[{"id":0,"href":"/posts/Short-Cut-for-GIST/","title":"Short Cut for Gist","section":"Blog","content":"Enter\nssh astrogroup@170.140.162.12 Password:\nNGC6814 Running # gistPipeline --config configFiles/MasterConfig --default-dir configFiles/defaultDir Upload directly from PowerShell # scp \"C:\\\\Users\\\\19175\\\\Desktop\\\\TNG Research\\\\GIST\\\\gistTutorial.tar.gz\" astrogroup@170.140.162.12:~/Erise/ scp \u0026ldquo;D:\\illustrateTNG Files\\SLOAN_MUSE.pyâ€ astrogroup@170.140.162.12:~/miniconda3/envs/gist/lib/python3.6/site-packages/gistPipeline/readData\nscp \u0026ldquo;D:\\illustrateTNG Files\\SLOAN_LR.pyâ€ astrogroup@170.140.162.12:~/miniconda3/envs/gist/lib/python3.6/site-packages/gistPipeline/readData\nThe Directory for Read-File\n~/miniconda3/envs/gist/lib/python3.6/site-packages/gistPipeline/readData CD command\nGo to gistTutorial cd ~/Erise/gistTutorial Open LogFile\nnano ~/Erise/gistTutorial/results/Test/LOGFILE Open masterConfig\nnano ~/Erise/gistTutorial/configFiles/MasterConfig Extract at your folder in Linux server:\ntar -xzvf gistTutorial.tar.gz ~/miniconda3/envs/gist/lib/python3.6/site-packages/vorbin/voronoi_2d_binning.py\nunzip the gz file:\ngzip -d -k TNG50-reds-0.035-angle-010-FOV-61-re_kpc-10-snap-98-460756.cube.fits.gz Finding\nfind ~/Erise/gistTutorial -name _______ Remove File\nrm Remove Dir\nrmdir C:\\Users\\19175\\Desktop\\TNG Research\\YinziyuanResults\\TNG50-reds-0.06-angle-010-FOV-61-re_kpc-15-snap-98-460756.cube.fits.gz\nscp \u0026ldquo;C:\\Users\\19175\\Desktop\\TNG Research\\YinziyuanResults\\TNG50-reds-0.06-angle-010-FOV-61-re_kpc-15-snap-98-460756.cube.fits.gz\u0026rdquo; astrogroup@170.140.162.12:~/Erise/gistTutorial/inputData\nC:\\Users\\19175\\Desktop\\TNG Research\\YinziyuanResults\\TNG50-reds-0.06-angle-010-FOV-61-re_kpc-15-snap-98-460756.cube.fits.gz\nUpload SAURON_LR\ngistpipline\nNGC0000Example\nQuestion\n"},{"id":1,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/01-Mid-Point-Method/","title":"01 Mid Point Method","section":"æ•°å€¼æ–¹æ³•","content":" Formula Derivation # The derivative approximation: $$ \\begin{aligned} f(t_i, u_i) \u0026amp;= \\frac{dy}{dt} \\Big|{t_i} \\approx \\frac{y(t{i+1}) - y(t_{i-1})}{2\\Delta t} + \\mathcal{O}(\\Delta t^2) \\end{aligned} $$\nUpdate formula:\n$$ \\begin{aligned} u_{i+1} \u0026amp;= u_{i-1} + 2\\Delta t f(t_i, u_i) \\end{aligned} $$\nInitial Conditions # Initial Condition Calculation: Given $y_0$ $$ \\begin{aligned} u_2 \u0026amp;= y_0 + 2\\Delta t f(t_1, u_1) \\end{aligned} $$\nPredictor-Corrector Steps:\n$$ \\begin{aligned} u_1^* \u0026amp;= y_0 + \\Delta t f(t_0, y_0), \\ u_1 \u0026amp;= y_0 + \\frac{\\Delta t}{2} (f(t_0, y_0) + f(t_1, u_1^*)) \\end{aligned} $$\nTwo-Step Method # Higher-order method that improves accuracy using information from two previous time steps. The method achieves second-order accuracy due to the central difference approximation. Absolute Stability # We consider the test equation:\n$$ \\begin{aligned} \\frac{dy}{dt} \u0026amp;= -\\lambda y, \\quad y(0) = y_0 \\end{aligned} $$\nwhere $\\lambda \u0026gt; 0$.\nDiscretization # The numerical update formula is:\n$$ \\begin{aligned} u_{n+1} \u0026amp;= u_{n-1} - 2\\Delta t \\lambda u_n \\end{aligned} $$\nRearranging:\n$$ \\begin{aligned} u_{n+1} + 2\\Delta t \\lambda u_n - u_{n-1} \u0026amp;= 0 \\end{aligned} $$\nIterative Computation # Starting with initial conditions ($y_0$, $y_1$):\n$$ \\begin{aligned} u_2 \u0026amp;= y_0 - 2\\Delta t \\lambda y_1, \\ u_3 \u0026amp;= y_1 - 2\\Delta t \\lambda u_2, \\ u_4 \u0026amp;= u_2 - 2\\Delta t \\lambda u_3, \\ u_5 \u0026amp;= u_3 - 2\\Delta t \\lambda u_4 \\end{aligned} $$\nThis formulation helps analyze the stability of the numerical scheme by checking whether the sequence $u_n$ grows or decays as $n \\to \\infty$.\nStability Analysis of the Numerical Scheme # We assume a solution of the form:\n$$ \\begin{aligned} u_i \u0026amp;= C \\beta^i \\end{aligned} $$\nSubstituting into the Recurrence Relation # $$ \\begin{aligned}\n\\end{aligned} $$ $$ \\begin{aligned} C \\beta^{i+1} + C 2\\Delta t \\lambda \\beta^i - C \\beta^{i-1} \u0026amp;= 0\\ \\beta^2 + 2\\Delta t \\lambda \\beta - 1 \u0026amp;= 0 \\quad \\text{devided by $C \\beta^{i-1}$.} \\end{aligned} $$ This is a characteristic equation for the recurrence relation. And solving for $\\beta$,\n$$ \\begin{aligned} \\beta \u0026amp;= \\frac{-2\\Delta t \\lambda \\pm \\sqrt{(2\\Delta t \\lambda)^2 + 4}}{2} \\end{aligned} $$ To ensure stability, we require:\n$$ \\begin{aligned} |\\beta_0|, |\\beta_1| \u0026amp;\\leq 1. \\end{aligned} $$\nGeneral Solution # Since the recurrence relation is second-order, the general solution is: $$ \\begin{aligned} u_i \u0026amp;= C_0 \\beta_0^i + C_1 \\beta_1^i. \\end{aligned} $$ From the initial conditions:\n$$ \\begin{aligned} u_0 \u0026amp;= C_0 + C_1 = y_0, \\\n\\nu_1 \u0026amp;= C_0 \\beta_0 + C_1 \\beta_1 = y_1 \\end{aligned} $$ which can be solved for $C_0$ and $C_1$.\nStability Condition # For stability, the roots $\\beta_0, \\beta_1$ must satisfy:\n$$ \\begin{aligned}|\\beta_0 \\beta_1| \u0026amp;= 1.\\end{aligned} $$\nFrom the characteristic equation:\n$$ \\begin{aligned}\\beta_0 \\beta_1 \u0026amp;= -\\frac{1}{\\beta_1}. \\end{aligned} $$\nEnsuring $|\\beta| \\leq 1$ determines the absolute stability region.\nFinite Difference Approximation and Stability Analysis # Finite Difference Approximation # $$ \\begin{aligned} \\alpha u_{i+1} + \\beta u_i + \\sigma u_{i-1} + \\delta u_{i-2} \u0026amp;= (\\alpha + \\beta + \\sigma + \\delta) u_i + \\mathcal{O}(\\Delta t^5) \\end{aligned} $$\nTime Discretization # $$ \\begin{aligned} \\frac{du}{dt} \\Big|{t_i} \u0026amp;= \\frac{u_i - u{i-1}}{\\Delta t} + \\mathcal{O}(\\Delta t^2) \\end{aligned} $$\nStability Analysis # $$ \\begin{aligned} \\alpha + 4\\beta \u0026amp;= 0, \\ \\alpha \u0026amp;= -4\\beta, \\ 4\\beta - 2\\beta \u0026amp;= 1 \\Rightarrow \\beta = \\frac{1}{2}, \\quad \\alpha = -2. \\end{aligned} $$\nThese constraints ensure numerical stability and proper convergence of the finite difference scheme.\nTaylor Expansions # Applying Taylor expansions to express the function values at different time steps: $$ \\begin{aligned} \u0026amp; \\alpha\\left[u_{i+1}=u_i-\\left.\\frac{d u}{d t}\\right|{t_i} \\Delta t+\\ldots\\right] \\ \u0026amp; \\beta\\left[u{i+2}=u_i-\\left.\\frac{d u}{d t}\\right|{t_i} 2 \\Delta t+\\ldots\\right] \\ \u0026amp; \\gamma\\left[u{i+3}=u_i-\\left.\\frac{d u}{d t}\\right|{t_i} 3 \\Delta t+\\ldots\\right] \\ \u0026amp; \\delta\\left[u{i+4}=u_i-\\left.\\frac{d u}{d t}\\right|_{t_i} 4 \\Delta t+\\ldots\\right] \\end{aligned} $$ Summing these expansions, we obtain the system of equations,\n$$ \\left{\\begin{aligned} -\\alpha-2 \\beta-3 \\gamma-4 \\delta \u0026amp; =1 \\ \\alpha+4 \\beta+8 \\gamma+16 \\delta \u0026amp; =0 \\end{aligned}\\right. $$\n"},{"id":2,"href":"/docs/Mathematics/MATH-351-352/Heat-Equation/","title":"çƒ­æ–¹ç¨‹","section":"åå¾®åˆ†æ–¹ç¨‹","content":"The general form of heat equation (diffusion equation) is represented as\n$$ u_t-k\\Delta u=0 $$\nwhere $x\\in U$ for $U \\in \\R^n$ (n-dimentional) s.t we have $u:\\bar{U} \\times [0,\\infty) \\to \\R$.\n$k$ is the thermal diffusivity of the material. prototype of parabolic PDEs normalized heat equation where $k=1$ is specified for theoretical studies (focusing on mathematical analysis) For 1-dimentional special case:\n$$ u_t-ku_{xx}=0 $$\nwhere ${x,t\\in(-\\infty,\\infty), [0, \\infty)}$.\nLecture 10 # ä¸‹é¢çš„æ’åºæ˜¯æŒ‰ç…§æ¨å¯¼çƒ­æ–¹ç¨‹è§£æè§£çš„é¡ºåºæ¥çš„ã€‚\nåŸºæœ¬æ€è·¯ï¼š\næˆ‘ä»¬å…ˆé€šè¿‡ æ ‡åº¦ä¸å˜æ€§ï¼ˆscale invarianceï¼‰æ‰¾åˆ°ä¸€ä¸ªæ‰€æœ‰è§£é€šç”¨çš„å½¢å¼ï¼Œå¹¶ä¸”åµŒå…¥åŸæ–¹ç¨‹è¿›è¡Œè¿ç®—ã€‚\næœ€ç»ˆï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ç»´çš„ï¼š\n10.1 çƒ­ä¼ å¯¼æ–¹ç¨‹çš„åŸºæœ¬è§£ # The Fundamental Solution to Heat Equation\n$$ \\Phi(x,t)=\\frac{1}{\\sqrt{4\\pi k t}} e^{-\\frac{x^2}{4kt}} $$\n10.1.1 æ­£æ€åˆ†å¸ƒå‡½æ•°çš„å±æ€§ # ä»¥åŠä¸å®ƒä½œä¸ºä¸€ä¸ªæ­£æ€åˆ†å¸ƒå‡½æ•°çš„å±æ€§ï¼š\nProperties of The Fundamental Solution\n10.2 å¯¹äºæŸ¯è¥¿é—®é¢˜çš„è§£ # æ¥ç€ï¼Œæˆ‘ä»¬å†ç”¨è¿‡å¹³ç§»ä¸å˜æ€§ **ï¼ˆ**translation invarianceï¼‰å’Œå·ç§¯æ¥è¿›ä¸€æ­¥ç¡®å®šæŸ¯è¥¿é—®é¢˜ï¼ˆ$t=0$ï¼‰çš„è§£ã€‚\nThe Solution to Cauchy Problem\n$$ u(x,t) = \\frac{1}{(4\\pi k t)^{1/2}} \\int^{+\\infty}_{-\\infty} e^{-\\frac{(x-y)^2}{4kt}} g(y) , dy $$\nå¹¶ä¸”å¯ä»¥ç”¨è¯¯å·®å‡½æ•°æ¥è¡¨ç¤ºè¿™ä¸ªè§£\n$$ u(x, t) = \\lim_{x \\to \\infty} \\text{erf}(x\\sqrt{4\\pi kt}) $$\nLecture 11 # 11.1 ä¸‰ç§ä¸åŒçš„è¾¹ç•Œæ¡ä»¶ # Different Types of Boundary Conditions\n11.2 åˆ†ç¦»å˜é‡æ³• # Separation of Variables Lecture 12 # 12.1 å‚…é‡Œå¶å±•å¼€å’Œå˜æ¢ # The Fourier Series\nLecture 13 # 13.1 çƒ­ä¼ å¯¼å…¬å¼è§£ # The Fourier Expansion for Heat Equation Solution\n13.2 Dirichleté—®é¢˜çš„è§£ # Non-Homogenous Dirichlet problem\nLecture 14 # 14.1 è§£çš„å”¯ä¸€æ€§ # Uniqueness of Solution\n14.2 ç”µæ¢¯æ–¹ç¨‹ # Lifting Function\nğŸ’¡ the transformed coefficient is defined as $$ \\hat \\mu =\\dfrac{\\mu}{(b-a)^2} $$\nLecture 15 # 15.1 æ–½å›¾å§†-åˆ˜ç»´å°”ç†è®º # Sturm Liouville Theory (SLE)\nLecture 16 # 16.1 æå¤§å€¼åŸç† # The Principle of Maximum\n"},{"id":3,"href":"/posts/Integrate-DeepL-Translation-Instruction/","title":"Integrate Deep L Translation Instruction","section":"Blog","content":" 1. Install R and Babeldown # 1.1 Install R # https://cran.r-project.org/.\n(The following task is using R console)\n1.2 Install Babeldown # More specific instruction, check here: https://docs.ropensci.org/babeldown/\n1.2.1 This command install \u0026lsquo;remotes\u0026rsquo; from CRAN if not already installed:\nif (!requireNamespace(\"remotes\", quietly = TRUE)) { install.packages(\"remotes\") } 1.2.2 Uses the â€˜remotesâ€™ package to install the â€˜babeldownâ€™ package from its GitHub repo:\ninstall.packages('babeldown', repos = c('https://ropensci.r-universe.dev', 'https://cloud.r-project.org')) 2. Set up DeepL API (Inside of R console) # Go to DeepL\u0026rsquo;s website and get an API key: https://www.deepl.com/en/your-account/keys 3. Connect Babeldown to DeepL API # Babeldown uses the DeepLÂ freeÂ API URL by default (no need to set up unless pro API).\n3.1 Download a keyring package (for secure API key retrieval) # install.packages(\"keyring\") 3.2 Keyring requests your API key # library(keyring) keyring::key_set(\"deepl\", prompt = \"API key:\") Enter your API key and then in any script you use babeldown, youâ€™d retrieve the key like so:\nSys.setenv(DEEPL_API_KEY = keyring::key_get(\"deepl\")) 4. (optional) Set up working directory # In R, use getwd() to check current working directory, and you may use setwd(\u0026quot;your absolute path\u0026quot;) to move your working directory for convenience.\nMy setup is:\nsetwd(\"/Users/erisehe/Documents/GitHub/erisehe.github.io\") 4. Translates # Since my working directory is at (\u0026quot;\u0026hellip;/erisehe.github.io\u0026quot;), so I runs relative path. The commands, for example, is:\nbabeldown::deepl_translate_hugo( post_path = \"content.en/docs/Mathematics/MATH 412 Real Analysis II/Lecture/name.md\", target_lang = \"ZH\", source_lang = \"EN\", force = TRUE ) It translate only one file at a time.\nHow to Translate an Entire Folder # You can use base R functions (or packages like purrr) to list the files and apply the translation function.\n# Define the source folder containing your markdown files source_dir \u003c- \"path/to/your/source_folder\" # Define the target folder where you want to save the translated files target_dir \u003c- \"path/to/your/target_folder\" if (!dir.exists(target_dir)) { dir.create(target_dir) } # List all markdown files in the source directory files \u003c- list.files(source_dir, pattern = \"\\\\.md$\", full.names = TRUE) # Loop through each file and translate it for (f in files) { # Translate the file using babeldown's deepl_translate_hugo function babeldown::deepl_translate_hugo( post_path = f, target_lang = \"ZH\", source_lang = \"EN\", force = TRUE ) # If the function writes the output file in a default location or with a predictable name, # you can move or copy it to your target directory. For example: output_file \u003c- file.path(dirname(f), paste0(\"translated_\", basename(f))) if (file.exists(output_file)) { file.copy(output_file, file.path(target_dir, basename(output_file))) } } "},{"id":4,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/02-Three-Point-Backward-Differentiation-Formula/","title":"02 Three Point Backward Differentiation Formula","section":"æ•°å€¼æ–¹æ³•","content":" 1. Ok Honestly I Have No Idea Where He Started # MATH 212 is useless - Alessandro Veneziani\nGiven the population problem #Implicit:\n$$ \\dfrac{d y}{d x} = A\\left( 1 - \\dfrac{y}{B}\\right)y $$ # Numerically, the problem is: $$ \\frac{u_{i+1} - u_i}{\\Delta t} = A \\left(1 - \\frac{u_{i+1}}{B} \\right) u_{i+1} $$ To solve this numerically, we rewrite the equation: $$ \\begin{align} x - u_i \u0026amp;= A \\left(1 - \\frac{x}{B} \\right) x \\ \u0026amp;= A x - \\frac{A}{B} x^2 \\end{align} $$\nusing $f(x)=0$, and we iterate using Newton\u0026rsquo;s method:\nyes he changed notation again\n[!remark|*] Newton\u0026rsquo;s Method $$\\underbrace{x^{(u+1)}}{y{\\text{new}}}=\\underbrace{x^{(u)}}{y{\\text{old}}}-\\frac{f(x^{(u)})}{f\u0026rsquo;(x^{(u)})}$$ We have $|y_{\\text{new}}-y_{\\text{old}}|\\leq \\text{tol}.$\n$$ \\begin{align} x - u_i \u0026amp; = \\Delta t A \\left(1 - \\frac{x}{B} \\right) x \\ \u0026amp; = \\Delta t A x - \\Delta t \\frac{A}{B} x^2 \\\n\\Longrightarrow \\quad x - u_{i}-\\Delta t A x + \\Delta t \\frac{A}{B} x^{2}\u0026amp;=0 \\end{align} $$ Define a function $g(x)$ from above: $$ \\begin{align} g(x) \u0026amp; = x - u_i - \\Delta t A x + \\Delta t \\frac{A}{B} x^{2}\\ \\end{align} $$ Taking the derivative: $$ \\begin{align} g\u0026rsquo;(x) \u0026amp;= \\frac{d}{dx} \\left( x - u_i - \\Delta t A x + \\Delta t \\frac{A}{B} x^2 \\right) \\ \u0026amp;= 1 - \\Delta t \\left(A - \\frac{2A}{B} x \\right) \\ \u0026amp;= 1 - \\Delta t f\u0026rsquo;(x). \\end{align} $$ For $f^{\\prime}(x)=A-\\frac{2 A}{B} x$.\n2. Approximate $\\frac{d y}{d x}$ Using a Three-Point Backward Differentiation Formula (BDF) # tracing back to last lecture on determination of coefficients $A, B, C$\n$$ \\begin{aligned} f(t_{i},y_{i})=\\left. \\frac{dy}{dx} \\right|{x_i} \u0026amp;\\approx \\frac{3}{2 \\Delta x} y_i - \\frac{4}{2 \\Delta x} y{i-1} + \\frac{1}{2 \\Delta x} y_{i-2} \\ \\ \\text{Taylor Expansion}\\Longrightarrow\\quad \u0026amp;\\left{ \\begin{aligned} y_{i-1} \u0026amp;= y_i - \\left. \\frac{dy}{dx} \\right|{x_i} \\Delta x \\dots \\ y{i-2} \u0026amp;= y_i - \\left. \\frac{dy}{dx} \\right|{x_i} 2 \\Delta x \\dots \\end{aligned} \\right. \\end{aligned}$$ we yield: $$ \\begin{align} \\frac{3}{2 \\Delta x} u_i\u0026amp;-\\frac{4}{2 \\Delta x} u{i-1}+\\frac{1}{2 \\Delta x} u_{i-2}=f\\left(t_i , u_i\\right)\\ \\end{align} $$\nImplicit formula: $$ u_i=\\frac{4}{3} u_{i-1}-\\frac{1}{3} u_{i-2}+\\frac{2}{3} \\Delta x\\left(t_i, u_i\\right) $$ we substitute $f(t_{i},u_{i})=\\lambda u_{i}$ to derive explicitly:\n$$\\begin{align} u_i - \\frac{4}{3} u_{i-1} + \\frac{1}{3} u_{i-2} - \u0026amp;\\frac{2}{3} \\Delta x f(t_i, u_i) = 0 \\ u_i - \\frac{4}{3} u_{i-1} + \\frac{1}{3} u_{i-2} - \u0026amp;\\frac{2}{3} \\Delta x \\lambda u_{i} = 0 \\ \\end{align} $$ Explicit formula: $$\n\\boxed{u_i = \\frac{1}{1 - \\frac{2}{3} \\Delta x\\lambda} \\left( \\frac{4}{3} u_{i-1} - \\frac{1}{3} u_{i-2} \\right) }\n$$\n[!definition|*] Generalized p-step BDF Form $$\\boxed{u_i-\\sum_{j=1}^{p} \\alpha_j u_{i-j}=\\Delta x \\beta_{-1} f\\left(t_i, u_i\\right)} $$ Generalized Implicit Multistep Method: $$\\boxed{u_{i+1}-\\sum_{j=1}^p \\alpha_j u_{i-j}=\\Delta x \\sum_{j={-1}}^p \\beta_j f\\left(t_{i-j}, u_{i+1-j}\\right)}$$\n"},{"id":5,"href":"/docs/Mathematics/%E4%BA%BA%E5%B7%A5%E4%B8%BB%E4%BD%93/","title":"äººå·¥ä¸»ä½“","section":"Mathematics","content":" Evolution of AI: Foundational Papers and Milestones (Chronological) # Below is a chronological list of influential papers that have shaped artificial intelligence â€“ from early symbolic reasoning and neural network concepts to the rise of deep learning and large language models. Each entry includes the workâ€™s main contribution, an influence rating, and a beginner-friendly explanation of its significance.\n1943 â€“ McCulloch \u0026amp; Pitts: â€œA Logical Calculus of the Ideas Immanent in Nervous Activityâ€\nContribution \u0026amp; Impact: Proposed the first mathematical model of how networks of artificial neurons could represent logical computations ( A Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia). This seminal work showed that simple on/off neurons with weighted inputs can compute any logical function, laying the groundwork for neural networks ( A Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia).\nInfluence (1â€“10): 10/10\nBeginner-Friendly Explanation: McCulloch and Pitts imagined the brain as a network of simple switches (neurons) that could be either on or off. They proved a bunch of these neuron-like switches could be connected to perform logical reasoning (like an electronic circuit). This was a foundational idea: it suggested machines could think by mimicking brain networks.\n1950 â€“ Alan Turing: â€œComputing Machinery and Intelligenceâ€\nContribution \u0026amp; Impact: Introduced the famous Turing Test as a criterion for machine intelligence ( Alan Turing\u0026rsquo;s Contributions to Artificial Intelligence : History of Information). Turing argued that instead of asking â€œCan machines think?â€, we should ask if a machine can imitate a human so well in conversation that an evaluator cannot tell the difference. This paper framed the philosophical and practical challenge of AI for decades.\nInfluence (1â€“10): 10/10\nBeginner-Friendly Explanation: Turing basically said: â€œIf you can chat with a computer and canâ€™t tell itâ€™s not human, then for all practical purposes, that computer is â€˜thinkingâ€™.â€ This idea â€“ a computer fooling a person in a conversation â€“ became a guiding goal for AI research and popular imagination.\n1956 â€“ Newell \u0026amp; Simon: The Logic Theorist (RAND Corporation Report \u0026amp; Dartmoor Demo)\nContribution \u0026amp; Impact: Demonstrated the first AI program deliberately engineered to mimic human problem-solving ( Newell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information). The Logic Theorist could prove mathematical theorems from Principia Mathematica, even finding an elegant proof for one theorem that was more efficient than the original ( Newell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information). This â€œheuristic searchâ€ approach showed digital computers can perform symbolic reasoning, launching the field of symbolic AI.\nInfluence (1â€“10): 9/10\nBeginner-Friendly Explanation: Newell and Simon built a program that solved logic puzzles (proving math theorems) in a way a person might, by searching through possible steps. It was the first time a computer did something â€œbrainyâ€ beyond pure calculations. This success convinced people that computers could manipulate symbols and logic to solve problems, not just crunch numbers.\n1958 â€“ Frank Rosenblatt: The Perceptron (Psychological Review \u0026amp; Mark I Perceptron)\nContribution \u0026amp; Impact: Introduced the perceptron, a simple neural network that learns from experience. Rosenblattâ€™s perceptron machine was the first computer that could learn new skills by trial and error using a neural network modeled on the brain ( Rosenblatt\u0026rsquo;s Perceptron Uses a Type of Neural Network : History of Information). It learned to classify patterns (like distinguishing shapes) by adjusting connection weights based on errors. This work pioneered the field of machine learning and inspired decades of neural network research.\nInfluence (1â€“10): 9/10\nBeginner-Friendly Explanation: The perceptron was essentially a mechanical â€œstudent.â€ It would make a guess about a pattern (for example, is this a picture of a dog or a cat?), then check if it was wrong. If it was wrong, it tweaked its internal settings to do better next time. Over many trials it gradually got more accurate ( Professorâ€™s perceptron paved the way for AI â€“ 60 years too soon | Cornell Chronicle). This was the first example of a machine learning from its mistakes â€“ a key idea in AI.\n1959 â€“ Arthur Samuel: â€œSome Studies in Machine Learning Using the Game of Checkersâ€\nContribution \u0026amp; Impact: Demonstrated one of the first successful self-learning programs. Samuelâ€™s checkers (draughts) program learned to improve at the game by playing against itself thousands of times. Importantly, it introduced mechanisms for a computer to learn from past games â€“ recording positions that led to wins or losses and updating its strategy accordingly ( The games that helped AI evolve | IBM). Samuel even coined the term â€œmachine learningâ€ for this approach. In 1962, his program was able to beat a respectable human player, proving that computers can learn complex tasks without being explicitly programmed for all situations.\nInfluence (1â€“10): 8/10\nBeginner-Friendly Explanation: Samuelâ€™s checkers program was like a rookie player that got better by practicing. It kept track of board positions and whether it eventually won or lost from them. Over time it favored moves that led to wins and avoided those leading to losses ( The games that helped AI evolve | IBM). This was revolutionary: the computer wasnâ€™t just following a fixed strategy given by a human â€“ it was figuring out a winning strategy by itself through experience.\n1969 â€“ Marvin Minsky \u0026amp; Seymour Papert: Perceptrons (MIT Press book)\nContribution \u0026amp; Impact: Delivered a thorough mathematical analysis of perceptrons and famously highlighted their limitations ( Minsky \u0026amp; Papertâ€™s â€œPerceptronsâ€ â€“ Building Babylon). They proved that a single-layer perceptron cannot learn certain simple functions (like the XOR problem â€“ determining if an input has an odd number of 1â€™s), unless it uses an exponentially large number of features. This critique (published as a book) effectively punctured the hype around neural networks at the time. It led to a significant shift in AI research focus from connectionist (neural network) methods to symbolic AI approaches in the 1970s, contributing to an â€œAI winterâ€ for neural nets ( Minsky \u0026amp; Papertâ€™s â€œPerceptronsâ€ â€“ Building Babylon).\nInfluence (1â€“10): 9/10\nBeginner-Friendly Explanation: Minsky and Papert showed that the perceptron â€“ this early learning neural network â€“ was very limited in what it could learn. For example, it couldnâ€™t correctly learn the simple logic of an â€œeither/orâ€ (XOR) condition because of its single-layer design ( Minsky \u0026amp; Papertâ€™s â€œPerceptronsâ€ â€“ Building Babylon). Their analysis basically said, â€œNeural nets are neat, but they canâ€™t handle some basic problems unless they get much more complex.â€ This turned many researchers away from neural networks for years, as they focused instead on logic and rule-based AI.\n1986 â€“ Rumelhart, Hinton \u0026amp; Williams: â€œLearning Representations by Back-Propagating Errorsâ€\nContribution \u0026amp; Impact: Introduced the backpropagation algorithm for training multi-layer neural networks efficiently (though the method had been conceptually described earlier, this paper popularized it). Backpropagation provided a practical way to adjust the weights in a network with many layers by propagating the error gradient backward from the output layer ( Backpropagation - Wikipedia). This breakthrough overcame the training difficulty of multi-layer perceptrons and sparked a resurgence of interest in neural network research in the late 1980s ( Backpropagation - Wikipedia). In short, it enabled â€œdeepâ€ neural networks to actually learn internal representations from data.\nInfluence (1â€“10): 10/10\nBeginner-Friendly Explanation: Backpropagation is an algorithm that finally let multi-layer neural networks learn. Think of it like teaching a multi-step math solution: you check the final answer, see how wrong it is, and then send feedback backward to correct each step. Similarly, backpropagation takes the error at the output and systematically adjusts each connection in all layers to reduce that error. Once this was introduced and shown to work ( Backpropagation - Wikipedia), researchers could train networks with several layers â€“ giving neural nets much more brain-like ability to form complex concepts (like recognizing shapes, then objects, then scenes). This revived neural networks as a viable AI approach.\n1988 â€“ Judea Pearl: Probabilistic Reasoning in Intelligent Systems (book)\nContribution \u0026amp; Impact: Established the field of Bayesian networks for reasoning under uncertainty. Pearl introduced a formalism where cause-and-effect relationships and uncertain knowledge could be encoded in a graphical model (a Bayesian network) and updated with probability theory. This was a paradigm shift from rule-based AI to probabilistic AI: instead of logic with strict true/false values, AI systems could handle gray areas and uncertainty in a principled way. Pearlâ€™s 1988 book became known as the â€œbibleâ€ of probabilistic AI ( Probabilistic Reasoning (1993â€“2011) â€” Making Things Think: How AI and Deep Learning Power the Products We Use) and his techniques for probabilistic inference laid the groundwork for modern AI systems that need to deal with real-world ambiguity (including everything from medical diagnosis expert systems to speech recognition).\nInfluence (1â€“10): 10/10\nBeginner-Friendly Explanation: Pearl taught AI how to handle uncertainty. Earlier AI often used rigid rules (e.g., â€œIF X then Yâ€), but real life is full of maybes. Pearlâ€™s Bayesian networks let a computer draw a graph of causes and effects (say, symptoms and diseases) and then reason with probabilities â€“ for example, â€œgiven these symptoms, thereâ€™s an 80% chance of flu.â€ This made AI much better at dealing with uncertain, real-world information, and it was a huge turning point that influenced everything from machine vision to natural language understanding. ( Probabilistic Reasoning (1993â€“2011) â€” Making Things Think: How AI and Deep Learning Power the Products We Use) ( Probabilistic Reasoning (1993â€“2011) â€” Making Things Think: How AI and Deep Learning Power the Products We Use)\n1989 â€“ Yann LeCun et al.: â€œBackpropagation Applied to Handwritten Zip Code Recognitionâ€\nContribution \u0026amp; Impact: Demonstrated the first real-world success of a deep neural network (a convolutional neural network, or CNN) trained end-to-end with backpropagation. LeCunâ€™s CNN, later known as LeNet-5, could read handwritten digits (like postal ZIP codes) from images with high accuracy ([\nDeep Neural Nets: 33 years ago and 33 years from now (Invited Post) Â· The ICLR Blog Track\n]( https://iclr-blog-track.github.io/2022/03/26/lecun1989/#:~:text=The%20Yann%20LeCun%20et%20al,I%20set%20out%20to%20reproduce)). It introduced the convolutional layer architecture that mimics the visual cortex, extracting features through local receptive fields and shared weights. This work was historically significant as an early proof that multi-layer neural networks can solve practical pattern-recognition problems that other methods struggled with, foreshadowing the deep learning breakthroughs decades later ([\nDeep Neural Nets: 33 years ago and 33 years from now (Invited Post) Â· The ICLR Blog Track\n]( https://iclr-blog-track.github.io/2022/03/26/lecun1989/#:~:text=The%20Yann%20LeCun%20et%20al,I%20set%20out%20to%20reproduce)).\nInfluence (1â€“10): 8/10\nBeginner-Friendly Explanation: LeCun and colleagues built a neural network that could read handwritten numbers â€“ for example, automatically recognizing zip code digits on mail. They designed special layers (now called convolutional layers) that help the network focus on small patches of an image, just like how our eyes notice local patterns. By training this multi-layer network with backpropagation, it got really good at digit recognition. This was one of the first times deep learning beat other methods on a real task, proving that these layered neural networks werenâ€™t just academic toys but could actually see things in images and make sense of them.\n1989 â€“ Chris Watkins: â€œLearning from Delayed Rewardsâ€ (PhD thesis introducing Q-Learning) Contribution \u0026amp; Impact: Introduced Q-learning, a foundational algorithm in reinforcement learning. Q-learning provided a model-free way for an agent to learn an optimal action policy by trial-and-error, even when outcomes (rewards) are delayed ( Q-learning - Wikipedia). Watkins proved that Q-learning converges to the optimal solution given sufficient exploration. This algorithm was crucial because it showed how an AI agent can learn to make sequences of decisions in an unknown environment to maximize reward, without needing a model of the environmentâ€™s dynamics. Q-learning (and the broader reinforcement learning framework) became a major branch of AI, underpinning later successes in game-playing AI, robotics, and beyond.\nInfluence (1â€“10): 9/10\nBeginner-Friendly Explanation: Q-learning is like learning to play a game by playing. Imagine youâ€™re dropped into a video game without instructions. You try moves at random and eventually figure out what gives you points or causes losses. Over time, you assign a value (â€œQ-valueâ€) to each move in each situation based on how good it turned out to be. Watkinsâ€™ Q-learning algorithm formalized this idea â€“ the computer updates its estimations of future rewards for actions as it experiments ( Q-learning - Wikipedia). The beauty is that the AI doesnâ€™t need to know the rules in advance; it learns what actions are best just from feedback. This became a cornerstone of how we teach AI agents to learn from experience (like teaching a robot to navigate a maze or an AI to play Atari games).\n1995 â€“ Cortes \u0026amp; Vapnik: â€œSupport-Vector Networksâ€ Contribution \u0026amp; Impact: Introduced Support Vector Machines (SVMs), a new supervised learning approach based on maximizing the margin between classes. SVMs framed learning as finding the optimal separating hyperplane in a high-dimensional space â€“ and by using the kernel trick, they could efficiently handle complex, non-linear decision boundaries by implicitly mapping data into higher dimensions ( Support vector machine - Wikipedia). SVMs offered strong theoretical guarantees (rooted in Vapnikâ€™s statistical learning theory) and delivered excellent performance on many tasks in the late 1990s and 2000s. They became one of the most dominant machine learning methods before the deep learning era, widely used in image recognition, text classification, and bioinformatics.\nInfluence (1â€“10): 8/10\nBeginner-Friendly Explanation: SVMs were a new way to do pattern recognition with math. Think of drawing a line to separate two groups of points on a paper: an SVM finds the line (or surface in higher dimensions) that not only separates the groups, but is as far away from all points as possible â€“ this is the maximum-margin idea ( Support vector machine - Wikipedia) ( Support vector machine - Wikipedia). If the groups arenâ€™t linearly separable, SVMs use a clever trick (the kernel) to imagine the data in a higher-dimensional space where a separation is possible, without having to enumerate all those dimensions explicitly. The result was a very powerful and robust classifier that for years was the go-to method when you wanted high accuracy in machine learning tasks.\n1997 â€“ Hochreiter \u0026amp; Schmidhuber: â€œLong Short-Term Memoryâ€ (Neural Computation) Contribution \u0026amp; Impact: Developed the Long Short-Term Memory (LSTM) network, a type of recurrent neural network designed to overcome the vanishing gradient problem for long sequence learning. LSTM introduced gating mechanisms (input, output, and forget gates) that allow the network to maintain information over long time lags ( Long short-term memory - Wikipedia) ( Long short-term memory - Wikipedia). This architecture enabled RNNs to retain long-term dependencies in sequence data (e.g. remembering context from far earlier in a text or time series). LSTMs proved enormously successful in the 2000s and 2010s for tasks like speech recognition, language modeling, and translation â€“ essentially any task involving sequential data â€“ and were a key component in state-of-the-art models until the transformer era.\nInfluence (1â€“10): 9/10\nBeginner-Friendly Explanation: LSTMs made neural networks much better at remembering. Standard RNNs have short memories â€“ they tend to â€œforgetâ€ things quickly as new inputs come in, partly because the error signal fades over time steps. LSTM units added gates that learn what to keep, what to throw away, and what to output ( Long short-term memory - Wikipedia) ( Long short-term memory - Wikipedia). For example, when processing a sentence, an LSTM can learn to carry forward the subject of the sentence so that many words later it still knows who or what the discussion is about. This was a big deal â€“ it meant AI could understand sequences (like text, speech, or music) with far better context and consistency.\n2006 â€“ Hinton et al.: â€œA Fast Learning Algorithm for Deep Belief Netsâ€ (Science) Contribution \u0026amp; Impact: Revived deep neural networks by introducing an effective training strategy using Deep Belief Networks (DBNs). Hinton showed that a deep multi-layer network could be trained by greedily training one layer at a time in an unsupervised fashion (using Restricted Boltzmann Machines), then fine-tuning with supervised learning. This breakthrough in 2006 was the first to successfully train networks with many layers, overcoming previous optimization difficulties ( [PDF] Why does Unsupervised Pre-training Help Deep Learning?). It proved that unsupervised pre-training could initialize deep networks in a good state, leading to much better results and reigniting research into â€œdeep learning.â€ This work directly influenced subsequent deep architectures and is seen as a turning point that led to the deep learning boom.\nInfluence (1â€“10): 8/10\nBeginner-Friendly Explanation: By the 2000s, neural nets had mostly one or two hidden layers because training more was too hard â€“ the signal just wouldnâ€™t propagate well. Hintonâ€™s team figured out a clever solution: train one layer at a time in a sort of self-supervised way (each layer learns to encode the data in a compressed form), then stack them up. With this layer-by-layer pre-training, they could finally train really deep networks (many layers) without things falling apart ( [PDF] Why does Unsupervised Pre-training Help Deep Learning?). This showed the community that deep networks could work in practice, and it set the stage for the breakthroughs that followed (especially once big data and GPUs became available a few years later).\n2012 â€“ Krizhevsky, Sutskever \u0026amp; Hinton: â€œImageNet Classification with Deep Convolutional Neural Networksâ€ Contribution \u0026amp; Impact: Better known as AlexNet, this paper rocked the computer vision world by winning the 2012 ImageNet challenge by a huge margin using a deep convolutional neural network ( AlexNet and ImageNet: The Birth of Deep Learning | Pinecone). AlexNet was an 8-layer CNN (with ReLU activations, dropout regularization, and GPU training) that achieved a top-5 error of 15.3% on ImageNet, while the next best approach was 26.2% â€“ an unprecedented 10% jump in accuracy ( AlexNet and ImageNet: The Birth of Deep Learning | Pinecone) ( AlexNet and ImageNet: The Birth of Deep Learning | Pinecone). It was the first widely acknowledged, practical success of deep learning in a large-scale task ( AlexNet and ImageNet: The Birth of Deep Learning | Pinecone) ( AlexNet and ImageNet: The Birth of Deep Learning | Pinecone), marking the start of the deep learning revolution in computer vision (and soon other fields). After AlexNet, the research community rapidly pivoted to deep neural networks for vision tasks.\nInfluence (1â€“10): 10/10\nBeginner-Friendly Explanation: AlexNetâ€™s victory was like an underdog team winning a championship by a landslide. In a contest of recognizing 1,000 different object types from images (ImageNet), this deep neural network crushed the traditional approaches â€“ it was much more accurate (about 16% error vs. 26% for the best non-neural method) ( AlexNet and ImageNet: The Birth of Deep Learning | Pinecone) ( AlexNet and ImageNet: The Birth of Deep Learning | Pinecone). People were stunned. This was proof that with enough data (millions of images), compute (GPUs), and a good architecture, neural networks dramatically outperformed other techniques. It instantly made deep learning the focus for anyone working on image recognition, and soon after, for speech and other areas as well.\n2014 â€“ Goodfellow et al.: â€œGenerative Adversarial Networksâ€ Contribution \u0026amp; Impact: Introduced Generative Adversarial Networks (GANs), a novel framework for training generative models by pitting two neural networks against each other â€“ a generator that tries to create fake data, and a discriminator that tries to detect fakes ( 10 AI milestones of the last 10 years | Royal Institution). This adversarial training scheme resulted in generative models that could produce remarkably realistic images (and other data) over time. GANs were a conceptual breakthrough in how to train networks to create rather than just recognize, and they spawned an entire subfield of research. They eventually led to high-fidelity image synthesis, deepfakes, and many creative AI applications.\nInfluence (1â€“10): 9/10\nBeginner-Friendly Explanation: A GAN is like having a counterfeiter and a detective who improve together. The counterfeiter (generator network) tries to make fake outputs â€“ say, fake images of faces â€“ that look real. The detective (discriminator network) looks at images and says â€œrealâ€ or â€œfake.â€ As they train, the counterfeiter gets better at fooling the detective, and the detective gets better at spotting fakes ( 10 AI milestones of the last 10 years | Royal Institution). Eventually, the fake outputs become so realistic that even humans canâ€™t easily tell theyâ€™re generated by a computer. This idea of two AIs dueling with each other turned out to be a powerful way for machines to imagine and create realistic data.\n2015 â€“ Mnih et al.: â€œHuman-Level Control Through Deep Reinforcement Learningâ€ Contribution \u0026amp; Impact: Demonstrated the power of deep reinforcement learning by introducing the Deep Q-Network (DQN) agent. This system combined Q-learning with a deep convolutional neural network, enabling an AI to learn to play Atari 2600 video games directly from raw pixel inputs ( Human-level control through deep reinforcement learning | Nature). Strikingly, the same DQN algorithm achieved human-level performance (or better) on dozens of Atari games â€“ using only the game screen pixels and score as input â€“ with no game-specific tweaks ( Human-level control through deep reinforcement learning | Nature). This was the first time a single AI agent learned a broad range of complex tasks end-to-end, bridging the gap between sensory perception and decision-making. It reinvigorated research in reinforcement learning and underscored the potential of combining deep learning with trial-and-error learning.\nInfluence (1â€“10): 10/10\nBeginner-Friendly Explanation: DeepMindâ€™s DQN agent was a breakthrough in game-playing AI. They took a neural network and trained it via reinforcement learning to play classic video games like Breakout, Pac-Man, and Space Invaders. The input was just the pixels on the screen and the game score, and it had to figure out what to do from that alone. Amazingly, after training, the AI could play many of these games as well as or better than a human â€“ using the same one algorithm for all ( Human-level control through deep reinforcement learning | Nature). For example, in Breakout it learned the clever strategy of tunneling around the bricks (a trick human experts use) without ever being told. This result showed that an AI can start from raw perception (seeing the screen) and learn intelligent control (playing the game) purely by trial and error, which was a big step toward more general learning systems.\n2016 â€“ Silver et al.: â€œMastering the Game of Go with Deep Neural Networks and Tree Searchâ€ (AlphaGo) Contribution \u0026amp; Impact: Achieved what was previously thought to be at least a decade away: a computer program defeating a top human professional in the game of Go ( Mastering the game of Go with deep neural networks and tree search | Nature). The AlphaGo system did this by combining deep policy networks (to choose moves) and value networks (to evaluate board positions) with Monte Carlo Tree Search. It learned first from expert human games and then via millions of games of self-play, refining its skills. In the published Nature paper, AlphaGo achieved a 99.8% win rate against other Go programs and beat the European Go champion 5â€“0 ( Mastering the game of Go with deep neural networks and tree search | Nature). This was a watershed moment for AI, showcasing the synthesis of deep learning and advanced search to conquer one of the most complex board games. It suggested that similar techniques could tackle other problems of high complexity.\nInfluence (1â€“10): 10/10\nBeginner-Friendly Explanation: For decades, Go was the ultimate challenge for AI â€“ far more complex than chess. AlphaGoâ€™s design had two brains: one that suggested likely good moves (a policy neural network) and one that judged board positions (a value neural network) ( Mastering the game of Go with deep neural networks and tree search | Nature). It also simulated future move sequences (tree search) but in a smarter way guided by those neural nets. The result was an AI that plays Go brilliantly. It first learned by studying human games, then got even better by playing against itself millions of times, each time learning from its mistakes. In 2016 it shocked the world by beating one of the best human Go players. This victory was about more than Go â€“ it meant AI could tackle extremely complex, subtle problems that were once thought to require human intuition.\n2017 â€“ Vaswani et al.: â€œAttention Is All You Needâ€ Contribution \u0026amp; Impact: Introduced the Transformer architecture, built entirely on self-attention mechanisms and devoid of recurrence or convolution. This paper showed that attention mechanisms alone can capture relationships in sequential input (like words in a sentence) more efficiently and with better parallelization than previous RNN/CNN approaches. The transformer architecture led to dramatic improvements in machine translation and natural language processing. It is the direct precursor to todayâ€™s large language models. Indeed, this work provided the technological foundation for LLMs, as transformers can read entire sequences and learn contextual dependencies with ease ( 10 AI milestones of the last 10 years | Royal Institution). Subsequent models like BERT and GPT are built on the transformer, validating the paperâ€™s title that â€œattention is all you need.â€\nInfluence (1â€“10): 10/10\nBeginner-Friendly Explanation: The Transformer architecture threw out the old playbook for processing sequences. Instead of reading words one-by-one in order (like an RNN) or focusing only on a fixed-size window (like a CNN), a Transformer looks at all the words at once and learns which words to pay attention to in order to understand the meaning ( 10 AI milestones of the last 10 years | Royal Institution). For example, to translate a sentence or answer a question, it can see how each word relates to every other word (using a mechanism called self-attention). This was revolutionary because it made language models much better at capturing context (who did what to whom, etc.) and it could be massively parallelized (so it could train on huge datasets). Nearly all modern large language models (like GPT-3, BERT) are based on this Transformer design, which shows how impactful this paper was.\n2018 â€“ Devlin et al.: â€œBERT: Pre-training of Deep Bidirectional Transformers for Language Understandingâ€ Contribution \u0026amp; Impact: Introduced BERT (Bidirectional Encoder Representations from Transformers), which demonstrated the power of pre-training a large transformer on unsupervised language tasks and then fine-tuning it for specific NLP tasks. BERTâ€™s bidirectional training (masked language modeling and next-sentence prediction objectives) produced deep language representations that achieved state-of-the-art results on a wide array of NLP benchmarks ( [PDF] BERT: Pre-training of Deep Bidirectional Transformers for Language \u0026hellip;). This paper popularized the paradigm of pre-train then fine-tune in NLP. After BERT, large pre-trained language models became the norm, fundamentally changing NLP research and leading to an explosion of models that improved on various tasks via fine-tuning rather than task-specific architectures from scratch.\nInfluence (1â€“10): 9/10\nBeginner-Friendly Explanation: BERT showed that an AI can learn a lot about language just by reading billions of words of text, without any specific task in mind. BERT is a huge Transformer network trained in a clever way: it hides some words in a sentence and forces itself to guess them (thatâ€™s masked language modeling). In doing so, it learns rich knowledge about syntax, semantics, and general language facts. Once trained on this â€œfill-in-the-blanksâ€ task across Wikipedia and books, BERT can be quickly taught to solve all sorts of language problems (question answering, sentiment analysis, etc.) by fine-tuning on a small task-specific dataset ( [PDF] BERT: Pre-training of Deep Bidirectional Transformers for Language \u0026hellip;). For NLP, this was a game-changer â€“ instead of training separate models from scratch for each little task, we now pre-train one giant model that knows a lot, and then just tweak it for the task at hand.\n2018 â€“ Radford et al.: â€œImproving Language Understanding by Generative Pre-Trainingâ€ (GPT-1) Contribution \u0026amp; Impact: This OpenAI work introduced the first Generative Pre-Trained Transformer (GPT) model. GPT-1 demonstrated that a transformer trained as a language model (predicting the next word on a massive corpus of books) could be fine-tuned to perform downstream tasks like question answering with excellent results â€“ even though it was not trained on those tasks directly. It established the effectiveness of unsupervised pre-training for transformers (using a left-to-right generative objective) and showed the versatility of the resulting representations. GPT-1 was able to generate coherent text and answer questions in a fluent way after pre-training on ~7000 unpublished books and then fine-tuning ( 10 AI milestones of the last 10 years | Royal Institution). While smaller in scale by todayâ€™s standards, GPT-1 laid the groundwork for the GPT series and the concept of large language models.\nInfluence (1â€“10): 7/10\nBeginner-Friendly Explanation: GPT-1 was the first step toward modern chatbots and LLMs. The idea was simple but powerful: train a Transformer to predict the next word in a sentence by feeding it a ton of books. By doing this, the model learns grammar, facts, and some reasoning just from the text. Then the researchers showed that you could take this generatively pre-trained model and fine-tune it on specific tasks (like having it answer questions or analyze sentiment) and it worked really well ( 10 AI milestones of the last 10 years | Royal Institution). In essence, GPT-1 was like teaching a student by letting it read an entire library (with the goal of guessing missing words), and then giving it a short internship for a specific job â€“ and it turned out the student had learned enough from reading to do the job impressively well.\n2020 â€“ Brown et al.: â€œLanguage Models are Few-Shot Learnersâ€ (GPT-3) Contribution \u0026amp; Impact: Introduced GPT-3, a 175-billion-parameter transformer that demonstrated astonishing capabilities in natural language generation and few-shot learning. GPT-3 showed that simply by scaling up model size and training on virtually all of the internetâ€™s text, a language model could perform a wide range of tasks without explicit training for each one â€“ given just a few examples in its prompt (a phenomenon called in-context learning). It achieved strong performance on translation, Q\u0026amp;A, and more by prompt alone ( [2005.14165] Language Models are Few-Shot Learners - arXiv). GPT-3â€™s release dazzled the world with its ability to generate human-like essays, code, and dialogues, revealing emergent properties of language models at scale. This paper marked the point where â€œlarge language modelâ€ entered the mainstream vocabulary and led to widespread deployment of LLMs in applications.\nInfluence (1â€“10): 10/10\nBeginner-Friendly Explanation: GPT-3 was a quantum leap in scale and ability for AI language models. The OpenAI team scaled up the number of neurons (parameters) in the model to 175 billion (hundreds of times larger than prior models) and trained it on text from the internet and books. The surprising finding was that GPT-3 can learn to do a task from just a few examples given in the prompt, without any further training â€“ for instance, give it two English-to-French translation examples in the prompt, and it can translate a new English sentence to French by analogy ( [2005.14165] Language Models are Few-Shot Learners - arXiv). This â€œfew-shotâ€ ability felt almost like instant learning. Moreover, GPT-3 could generate paragraphs of coherent, often insightful text on almost any topic you prompted it with. It was the first AI that really felt like a general-purpose language generator, and it set the stage for the chatbots and creative AI that followed.\n2020 â€“ Jumper et al.: â€œHigh Accuracy Protein Structure Prediction Using Deep Learning (AlphaFold)â€ Contribution \u0026amp; Impact: Solved a 50-year grand challenge in biology â€“ the protein folding problem â€“ using deep learning. AlphaFold2 (described in a 2020 Nature paper and CASP competition results) employed an attention-based neural network (in fact, a modified transformer) to predict 3D protein structures from amino acid sequences with atomic-level accuracy ( 10 AI milestones of the last 10 years | Royal Institution). It trained on the sequences and known structures in public databases and dramatically outperformed all prior methods, achieving accuracies comparable to experimental laboratory techniques. This was a milestone not just for AI but for science, showing that AI can make major contributions to scientific problems. AlphaFoldâ€™s success has accelerated research in drug discovery and biology, and it demonstrated the versatility of deep learning beyond traditional â€œAI tasks.â€\nInfluence (1â€“10): 10/10\nBeginner-Friendly Explanation: AlphaFold was like an â€œAI biochemistâ€ that figured out how proteins fold into their 3D shapes. Proteins are molecular machines in our bodies, and their function depends on their shape. Determining that shape used to take scientists years in the lab, but AlphaFold learned to predict shapes in hours with incredible accuracy ( 10 AI milestones of the last 10 years | Royal Institution). How? It looked at tens of thousands of known protein shapes and learned patterns. Using a transformer neural network, it could then take a new proteinâ€™s sequence (its amino acid recipe) and compute its likely folded structure. This achievement was considered a major scientific breakthrough â€“ something many experts thought AI wouldnâ€™t crack for a long time. It showed AI can not only play games or chat, but also solve hard scientific puzzles, potentially leading to new medicines and understandings of biology.\n2022 â€“ Ouyang et al.: â€œTraining Language Models to Follow Instructions with Human Feedbackâ€ (InstructGPT) Contribution \u0026amp; Impact: Demonstrated a successful approach to AI alignment by fine-tuning a large language model using human feedback. The paper introduced InstructGPT, a version of GPT-3 fine-tuned with Reinforcement Learning from Human Feedback (RLHF). Human evaluators ranked outputs, and those rankings were used to train a reward model which then guided the policy optimization. The result was an LLM that followed user instructions much more reliably and produced outputs that were more truthful and less toxic than the base GPT-3 ( Training language models to follow instructions with human feedback). InstructGPT showed that large models can be steered toward helpful behavior, and its techniques directly underlie OpenAIâ€™s ChatGPT. This was a key step in making LLMs practically useful and safer for wide deployment.\nInfluence (1â€“10): 9/10\nBeginner-Friendly Explanation: InstructGPT took a big language model and basically gave it a manners class with human teachers. Instead of just predicting the next word in general, it was trained to follow instructions. Researchers accomplished this by showing the model many examples of prompts and ideal answers (crafted or rated by humans), and even having humans rank different AI responses to the same prompt. The model was then tweaked (using a reinforcement learning process) to prefer responses that humans liked ( Training language models to follow instructions with human feedback). The outcome: compared to the original GPT-3, InstructGPT is much better at doing what you ask â€“ if you prompt it to be polite and to avoid certain topics, it will, and it makes fewer factual errors. This approach of using human feedback became crucial for turning big general models into helpful assistants (itâ€™s essentially how ChatGPT was trained).\n2023 â€“ OpenAI: â€œGPT-4 Technical Reportâ€ Contribution \u0026amp; Impact: Described GPT-4, a large-scale multimodal model that represents the state-of-the-art in 2023. GPT-4 is multimodal, accepting both image and text inputs and generating text outputs ( [2303.08774] GPT-4 Technical Report - arXiv). It further improved the capability and alignment of LLMs â€“ exhibiting more advanced reasoning, fewer mistakes, and the ability to handle much more complex instructions than its predecessors. While many details (like model size) remain unpublished, GPT-4â€™s impact has been to push the envelope of what AI systems can do (such as passing standardized tests, coding large programs, or analyzing images in detail) and to highlight new challenges (like hallucination reduction and transparency). GPT-4â€™s release solidified that LLMs are here to stay, being integrated into products and society at large, and it raised the urgency of addressing AIâ€™s open problems.\nInfluence (1â€“10): 9/10\nBeginner-Friendly Explanation: GPT-4 is currently the most advanced iteration of the GPT series. One big new feature is that it can accept images as part of the prompt â€“ so you can show it a picture and ask questions about it, and it will respond (for example, describing an image or interpreting a meme) ( [2303.08774] GPT-4 Technical Report - arXiv). Itâ€™s also significantly smarter in many ways: it can handle much longer essays, itâ€™s harder to trick into giving harmful answers, and it scores impressively on exams in math, law, medicine, etc. People have used GPT-4 to draft legal documents, create apps from scratch, and tutor themselves on complex topics. In short, GPT-4 pushed the boundary of AI capabilities forward, while also making it clear that we need to deal with issues like the model sometimes being confidently wrong (hallucinations) or the need for even better alignment with human values.\nOpen Problems and Future Directions # Despite these milestones and the immense progress in AI, several fundamental challenges remain unsolved:\nExplainability: Many advanced AI models (especially deep neural networks) operate as â€œblack boxes,â€ making it hard to understand or trust their decisions ( The Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Developing AI that can explain why it made a certain decision is an ongoing area of research.\nGeneralization and Robustness: AI systems can be brittle â€“ a model trained on certain data may fail in unexpected ways when faced with novel situations or adversarial inputs ( The Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Achieving human-like adaptability and reliability in the open world (not just controlled settings) is still an open problem.\nBias and Fairness: Models often inherit biases present in their training data, which can lead to unfair or incorrect outcomes, especially in sensitive applications ( The Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Eliminating harmful biases and ensuring AI systems treat different groups fairly is crucial and challenging.\nCommon-Sense Reasoning: AI still notably lacks common sense. Tasks that are trivial for humans â€“ understanding unstated assumptions, basic physical or social logic â€“ can stump AI systems ( The Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Building AI with a robust common-sense understanding of the world is an important unsolved challenge.\nSafety and Alignment: As AI systems become more powerful, ensuring they align with human values and intent is paramount ( The Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). This involves preventing unintended harmful behavior, controlling misinterpretations of instructions, and, in the long term, making sure AI goals remain tethered to what humans actually want. Techniques like the RLHF used in InstructGPT are initial steps, but deeper solutions in AI safety, robustness, and governance are actively being sought.\nIn summary, the evolution of AI has been driven by a series of conceptual and technical breakthroughs â€“ from the theoretical underpinnings of symbolic reasoning and neural networks to the empirical triumphs of deep learning and large language models. Each foundational paper above either opened a new avenue of research or dramatically accelerated progress in an existing one. A reader with a strong analytical background can appreciate how each innovation addressed a core limitation of the previous generation: giving machines the ability to reason with logic, to learn from data, to handle uncertainty, to perceive patterns, to remember sequences, to leverage big data and compute, and to align with human goals. While many â€œimpossibleâ€ feats have now been achieved (playing Go, folding proteins, human-level dialogue), AI is not a solved problem â€“ far from it. The community is now wrestling with making AI more understandable, general, fair, and safe. These open problems will define the next chapter of AI research, as we move from simply building powerful systems to ensuring those systems behave intelligently and beneficially in the messy, nuanced world we live in.\nSources: The descriptions above draw from the original papers and subsequent analyses, including historical accounts and summaries from Wikipedia and other secondary sources. Key references include McCulloch \u0026amp; Pitts (1943) ( A Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia) ( A Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia), Turing (1950) ( Alan Turing\u0026rsquo;s Contributions to Artificial Intelligence : History of Information), Newell \u0026amp; Simon (1956) ( Newell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information) ( Newell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information), Rosenblattâ€™s perceptron (1958) ( Rosenblatt\u0026rsquo;s Perceptron Uses a Type of Neural Network : History of Information) ( Professorâ€™s perceptron paved the way for AI â€“ 60 years too soon | Cornell Chronicle), Samuelâ€™s checkers (1959) ( The games that helped AI evolve | IBM), Minsky \u0026amp; Papert (1969) ( Minsky \u0026amp; Papertâ€™s â€œPerceptronsâ€ â€“ Building Babylon) ( Minsky \u0026amp; Papertâ€™s â€œPerceptronsâ€ â€“ Building Babylon), backpropagation (1986) ( Backpropagation - Wikipedia), Pearlâ€™s Bayesian networks (1988) ( Probabilistic Reasoning (1993â€“2011) â€” Making Things Think: How AI and Deep Learning Power the Products We Use) ( Probabilistic Reasoning (1993â€“2011) â€” Making Things Think: How AI and Deep Learning Power the Products We Use), LeCunâ€™s CNN (1989) ([\nDeep Neural Nets: 33 years ago and 33 years from now (Invited Post) Â· The ICLR Blog Track\n]( https://iclr-blog-track.github.io/2022/03/26/lecun1989/#:~:text=The%20Yann%20LeCun%20et%20al,I%20set%20out%20to%20reproduce)), Watkinsâ€™ Q-learning (1989) ( Q-learning - Wikipedia), Cortes \u0026amp; Vapnik (1995) on SVMs ( Support vector machine - Wikipedia), Hochreiter \u0026amp; Schmidhuber (1997) on LSTM ( Long short-term memory - Wikipedia), Hintonâ€™s deep belief nets (2006) ( [PDF] Why does Unsupervised Pre-training Help Deep Learning?), AlexNet (2012) ( AlexNet and ImageNet: The Birth of Deep Learning | Pinecone) ( AlexNet and ImageNet: The Birth of Deep Learning | Pinecone), GANs (2014) ( 10 AI milestones of the last 10 years | Royal Institution), DeepMindâ€™s DQN (2015) ( Human-level control through deep reinforcement learning | Nature), AlphaGo (2016) ( Mastering the game of Go with deep neural networks and tree search | Nature), the Transformer (2017) ( 10 AI milestones of the last 10 years | Royal Institution), BERT (2018) ( [PDF] BERT: Pre-training of Deep Bidirectional Transformers for Language \u0026hellip;), GPT-1 (2018) ( 10 AI milestones of the last 10 years | Royal Institution), GPT-3 (2020) ( [2005.14165] Language Models are Few-Shot Learners - arXiv), AlphaFold (2020) ( 10 AI milestones of the last 10 years | Royal Institution), InstructGPT (2022) ( Training language models to follow instructions with human feedback), and GPT-4 (2023) ( [2303.08774] GPT-4 Technical Report - arXiv), as well as discussions on AIâ€™s remaining challenges ( The Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium) ( The Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). These works collectively chart the conceptual history of AIâ€™s evolution and the road ahead.\n"},{"id":6,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/03-Numerical-Solution-of-Ordinary-Differential-Equations/","title":"å¸¸å¾®åˆ†æ–¹ç¨‹çš„æ•°å€¼è§£","section":"æ•°å€¼æ–¹æ³•","content":" Introduction # The subject of this Chapter is the numerical approximation of the Cauchy problem:\n$$(1) \\quad \\frac{dy}{dt} = f(t,y) \\quad \\text{in } t \u0026gt; 0 \\quad (I.C.)$$\nwith $$y(0) = y_0 \\text{ given}.$$\nor, more in general, a system:\n$$(2) \\quad \\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(t,\\mathbf{y}) \\quad \\text{in } t \u0026gt; 0$$\nwith $$\\mathbf{y}(0) = \\mathbf{y}_0.$$\nLet\u0026rsquo;s recall some basic results on the function $f$. It is said to be Lipschitz continuous in an interval $I$ if there exists a positive $L \u0026gt; 0$ s.t.\n$$|f(t,y_1) - f(t,y_2)| \u0026lt; L |y_1 - y_2|.$$\nClearly, if $f$ is differentiable with continuity in $I$, it is also Lipschitz continuous with $L \\leq \\max_I \\left|\\frac{\\partial f}{\\partial y}\\right|$.\nLocal Theorem # If $f$ is Lipschitz continuous in a range $t \\in I_1$ and $y \\in I \\subseteq \\mathbb{R}$, then $\\exists$ an interval $\\hat{I} \\subseteq I$ where the solution to $(1)$ exists and is unique.\nGlobal Theorem # If the $f$ is Lipschitz continuous $\\forall t \\in I$ and $y \\in \\mathbb{R}$, then the solution $\\exists$ uniquely in $I$.\nStability Definitions # From the practical point of view, it is important to consider also the perturbed case:\n$$(1_\\epsilon): \\quad \\frac{dy_\\epsilon}{dt} = f(t, y_\\epsilon) + \\delta(t) \\quad t \\geq 0$$\n$$y^\\epsilon(0) = y_0 + \\epsilon$$\nwith $|\\delta(t)| \\leq \\epsilon \\quad \\forall t \\geq 0$\nIf there exists a finite constant $C$ such that\n$$|y - y_\\epsilon| \u0026lt; C\\epsilon \\quad (*)$$\nthen we say that the solution is Lyapunov stable.\nIn general, $C$ may depend on $t$, so that $(*)$ may hold on finite intervals (and not over the entire $[0,\\infty)$ axis).\nTo have a stronger concept, we advocate the ASYMPTOTIC STABILITY:\n$$\\lim_{t \\to \\infty} |y(t) - y_\\epsilon(t)| = 0.$$\nFrom the engineering point of view, this is a central concept, as the theory of control (Automatical Control, Feedback control) relies on this definition.\nRemark # The Cauchy problem has a formal (quite useless) solution:\n$$y(t) = y_0 + \\int_0^t f(\\tau, y(\\tau))d\\tau$$\nconnecting the solution in $t$ with its past.\nThis is basically an alternative formulation of the problem and, in fact, will be used to formulate possible numerical approximation alternative to the ones based on $(1)$.\nSome Simple Examples # To begin with, we can split the time interval in subintervals, for instance with a uniform reticulation with step $\\Delta t$.\n![Time discretization with points at 0, Î”t, 2Î”t, 3Î”t\u0026hellip;]\nThen, we can use the formula:\n$$\\frac{dy}{dt}(t_i) \\simeq \\frac{y(t_{i+1}) - y(t_i)}{\\Delta t}$$\nthat we know is accurate with an error scaling with $\\Delta t$. In this way, we have:\n[From now on, the approximation of the solution $y(t)$ in $t_i$ will be denoted with $u_i$]\n$$\\frac{u_{i+1} - u_i}{\\Delta t} = f(t_i, u_i)$$\nIn practice, starting at $t = 0$: $$\\begin{align} u_1 \u0026amp;= u_0 + \\Delta t , f(t_0, u_0) \\quad \\rightsquigarrow \\quad (u_0 = y_0) \\ u_2 \u0026amp;= u_1 + \\Delta t , f(t_1, u_1) \\end{align}$$\nWe can easily compute the approximation $u_i$ of $y(t_i)$.\nOn the other hand, we could do:\n$$\\frac{u_i - u_{i-1}}{\\Delta t} = f(t_i, u_i)$$\nleading to:\n$$u_1 = u_0 + \\Delta t , f(t_1, u_1) \\quad (u_0 = y_0)$$\nThis is not as easy as before: it\u0026rsquo;s a NONLINEAR (ALGEBRAIC) EQUATION; we know how to solve it (first Chapter), but it is a computational additional cost. Then, similarly, we have:\n$$u_2 = u_1 + \\Delta t , f(t_2, u_2)$$\nWe have also another option:\n$$\\frac{dy}{dt}(t_i) \\simeq \\frac{u_{i+1} - u_{i-1}}{2\\Delta t}$$\nIn this case, the error scales with $O(\\Delta t^2)$. So, in practice we have:\n$$u_{i+1} = 2\\Delta t , f(t_i, u_i) + u_{i-1}$$\nor specifically: $u_2 = 2\\Delta t , f(t_1, u_1) + u_0 \\quad (u_0 = y_0)$\nI need to know $u_1$, not just $u_0$, then we can use the method.\nWith these three examples, we have already found many possible types of methods:\nImplicit vs Explicit\nImplicit: Solve a non-linear equation Explicit: No need of solving equations One Step vs Multistep\nOne Step: $u_{i+1} = g(\\Delta t, u_i)$ Multistep: $u_{i+1} = g(\\Delta t, u_i, u_{i-1}, u_{i-2}\u0026hellip;)$ At the top of this, we have the problem of the accuracy and - even most importantly- of the CONVERGENCE.\nIn fact, the basic requirement we need is that the method is convergent:\n$$\\lim_{\\Delta t \\to 0} |y(t_i) - u_i| = 0$$\nThen, if we find that $|y(t_i) - u_i| \\sim O(\\Delta t^p)$ then the accuracy or the order of the method is $p$.\nNature (implicit/explicit), Number of steps and most important CONVERGENCE and accuracy (order) are the categories for analyzing and ranking different methods.\nBefore we embark ourselves in a general analysis, however, let\u0026rsquo;s focus on a specific case, where important concepts will be highlighted.\nAnalysis of Forward Euler # The method:\n$$\\frac{u_{i+1} - u_i}{\\Delta t} = f(t_i, u_i)$$\nis called Forward Euler.\nLet\u0026rsquo;s consider it in detail.\nTo start with, let\u0026rsquo;s introduce the distinction of \u0026ldquo;consistency\u0026rdquo; and truncation error.\nIf we have the exact solution $y_{ex}(t)$ it is easily realized that\n$$\\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\\Delta t} \\neq f(t_i, y_{ex}(t_i))$$\nFor instance,\n$$\\frac{dy}{dt} = \\lambda y \\quad y(0) = 1 \\implies y_{ex} = e^{\\lambda t}$$\nthen\n$$\\frac{e^{\\lambda(t_i+\\Delta t)} - e^{\\lambda t_i}}{\\Delta t} = e^{\\lambda t_i} \\frac{e^{\\lambda \\Delta t} - 1}{\\Delta t} \\neq \\lambda e^{\\lambda t_i} \\quad (\\lambda y(t_i))$$\nWe can be more specific:\n$$y_{ex}(t_{i+1}) = y_{ex}(t_i) + \\frac{dy_{ex}}{dt}(t_i)\\Delta t + \\frac{1}{2}\\frac{d^2y_{ex}}{dt^2}(t_i)\\Delta t^2 + \u0026hellip;$$\nNow:\n$$\\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\\Delta t} = \\frac{dy_{ex}(t_i)}{dt} + \\frac{1}{2}\\frac{d^2y_{ex}(t_i)}{dt^2}\\Delta t + \u0026hellip;$$\nThis gives us:\n$$\\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\\Delta t} = f(t_i, y_{ex}(t_i)) + \\left[\\frac{\\Delta t}{2}\\frac{d^2y_{ex}}{dt^2}\\right]$$\nForward Euler $\\quad \\quad \\quad$ Local Truncation Error (LTE)\nIn some sense, the truncation error is the residual we get when we pretend the exact solution to solve the numerical scheme.\nNow, to investigate how the error of Forward Euler works, let\u0026rsquo;s consider the following picture:\n![Error propagation diagram showing exact solution trajectory and numerical approximation]\nFrom the picture it is evident that the error:\n$$e_{i+1} = y_{ex}(t_{i+1}) - u_{i+1}$$\nis the result of two contributions:\n$$e_{i+1} = \\underbrace{y_{ex}(t_{i+1}) - u^{i+1}}{\\text{generated at the local step}} + \\underbrace{u^{i+1} - u{i+1}}_{\\text{propagated from previous steps}}$$\nFrom the previous definition:\n$$y_{ex}(t_{i+1}) = y_{ex}(t_i) + \\Delta t , f(t_i, y_{ex}(t_i)) + \\Delta t , \\tau_{i+1}$$\nwhere $\\tau_{i+1} = \\frac{\\Delta t}{2}\\frac{d^2y_{ex}(t_i)}{dt^2}$ is the local TE.\n$$u^*{i+1} = y{ex}(t_i) + \\Delta t , f(t_i, y_{ex}(t_i))$$\n$$y_{ex}(t_{i+1}) - u^*_{i+1} = \\Delta t , \\tau_i$$\nNow, let\u0026rsquo;s focus on the other part.\nThis second component $u^*{i+1} - u{i+1}$ is inherited from the previous errors.\n$$u^*{i+1} = y{ex}(t_i) + \\Delta t , f(t_i, y_{ex}(t_i))$$ $$u_{i+1} = u_i + \\Delta t , f(t_i, u_i)$$\nUsing the Lipschitz assumption $|f(t,y) - f(t,u)| \\leq L |y-u|$ we have:\n$$|u^*{i+1} - u{i+1}| \\leq |e_i| + \\Delta t , L |e_i| = (1 + \\Delta t , L)|e_i|$$\nwhere $e_i = y_{ex}(t_i) - u_i$\nAll together, we have:\n$$|e_{i+1}| \\leq \\Delta t |\\tau_i| + (1 + \\Delta t , L)|e_i|$$\nNow, take $|\\tau^*| = \\max_i |\\tau_i| = \\text{GLOBAL TRUNCATION ERROR}$.\nThen:\n$$|e_{i+1}| \\leq \\underbrace{\\Delta t |\\tau^*|}{\\text{local}} + \\underbrace{(1 + \\Delta t , L)}{\\text{propagated}}|e_i|$$\nAssume that $e_0 = 0$ (no errors on the initial conditions). Then we have:\n$$|e_1| \\leq \\Delta t |\\tau^|$$ $$|e_2| \\leq \\Delta t |\\tau^| + (1 + \\Delta t , L)|e_1| \\leq \\Delta t |\\tau^|(1 + (1+\\Delta t , L))$$ $$|e_3| \\leq \\Delta t |\\tau^| + (1+\\Delta t , L)|e_2| \\leq \\Delta t |\\tau^*|(1 + (1+\\Delta t , L) + (1+\\Delta t , L)^2)$$\nWe infer:\n$$|e_K| \\leq \\Delta t |\\tau^| \\sum_{j=0}^{K-1}(1+\\Delta t , L)^j = \\Delta t |\\tau^|\\frac{(1+\\Delta t , L)^K - 1}{1 - 1 - \\Delta t , L} = \\frac{|\\tau^*|}{L}((1+\\Delta t , L)^K - 1)$$\nNotice that:\n$$(1 + x)^K \\leq \\exp(xK)$$\nSo:\n$$|e_K| \\leq \\frac{|\\tau^|}{L}(\\exp(LKh) - 1) = \\frac{|\\tau^|}{L}(\\exp(Lt_K) - 1)$$\nwhere $K\\Delta t = t_K$\nNow, if we want to have a bound on the error in the interval $[0,T]$, we have:\n$$|e| \\leq \\frac{|\\tau^*|}{L}(\\exp(LT) - 1)$$\nWe have proved the following Theorem:\nConvergence of Forward Euler # If the initial data are exact, and $f$ is Lipschitz continuous with respect to the $y$-argument, with a solution $\\in C^2(0,T)$, then FE converges.\nIn fact:\n$$|\\tau^*| = \\frac{1}{2}\\Delta t \\max_i |y^{\u0026rsquo;\u0026rsquo;}| \\xrightarrow{\\Delta t \\to 0} 0$$\nand $|e| \\leq |\\tau^*|\\frac{\\exp(LT) - 1}{L} \\xrightarrow{\\Delta t \\to 0} 0$ is bounded.\nFE is convergent with order 1.\nBeyond the Theorem per se (we will generalize it to other methods), it is important to notice that the convergence is the consequences of two peculiarities:\n(1) $\\tau^* \\xrightarrow{\\Delta t \\to 0} 0$ the LTE/GTE vanishes as $\\Delta t \\to 0$, so locally the error is under control.\nThis property is called CONSISTENCY.\n(2) The factor $\\frac{\\exp(LT) - 1}{L}$ is independent of $\\Delta t$ (or, in general, doesn\u0026rsquo;t blow up for $\\Delta t \\to 0$). This is related to the way the error propagates so it is a \u0026ldquo;global\u0026rdquo; property through the constant $L$.\nThe control of the error in time is called STABILITY.\nIn some sense, we can say that:\nCONVERGENCE = CONSISTENCY + STABILITY\nIn spite of the intuitive arguments we used here, we will see that this is a good representation of general results (Lax-Richtmyer equivalence Theorem) holding for method using linear combinations of the function $f$ evaluated in the points $(t_i, u_i)$.\nOther One-Step Methods # So far, we have two one-step methods (forward and backward Euler). Let\u0026rsquo;s see other two. It is instructive to see how they are devised.\nCrank-Nicolson # From $y(t) = y_0 + \\int_0^t f(\\tau, y(\\tau))d\\tau$ we can organize the following method:\n![Timeline showing points $t_0, t_1, t_2, t_3, t_4, t_5, t_6, t_7$]\nLocalize: $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(\\tau, y(\\tau)) d\\tau$\nDiscretize: approximate the integral with the trapezoidal rule:\n$$u_{n+1} = u_n + \\frac{f(t_{n+1}, u_{n+1}) + f(t_n, u_n)}{2} \\Delta t$$\nThis is a Second Order (EXERCISE) one-step implicit method.\nHeun # Let\u0026rsquo;s start from Crank-Nicolson and make it \u0026ldquo;explicit\u0026rdquo;.\nOn the right hand side of CN we set:\n$$u_{n+1} \\simeq u_n + \\Delta t , f(t_n, u_n) \\quad \\text{(Explicit Euler)}$$\nWe obtain the scheme:\n$$u_{n+1} = u_n + \\Delta t \\frac{f(t_{n+1}, u_n+\\Delta t f(t_n, u_n)) + f(t_n, u_n)}{2}$$\nThis is called Heun. It is One-step, still 2nd order (EXERCISE).\nThe Concept of Zero-Stability # Let\u0026rsquo;s start applying the concept to one-step methods in the form:\n$$u_{n+1} = u_n + \\Delta t , \\Phi(u_n, t_n, f_n; \\Delta t)$$\nLet\u0026rsquo;s consider the perturbed scheme:\n$$\\begin{cases} w_{n+1} = w_n + \\Delta t (\\Phi(z_n, t_n, f(t_n, z_n); \\Delta t) + \\delta_n) \\ w_0 = y_0 + \\delta_0 \u0026amp; \\text{with } |\\delta_i| \\leq \\varepsilon \\end{cases}$$\nWe say that the method is zero-stable if for $\\Delta t \u0026lt; \\Delta t_0$, there exists a constant $C \u0026gt; 0$ such that\n$$|u_n - z_n| \\leq C\\varepsilon$$\nfor $\\varepsilon \u0026gt; 0$ sufficiently small. ($C$ and $\\Delta t_0$ depend on problem data, $T_{fin}$, $f$).\nIt is possible to prove the following theorem:\nTheorem: If $\\Phi$ is Lipschitz continuous with respect to the dependence on $u_n$ ($\\exists \\Delta \u0026gt; 0$: $|\\Phi(u_n) - \\Phi(z_n)| \\leq \\Delta |u_n - z_n|$), then the One-step method is zero-stable.\nLipschitz continuity in this case is enough for zero-stability (and it is generally a consequence of Lipschitz continuity of $f$).\nThen, we have another theorem. It generalizes the theorem for the explicit Euler.\nTheorem: If $\\Phi$ is like in the previous theorem, then:\n$$|y(t_n) - u_n| \\leq \\left(|y_0 - u_0| + t_n , \\tau(\\Delta t)\\right) e^{L t_n}.$$\nTherefore, if:\n$\\tau(\\Delta t) \\to 0$ with $\\Delta t$ $y_0 - u_0 \\to 0$ with $\\Delta t$ the method is convergent\n(and the order is $\\Delta t^p$, with $p = \\min(p_1, p_2)$ where:\n$\\tau(\\Delta t) \\sim O(\\Delta t^{p_1})$ $y_0 - u_0 \\sim O(\\Delta t^{p_2})$ (generally $p = p_1$). In other terms:\nFor one-step method (not true for multi-step):\n$$\\Phi \\text{ Lipschitz continuous } \\Rightarrow \\text{ Method zero-stable} + \\text{Consistency} \\Rightarrow \\text{CONVERGENCE}$$\n$\\Rightarrow$ If $\\Phi$ is Lipschitz continuous, consistency $\\Rightarrow$ convergence.\nThe Concept of Absolute Stability # The zero-stability is a property of the numerical scheme, required by the presence of roundoff errors and other possible perturbations.\nHowever, there is another (maybe more engineering) concept of stability related to the nature of the problem to solve.\nIn simple words, we want that, if a problem is asymptotically stable, the numerical approximation is asymptotically stable too.\nIs this happening? Let\u0026rsquo;s consider the prototype of asymptotically stable problem (Model Problem).\nLet\u0026rsquo;s consider the Cauchy problem:\n$$\\begin{cases} \\frac{dy}{dt} = \\lambda y \u0026amp; t \u0026gt; 0 \\ y(0) = y_0 \\end{cases}$$\nWe know that the solution is asymptotically stable for $\\lambda \u0026lt; 0$ $(y_{ex} = y_0 e^{\\lambda t} \\xrightarrow{t \\to \\infty} 0 \\text{ for } \\lambda \u0026lt; 0)$\nIn fact: $\\frac{dz}{dt} = \\lambda z$ with $z(0) = y_0 + \\varepsilon \\Rightarrow z - y = \\varepsilon e^{\\lambda t} \\xrightarrow{t\\to\\infty} 0$ for $\\lambda \u0026lt; 0$.\nRemark # For a system: $\\begin{cases} \\frac{d\\mathbf{y}}{dt} = A\\mathbf{y} \u0026amp; \\mathbf{y} \\in \\mathbb{R}^n \\ \\mathbf{y}(0) = \\mathbf{y}_0 \u0026amp; A \\in \\mathbb{R}^{n \\times n} \\end{cases}$\nthe asymptotic stability is guaranteed if THE REAL PART OF ALL THE EIGENVALUES is negative.\nTo be general, from now on we will consider $\\lambda \\in \\mathbb{C}$ also for the scalar case. In particular, the left-plane $\\text{Real}(\\lambda) \u0026lt; 0$ is the region of the complex plane where the original problem is asymptotically stable.\nQuestion: is the solution of the model problem with Explicit Euler asymptotically vanishing as the exact solution? # Notice that the question is not related to the behavior of the solution for $\\Delta t \\to 0$, but for $\\Delta t$ given and $t \\to +\\infty$.\nLet\u0026rsquo;s see: $$\\frac{dy}{dt} = \\lambda y \\quad \\text{EE}: \\frac{u_{i+1} - u_i}{\\Delta t} = \\lambda u_i$$\n$$u_{i+1} = (1 + \\lambda \\Delta t) u_i \\quad (\\text{Re}(\\lambda) \u0026lt; 0)$$\n$$|u_{i+1}| = |1 + \\lambda \\Delta t| |u_i| \\Rightarrow |u_{i+1}| = |1 + \\lambda \\Delta t|^{i+1} |u_0|$$\nThe solution asymptotically vanishes if $|1 + \\lambda \\Delta t| \u0026lt; 1$\nIntuitively, if $\\lambda$ is Real and negative:\n$$|1 + \\lambda \\Delta t| \u0026lt; 1$$ $$\\Downarrow$$ $$-1 \u0026lt; 1 + \\lambda \\Delta t \u0026lt; 1$$ $$\\Downarrow$$ $$-2 \u0026lt; \\lambda \\Delta t \u0026lt; 0$$ $$\\Downarrow$$ $$\\Delta t \u0026lt; \\frac{2}{|\\lambda|}$$\nSo, if $\\Delta t \u0026gt; \\frac{2}{|\\lambda|}$ the numerical solution is not stable.\nFor $\\lambda$ complex, we can draw the region of the plane $\\lambda \\Delta t$ where the solution is stable.\n![Complex plane diagram showing unit circle with center at (-1,0)]\nUnit circle with center in $(-1, 0)$\n(In magenta the region of stability of the problem).\nSo, the method is reproducing the asymptotic behavior of the exact solution ONLY UNDER THE CONDITION THAT:\n$$\\lambda \\Delta t \\in \\text{Unit Circle centered in } (-1, 0).$$\nWhat happens with Implicit Euler? # $$\\frac{1}{|1 - \\lambda \\Delta t|} \u0026lt; 1 \\quad \\forall \\Delta t,$$\nso $u^{n+1} \\xrightarrow{n \\to \\infty} 0 \\quad \\forall \\Delta t$\nThere is a big difference between the two Eulers: one is stable under some conditions, the other is unconditionally stable.\nWhat about Crank-Nicolson? # $$u^{n+1} = u^n + \\Delta t \\frac{\\lambda u^{n+1} + \\lambda u^n}{2} \\Rightarrow u^{n+1} = \\frac{1 + \\frac{\\Delta t}{2}\\lambda}{1 - \\frac{\\Delta t}{2}\\lambda} u^n$$\nAgain, for $\\lambda \\in \\mathbb{C}$ we have:\n$$\\left|\\frac{1 + \\frac{\\Delta t}{2}\\lambda}{1 - \\frac{\\Delta t}{2}\\lambda}\\right| \u0026lt; 1 \\quad \\forall \\Delta t$$\nso also CN is stable with no condition.\nDEFINITION # A method is said to be ABSOLUTELY STABLE if the solution of the model problem $\\frac{dy}{dt} = \\lambda y$ vanishes asymptotically when $t \\to +\\infty$.\nWe say that the method is UNCONDITIONALLY ABSOLUTELY STABLE if this happens $\\forall \\Delta t \u0026gt; 0$. On the contrary, it is said to be CONDITIONALLY STABLE if we have limitations on $\\Delta t$.\nAnother Example: Heun # $$u^{n+1} = u^n + \\frac{\\Delta t}{2}(\\lambda(u^n + \\Delta t \\lambda u^n) + \\lambda u^n) = \\left(1 + \\Delta t \\lambda + \\frac{\\Delta t^2 \\lambda^2}{2}\\right) u^n$$\nNow, consider the curve $\\frac{\\Delta t^2 \\lambda^2}{2} + \\Delta t \\lambda + 1$ for $\\lambda \u0026lt; 0$\nWe see that we need:\n$$0 \u0026lt; \\Delta t \u0026lt; \\frac{2}{|\\lambda|}$$\n(as for Explicit Euler).\nIn the complex plane, the region is slightly larger than for Explicit Euler.\nREMARK: We found that the explicit methods are conditionally stable, while the implicit ones are unconditionally stable. # In general, we do not have unconditionally stable explicit methods, but we do have implicit methods that are only conditionally stable.\nThe region of absolute stability is the portion of $\\mathbb{C}^-$ for $\\lambda \\Delta t$ to belong to such that the method is absolutely stable. Unconditional absolute stability (a.k.a A-stability) is when this region is the entire left plane $\\mathbb{C}^-$.\nThe concept of absolute stability somehow clarifies what are the criteria to select a method for a problem.\nIn fact, let\u0026rsquo;s first consider the general case(s):\n$\\frac{dy}{dt} = f(t,y) \\simeq f(t,y_0) + \\frac{\\partial f}{\\partial t}(t-t_0) + \\frac{\\partial f}{\\partial y}(t,y_0)(y-y_0)$\nso we can locally take $\\lambda \\simeq \\frac{\\partial f}{\\partial y}(t,y_0)$\nFor a system: $\\frac{d\\mathbf{y}}{dt} = A \\mathbf{y} \\Rightarrow \\lambda = eig(A)$\n$\\frac{d\\mathbf{y}}{dt} = \\mathbf{F}(t,\\mathbf{y})$ (nonlinear system)\n$\\Rightarrow \\lambda = eig\\left(\\frac{\\partial \\mathbf{F}}{\\partial \\mathbf{y}}(t_0, y_0) \\right)$ - Jacobian\nNow, for a general problem, we have:\nMethod Nature Accuracy Limitations on $\\Delta t$ FE Explicit 1 $\\Delta t \u0026lt; \\frac{2}{ BE Implicit 1 NO CN Implicit 2 NO H Explicit 2 $\\Delta t \u0026lt; \\frac{2}{ Implicit Methods are more computationally expensive.\nIn an extreme synthesis:\n![Comparison of FE and BE with timeline]\nWith FE each step is low-cost (Explicit) but we may need to do many for the conditional stability.\nWith BE each step is more expensive, but we need to do (generally) fewer steps.\n$\\Rightarrow$ The optimal choice is largely problem dependent.\nMultistep Methods # The mid-point method is just an example of multi-step methods:\n$$u_{n+1} = u_{n-1} + 2\\Delta t , f(t_n, u_n)$$\nIn general, a multistep method with $p$ steps take the form:\n$$u_{n+1} - \\sum_0^p a_j u_{n-j} = \\Delta t \\sum_{j=-1}^p b_j f(t_{n-j}, u_{n-j})$$\nThe method is IMPLICIT when $b_{-1} \\neq 0$.\nExample: # $$y(t_{n+1}) = y(t_{n-1}) + \\int_{t_{n-1}}^{t_{n+1}} f(\\tau, y(\\tau)) d\\tau$$\n$$\\Downarrow \\text{ SIMPSON}$$\n$$u_{n+1} = u_{n-1} + 2\\Delta t \\frac{f(t_{n+1}, u_{n+1}) + 4f(t_n, u_n) + f(t_{n-1}, u_{n-1})}{6} =$$\n$$= u_{n-1} + \\frac{\\Delta t}{3}(f(t_{n+1}, u_{n+1}) + 4f(t_n, u_n) + f(t_{n-1}, u_{n-1}))$$\n$$\\mathbf{a} = \\begin{bmatrix} 0 \\ 1 \\end{bmatrix} a_0, a_1 \\quad \\mathbf{b} = \\begin{bmatrix} \\frac{1}{3} \\ \\frac{4}{3} \\ \\frac{1}{3} \\end{bmatrix} b_{-1}, b_0, b_1$$\nIn general, we have two approaches for deriving a Multi-step methods:\nBDF (Backward Difference Formulas) # $$\\frac{dy}{dt}(t_{n+1}) = f(t_{n+1}, u_{n+1})$$ $$\\downarrow$$ approximate this with a backward finite difference, e.g.,\n$$\\frac{dy}{dt}(t_{n+1}) \\simeq \\frac{\\frac{3}{2}u_{n+1} - 2u_n + \\frac{1}{2}u_{n-1}}{\\Delta t}$$\n$$\\Rightarrow u_{n+1} = \\frac{4}{3}u_n - \\frac{1}{3}u_{n-1} + \\Delta t f(t_{n+1}, u_{n+1})$$\nThey are all in the form:\n$$\\mathbf{b} = \\begin{bmatrix} 1 \\ 0 \\ \\vdots \\ 0 \\end{bmatrix}$$\nAdams # In this case, we start from:\n$$y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(\\tau, y(\\tau)) d\\tau$$\nNow, we replace $f$ with an interpolation:\nWe interpolate $f$ over the nodes: $n-p, n-p+1, \u0026hellip; n$ so to have an explicit method (Adams-Bashforth) We interpolate $f$ over the nodes: $n-p, n-p+1, \u0026hellip; n, n+1$ Adams methods have always:\n$$\\mathbf{a} = \\begin{bmatrix} 1 \\ 0 \\ \\vdots \\ 0 \\end{bmatrix}$$\nA Rapid Recall of Difference Equations Theory # A linear difference equation is an equation in the form:\n$$u_{n+p} + a_{n+p-1}u_{n+p-1} + a_{n+p-2}u_{n+p-2} + \\ldots + a_0u_n = \\varphi_{n+p}$$\nwhere the solution is the set ${u_j}$ and initial conditions $u_0, u_1, \\ldots u_{p-1}$ are given.\nThe theory on difference equations has several similarities with the theory on differential equations. In particular, the general solution of a linear difference equation is the linear combination of a particular solution of the equation and the general solution of the homogeneous one.\nThe general solution of the homogeneous takes the form:\n$$u_n = \\sum_{j=0}^{N}\\left(\\sum_{s=0}^{m_j-1} V_{js}n^s \\right)r_j^n$$\nwhere $r_j$ are the roots of:\n$$r^p + a_{p-1}r^{p-1} + a_{p-2}r^{p-2} + \\ldots + a_0 = 0$$\nand\n$N$ is the number of distinct roots $m_j$ is the multiplicity of $r_j$ We will see a strong connection between this theory and the analysis of the linear multi-step methods.\nIn fact, a LMM reads like:\n$$u_{n+1} - \\sum_{j=0}^p a_j u_{n-j} = \\Delta t \\sum_{j=-1}^p b_j f_{n-j}$$\nIf we consider the model problem, we are lead to the linear difference equation:\n$$u_{n+1} - \\sum_{j=0}^p a_j u_{n-j} - \\Delta t \\lambda \\sum_{j=-1}^p b_j u_{n-j} = 0$$\nso we need to solve exactly a linear difference equation.\nMore precisely, we still define the LTE as the residual of the exact solution in the numerical scheme:\n$$\\Delta t , \\tau_{n+1} = y_{ex}(t_{n+1}) - \\sum_{j=0}^p a_j y_{ex}(t_{n-j}) - \\Delta t \\sum_{j=-1}^p b_j \\frac{dy_{ex}}{dt}(t_{n-j})$$\nThe method is CONSISTENT when $\\tau_{n+1} \\xrightarrow{\\Delta t \\to 0} 0 \\quad \\forall n$\nZero-Stability Definition # The definition of zero-stability is similar to the one for One-step Methods.\nAlso, we define:\nFirst characteristic polynomial:\n$$\\rho(z) = z^{p+1} - \\sum_{j=0}^p a_j z^{p-j}$$\nSecond characteristic polynomial:\n$$\\sigma(z) = b_{-1}z^{p+1} - \\sum_{j=0}^p b_j z^{p-j}$$\nand the polynomial: $\\Pi(z) = \\rho(z) - \\Delta t \\lambda \\sigma(z)$ (this is the polynomial found for the model problem)\nBased on this we define:\nRoot Condition: Call $r_i$ the roots of $\\rho(z)$. The polynomial (or the associated LMM) fulfills the root condition when:\n(1) $|r_i| \\leq 1 \\quad \\forall i$ (2) The roots with $|r| = 1$ have multiplicity 1.\nStrong Root Condition: In addition to the R.C., only $r_0$ is s.t. $|r_0| = 1$, all the other roots $|r_j| \u0026lt; 1$ $(j \u0026gt; 1, \\ldots, p)$.\nAbsolute R.C.: $\\exists \\Delta t \\leq \\overline{\\Delta t}$ s.t. all the roots $r_j(\\Delta t)$ of $\\Pi_{\\Delta t}(z)$ are s.t. $|r_j(\\Delta t)| \u0026lt; 1$, $j = 0, \\ldots, p$, $\\Delta t \\leq \\overline{\\Delta t}$.\nAnalysis of Multistep Methods # We have a sequence of theorems (no proofs):\nA LMM is consistent if and only if:\n$$\\sum_{j=0}^p a_j = 1 \\quad -\\sum_{j=0}^p j a_j + \\sum_{j=-1}^p b_j = 1$$\nAlso, the method is at least of order $p$ if the solution is $\\in C^{p+1}(I)$ and\n$$(*)\\ \\sum_{j=0}^p (j)^k a_j + k \\sum_{j=-1}^p (j)^{k-1} b_j = 1 \\quad k = 1, 2, \\ldots q$$\nRemark: The condition $\\sum_{j=0}^p a_j = 1$ means that the first characteristic polynomial $\\rho(z)$ has at least one root in 1.\nA consistent method is zero-stable if and only if it fulfills the root condition.\nWith Theorems (1) + (2) we have the convergence and order analysis.\n(1) + (2): If the LMM method falls into (1) and (2) with condition (*) (not true for q+1) and the initial conditions ($u_i \\to y_{ex}(t_i)$ $i = 0, \\ldots, p$) converge to the exact ones with order $q$, the resulting method is convergent with order $q$.\nRemark (First Dahlquist Barrier): There is no zero-stable $p$-LMM with order\n$\u0026gt; p+1$ for $p$ odd $\u0026gt; p+2$ for $p$ even Let\u0026rsquo;s turn now to the Absolute stability.\nThe absolute root condition is necessary and sufficient for the absolute stability. In fact, if $\\overline{\\Delta t} = +\\infty$, the absolute stability is unconditional. The analysis of a LMM is completed by analyzing the coefficients of the method and the roots of the two polynomials $\\rho$ and $\\Pi$.\nThe roots of $\\Pi$ can be analyzed numerically (and are tabulated for most of the methods) by Matlab/Python subroutines.\nSEE EXAMPLES (NODEPY library in Python, QSS in Matlab)\nRemark (Second Dahlquist Barrier): There are no explicit LMM absolutely unconditionally stable. There are no LMM absolutely unconditionally stable with order $q \u0026gt; 2$.\nA Clarification on Stability Concepts # To clarify the different stability concepts:\nZero-stability:\n$$|u_j| \\leq C_{T_{fin}} (|u_0| + \\ldots |u_p|)$$\nwhere the $C$ may depend on $T_{fin}$\nAbsolute stability:\n$$C_{T_{fin}} \\xrightarrow{T_{fin} \\to \\infty} 0$$\n$C$ is bounded independently of $T_{fin}$\nWe call this \u0026ldquo;relative stability\u0026rdquo;\n$$\\text{for a consistent scheme} \\quad \\text{R.C.} \\Leftarrow \\text{Strong R.C.} \\Leftarrow\\text{A.R.C.}$$ $$\\text{CONVERGENCE} \\Leftarrow \\text{Zero-Stability} \\Leftarrow \\text{Relative Stability} \\Leftarrow \\text{Absolute Stability}$$\nExample (Extreme) # Mid-point:\n$$u_{n+1} = u_{n-1} + 2\\Delta t , f(t_n, u_n)$$\n$$a_0 = 0 \\quad a_1 = 1 \\quad \\Rightarrow \\rho(z) = z^2 - 1 = 0 \\quad \\Rightarrow \\rho = \\pm 1$$\nR.C.: OK\n$$\\Pi_{\\Delta t}(z) = z^2 - 2\\lambda z - 1$$\nThe product of the two roots is always -1, if one root is \u0026lt; 1 in magnitude, the other is \u0026gt; 1.\nUnconditionally Absolutely UNSTABLE\nPredictor-Corrector Methods # A clear message from the previous examples is that, in general, explicit methods are less absolutely stable, they are never unconditionally stable and have smaller regions of stability.\nHowever, let\u0026rsquo;s reconsider implicit methods too.\nFor instance, let\u0026rsquo;s consider a generic LMM:\n$$u_{n+1} - \\sum_{j=0}^p a_j u_{n-j} = b_{-1} \\Delta t , f(t_{n+1}, u_{n+1}) + \\Delta t \\sum_{j=0}^p b_j f(t_{n-j}, u_{n-j})$$\nTo solve this nonlinear equation, we could use a Newton method but also a fixed-point method and a natural candidate is the following:\n$$u_{n+1}^{(m)} = \\sum_{j=0}^p a_j u_{n-j} + \\Delta t \\sum_{j=0}^p b_j f_{n-j} + \\Delta t b_{-1} f(t_{n+1}, u_{n+1}^{(m-1)})$$\nNotice that this method converges if:\n$$\\left|\\Delta t , b_{-1} , f\u0026rsquo;(t_{n+1}, u_{n+1}) \\right| \u0026lt; 1$$\nSo we have the condition:\n$$\\Delta t \u0026lt; \\frac{1}{|b_{-1}||f\u0026rsquo;|}$$\nEven if the method is unconditionally stable, we may have a condition on $\\Delta t$ for the convergence of the fixed-point.\nIn any case, a good initial condition is required, because with a good $u^{(0)}$ the number of iterations can be reduced: an explicit method can be used to this.\nThese considerations lead to the design of a new class of methods, Heun being one of the possible examples.\nPredictor [P] is an explicit method of order $q_P$\nCorrector [C] is an implicit method of order $q_C$\nPredictor Corrector method:\nP: do one step of P $\\Rightarrow u_{n+1}^{(0)}$\nE: evaluate $f(t_{n+1}, u_{n+1}^{(0)})$\nC: compute $m$ fixed-point iterations of C\nOptional: E: compute $f_{n+1} = f(t_{n+1}, u_{n+1}^{(m)})$\nThese methods go under the name of $\\text{PEC}^m$ or $\\text{PEC}^m\\text{E}$ if the last step is taken.\nExamples: # P = Adams-Bashforth of order 2 C = Adams-Moulton of order 3 $m = 1$\nPEC: $$\\begin{cases} u_{n+1}^{(0)} = u_n^{(1)} + \\frac{\\Delta t}{2}(3f_n^{(0)} - f_{n-1}^{(0)}) \\ f_{n+1}^{(0)} = f(t_{n+1}, u_{n+1}^{(0)}) \\ u_{n+1}^{(1)} = u_n^{(1)} + \\frac{\\Delta t}{12}(5f_{n+1}^{(0)} + 8f_n^{(0)} - f_{n-1}^{(0)}) \\end{cases}$$\nPECE: $$\\begin{cases} u_{n+1}^{(0)} = u_n^{(1)} + \\frac{\\Delta t}{2}(3f_n^{(1)} - f_{n-1}^{(1)}) \\ f_{n+1}^{(0)} = f(t_{n+1}, u_{n+1}^{(0)}) \\ u_{n+1}^{(1)} = u_n^{(1)} + \\frac{\\Delta t}{12}(5f_{n+1}^{(0)} + 8f_n^{(1)} - f_{n-1}^{(1)}) \\ f_{n+1}^{(1)} = f(t_{n+1}, u_{n+1}^{(1)}) \\end{cases}$$\nA natural question is: what is the accuracy of $\\text{PEC}^m$ or $\\text{PECE}^m$? What is the region of absolute stability?\nTheorem (Accuracy): $\\text{PECE}$ methods have (under regularity assumptions on the solution) order of accuracy:\n$$q_{PC} = min(q_P + m, q_C)$$\nFor the region of absolute stability, in general:\n$$\\text{Region}(P) \\subseteq \\text{Region}(\\text{PECE}) \\subseteq \\text{Region}(C)$$\nand $\\text{Region}(\\text{PECE}) \\xrightarrow{m \\to +\\infty} \\text{Region}(C)$\nRunge-Kutta Methods # Heun is also an RK method:\n$$u_{n+1} = u_n + \\Delta t\\left(f_n + f(t_{n+1}, u_n + \\Delta t f_n)\\right)$$\nThis is a 2nd order method but the accuracy is not obtained using more steps, but giving up the linearity of the method.\nIn general, RK are in the form:\n$$u_{n+1} = u_n + \\Delta t , \\mathbf{K} \\cdot \\mathbf{b}$$\nwhere $\\mathbf{K}, \\mathbf{b} \\in \\mathbb{R}^s$\nand $[\\mathbf{K}]_i = f(t_n + c_i \\Delta t, u_n + \\Delta t[A\\mathbf{K}]_i)$\nwhere $[\\mathbf{A}\\mathbf{K}]_i$ is the $i^{th}$ entry of the vector $\\mathbf{A}\\mathbf{K}$\nThe method is characterized by: $s$ (stages), $\\mathbf{b}$, $\\mathbf{c}$ and $\\mathbf{A} \\in \\mathbb{R}^{s \\times s}$\nIn particular, the three \u0026ldquo;ingredients\u0026rdquo; are generally written as:\n$$\\frac{\\mathbf{c} | \\mathbf{A}}{\\mathbf{b}^T} \\quad \\text{(Butcher array)}$$\nIt is assumed that $c_i = \\sum_{j=1}^s a_{ij} \\quad \\forall i = 1, \\ldots s$\nIf $\\mathbf{A}$ is such that $a_{ij} = 0 \\quad \\forall j \\geq i$ then:\n$$K_i = [\\mathbf{K}]i = f(t_n + \\Delta t c_i, u_n + \\Delta t \\sum{j \u0026lt; i} a_{ij} K_j)$$\nso the computation of $K_i$ is immediate. We call this case an explicit RK scheme.\nIf $a_{ij} = 0 \\quad \\forall j \u0026gt; i$, then:\n$$K_i = [\\mathbf{K}]i = f(t_n + \\Delta t c_i, u_n + \\Delta t \\sum{j \u0026lt; i} a_{ij} K_j + \\Delta t a_{ii} K_i)$$\nso we need to solve a non-linear equation to obtain $K_i$. We call this a semi-implicit method.\nIn general, computing $\\mathbf{K}$ requires the solution of a non-linear system (implicit method).\nClearly, the computational cost increases in the three cases.\nDerivation of an Explicit RK Method # A possible strategy to devise an explicit method is to maximize the order of the LTE with Taylor expansion analysis.\nFor instance, for $s = 2$:\n$$\\begin{pmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\ c_2 \u0026amp; a_{21} \u0026amp; 0 \\ \\hline \u0026amp; b_1 \u0026amp; b_2 \\end{pmatrix}$$\nWe have three parameters, but we set $a_{21} = c_2$.\n$$u_{n+1} = u_n + \\Delta t(b_1 f(t_n, u_n) + b_2 f(t_n + c_2 \\Delta t, u_n + \\Delta t c_2 f(t_n, u_n)))$$\n$$y_{ex}(t_n) = y_{ex}(t_n) + \\Delta t \\frac{dy_{ex}}{dt}(t_n) + \\frac{\\Delta t^2}{2}\\frac{d^2y_{ex}}{dt^2}(t_n) + \\frac{\\Delta t^3}{3!}\\frac{d^3y_{ex}}{dt^3}(t_n) + H.O.T.$$\n$$f(t_n + c_2 \\Delta t, u_n + \\Delta t c_2 f(t_n, u_n)) =$$\n$$= f(t_n, u_n) + \\frac{\\partial f}{\\partial t}(t_n, y_n)c_2 \\Delta t + \\frac{\\partial f}{\\partial y}(t_n, y_n)c_2 \\Delta t f(t_n, u_n) =$$\nNotice that: $$\\frac{d^2y}{dt^2} = \\frac{df}{dt} = \\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{dy}\\frac{dy}{dt} = \\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{\\partial y}f$$\n$$\\Rightarrow f(t_n, u_n) + c_2 \\Delta t \\frac{d^2y}{dt^2}(t_n)$$\nThe Taylor expansion applied to the scheme reads:\n$$u_{n+1} = u_n + \\Delta t(b_1 + b_2)f + c_2 b_2 \\Delta t^2 \\frac{d^2y}{dt^2}$$\nWe match therefore the first terms of the Taylor expansion for:\n$$\\begin{align} b_1 + b_2 \u0026amp;= 1 \\ b_2 c_2 \u0026amp;= \\frac{1}{2} \\end{align}$$\nFor $b_2 = \\frac{1}{2}$ we obtain the Heun method.\nThe L.T.E is $\\Delta t , \\tau \\sim O(\\Delta t^3)$\nso the method is 2nd order.\nImplicit methods can be devised from Gaussian quadratures.\nAnalysis of RK # CONSISTENCY: As the previous example shows, we need $\\sum b_i = 1$. This is necessary and sufficient for the consistency.\nZERO-STABILITY: RK are One-Step methods, if $f$ is Lipschitz-Continuous, they are zero-stable.\nORDER: In general, the order we can obtain depends on the number of stages in a (non-trivial) way:\nExplicit RK: ORDER 1 2 3 4 5 6 7 8 $s_{min}$ 1 2 3 4 6 7 9 11 $s_{min}$ = minimum number of stages to obtain the corresponding order.\nABSOLUTE STABILITY # If we write the method for the model problem, we can write:\n$$u_{n+1} = \\mathcal{R}(\\Delta t \\lambda) u_n$$\nThe region of absolute stability is, in general, the non-trivial region where $|\\mathcal{R}(\\Delta t \\lambda)| \u0026lt; 1$ (in $\\mathbb{C}$).\nWhy are RK so popular? # RK are extremely popular, because they can be high order with \u0026ldquo;only\u0026rdquo; one-step.\nOne-step means that:\nwe do not need high-order approximation of the initial data needed by LMM (the initial condition is enough) we can easily perform the time-step ADAPTIVITY (much more difficult with LMM). For the adaptivity, there are smart combinations of RK that obtain the adaptivity (i.e. an error estimate to adapt the step) in an efficient way.\nOne of the most popular is: RK45 Fehlberg: smart combination of an explicit method of order 4 and an explicit of order 5 to adapt the step.\nA Final Note on Stiff problems # Many of the concepts used here can be extended to systems of ODEs:\n$$\\begin{cases} \\frac{d\\mathbf{y}}{dt} = \\mathbf{F}(t, \\mathbf{y}) \\ \\mathbf{y}(0) = \\mathbf{y}_0 \\end{cases}$$\nIn the case of a linear ODE system:\n$$\\mathbf{F} = A , \\mathbf{y} \\quad \\downarrow \\quad \\text{matrix}$$\nThere is, however, an important concept to clarify.\nConsider a simple 2Ã—2 problem:\n$$\\frac{d\\mathbf{y}}{dt} = A\\mathbf{y}$$\nwhere $A$ has the two eigenvalues: $\\begin{cases} \\lambda_1 = -10^6 \\ \\lambda_2 = -1 \\end{cases}$\nIf we use Explicit Euler:\n$$\\mathbf{y}^{n+1} = \\mathbf{y}^n + \\Delta t , A \\mathbf{y}^n$$\nthe region of absolute stability is:\n$$\\Delta t \\leq \\min\\left(\\frac{2}{10^6}, \\frac{2}{1}\\right) = 2 \\cdot 10^{-6}$$\nThe solution, on the other hand, is the linear combination of the two functions:\n$$e^{-10^6 t}, e^{-t}$$\n[Graph showing rapid decay of $e^{-10^6 t}$ vs slower decay of $e^{-t}$]\nTo capture the fast dynamics ($\\lambda = 10^{-6}$), that fades away immediately, we need to take $\\Delta t \\sim 10^{-6}$!!!\nAn explicit method is certainly not a good choice here.\nIn general, we say that a problem is \u0026ldquo;stiff\u0026rdquo; when it may require very stringent time-step in a non-efficient way. The name \u0026ldquo;stiff\u0026rdquo; originates from the coupling of springs with different stiffness:\n[Simple diagram of a mass connected to two springs with different spring constants]\nto study the dynamics of the two real balls, one can write an ODE system.\nIf $K_1 \u0026laquo; K_2$ this is a stiff problem.\nEXERCISE on LMM # Consider the following family of methods (LMM):\n$$u_{n+1} = \\alpha u_n + (1-\\alpha)u_{n-1} + \\Delta t , \\gamma f_{n+1}$$\nInvestigate the convergence properties of the method as function of $\\alpha$ and $\\gamma$. Sol: For $\\alpha \\neq 1$, the method is 2-step. For $\\gamma \\neq 0$, the method is implicit.\nIt\u0026rsquo;s a LMM with: $\\mathbf{a} = \\begin{bmatrix} \\alpha \\ 1-\\alpha \\ 0 \\end{bmatrix} \\begin{bmatrix} 0 \\ 1 \\ 0 \\end{bmatrix}$, $\\mathbf{b} = \\begin{bmatrix} \\gamma \\ 0 \\ 0 \\end{bmatrix} \\begin{bmatrix} -1 \\ 0 \\ 1 \\end{bmatrix}$\nConsistency: $\\sum a_j = 1$: $\\alpha + (1-\\alpha) = 1$ âœ“OK $-\\sum j a_j + \\sum b_j = 1$: $0 \\cdot \\alpha - 1 \\cdot (1-\\alpha) + \\gamma = 1$ $\\Rightarrow \\gamma = 2 - \\alpha$\nThe methods: $u_{n+1} = \\alpha u_n + (1-\\alpha)u_{n-1} + \\Delta t (2-\\alpha)f_{n+1}$ are consistent.\nOrder: $\\sum (j)^2 a_j + 2\\sum (-j)^1 b_j = 1$ ? $\\Rightarrow (1-\\alpha) + 2(2-\\alpha) = 1 \\Rightarrow \\alpha = \\frac{4}{3}, \\gamma = \\frac{2}{3}$\nInvestigate the absolute stability of the method with order 2. The method: $u_{n+1} = \\frac{4}{3}u_n - \\frac{1}{3}u_{n-1} + \\Delta t \\frac{2}{3}f_{n+1}$\nis of order 2 (it is, in fact, a BDF of order 2).\n$\\rho(z) = z^2 - \\frac{4}{3}z + \\frac{1}{3} = 0 \\quad r_{1,2} = \\frac{\\frac{4}{3} \\pm \\sqrt{\\frac{16}{9} - \\frac{4}{3}}}{2} = \\frac{4 \\pm 2}{6} = \\frac{2 \\pm 1}{3}$\nR.C. âœ“\n$\\Pi_{\\Delta t}(z) = z^2 - \\frac{4}{3}z + \\frac{1}{3} - \\Delta t \\lambda \\frac{2}{3}z^2 = 0$\nLet\u0026rsquo;s consider $\\lambda \\in \\mathbb{R}^-$:\n$(3 - \\Delta t \\lambda 2)z^2 - 4z + 1 = 0$\n$z^2 - \\frac{4z}{3 + 2\\Delta t|\\lambda|} + \\frac{1}{3 + 2\\Delta t|\\lambda|} = 0$\n$r_1 \\cdot r_2 = \\frac{1}{3 + 2\\Delta t|\\lambda|}$\n$r_1 = \\frac{2 + \\sqrt{1 - 2\\Delta t|\\lambda|}}{3 + 2\\Delta t|\\lambda|} \\xrightarrow{\\Delta t \\to 0} 1$\n$|r_1(\\Delta t)| \u0026lt; 1 \\quad \\forall \\Delta t \u0026gt; 0$\n$r_2 = \\frac{2 - \\sqrt{1 - 2\\Delta t|\\lambda|}}{3 + 2\\Delta t|\\lambda|} \\xrightarrow{\\Delta t \\to 0} \\frac{1}{3}$\n$|r_2(\\Delta t)| \u0026lt; \\frac{1}{3}$\nMethod unconditionally stable (Verify with Python/Matlab).\nSolve $$\\begin{cases} \\frac{dy}{dt} = -(1 + t_g(t))y \u0026amp; t \\in [0, 1] \\ y(0) = 1 \\end{cases}$$ with the BDF2 method found and verify your results, knowing that the exact solution is $y_{ex} = e^{-x}\\cos(x)$.\nUsing MATLAB, the problem is easily solved with the QSS subroutines:\nqssstab.m (draws the region of absolute stability) qssmulti.m (solves with a generic LMM)\nWith Python there are many libraries: SciPy, odeint that uses RK, ode, and allows the selection of BDF methods, but generally with adaptive time step.\nNODEPY is potentially an excellent library but buggy.\nIt\u0026rsquo;s excellent for drawing the stability region (see hmm.py on Canvas)\nI have written a simple LMM solver with fixed-point iterations for implicit methods.\nUsing my-hmm.py you can verify that our method is 2nd order:\nmax error $3 \\cdot 10^{-4}$ $8 \\cdot 10^{-5}$ $2 \\cdot 10^{-5}$ $\\Delta t$ 0.05 0.025 0.0125 [Graph showing exact vs numerical solution]\n[Stability region diagram showing a circle in the complex plane] Red = Region of Stability\n"},{"id":7,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/Exercise/","title":"ä¹ é¢˜","section":"ç¬¬ä¹ç« ","content":" Problem 1: Finite-Difference Solution # We wish to discretize and solve the boundary-value problem\n$$-\\mu\\frac{d^2u}{dx^2} + \\beta\\frac{du}{dx} = f(x),\\quad x\\in(0,1),\\quad u(0)=u(1)=0$$\nOr equivalently:\n$$\\begin{cases} -\\mu u\u0026rsquo;\u0026rsquo;(x) + \\beta u\u0026rsquo;(x) = f(x), \u0026amp; 0\u0026lt;x\u0026lt;1,\\ u(0)=0,; u(1)=0. \\end{cases}$$\nWhere $\\mu\u0026gt;0$ and $\\beta$ is a constant (possibly negative), and $f\\in C^0(0,1)$.\n1. Finite-Difference Discretization # Divide $[0,1]$ into $N$ equal subintervals so that $\\Delta x = \\frac{1}{N}$. Let\n$$x_j = j\\Delta x,\\quad j=0,1,2,\\dots,N,$$\nso that $x_0=0$ and $x_N=1$. We approximate $u(x_j)\\approx u_j$. The boundary conditions become $u_0=0$ and $u_N=0$.\n1.1 Central Difference Scheme (for the interior points) # A standard centered second-difference for $u\u0026rsquo;\u0026rsquo;(x)$ at $x_j$ is:\n$$u\u0026rsquo;\u0026rsquo;(x_j) \\approx \\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2}.$$\nA standard centered first-difference for $u\u0026rsquo;(x)$ at $x_j$ is:\n$$u\u0026rsquo;(x_j) \\approx \\frac{u_{j+1} - u_{j-1}}{2\\Delta x}.$$\nHence, the PDE $-\\mu u\u0026rsquo;\u0026rsquo;(x) + \\beta u\u0026rsquo;(x)=f(x)$ becomes for $j=1,\\dots,N-1$:\n$$-\\mu\\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1} - u_{j-1}}{2\\Delta x} = f(x_j).$$\nWe impose $u_0 = 0$ and $u_N=0$.\n2. Local Truncation Error # The second-order central difference for $u\u0026rsquo;\u0026rsquo;$ is $O((\\Delta x)^2)$ accurate. The central difference for $u\u0026rsquo;$ is also $O((\\Delta x)^2)$ accurate. Hence the local truncation error of the combined scheme is $O((\\Delta x)^2)$.\n3. Matrix Form # Collect unknowns $u_1,u_2,\\dots,u_{N-1}$ into a vector $\\mathbf{u}=(u_1,\\dots,u_{N-1})^T$. The boundary values $u_0=0$ and $u_N=0$ are known.\nRewrite the finite-difference equation for an interior index $j$:\n$$-\\mu\\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1} - u_{j-1}}{2\\Delta x} = f(x_j).$$\nWe can factor out coefficients:\nLet $\\alpha \\equiv \\frac{\\mu}{(\\Delta x)^2}$. Let $\\gamma \\equiv \\frac{\\beta}{2\\Delta x}$. Then the coefficient of $u_j$ is $2\\alpha$, the coefficient of $u_{j+1}$ is $-\\alpha + \\gamma$, and the coefficient of $u_{j-1}$ is $-\\alpha - \\gamma$. Thus, in matrix form:\n$$\\begin{pmatrix} 2\\alpha \u0026amp; -\\alpha+\\gamma \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\ -\\alpha-\\gamma \u0026amp; 2\\alpha \u0026amp; -\\alpha+\\gamma \u0026amp; \\cdots \u0026amp; 0\\ 0 \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\vdots\\ \\vdots \u0026amp; \u0026amp; -\\alpha-\\gamma \u0026amp; 2\\alpha \u0026amp; -\\alpha+\\gamma\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; -\\alpha-\\gamma \u0026amp; 2\\alpha \\end{pmatrix} \\begin{pmatrix} u_1\\ u_2\\ \\vdots \\ u_{N-2}\\ u_{N-1} \\end{pmatrix} # \\begin{pmatrix} f(x_1)\\ f(x_2)\\ \\vdots \\ f(x_{N-2})\\ f(x_{N-1}) \\end{pmatrix}.$$\nThis is a tridiagonal linear system, solvable by standard methods (e.g., Thomas algorithm).\n4. Quality of the Solution vs. $\\beta/\\mu$ # The ratio $\\frac{\\beta}{\\mu}$ often plays the role of a PÃ©clet-type number in advection-diffusion problems.\nIf $\\frac{\\beta}{\\mu}$ is small (diffusion-dominated), the solution is usually smooth and well-behaved under central differencing. If $\\frac{\\beta}{\\mu}$ is large (advection-dominated), pure central differences may produce spurious oscillations unless $\\Delta x$ is refined or upwinding techniques are used to stabilize the discrete solution. 5. Upwind Method (First Order) for $\\beta\u0026lt;0$ # When $\\beta\u0026lt;0$, the \u0026ldquo;flow\u0026rdquo; is from right to left, so an upwind difference for the first derivative $\\beta u\u0026rsquo;(x)$ uses values on the \u0026ldquo;right\u0026rdquo; side at each $j$. Concretely, for $\\beta\u0026lt;0$, we replace:\n$$u\u0026rsquo;(x_j) \\approx \\frac{u_{j+1} - u_j}{\\Delta x} \\quad \\text{(a \u0026ldquo;backward\u0026rdquo; upwind if flow is leftward)}.$$\nHence, the difference equation becomes:\n$$-\\mu\\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1} - u_j}{\\Delta x} = f(x_j), \\quad j=1,\\dots,N-1.$$\n(This replaces the central difference in the advective term by a one-sided upwind difference.)\n5.1 No Spurious Oscillations # The first-order upwind scheme for linear advection-diffusion is known to be monotone for any $\\Delta x\u0026gt;0$ when $\\beta\u0026lt;0$ (or, more generally, for any sign of $\\beta$ if we choose the correct upwind direction). Monotonicity prevents nonphysical oscillations. In short:\nCentral difference can oscillate if $|\\beta|$ is large relative to $\\mu$. Upwind difference sacrifices some accuracy (only first order in $\\Delta x$ for the advective term) but remains stable and nonoscillatory for any step size $\\Delta x$. Thus, with $\\beta\u0026lt;0$, the upwind approach $u\u0026rsquo;(x_j)\\approx (u_{j+1}-u_j)/\\Delta x$ ensures a stable, physically plausible solution without oscillations.\n6. Summary # Equation \u0026amp; Discretization\n$$-\\mu u\u0026rsquo;\u0026rsquo; + \\beta u\u0026rsquo; = f(x), \\quad u(0)=u(1)=0 \\longrightarrow \\begin{cases} -\\mu\\frac{u_{j+1}-2u_j+u_{j-1}}{(\\Delta x)^2} +\\beta\\frac{u_{j+1}-u_{j-1}}{2\\Delta x} = f(x_j),\\ u_0=0,;u_N=0. \\end{cases}$$\nLocal Truncation Error is $O((\\Delta x)^2)$ for the centered scheme.\nMatrix Form: A standard tridiagonal system with bands $-\\alpha\\mp \\gamma$, $2\\alpha$, $-\\alpha\\pm \\gamma$.\nEffect of $\\beta/\\mu$: If $|\\beta|$ is large relative to $\\mu$, central differences can produce oscillatory solutions; upwind methods help.\nUpwind Method (for $\\beta\u0026lt;0$): $$-\\mu\\frac{u_{j+1}-2u_j+u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1}-u_j}{\\Delta x} = f(x_j)$$ prevents oscillations for any $\\Delta x$.\nProblem 2 # Solution Outline\nWe have the linear advection equation\n$$\\frac{\\partial u}{\\partial t} ;+; a,\\frac{\\partial u}{\\partial x} ;=; 0,\\quad x\\in \\mathbb{R},;t\u0026gt;0,$$ with initial condition $u(x,0) = u_0(x)$. We wish to:\nDerive the Laxâ€“Wendroff scheme for this PDE. Determine the CFL condition for stability, i.e.\\ find the condition on $\\displaystyle \\frac{|a|\\Delta t}{\\Delta x}$. 1) Derivation of the Laxâ€“Wendroff Scheme # A succinct way to derive Laxâ€“Wendroff is via a second-order Taylor expansion in time about $t^n$:\n$$u^{n+1}i ;\\approx; u(x_i,,t_n + \\Delta t) ;=; u(x_i,t_n) ;+;\\Delta t,\\frac{\\partial u}{\\partial t} ;+;\\tfrac{(\\Delta t)^2}{2},\\frac{\\partial^2 u}{\\partial t^2};\\bigg|{(x_i,t_n)}.$$\nFrom the PDE $\\partial_t u = -,a,\\partial_x u$, we can replace time-derivatives by spatial derivatives:\nFirst derivative in time: $$\\frac{\\partial u}{\\partial t} ;=; -,a,\\frac{\\partial u}{\\partial x}.$$\nSecond derivative in time: $$\\frac{\\partial^2 u}{\\partial t^2} ;=; \\frac{\\partial}{\\partial t}\\Bigl(-,a,\\frac{\\partial u}{\\partial x}\\Bigr) ;=; -,a,\\frac{\\partial}{\\partial x}\\Bigl(\\frac{\\partial u}{\\partial t}\\Bigr) ;=; -,a,\\frac{\\partial}{\\partial x}\\Bigl(-,a,\\frac{\\partial u}{\\partial x}\\Bigr) ;=; a^2,\\frac{\\partial^2 u}{\\partial x^2}.$$\nHence,\n$$u^{n+1}_i ;\\approx; u^{n}i ;-; a,\\Delta t ,\\frac{\\partial u}{\\partial x} ;+; \\frac{a^2(\\Delta t)^2}{2},\\frac{\\partial^2 u}{\\partial x^2} ;\\Bigg|{(x_i,t_n)}.$$\nDiscretizing the spatial derivatives # We replace the first and second spatial derivatives by standard centered finite differences:\n$$\\frac{\\partial u}{\\partial x}\\bigg|{x_i} \\approx \\frac{u{i+1}^n - u_{i-1}^n}{2,\\Delta x}, \\qquad \\frac{\\partial^2 u}{\\partial x^2}\\bigg|{x_i} \\approx \\frac{u{i+1}^n - 2,u_i^n + u_{i-1}^n}{(\\Delta x)^2}.$$\nPutting this all together:\n$$u_{i}^{n+1} ;=; u_{i}^{n} ;-; a,\\Delta t ,\\frac{u_{i+1}^{n} - u_{i-1}^{n}}{2,\\Delta x} ;+; \\frac{a^2,(\\Delta t)^2}{2}, \\frac{u_{i+1}^{n} - 2,u_{i}^{n} + u_{i-1}^{n}}{(\\Delta x)^2}.$$\nIt is common to set $\\displaystyle \\nu ;=;\\frac{a,\\Delta t}{\\Delta x}$. Then the scheme reads\n$$\\boxed{ u_{i}^{n+1} ;=; u_{i}^{n} ;-;\\frac{\\nu}{2},\\bigl(u_{i+1}^{n} - u_{i-1}^{n}\\bigr) ;+;\\frac{\\nu^{2}}{2}, \\bigl(u_{i+1}^{n} ;-;2,u_{i}^{n} ;+;u_{i-1}^{n}\\bigr). }$$\nThis is the Laxâ€“Wendroff scheme for the linear advection equation.\n2) The CFL Stability Condition # A standard von Neumann (Fourier) stability analysis, or the usual Laxâ€“Richtmyer theory for hyperbolic PDEs, shows that Laxâ€“Wendroff is stable if and only if the Courant number satisfies\n$$\\bigl|,\\nu,\\bigr| ;=; \\biggl|\\frac{a,\\Delta t}{\\Delta x}\\biggr| ;\\le; 1.$$\nTherefore among the multiple-choice options, the correct condition is\n$$\\boxed{;; \\bigl|\\tfrac{a,\\Delta t}{\\Delta x}\\bigr| ;\\le; 1.}$$\n3) Motivation # Domain of dependence argument. For the PDE $\\partial_t u + a,\\partial_x u = 0,$ characteristics travel with speed $a$. Numerically, we must ensure that information from these characteristics is captured on the grid from one time step to the next; that is the essence of the CFL condition. If $|a|\\Delta t \u0026gt; \\Delta x$, the method â€œjumps overâ€ grid cells and fails to remain stable.\nVon Neumann analysis. Substituting $u_i^n = \\lambda^n e^{ikx_i}$ into the scheme shows that the amplification factor $|\\lambda|$ is $\\le 1$ if and only if $\\bigl|\\frac{a\\Delta t}{\\Delta x}\\bigr|\\le 1.$\nHence the short answer is:\nLaxâ€“Wendroff has the form $$u_{i}^{n+1} # u_{i}^{n} # \\frac{\\nu}{2},(u_{i+1}^{n} - u_{i-1}^{n}) + \\frac{\\nu^{2}}{2},(u_{i+1}^{n}-2u_{i}^{n}+u_{i-1}^{n}),$$ The method is stable if and only if $\\displaystyle \\bigl|\\frac{a\\Delta t}{\\Delta x}\\bigr|\\le 1.$ This condition is exactly the usual CFL requirement ensuring the numerical domain of dependence covers the PDEâ€™s domain of dependence. Problem 3 # Below is a fairly detailed derivation and discussion of the Crankâ€“Nicolson (CN) method (the $\\theta$-method with $\\theta = \\tfrac12$) for the heat equation\n$$\\frac{\\partial u}{\\partial t};-;\\frac{\\partial^2 u}{\\partial x^2};=;f, \\quad x\\in(0,1),;t\u0026gt;0, \\quad u(0,t) ;=;u(1,t);=;0, \\quad u(x,0);=;u_0(x).$$\nSince the PDE can be written as $$u_t ;=; u_{xx} + f,$$ we will discretize in both space and time.\n1. The Crankâ€“Nicolson Discretization # Let $\\Delta x = \\frac{1}{M}$ partition $[0,1]$ into $M+1$ grid points $x_i = i,\\Delta x$, $i=0,\\dots,M$, and let $\\Delta t$ be a time step, so $t^n = n,\\Delta t$. We write $u_i^n\\approx u(x_i,t^n)$. The standard secondâ€order central difference for $u_{xx}$ is\n$$u_{xx}(x_i,t^n) ;\\approx; \\frac{u_{i+1}^n - 2,u_i^n + u_{i-1}^n}{(\\Delta x)^2}.$$\nA $\\theta$â€method (also called the $\\theta$-scheme) for $u_t = u_{xx} + f$ in time is:\n$$\\frac{u_i^{n+1} - u_i^n}{\\Delta t} ;=; \\theta\\Bigl[\\underbrace{\\tfrac{u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}}{(\\Delta x)^2} ;+; f_i^{n+1}}{\\text{â€œimplicitâ€ part}}\\Bigr] ;+; \\bigl(1-\\theta\\bigr)\\Bigl[\\underbrace{\\tfrac{u{i+1}^{n} - 2u_i^{n} + u_{i-1}^{n}}{(\\Delta x)^2};+; f_i^{n}}_{\\text{â€œexplicitâ€ part}}\\Bigr].$$\nCrankâ€“Nicolson is the special case $\\theta = \\tfrac12$. Substituting $\\theta=\\tfrac12$, we get\n$$\\frac{u_i^{n+1} - u_i^n}{\\Delta t} ;=; \\tfrac12\\Bigl[\\tfrac{u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}}{(\\Delta x)^2} + f_i^{n+1}\\Bigr] ;+; \\tfrac12\\Bigl[\\tfrac{u_{i+1}^{n} - 2u_i^{n} + u_{i-1}^{n}}{(\\Delta x)^2} + f_i^n\\Bigr].$$\nRearranging terms gives $$u_i^{n+1} ;-; \\frac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}\\bigr) ;=; u_i^{n} ;+; \\frac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n} - 2u_i^n + u_{i-1}^{n}\\bigr) ;+; \\frac{\\Delta t}{2},\\bigl(f_i^{n+1} + f_i^n\\bigr).$$ One may think of this as the linear system $$\\bigl[I + \\tfrac{\\Delta t}{2,(\\Delta x)^2},A\\bigr],\\mathbf{u}^{n+1} ;=; \\bigl[I - \\tfrac{\\Delta t}{2,(\\Delta x)^2},A\\bigr],\\mathbf{u}^{n} ;+;\\frac{\\Delta t}{2},\\bigl(\\mathbf{f}^{n+1} + \\mathbf{f}^n\\bigr),$$ where $A$ is the usual tridiagonal matrix corresponding to the secondâ€difference operator (and where $\\mathbf{u}^n$ is the vector of $u_i^n$). After applying boundary conditions $u_0^n = u_M^n = 0$, one solves this tridiagonal system at each time step.\nBoundary Conditions # Because $u(0,t)=u(1,t)=0$, we set $u_0^n=0$ and $u_M^n=0$ for all $n$. The updates are applied only for $i=1,\\dots,M-1$.\nSummary of the CN Update # In â€œindex form,â€ the Crankâ€“Nicolson scheme is: $$\\boxed{ \\begin{aligned} \u0026amp;\\text{For }i=1,\\dots,M-1:\\quad u_i^{n+1} ;-; \\tfrac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}\\bigr) \\ \u0026amp;\\qquad\\quad;=; u_i^{n} ;+; \\tfrac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n} - 2u_i^n + u_{i-1}^{n}\\bigr) ;+;\\frac{\\Delta t}{2},\\bigl(f_i^{n+1} + f_i^n\\bigr). \\end{aligned} }$$ This is solved simultaneously for all $i$, respecting $u_0^{n+1}=u_M^{n+1}=0$.\n2. Properties: Stability \u0026amp; Accuracy # Stability:\nUnconditional stability for the heat equation. In other words, there is no restriction on $\\Delta t$ relative to $\\Delta x$ needed solely for stability (unlike the explicit forwardâ€Euler method, which requires $\\Delta t \\le \\tfrac12 (\\Delta x)^2$). More precisely, CN is Aâ€stable as an ODE solver applied to the linear diffusion operator. One can show by a von Neumann analysis or standard Laxâ€“Richtmyer theory that errors do not grow unboundedly for any $\\Delta t\u0026gt;0$. Accuracy:\nIn time, Crankâ€“Nicolson is secondâ€order accurate, because it is essentially the trapezoidal rule in time (it uses $\\frac12$ of the â€œnewâ€ timeâ€levelâ€™s spatial derivative plus $\\frac12$ of the â€œoldâ€ timeâ€levelâ€™s spatial derivative). In space, if we use the standard secondâ€difference approximation, the scheme is also secondâ€order in $\\Delta x$. Overall, we often say â€œCN is secondâ€order in both space and time (for sufficiently smooth solutions).â€ 3. BONUS: Discontinuous Initial Condition # Even if $u_0(x)$ is not continuous, the heat equation itself is smoothing: for $t\u0026gt;0$, the exact solution becomes infinitely differentiable in $x$. Numerically:\nThe scheme remains stable and convergent. Because it is a diffusionâ€type PDE, any jump discontinuity in the initial data gets smoothed out instantly as $t$ increases. Crankâ€“Nicolson will faithfully capture that smoothing. You may see large gradients at early time steps near the discontinuity, but the method will not become unstable. Hence having a discontinuous initial condition does not cause instability for the heat equation with Crankâ€“Nicolson. The scheme still converges (secondâ€order in both space and time) to the unique smooth solution that the parabolic PDE defines for $t\u0026gt;0$.\nProblem 4 æ±‚è§£äºŒç»´æ‹‰æ™®æ‹‰æ–¯æ–¹ç¨‹çš„äº”ç‚¹å·®åˆ†æ ¼å¼ # æˆ‘ä»¬è€ƒè™‘å¦‚ä¸‹çš„æ¤­åœ†æ–¹ç¨‹ï¼ˆPoisson å‹ï¼‰ï¼š $$-,\\Delta u ;=; f, \\quad (x,y)\\in [0,1] \\times [0,1].$$\nå…¶ä¸­ $$\\Delta ;=;\\frac{\\partial^2}{\\partial x^2} ;+;\\frac{\\partial^2}{\\partial y^2}$$ æ˜¯äºŒç»´æ‹‰æ™®æ‹‰æ–¯ç®—å­ï¼Œ$f$ æ˜¯å·²çŸ¥çš„å‡½æ•°ã€‚\n1. å»ºç«‹ç½‘æ ¼ # ä»¤ $N_x$ å’Œ $N_y$ åˆ†åˆ«è¡¨ç¤ºåœ¨ $x$ å’Œ $y$ æ–¹å‘ä¸Šçš„ç½‘æ ¼åˆ’åˆ†æ•°ç›®ï¼ˆä»…æŒ‡å†…éƒ¨èŠ‚ç‚¹æ•°ï¼Œä¸å«è¾¹ç•Œï¼‰ï¼Œåˆ™æ­¥é•¿ä¸º $$\\delta x ;=;\\frac{1}{N_x+1}, \\quad \\delta y ;=;\\frac{1}{N_y+1}.$$ æˆ‘ä»¬åœ¨åŒºé—´ $[0,1]\\times[0,1]$ å†…å–ç¦»æ•£ç½‘æ ¼ç‚¹ $$x_i ;=; i,\\delta x, \\quad y_j ;=; j,\\delta y,$$ å…¶ä¸­ $i=0,1,2,\\dots,N_x+1$, $j=0,1,2,\\dots,N_y+1$ã€‚\nåœ¨å†…éƒ¨èŠ‚ç‚¹ $(x_i,y_j)$ ä¸Šï¼Œæˆ‘ä»¬ç”¨ $u_{i,j}$ è¡¨ç¤ºå¯¹çœŸè§£ $u(x_i,y_j)$ çš„æ•°å€¼è¿‘ä¼¼ã€‚\n2. äº”ç‚¹å·®åˆ†æ ¼å¼ # æ–¹ç¨‹ $-\\Delta u = f$ å¯å†™æˆ $$-\\left( \\frac{\\partial^2 u}{\\partial x^2} ;+; \\frac{\\partial^2 u}{\\partial y^2} \\right) ;=; f,$$ å³ $$\\frac{\\partial^2 u}{\\partial x^2} ;+; \\frac{\\partial^2 u}{\\partial y^2} ;=; -,f.$$\nåœ¨ç½‘æ ¼ä¸Šï¼ŒäºŒé˜¶å¯¼æ•°çš„ä¸­å¿ƒå·®åˆ†è¿‘ä¼¼åˆ†åˆ«ä¸ºï¼š\nåœ¨ $x$ æ–¹å‘ï¼š $$\\frac{\\partial^2 u}{\\partial x^2}\\bigg|{(x_i,y_j)} ;\\approx; \\frac{u{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2},$$ åœ¨ $y$ æ–¹å‘ï¼š $$\\frac{\\partial^2 u}{\\partial y^2}\\bigg|{(x_i,y_j)} ;\\approx; \\frac{u{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2}.$$ å› æ­¤ï¼Œ$\\Delta u$ åœ¨ç¦»æ•£åŒ–åå¯å†™ä¸º $$\\Delta u_{i,j} ;=; \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2}.$$\nç”±äºæ–¹ç¨‹æ˜¯ $-\\Delta u = f$ï¼Œåˆ™å¯¹åº”çš„äº”ç‚¹å·®åˆ†æ ¼å¼ä¸º\n$$-\\left[, \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2} \\right] ;=; f_{i,j},$$ æˆ–ç­‰ä»·åœ°å†™æˆ $$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2} ;=; -,f_{i,j}.$$\nè¿™é‡Œ $f_{i,j} = f(x_i,y_j)$ è¡¨ç¤ºåœ¨ç½‘æ ¼ç‚¹å¤„çš„å‡½æ•°å–å€¼ã€‚\n3. ç²¾åº¦é˜¶æ¬¡ # ä¸Šè¿°ä¸­å¿ƒå·®åˆ†æ ¼å¼å¯¹äºŒé˜¶å¯¼æ•°åœ¨ç©ºé—´æ­¥é•¿ä¸Šå…·æœ‰ äºŒé˜¶ç²¾åº¦ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬å°† $\\delta x$ å’Œ $\\delta y$ åŒæ­¥ç¼©å°ï¼ˆå‡è®¾ç½‘æ ¼ç­‰è·ï¼‰ï¼Œåˆ™ç¦»æ•£è§£ç›¸å¯¹äºçœŸè§£çš„è¯¯å·®åœ¨ $\\delta x, \\delta y \\to 0$ æ—¶æ»¡è¶³ $$\\mathcal{O}\\bigl((\\delta x)^2 + (\\delta y)^2\\bigr).$$\nç®€è€Œè¨€ä¹‹ï¼Œå¯¹æ‹‰æ™®æ‹‰æ–¯æ–¹ç¨‹é‡‡ç”¨è¿™ç§äº”ç‚¹ä¸­å¿ƒå·®åˆ†æ ¼å¼ï¼Œåœ¨å‡åŒ€ç½‘æ ¼ä¸‹æ˜¯äºŒé˜¶ç²¾åº¦ã€‚\n4. æœ€ç»ˆå½¢æˆçš„çº¿æ€§æ–¹ç¨‹ç»„ # å¯¹æ‰€æœ‰å†…éƒ¨èŠ‚ç‚¹ $(i,j)$ï¼ˆå³ $1\\le i\\le N_x$, $1\\le j\\le N_y$ï¼‰åº”ç”¨ä¸Šè¿°ç¦»æ•£æ–¹ç¨‹ï¼Œæˆ‘ä»¬ä¾¿å¾—åˆ°ä¸€ä¸ªå…³äºæ‰€æœ‰æœªçŸ¥é‡ ${u_{i,j}}$ çš„çº¿æ€§æ–¹ç¨‹ç»„ã€‚è‹¥å†ç»“åˆè¾¹ç•Œæ¡ä»¶ï¼ˆä¾‹å¦‚å·²çŸ¥è¾¹ç•Œä¸Šçš„ $u_{0,j},u_{N_x+1,j},u_{i,0},u_{i,N_y+1}$ï¼‰ï¼Œå³å¯å®Œå…¨æ±‚è§£ã€‚\nåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥ç”¨å„ç§è¿­ä»£æ³•ï¼ˆå¦‚ Jacobiã€Gauss-Seidelã€SOR ç­‰ï¼‰æˆ–ç›´æ¥æ³•ï¼ˆå¦‚ LU åˆ†è§£ç­‰ï¼‰æ¥æ±‚è§£è¿™ä¸ªç¦»æ•£æ–¹ç¨‹ç»„ã€‚\n5. æ€»ç»“ # äº”ç‚¹å·®åˆ†æ ¼å¼çš„ç¦»æ•£æ–¹ç¨‹ï¼ˆåœ¨äºŒç»´æƒ…å†µä¸‹ï¼‰æ˜¯ï¼š $$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2} ;=; -,f_{i,j}.$$ åœ¨æ ‡å‡†çš„ï¼ˆå‡åŒ€ï¼‰ç½‘æ ¼ä¸‹ï¼Œè¯¥æ–¹æ³•çš„ç©ºé—´ç¦»æ•£ç²¾åº¦æ˜¯äºŒé˜¶ã€‚ "},{"id":8,"href":"/posts/creating-a-new-theme/","title":"Creating a New Theme","section":"Blog","content":" Introduction # This tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\nWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\nIn this tutorial, commands that you enter will start with the \u0026ldquo;$\u0026rdquo; prompt. The output will follow. Lines that start with \u0026ldquo;#\u0026rdquo; are comments that I\u0026rsquo;ve added to explain a point. When I show updates to a file, the \u0026ldquo;:wq\u0026rdquo; on the last line means to save the file.\nHere\u0026rsquo;s an example:\n## this is a comment $ echo this is a command this is a command ## edit the file $ vi foo.md +++ date = \"2014-09-28\" title = \"creating a new theme\" +++ bah and humbug :wq ## show it $ cat foo.md +++ date = \"2014-09-28\" title = \"creating a new theme\" +++ bah and humbug $ Some Definitions # There are a few concepts that you need to understand before creating a theme.\nSkins # Skins are the files responsible for the look and feel of your site. Itâ€™s the CSS that controls colors and fonts, itâ€™s the Javascript that determines actions and reactions. Itâ€™s also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors.\nYou have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you donâ€™t have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin.\nYour second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. Itâ€™s extra work, though, so why bother with it?\nThe difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ canâ€™t be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it.\nThe rest of this tutorial will call a skin created in the themes/ directory a theme.\nNote that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you wonâ€™t need to update the siteâ€™s configuration file to use a theme.\nThe Home Page # The home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html.\nSite Configuration File # When Hugo runs, it looks for a configuration file that contains settings that override default values for the entire site. The file can use TOML, YAML, or JSON. I prefer to use TOML for my configuration files. If you prefer to use JSON or YAML, youâ€™ll need to translate my examples. Youâ€™ll also need to change the name of the file since Hugo uses the extension to determine how to process it.\nHugo translates Markdown files into HTML. By default, Hugo expects to find Markdown files in your content/ directory and template files in your themes/ directory. It will create HTML files in your public/ directory. You can change this by specifying alternate locations in the configuration file.\nContent # Content is stored in text files that contain two sections. The first section is the â€œfront matter,â€ which is the meta-information on the content. The second section contains Markdown that will be converted to HTML.\nFront Matter # The front matter is information about the content. Like the configuration file, it can be written in TOML, YAML, or JSON. Unlike the configuration file, Hugo doesnâ€™t use the fileâ€™s extension to know the format. It looks for markers to signal the type. TOML is surrounded by â€œ+++â€, YAML by â€œ---â€, and JSON is enclosed in curly braces. I prefer to use TOML, so youâ€™ll need to translate my examples if you prefer YAML or JSON.\nThe information in the front matter is passed into the template before the content is rendered into HTML.\nMarkdown # Content is written in Markdown which makes it easier to create the content. Hugo runs the content through a Markdown engine to create the HTML which will be written to the output file.\nTemplate Files # Hugo uses template files to render content into HTML. Template files are a bridge between the content and presentation. Rules in the template define what content is published, where it\u0026rsquo;s published to, and how it will rendered to the HTML file. The template guides the presentation by specifying the style to use.\nThere are three types of templates: single, list, and partial. Each type takes a bit of content as input and transforms it based on the commands in the template.\nHugo uses its knowledge of the content to find the template file used to render the content. If it canâ€™t find a template that is an exact match for the content, it will shift up a level and search from there. It will continue to do so until it finds a matching template or runs out of templates to try. If it canâ€™t find a template, it will use the default template for the site.\nPlease note that you can use the front matter to influence Hugoâ€™s choice of templates.\nSingle Template # A single template is used to render a single piece of content. For example, an article or post would be a single piece of content and use a single template.\nList Template # A list template renders a group of related content. That could be a summary of recent postings or all articles in a category. List templates can contain multiple groups.\nThe homepage template is a special type of list template. Hugo assumes that the home page of your site will act as the portal for the rest of the content in the site.\nPartial Template # A partial template is a template that can be included in other templates. Partial templates must be called using the â€œpartialâ€ template command. They are very handy for rolling up common behavior. For example, your site may have a banner that all pages use. Instead of copying the text of the banner into every single and list template, you could create a partial with the banner in it. That way if you decide to change the banner, you only have to change the partial template.\nCreate a New Site # Let\u0026rsquo;s use Hugo to create a new web site. I\u0026rsquo;m a Mac user, so I\u0026rsquo;ll create mine in my home directory, in the Sites folder. If you\u0026rsquo;re using Linux, you might have to create the folder first.\nThe \u0026ldquo;new site\u0026rdquo; command will create a skeleton of a site. It will give you the basic directory structure and a useable configuration file.\n$ hugo new site ~/Sites/zafta $ cd ~/Sites/zafta $ ls -l total 8 drwxr-xr-x 7 quoha staff 238 Sep 29 16:49 . drwxr-xr-x 3 quoha staff 102 Sep 29 16:49 .. drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static $ Take a look in the content/ directory to confirm that it is empty.\nThe other directories (archetypes/, layouts/, and static/) are used when customizing a theme. That\u0026rsquo;s a topic for a different tutorial, so please ignore them for now.\nGenerate the HTML For the New Site # Running the hugo command with no options will read all the available content and generate the HTML files. It will also copy all static files (that\u0026rsquo;s everything that\u0026rsquo;s not content). Since we have an empty site, it won\u0026rsquo;t do much, but it will do it very quickly.\n$ hugo --verbose INFO: 2014/09/29 Using config file: config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ The \u0026ldquo;--verbose\u0026rdquo; flag gives extra information that will be helpful when we build the template. Every line of the output that starts with \u0026ldquo;INFO:\u0026rdquo; or \u0026ldquo;WARN:\u0026rdquo; is present because we used that flag. The lines that start with \u0026ldquo;WARN:\u0026rdquo; are warning messages. We\u0026rsquo;ll go over them later.\nWe can verify that the command worked by looking at the directory again.\n$ ls -l total 8 drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static $ See that new public/ directory? Hugo placed all generated content there. When you\u0026rsquo;re ready to publish your web site, that\u0026rsquo;s the place to start. For now, though, let\u0026rsquo;s just confirm that we have what we\u0026rsquo;d expect from a site with no content.\n$ ls -l public total 16 -rw-r--r-- 1 quoha staff 416 Sep 29 17:02 index.xml -rw-r--r-- 1 quoha staff 262 Sep 29 17:02 sitemap.xml $ Hugo created two XML files, which is standard, but there are no HTML files.\nTest the New Site # Verify that you can run the built-in web server. It will dramatically shorten your development cycle if you do. Start it by running the \u0026ldquo;server\u0026rdquo; command. If it is successful, you will see output similar to the following:\n$ hugo server --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms Serving pages from /Users/quoha/Sites/zafta/public Web Server is available at http://localhost:1313 Press Ctrl+C to stop Connect to the listed URL (it\u0026rsquo;s on the line that starts with \u0026ldquo;Web Server\u0026rdquo;). If everything is working correctly, you should get a page that shows the following:\nindex.xml sitemap.xml That\u0026rsquo;s a listing of your public/ directory. Hugo didn\u0026rsquo;t create a home page because our site has no content. When there\u0026rsquo;s no index.html file in a directory, the server lists the files in the directory, which is what you should see in your browser.\nLetâ€™s go back and look at those warnings again.\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] That second warning is easier to explain. We havenâ€™t created a template to be used to generate â€œpage not found errors.â€ The 404 message is a topic for a separate tutorial.\nNow for the first warning. It is for the home page. You can tell because the first layout that it looked for was â€œindex.html.â€ Thatâ€™s only used by the home page.\nI like that the verbose flag causes Hugo to list the files that it\u0026rsquo;s searching for. For the home page, they are index.html, _default/list.html, and _default/single.html. There are some rules that we\u0026rsquo;ll cover later that explain the names and paths. For now, just remember that Hugo couldn\u0026rsquo;t find a template for the home page and it told you so.\nAt this point, you\u0026rsquo;ve got a working installation and site that we can build upon. All thatâ€™s left is to add some content and a theme to display it.\nCreate a New Theme # Hugo doesn\u0026rsquo;t ship with a default theme. There are a few available (I counted a dozen when I first installed Hugo) and Hugo comes with a command to create new themes.\nWe\u0026rsquo;re going to create a new theme called \u0026ldquo;zafta.\u0026rdquo; Since the goal of this tutorial is to show you how to fill out the files to pull in your content, the theme will not contain any CSS. In other words, ugly but functional.\nAll themes have opinions on content and layout. For example, Zafta uses \u0026ldquo;post\u0026rdquo; over \u0026ldquo;blog\u0026rdquo;. Strong opinions make for simpler templates but differing opinions make it tougher to use themes. When you build a theme, consider using the terms that other themes do.\nCreate a Skeleton # Use the hugo \u0026ldquo;new\u0026rdquo; command to create the skeleton of a theme. This creates the directory structure and places empty files for you to fill out.\n$ hugo new theme zafta $ ls -l total 8 drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static drwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes $ find themes -type f | xargs ls -l -rw-r--r-- 1 quoha staff 1081 Sep 29 17:31 themes/zafta/LICENSE.md -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/archetypes/default.md -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html -rw-r--r-- 1 quoha staff 93 Sep 29 17:31 themes/zafta/theme.toml $ The skeleton includes templates (the files ending in .html), license file, a description of your theme (the theme.toml file), and an empty archetype.\nPlease take a minute to fill out the theme.toml and LICENSE.md files. They\u0026rsquo;re optional, but if you\u0026rsquo;re going to be distributing your theme, it tells the world who to praise (or blame). It\u0026rsquo;s also nice to declare the license so that people will know how they can use the theme.\n$ vi themes/zafta/theme.toml author = \"michael d henderson\" description = \"a minimal working template\" license = \"MIT\" name = \"zafta\" source_repo = \"\" tags = [\"tags\", \"categories\"] :wq ## also edit themes/zafta/LICENSE.md and change ## the bit that says \"YOUR_NAME_HERE\" Note that the the skeleton\u0026rsquo;s template files are empty. Don\u0026rsquo;t worry, we\u0026rsquo;ll be changing that shortly.\n$ find themes/zafta -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html $ Update the Configuration File to Use the Theme # Now that we\u0026rsquo;ve got a theme to work with, it\u0026rsquo;s a good idea to add the theme name to the configuration file. This is optional, because you can always add \u0026ldquo;-t zafta\u0026rdquo; on all your commands. I like to put it the configuration file because I like shorter command lines. If you don\u0026rsquo;t put it in the configuration file or specify it on the command line, you won\u0026rsquo;t use the template that you\u0026rsquo;re expecting to.\nEdit the file to add the theme, add a title for the site, and specify that all of our content will use the TOML format.\n$ vi config.toml theme = \"zafta\" baseurl = \"\" languageCode = \"en-us\" title = \"zafta - totally refreshing\" MetaDataFormat = \"toml\" :wq $ Generate the Site # Now that we have an empty theme, let\u0026rsquo;s generate the site again.\n$ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ Did you notice that the output is different? The warning message for the home page has disappeared and we have an additional information line saying that Hugo is syncing from the theme\u0026rsquo;s directory.\nLet\u0026rsquo;s check the public/ directory to see what Hugo\u0026rsquo;s created.\n$ ls -l public total 16 drwxr-xr-x 2 quoha staff 68 Sep 29 17:56 css -rw-r--r-- 1 quoha staff 0 Sep 29 17:56 index.html -rw-r--r-- 1 quoha staff 407 Sep 29 17:56 index.xml drwxr-xr-x 2 quoha staff 68 Sep 29 17:56 js -rw-r--r-- 1 quoha staff 243 Sep 29 17:56 sitemap.xml $ Notice four things:\nHugo created a home page. This is the file public/index.html. Hugo created a css/ directory. Hugo created a js/ directory. Hugo claimed that it created 0 pages. It created a file and copied over static files, but didn\u0026rsquo;t create any pages. That\u0026rsquo;s because it considers a \u0026ldquo;page\u0026rdquo; to be a file created directly from a content file. It doesn\u0026rsquo;t count things like the index.html files that it creates automatically. The Home Page # Hugo supports many different types of templates. The home page is special because it gets its own type of template and its own template file. The file, layouts/index.html, is used to generate the HTML for the home page. The Hugo documentation says that this is the only required template, but that depends. Hugo\u0026rsquo;s warning message shows that it looks for three different templates:\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] If it can\u0026rsquo;t find any of these, it completely skips creating the home page. We noticed that when we built the site without having a theme installed.\nWhen Hugo created our theme, it created an empty home page template. Now, when we build the site, Hugo finds the template and uses it to generate the HTML for the home page. Since the template file is empty, the HTML file is empty, too. If the template had any rules in it, then Hugo would have used them to generate the home page.\n$ find . -name index.html | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 20:21 ./public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 ./themes/zafta/layouts/index.html $ The Magic of Static # Hugo does two things when generating the site. It uses templates to transform content into HTML and it copies static files into the site. Unlike content, static files are not transformed. They are copied exactly as they are.\nHugo assumes that your site will use both CSS and JavaScript, so it creates directories in your theme to hold them. Remember opinions? Well, Hugo\u0026rsquo;s opinion is that you\u0026rsquo;ll store your CSS in a directory named css/ and your JavaScript in a directory named js/. If you don\u0026rsquo;t like that, you can change the directory names in your theme directory or even delete them completely. Hugo\u0026rsquo;s nice enough to offer its opinion, then behave nicely if you disagree.\n$ find themes/zafta -type d | xargs ls -ld drwxr-xr-x 7 quoha staff 238 Sep 29 17:38 themes/zafta drwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes/zafta/archetypes drwxr-xr-x 5 quoha staff 170 Sep 29 17:31 themes/zafta/layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/_default drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/partials drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/static drwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/css drwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/js $ The Theme Development Cycle # When you\u0026rsquo;re working on a theme, you will make changes in the theme\u0026rsquo;s directory, rebuild the site, and check your changes in the browser. Hugo makes this very easy:\nPurge the public/ directory. Run the built in web server in watch mode. Open your site in a browser. Update the theme. Glance at your browser window to see changes. Return to step 4. Iâ€™ll throw in one more opinion: never work on a theme on a live site. Always work on a copy of your site. Make changes to your theme, test them, then copy them up to your site. For added safety, use a tool like Git to keep a revision history of your content and your theme. Believe me when I say that it is too easy to lose both your mind and your changes.\nCheck the main Hugo site for information on using Git with Hugo.\nPurge the public/ Directory # When generating the site, Hugo will create new files and update existing ones in the public/ directory. It will not delete files that are no longer used. For example, files that were created in the wrong directory or with the wrong title will remain. If you leave them, you might get confused by them later. I recommend cleaning out your site prior to generating it.\nNote: If you\u0026rsquo;re building on an SSD, you should ignore this. Churning on a SSD can be costly.\nHugo\u0026rsquo;s Watch Option # Hugo\u0026rsquo;s \u0026ldquo;--watch\u0026rdquo; option will monitor the content/ and your theme directories for changes and rebuild the site automatically.\nLive Reload # Hugo\u0026rsquo;s built in web server supports live reload. As pages are saved on the server, the browser is told to refresh the page. Usually, this happens faster than you can say, \u0026ldquo;Wow, that\u0026rsquo;s totally amazing.\u0026rdquo;\nDevelopment Commands # Use the following commands as the basis for your workflow.\n## purge old files. hugo will recreate the public directory. ## $ rm -rf public ## ## run hugo in watch mode ## $ hugo server --watch --verbose Here\u0026rsquo;s sample output showing Hugo detecting a change to the template for the home page. Once generated, the web browser automatically reloaded the page. I\u0026rsquo;ve said this before, it\u0026rsquo;s amazing.\n$ rm -rf public $ hugo server --watch --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms Watching for changes in /Users/quoha/Sites/zafta/content Serving pages from /Users/quoha/Sites/zafta/public Web Server is available at http://localhost:1313 Press Ctrl+C to stop INFO: 2014/09/29 File System Event: [\"/Users/quoha/Sites/zafta/themes/zafta/layouts/index.html\": MODIFY|ATTRIB] Change detected, rebuilding site WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 1 ms Update the Home Page Template # The home page is one of a few special pages that Hugo creates automatically. As mentioned earlier, it looks for one of three files in the theme\u0026rsquo;s layout/ directory:\nindex.html _default/list.html _default/single.html We could update one of the default templates, but a good design decision is to update the most specific template available. That\u0026rsquo;s not a hard and fast rule (in fact, we\u0026rsquo;ll break it a few times in this tutorial), but it is a good generalization.\nMake a Static Home Page # Right now, that page is empty because we don\u0026rsquo;t have any content and we don\u0026rsquo;t have any logic in the template. Let\u0026rsquo;s change that by adding some text to the template.\n$ vi themes/zafta/layouts/index.html \u003c!DOCTYPE html\u003e hugo says hello!\n:wq $ Build the web site and then verify the results.\n$ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 78 Sep 29 21:26 public/index.html $ cat public/index.html \u003c!DOCTYPE html\u003e hugo says hello!\nLive Reload # Note: If you\u0026rsquo;re running the server with the --watch option, you\u0026rsquo;ll see different content in the file:\n$ cat public/index.html \u003c!DOCTYPE html\u003e hugo says hello!\nWhen you use --watch, the Live Reload script is added by Hugo. Look for live reload in the documentation to see what it does and how to disable it.\nBuild a \u0026ldquo;Dynamic\u0026rdquo; Home Page # \u0026ldquo;Dynamic home page?\u0026rdquo; Hugo\u0026rsquo;s a static web site generator, so this seems an odd thing to say. I mean let\u0026rsquo;s have the home page automatically reflect the content in the site every time Hugo builds it. We\u0026rsquo;ll use iteration in the template to do that.\nCreate New Posts # Now that we have the home page generating static content, let\u0026rsquo;s add some content to the site. We\u0026rsquo;ll display these posts as a list on the home page and on their own page, too.\nHugo has a command to generate a skeleton post, just like it does for sites and themes.\n$ hugo --verbose new post/first.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/first.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/default.md ERROR: 2014/09/29 Unable to Cast to map[string]interface{} $ That wasn\u0026rsquo;t very nice, was it?\nThe \u0026ldquo;new\u0026rdquo; command uses an archetype to create the post file. Hugo created an empty default archetype file, but that causes an error when there\u0026rsquo;s a theme. For me, the workaround was to create an archetypes file specifically for the post type.\n$ vi themes/zafta/archetypes/post.md +++ Description = \"\" Tags = [] Categories = [] +++ :wq $ find themes/zafta/archetypes -type f | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 21:53 themes/zafta/archetypes/default.md -rw-r--r-- 1 quoha staff 51 Sep 29 21:54 themes/zafta/archetypes/post.md $ hugo --verbose new post/first.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/first.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md INFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/first.md /Users/quoha/Sites/zafta/content/post/first.md created $ hugo --verbose new post/second.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/second.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md INFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/second.md /Users/quoha/Sites/zafta/content/post/second.md created $ ls -l content/post total 16 -rw-r--r-- 1 quoha staff 104 Sep 29 21:54 first.md -rw-r--r-- 1 quoha staff 105 Sep 29 21:57 second.md $ cat content/post/first.md +++ Categories = [] Description = \"\" Tags = [] date = \"2014-09-29T21:54:53-05:00\" title = \"first\" +++ my first post $ cat content/post/second.md +++ Categories = [] Description = \"\" Tags = [] date = \"2014-09-29T21:57:09-05:00\" title = \"second\" +++ my second post $ Build the web site and then verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\"category\":\"categories\", \"tag\":\"tags\"} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ The output says that it created 2 pages. Those are our new posts:\n$ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 78 Sep 29 22:13 public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/second/index.html $ The new files are empty because because the templates used to generate the content are empty. The homepage doesn\u0026rsquo;t show the new content, either. We have to update the templates to add the posts.\nList and Single Templates # In Hugo, we have three major kinds of templates. There\u0026rsquo;s the home page template that we updated previously. It is used only by the home page. We also have \u0026ldquo;single\u0026rdquo; templates which are used to generate output for a single content file. We also have \u0026ldquo;list\u0026rdquo; templates that are used to group multiple pieces of content before generating output.\nGenerally speaking, list templates are named \u0026ldquo;list.html\u0026rdquo; and single templates are named \u0026ldquo;single.html.\u0026rdquo;\nThere are three other types of templates: partials, content views, and terms. We will not go into much detail on these.\nAdd Content to the Homepage # The home page will contain a list of posts. Let\u0026rsquo;s update its template to add the posts that we just created. The logic in the template will run every time we build the site.\n$ vi themes/zafta/layouts/index.html \u003c!DOCTYPE html\u003e {{ range first 10 .Data.Pages }} {{ .Title }} {{ end }} :wq $ Hugo uses the Go template engine. That engine scans the template files for commands which are enclosed between \u0026ldquo;{{\u0026rdquo; and \u0026ldquo;}}\u0026rdquo;. In our template, the commands are:\nrange .Title end The \u0026ldquo;range\u0026rdquo; command is an iterator. We\u0026rsquo;re going to use it to go through the first ten pages. Every HTML file that Hugo creates is treated as a page, so looping through the list of pages will look at every file that will be created.\nThe \u0026ldquo;.Title\u0026rdquo; command prints the value of the \u0026ldquo;title\u0026rdquo; variable. Hugo pulls it from the front matter in the Markdown file.\nThe \u0026ldquo;end\u0026rdquo; command signals the end of the range iterator. The engine loops back to the top of the iteration when it finds \u0026ldquo;end.\u0026rdquo; Everything between the \u0026ldquo;range\u0026rdquo; and \u0026ldquo;end\u0026rdquo; is evaluated every time the engine goes through the iteration. In this file, that would cause the title from the first ten pages to be output as heading level one.\nIt\u0026rsquo;s helpful to remember that some variables, like .Data, are created before any output files. Hugo loads every content file into the variable and then gives the template a chance to process before creating the HTML files.\nBuild the web site and then verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:23 public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/second/index.html $ cat public/index.html \u003c!DOCTYPE html\u003e second first $ Congratulations, the home page shows the title of the two posts. The posts themselves are still empty, but let\u0026rsquo;s take a moment to appreciate what we\u0026rsquo;ve done. Your template now generates output dynamically. Believe it or not, by inserting the range command inside of those curly braces, you\u0026rsquo;ve learned everything you need to know to build a theme. All that\u0026rsquo;s really left is understanding which template will be used to generate each content file and becoming familiar with the commands for the template engine.\nAnd, if that were entirely true, this tutorial would be much shorter. There are a few things to know that will make creating a new template much easier. Don\u0026rsquo;t worry, though, that\u0026rsquo;s all to come.\nAdd Content to the Posts # We\u0026rsquo;re working with posts, which are in the content/post/ directory. That means that their section is \u0026ldquo;post\u0026rdquo; (and if we don\u0026rsquo;t do something weird, their type is also \u0026ldquo;post\u0026rdquo;).\nHugo uses the section and type to find the template file for every piece of content. Hugo will first look for a template file that matches the section or type name. If it can\u0026rsquo;t find one, then it will look in the _default/ directory. There are some twists that we\u0026rsquo;ll cover when we get to categories and tags, but for now we can assume that Hugo will try post/single.html, then _default/single.html.\nNow that we know the search rule, let\u0026rsquo;s see what we actually have available:\n$ find themes/zafta -name single.html | xargs ls -l -rw-r--r-- 1 quoha staff 132 Sep 29 17:31 themes/zafta/layouts/_default/single.html We could create a new template, post/single.html, or change the default. Since we don\u0026rsquo;t know of any other content types, let\u0026rsquo;s start with updating the default.\nRemember, any content that we haven\u0026rsquo;t created a template for will end up using this template. That can be good or bad. Bad because I know that we\u0026rsquo;re going to be adding different types of content and we\u0026rsquo;re going to end up undoing some of the changes we\u0026rsquo;ve made. It\u0026rsquo;s good because we\u0026rsquo;ll be able to see immediate results. It\u0026rsquo;s also good to start here because we can start to build the basic layout for the site. As we add more content types, we\u0026rsquo;ll refactor this file and move logic around. Hugo makes that fairly painless, so we\u0026rsquo;ll accept the cost and proceed.\nPlease see the Hugo documentation on template rendering for all the details on determining which template to use. And, as the docs mention, if you\u0026rsquo;re building a single page application (SPA) web site, you can delete all of the other templates and work with just the default single page. That\u0026rsquo;s a refreshing amount of joy right there.\nUpdate the Template File # $ vi themes/zafta/layouts/_default/single.html \u003c!DOCTYPE html\u003e {{ .Title }} {{ .Title }} {{ .Content }} :wq $ Build the web site and verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:40 public/index.html -rw-r--r-- 1 quoha staff 125 Sep 29 22:40 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:40 public/post/index.html -rw-r--r-- 1 quoha staff 128 Sep 29 22:40 public/post/second/index.html $ cat public/post/first/index.html \u003c!DOCTYPE html\u003e first first my first post\n$ cat public/post/second/index.html \u003c!DOCTYPE html\u003e second second my second post\n$ Notice that the posts now have content. You can go to localhost:1313/post/first to verify.\nLinking to Content # The posts are on the home page. Let\u0026rsquo;s add a link from there to the post. Since this is the home page, we\u0026rsquo;ll update its template.\n$ vi themes/zafta/layouts/index.html \u003c!DOCTYPE html\u003e {{ range first 10 .Data.Pages }} {{ .Title }} {{ end }} Build the web site and verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 149 Sep 29 22:44 public/index.html -rw-r--r-- 1 quoha staff 125 Sep 29 22:44 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:44 public/post/index.html -rw-r--r-- 1 quoha staff 128 Sep 29 22:44 public/post/second/index.html $ cat public/index.html \u003c!DOCTYPE html\u003e second first $ Create a Post Listing # We have the posts displaying on the home page and on their own page. We also have a file public/post/index.html that is empty. Let\u0026rsquo;s make it show a list of all posts (not just the first ten).\nWe need to decide which template to update. This will be a listing, so it should be a list template. Let\u0026rsquo;s take a quick look and see which list templates are available.\n$ find themes/zafta -name list.html | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html As with the single post, we have to decide to update _default/list.html or create post/list.html. We still don\u0026rsquo;t have multiple content types, so let\u0026rsquo;s stay consistent and update the default list template.\nCreating Top Level Pages # Let\u0026rsquo;s add an \u0026ldquo;about\u0026rdquo; page and display it at the top level (as opposed to a sub-level like we did with posts).\nThe default in Hugo is to use the directory structure of the content/ directory to guide the location of the generated html in the public/ directory. Let\u0026rsquo;s verify that by creating an \u0026ldquo;about\u0026rdquo; page at the top level:\n$ vi content/about.md +++ title = \"about\" description = \"about this site\" date = \"2014-09-27\" slug = \"about time\" +++ ## about us i'm speechless :wq Generate the web site and verify the results.\n$ find public -name '*.html' | xargs ls -l -rw-rw-r-- 1 mdhender staff 334 Sep 27 15:08 public/about-time/index.html -rw-rw-r-- 1 mdhender staff 527 Sep 27 15:08 public/index.html -rw-rw-r-- 1 mdhender staff 358 Sep 27 15:08 public/post/first-post/index.html -rw-rw-r-- 1 mdhender staff 0 Sep 27 15:08 public/post/index.html -rw-rw-r-- 1 mdhender staff 342 Sep 27 15:08 public/post/second-post/index.html Notice that the page wasn\u0026rsquo;t created at the top level. It was created in a sub-directory named \u0026lsquo;about-time/\u0026rsquo;. That name came from our slug. Hugo will use the slug to name the generated content. It\u0026rsquo;s a reasonable default, by the way, but we can learn a few things by fighting it for this file.\nOne other thing. Take a look at the home page.\n$ cat public/index.html \u003c!DOCTYPE html\u003e creating a new theme about second first Notice that the \u0026ldquo;about\u0026rdquo; link is listed with the posts? That\u0026rsquo;s not desirable, so let\u0026rsquo;s change that first.\n$ vi themes/zafta/layouts/index.html \u003c!DOCTYPE html\u003e posts {{ range first 10 .Data.Pages }} {{ if eq .Type \"post\"}} {{ .Title }} {{ end }} {{ end }} pages {{ range .Data.Pages }} {{ if eq .Type \"page\" }} {{ .Title }} {{ end }} {{ end }} :wq Generate the web site and verify the results. The home page has two sections, posts and pages, and each section has the right set of headings and links in it.\nBut, that about page still renders to about-time/index.html.\n$ find public -name '*.html' | xargs ls -l -rw-rw-r-- 1 mdhender staff 334 Sep 27 15:33 public/about-time/index.html -rw-rw-r-- 1 mdhender staff 645 Sep 27 15:33 public/index.html -rw-rw-r-- 1 mdhender staff 358 Sep 27 15:33 public/post/first-post/index.html -rw-rw-r-- 1 mdhender staff 0 Sep 27 15:33 public/post/index.html -rw-rw-r-- 1 mdhender staff 342 Sep 27 15:33 public/post/second-post/index.html Knowing that hugo is using the slug to generate the file name, the simplest solution is to change the slug. Let\u0026rsquo;s do it the hard way and change the permalink in the configuration file.\n$ vi config.toml [permalinks] page = \"/:title/\" about = \"/:filename/\" Generate the web site and verify that this didn\u0026rsquo;t work. Hugo lets \u0026ldquo;slug\u0026rdquo; or \u0026ldquo;URL\u0026rdquo; override the permalinks setting in the configuration file. Go ahead and comment out the slug in content/about.md, then generate the web site to get it to be created in the right place.\nSharing Templates # If you\u0026rsquo;ve been following along, you probably noticed that posts have titles in the browser and the home page doesn\u0026rsquo;t. That\u0026rsquo;s because we didn\u0026rsquo;t put the title in the home page\u0026rsquo;s template (layouts/index.html). That\u0026rsquo;s an easy thing to do, but let\u0026rsquo;s look at a different option.\nWe can put the common bits into a shared template that\u0026rsquo;s stored in the themes/zafta/layouts/partials/ directory.\nCreate the Header and Footer Partials # In Hugo, a partial is a sugar-coated template. Normally a template reference has a path specified. Partials are different. Hugo searches for them along a TODO defined search path. This makes it easier for end-users to override the theme\u0026rsquo;s presentation.\n$ vi themes/zafta/layouts/partials/header.html \u003c!DOCTYPE html\u003e {{ .Title }} :wq $ vi themes/zafta/layouts/partials/footer.html :wq Update the Home Page Template to Use the Partials # The most noticeable difference between a template call and a partials call is the lack of path:\n{{ template \"theme/partials/header.html\" . }} versus\n{{ partial \"header.html\" . }} Both pass in the context.\nLet\u0026rsquo;s change the home page template to use these new partials.\n$ vi themes/zafta/layouts/index.html {{ partial \"header.html\" . }} posts {{ range first 10 .Data.Pages }} {{ if eq .Type \"post\"}} {{ .Title }} {{ end }} {{ end }} pages {{ range .Data.Pages }} {{ if or (eq .Type \"page\") (eq .Type \"about\") }} {{ .Type }} - {{ .Title }} - {{ .RelPermalink }} {{ end }} {{ end }} {{ partial \"footer.html\" . }} :wq Generate the web site and verify the results. The title on the home page is now \u0026ldquo;your title here\u0026rdquo;, which comes from the \u0026ldquo;title\u0026rdquo; variable in the config.toml file.\nUpdate the Default Single Template to Use the Partials # $ vi themes/zafta/layouts/_default/single.html {{ partial \"header.html\" . }} {{ .Title }} {{ .Content }} {{ partial \"footer.html\" . }} :wq Generate the web site and verify the results. The title on the posts and the about page should both reflect the value in the markdown file.\nAdd â€œDate Publishedâ€ to Posts # It\u0026rsquo;s common to have posts display the date that they were written or published, so let\u0026rsquo;s add that. The front matter of our posts has a variable named \u0026ldquo;date.\u0026rdquo; It\u0026rsquo;s usually the date the content was created, but let\u0026rsquo;s pretend that\u0026rsquo;s the value we want to display.\nAdd â€œDate Publishedâ€ to the Template # We\u0026rsquo;ll start by updating the template used to render the posts. The template code will look like:\n{{ .Date.Format \"Mon, Jan 2, 2006\" }} Posts use the default single template, so we\u0026rsquo;ll change that file.\n$ vi themes/zafta/layouts/_default/single.html {{ partial \"header.html\" . }} {{ .Title }} {{ .Date.Format \"Mon, Jan 2, 2006\" }} {{ .Content }} {{ partial \"footer.html\" . }} :wq Generate the web site and verify the results. The posts now have the date displayed in them. There\u0026rsquo;s a problem, though. The \u0026ldquo;about\u0026rdquo; page also has the date displayed.\nAs usual, there are a couple of ways to make the date display only on posts. We could do an \u0026ldquo;if\u0026rdquo; statement like we did on the home page. Another way would be to create a separate template for posts.\nThe \u0026ldquo;if\u0026rdquo; solution works for sites that have just a couple of content types. It aligns with the principle of \u0026ldquo;code for today,\u0026rdquo; too.\nLet\u0026rsquo;s assume, though, that we\u0026rsquo;ve made our site so complex that we feel we have to create a new template type. In Hugo-speak, we\u0026rsquo;re going to create a section template.\nLet\u0026rsquo;s restore the default single template before we forget.\n$ mkdir themes/zafta/layouts/post $ vi themes/zafta/layouts/_default/single.html {{ partial \"header.html\" . }} {{ .Title }} {{ .Content }} {{ partial \"footer.html\" . }} :wq Now we\u0026rsquo;ll update the post\u0026rsquo;s version of the single template. If you remember Hugo\u0026rsquo;s rules, the template engine will use this version over the default.\n$ vi themes/zafta/layouts/post/single.html {{ partial \"header.html\" . }} {{ .Title }} {{ .Date.Format \"Mon, Jan 2, 2006\" }} {{ .Content }} {{ partial \"footer.html\" . }} :wq Note that we removed the date logic from the default template and put it in the post template. Generate the web site and verify the results. Posts have dates and the about page doesn\u0026rsquo;t.\nDon\u0026rsquo;t Repeat Yourself # DRY is a good design goal and Hugo does a great job supporting it. Part of the art of a good template is knowing when to add a new template and when to update an existing one. While you\u0026rsquo;re figuring that out, accept that you\u0026rsquo;ll be doing some refactoring. Hugo makes that easy and fast, so it\u0026rsquo;s okay to delay splitting up a template.\n"},{"id":9,"href":"/posts/migrate-from-jekyll/","title":"Migrating from Jekyll","section":"Blog","content":" Move static content to static # Jekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like\nâ–¾ \u0026lt;root\u0026gt;/ â–¾ images/ logo.png should become\nâ–¾ \u0026lt;root\u0026gt;/ â–¾ static/ â–¾ images/ logo.png Additionally, you\u0026rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.\nCreate your Hugo configuration file # Hugo can read your configuration as JSON, YAML or TOML. Hugo supports parameters custom configuration too. Refer to the Hugo configuration documentation for details.\nSet your configuration publish folder to _site # The default is for Jekyll to publish to _site and for Hugo to publish to public. If, like me, you have _site mapped to a git submodule on the gh-pages branch, you\u0026rsquo;ll want to do one of two alternatives:\nChange your submodule to point to map gh-pages to public instead of _site (recommended).\ngit submodule deinit _site git rm _site git submodule add -b gh-pages git@github.com:your-username/your-repo.git public Or, change the Hugo configuration to use _site instead of public.\n{ .. \u0026quot;publishdir\u0026quot;: \u0026quot;_site\u0026quot;, .. } Convert Jekyll templates to Hugo templates # That\u0026rsquo;s the bulk of the work right here. The documentation is your friend. You should refer to Jekyll\u0026rsquo;s template documentation if you need to refresh your memory on how you built your blog and Hugo\u0026rsquo;s template to learn Hugo\u0026rsquo;s way.\nAs a single reference data point, converting my templates for heyitsalex.net took me no more than a few hours.\nConvert Jekyll plugins to Hugo shortcodes # Jekyll has plugins; Hugo has shortcodes. It\u0026rsquo;s fairly trivial to do a port.\nImplementation # As an example, I was using a custom image_tag plugin to generate figures with caption when running Jekyll. As I read about shortcodes, I found Hugo had a nice built-in shortcode that does exactly the same thing.\nJekyll\u0026rsquo;s plugin:\nmodule Jekyll class ImageTag \u0026lt; Liquid::Tag @url = nil @caption = nil @class = nil @link = nil // Patterns IMAGE_URL_WITH_CLASS_AND_CAPTION = IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;(\\s+)-\u0026gt;((https?:\\/\\/|\\/)(\\S+))(\\s*)/i IMAGE_URL_WITH_CAPTION = /((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;/i IMAGE_URL_WITH_CLASS = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))/i IMAGE_URL = /((https?:\\/\\/|\\/)(\\S+))/i def initialize(tag_name, markup, tokens) super if markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK @class = $1 @url = $3 @caption = $7 @link = $9 elsif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION @class = $1 @url = $3 @caption = $7 elsif markup =~ IMAGE_URL_WITH_CAPTION @url = $1 @caption = $5 elsif markup =~ IMAGE_URL_WITH_CLASS @class = $1 @url = $3 elsif markup =~ IMAGE_URL @url = $1 end end def render(context) if @class source = \u0026quot;\u0026lt;figure class='#{@class}'\u0026gt;\u0026quot; else source = \u0026quot;\u0026lt;figure\u0026gt;\u0026quot; end if @link source += \u0026quot;\u0026lt;a href=\\\u0026quot;#{@link}\\\u0026quot;\u0026gt;\u0026quot; end source += \u0026quot;\u0026lt;img src=\\\u0026quot;#{@url}\\\u0026quot;\u0026gt;\u0026quot; if @link source += \u0026quot;\u0026lt;/a\u0026gt;\u0026quot; end source += \u0026quot;\u0026lt;figcaption\u0026gt;#{@caption}\u0026lt;/figcaption\u0026gt;\u0026quot; if @caption source += \u0026quot;\u0026lt;/figure\u0026gt;\u0026quot; source end end end Liquid::Template.register_tag('image', Jekyll::ImageTag) is written as this Hugo shortcode:\n\u0026lt;!-- image --\u0026gt; \u0026lt;figure {{ with .Get \u0026quot;class\u0026quot; }}class=\u0026quot;{{.}}\u0026quot;{{ end }}\u0026gt; {{ with .Get \u0026quot;link\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt;{{ end }} \u0026lt;img src=\u0026quot;{{ .Get \u0026quot;src\u0026quot; }}\u0026quot; {{ if or (.Get \u0026quot;alt\u0026quot;) (.Get \u0026quot;caption\u0026quot;) }}alt=\u0026quot;{{ with .Get \u0026quot;alt\u0026quot;}}{{.}}{{else}}{{ .Get \u0026quot;caption\u0026quot; }}{{ end }}\u0026quot;{{ end }} /\u0026gt; {{ if .Get \u0026quot;link\u0026quot;}}\u0026lt;/a\u0026gt;{{ end }} {{ if or (or (.Get \u0026quot;title\u0026quot;) (.Get \u0026quot;caption\u0026quot;)) (.Get \u0026quot;attr\u0026quot;)}} \u0026lt;figcaption\u0026gt;{{ if isset .Params \u0026quot;title\u0026quot; }} {{ .Get \u0026quot;title\u0026quot; }}{{ end }} {{ if or (.Get \u0026quot;caption\u0026quot;) (.Get \u0026quot;attr\u0026quot;)}}\u0026lt;p\u0026gt; {{ .Get \u0026quot;caption\u0026quot; }} {{ with .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt; {{ end }} {{ .Get \u0026quot;attr\u0026quot; }} {{ if .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/p\u0026gt; {{ end }} \u0026lt;/figcaption\u0026gt; {{ end }} \u0026lt;/figure\u0026gt; \u0026lt;!-- image --\u0026gt; Usage # I simply changed:\n{% image full http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg \u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were \u0026quot;having fun\u0026quot; and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; -\u0026gt;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/ %} to this (this example uses a slightly extended version named fig, different than the built-in figure):\n{{% fig class=\u0026quot;full\u0026quot; src=\u0026quot;http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg\u0026quot; title=\u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were having fun and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; link=\u0026quot;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/\u0026quot; %}} As a bonus, the shortcode named parameters are, arguably, more readable.\nFinishing touches # Fix content # Depending on the amount of customization that was done with each post with Jekyll, this step will require more or less effort. There are no hard and fast rules here except that hugo server --watch is your friend. Test your changes and fix errors as needed.\nClean up # You\u0026rsquo;ll want to remove the Jekyll configuration at this point. If you have anything else that isn\u0026rsquo;t used, delete it.\nA practical example in a diff # Hey, it\u0026rsquo;s Alex was migrated in less than a father-with-kids day from Jekyll to Hugo. You can see all the changes (and screw-ups) by looking at this diff.\n"},{"id":10,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.6-Lebesgue-Convergence-Theorem/","title":"8.6 å‹’è´æ ¼æ”¶æ•›å®šç†","section":"ç¬¬å…«ç«  åº¦é‡ç†è®º","content":" Main Question # When do we have: $$\\lim_{n\\to\\infty}\\int_A f_n(x)dx = \\int_A(\\lim_{n\\to\\infty}f_n(x))dx?$$\nLebesgue Monotone Convergence Theorem (Theorem 8.6.1) # Let $g_n: [0,1] \\to \\mathbb{R}$ be a sequence of non-negative measurable functions such that:\n$g_{n+1}(x) \\leq g_n(x)$ $\\forall x$ (decreasing sequence) $\\lim_{n\\to\\infty} g_n(x) = 0$ $\\forall x \\in [0,1]$ Then: $$\\lim_{n\\to\\infty}\\int_0^1 g_n(x)dx = \\int_0^1 0 dx = 0$$\nProof Details # For LMCT to hold, we only need:\n$f_n(x) \\leq f_{n+1}(x) \\leq f(x)$ $\\forall x$ $f_n(x) \\to f(x)$, $\\forall x$ This implies: $f_n(x) \\uparrow f(x)$\nThe assumption $A \\subset [0,1] \\subset \\mathbb{R}$ is not essential. Result is true for any set $A \\subset \\mathbb{R}^n$.\nThe Monotonicity Assumption Cannot Be Removed # Example # $g_n(x) = \\begin{cases} n, \u0026amp; 0 \u0026lt; x \u0026lt; \\frac{1}{n} \\ 0, \u0026amp; \\text{else} \\end{cases}$\nNote that:\n$g_n(x) \\to 0$ $\\forall x \\in [0,1]$ $\\int_0^1 g_n(x)dx = 1$ $\\forall n$ Therefore $\\lim_{n\\to\\infty}\\int_0^1 g_n(x)dx = 1 \\neq 0 = \\int_0^1 0 dx$\n2nd Part of LMCT # Lemma # Suppose $f: [0,1] \\to \\mathbb{R}$ is measurable with $|f| \\leq M$ and $\\int_0^1 f \\geq \\alpha \u0026gt; 0$. Then the set: $E = {x \\in [0,1]: f(x) \\geq \\frac{\\alpha}{2}}$ contains a finite union of disjoint open intervals of total length $\\geq \\frac{\\alpha}{4M}$\nResult # $0 \\leq \\int_0^1 f - L(f,P) \\leq \\frac{\\alpha}{4}$\nWhere $L$ denotes the total length of intervals $I \\in P$ with $I \\subset E$.\nThen: $$\\frac{3\\alpha}{4} \u0026lt; L(f, P) = \\sum_{I \\in P} \\left(\\inf_{I}(f(x))\\right)\\ell(I)$$\n$$= \\sum_{I \\subset E} + \\sum_{I \\not\\subset E} \\left(\\inf_{I}(f(x))\\right)\\ell(I)$$\n$$\\leq \\sum_{I \\subset E} M\\ell(I) + \\sum_{I} \\frac{\\alpha}{2}\\ell(I)$$\n$$\\leq \\ell M + \\frac{\\alpha}{2} \\cdot 1$$\nSince $\\ell M \u0026gt; \\frac{3\\alpha}{4} - \\frac{\\alpha}{2} = \\frac{\\alpha}{4}$, we can conclude:\n$$\\ell \u0026gt; \\frac{\\alpha}{4M}$$\nThis completes the proof of the lemma, demonstrating that the set $E$ contains intervals with sufficient total length, which is a key component in establishing the Lebesgue Monotone Convergence Theorem.\nThe boxed result at the end confirms that $\\ell \u0026gt; \\frac{\\alpha}{4M}$, which validates our earlier assertion about the minimum total length of the intervals in set $E$.\nProof of Theorem 8.6.1 (LMCT) # Step 1: Setup # Given:\n$0 \\leq g_{n+1} \\leq g_n$ implies $\\int_0^1 g_{n+1}(x) \\leq \\int_0^1 g_n(x)$ This leads to the limit: $\\lim_{n\\to\\infty}\\int_0^1 g_n(x) = \\lambda \\geq 0$ We need to show that $\\lambda = 0$.\nAssuming $\\lambda \u0026gt; 0$ will lead to a contradiction (using our assumption that $g_n(x) \\to 0$ $\\forall x \\in [0,1]$).\nStep 2: Application of Lemma # We apply the previously established lemma to the cut-off function:\n$$(g_n)_M = \\begin{cases} g_n(x), \u0026amp; g_n(x) \\leq M \\ M, \u0026amp; g_n(x) \u0026gt; M \\end{cases}$$\nThis implies:\n$$\\int_0^1 g_n(x) dx = \\lim_{M \\to \\infty} \\int_0^1 (g_n)M$$ é€‰æ‹© $m\u0026gt; \\frac{2\\lambda}{5}$ s.t. $$0 \\leq \\int^{1}{0}(g_{n}-(g_{n}){M})$\\leq \\int^{1}{0}(g_{1}-(g_{1}){M})\\leq \\frac{\\lambda}{5}$$ æˆ‘ä»¬è®©$E{n}=\\left{ x\\in[1,0]:g_{n}(x)\\geq \\frac{2\\lambda}{5} \\right}$ï¼Œç„¶å\n$E_{n+1}\\subset E_{n}$ ${x \\in [0,1] : (g_n)_M(x) \\geq \\frac{\\alpha}{2}} \\subset E_n$ where we choose $\\alpha$ such that $\\frac{2\\lambda}{5} = \\frac{\\alpha}{2}$, giving us $\\alpha = \\frac{4\\lambda}{5}$ Key Step: # Applying the lemma to $(g_n)_M$ with $\\alpha = \\frac{4\\lambda}{5}$: this implies $E_n$ contains a finite union of disjoint open intervals of total length: $$\\ell \\geq \\frac{\\alpha}{4M} = \\frac{\\lambda}{5M}$$\nStep 3: # Show that $\\bigcap_{n=1}^{\\infty} E_n = \\emptyset$\nLet $D = \\bigcap_{n=1}^{\\infty} {x \\in [0,1] : g_n \\text{ not converging to } 0} = \\bigcap_{n=1}^{\\infty} D_n$\nThen $g_{n}$ intagrable $\\Rightarrow m(D_n) = 0 = m(D) = 0$.\nThus $D$ is covered by $U$, a countable union of open intervals of total length $\u0026lt; \\frac{\\lambda}{5M}$.\nBy Step 2,\n$E_n \\subset U$\n"},{"id":11,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.6-Morse-Lemma/","title":"7.6 è«å°”æ–¯å¼•ç†","section":"ç¬¬ä¸ƒç«  é€†å‡½æ•°å’Œéšå‡½æ•°å®šç†","content":" Morse Theory: Local Behavior Near a Critical Point # Let $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$ be of class $C^2$ and $x_0$ is a critical point $\\rightarrow$ one can use $H(x_0)$ to classify critical points.\nMorse theory: makes this classification more precise.\nMorse Lemma # [!lemma|*] Let $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$ be of class $C^2$ with critical point $x_0 \\in A$. If $H(x_0)$ is non-degenerate, then there exists a neighborhood of $x_0$ and a diffeomorphism $g$ such that the function $f \\circ g$ has the form: $$f \\circ g(y) = f(x_0) + \\sum_{i=1}^{\\lambda} -y_i^2 + \\sum_{i=\\lambda+1}^n y_i^2$$ where $\\lambda$ is an integer called the index of $f$ at $x_0$.\nApplications # $\\lambda = 0$: $x_0$ is local minimum (paraboloid opens upward) $\\lambda = n$: $x_0$ is local maximum (paraboloid opens downward) $0 \u0026lt; \\lambda \u0026lt; n$: $x_0$ is saddle point (hyperboloid) Idea: Diagonalization of Hessian Matrix # $H(f) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} \u0026amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \u0026amp; \\cdots \\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} \u0026amp; \\frac{\\partial^2 f}{\\partial x_2^2} \u0026amp; \\cdots \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \\end{pmatrix}$\n$\\lambda$ = # of negative eigenvalues of $H(f)(x_0)$\nExample: Determine the shape of the surface $z = f(x,y)$ near $(0,0)$ # Solution: At $(0,0)$, $f_x = 0$, $f_y = 0$\nCompute eigenvalues of the Hessian matrix. If there is one negative eigenvalue, $\\lambda = 1$, it\u0026rsquo;s a saddle point (hyperboloid).\nConstrained Extremal Problem # Goal: To maximize or minimize a function $f(x): \\mathbb{R}^n \\to \\mathbb{R}$ under the condition $g(x) = c$.\nLagrange Multiplier Method # A Necessary Condition # Theorem: Let $f, g: U \\subset \\mathbb{R}^n \\to \\mathbb{R}$ be of class $C^1$. Assume $g(x_0) = c_0$ with $\\nabla g(x_0) \\neq 0$. If $f$ restricted to the surface $S = {g(x) = c_0}$ has a max or min at $x_0$, then there exists a real number $\\lambda$ such that:\n$$\\nabla f(x_0) = \\lambda \\nabla g(x_0)$$\nGeometric Meaning # $\\nabla f(x_0)$ points in the direction of steepest ascent At the extremum, this direction is perpendicular to the surface $S$ Geometric Proof # Let $c(t)$ be a fixed curve on $S = {g(x) = c_0}$ passing through $x_0$ at $t = t_0$. If $f$ restricted to $S$ has a max at $x_0$, then $f(c(t))$ has max at $t_0$.\nTherefore: $\\frac{d}{dt}f(c(t))|_{t=t_0} = 0$\nBy chain rule: $\\nabla f(c(t_0)) \\cdot c\u0026rsquo;(t_0) = 0$\nSince $c\u0026rsquo;(t_0)$ is tangent to $S$ at $x_0$, $\\nabla f(x_0)$ must be perpendicular to the tangent space of $S$ at $x_0$. Therefore, it must be parallel to $\\nabla g(x_0)$, which is normal to the surface.\nProcedure to Solve Extremal Problem # Step 1: Solve the system of equations $$\\nabla f(x) = \\lambda \\nabla g(x)$$ $$g(x) = c$$\nThis gives $n+1$ equations with $n+1$ variables $(x_1, x_2, \u0026hellip;, x_n, \\lambda)$\nStep 2: Compute values of $f$ at these critical points and determine which are maxima, minima, or saddle points\nExample: Find the extrema of # $$f(x,y) = x^2 y^2$$ subject to the condition $x^2 + y^2 = 1$\nSolution:\nApplying Lagrange multiplier method: $\\nabla f = (2xy^2, 2x^2y) = \\lambda \\nabla g = \\lambda(2x, 2y)$\nThis gives us:\n$xy^2 = \\lambda x$ $x^2y = \\lambda y$ Analyzing by cases:\nCase 1: If $x = 0$, then $y = \\pm 1$ (from constraint) Case 2: If $y = 0$, then $x = \\pm 1$ (from constraint) Case 3: If $x \\neq 0$ and $y \\neq 0$: From the first equation: $y^2 = \\lambda$ (dividing by $x$) From the second equation: $x^2 = \\lambda$ (dividing by $y$) This gives $x^2 = y^2$ With the constraint $x^2 + y^2 = 1$, we get $2x^2 = 1$, so $x = \\pm\\frac{1}{\\sqrt{2}}$ and $y = \\pm\\frac{1}{\\sqrt{2}}$ Critical points: $(0, \\pm 1), (\\pm 1, 0), (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}), (\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}), (-\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}), (-\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}})$\nEvaluating $f(x,y) = x^2y^2$ at these points:\n$f(0, \\pm 1) = 0$ $f(\\pm 1, 0) = 0$ $f(\\pm\\frac{1}{\\sqrt{2}}, \\pm\\frac{1}{\\sqrt{2}}) = (\\frac{1}{2})^2 = \\frac{1}{4}$ Therefore, the maximum value is $\\frac{1}{4}$ at $(\\pm\\frac{1}{\\sqrt{2}}, \\pm\\frac{1}{\\sqrt{2}})$, and the minimum value is $0$ at $(0, \\pm 1)$ and $(\\pm 1, 0)$.\nExtremal Problem with Multiple Constraints # Maximize/minimize $f(x)$ with constraints:\n$g_1(x) = c_1$ $g_2(x) = c_2$ \u0026hellip; $g_m(x) = c_m$ Procedure: Solve the system of equations # $$\\nabla f(x) = \\lambda_1 \\nabla g_1(x) + \\lambda_2 \\nabla g_2(x) + \u0026hellip; + \\lambda_m \\nabla g_m(x)$$ $$g_1(x) = c_1, g_2(x) = c_2, \u0026hellip;, g_m(x) = c_m$$\nWith $m+n$ equations and $m+n$ variables $(x_1,\u0026hellip;,x_n, \\lambda_1,\u0026hellip;,\\lambda_m)$\nAnalytical Proof of the Theorem (Lagrange Multiplier) # We want to substitute the condition $g(x) = c_0$ into the function $f(x)$ to eliminate the constraint.\nSince $\\nabla g(x_0) \\neq 0$, we may assume without loss of generality that $\\frac{\\partial g}{\\partial x_n} \\neq 0$ at $x_0$.\nBy the implicit function theorem, the equation $g(x_1, x_2, \u0026hellip;, x_n) = c_0$ can be solved for $x_n$ in a neighborhood of $x_0$: $$x_n = h(x_1, x_2, \u0026hellip;, x_{n-1})$$\nLet $k(x_1, x_2, \u0026hellip;, x_{n-1}) = f(x_1, x_2, \u0026hellip;, x_{n-1}, h(x_1, x_2, \u0026hellip;, x_{n-1}))$\nThus, an extremum of $f$ subject to the constraint corresponds to an extremum of $k$ without constraints.\nAt an extremum of $k$, we have: $$0 = \\frac{\\partial k}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} + \\frac{\\partial f}{\\partial x_n}\\frac{\\partial h}{\\partial x_i}, \\quad i = 1, 2, \u0026hellip;, n-1$$\nSince $g(x_1, x_2, \u0026hellip;, x_{n-1}, h(x_1, x_2, \u0026hellip;, x_{n-1})) = c_0$ identically, we can differentiate with respect to $x_i$:\n$$\\frac{\\partial g}{\\partial x_i} + \\frac{\\partial g}{\\partial x_n}\\frac{\\partial h}{\\partial x_i} = 0, \\quad i = 1, 2, \u0026hellip;, n-1$$\nSolving for $\\frac{\\partial h}{\\partial x_i}$:\n$$\\frac{\\partial h}{\\partial x_i} = -\\frac{\\frac{\\partial g}{\\partial x_i}}{\\frac{\\partial g}{\\partial x_n}}, \\quad i = 1, 2, \u0026hellip;, n-1$$\nSubstituting this into our extremum condition:\n$$\\frac{\\partial f}{\\partial x_i} - \\frac{\\partial f}{\\partial x_n}\\frac{\\frac{\\partial g}{\\partial x_i}}{\\frac{\\partial g}{\\partial x_n}} = 0, \\quad i = 1, 2, \u0026hellip;, n-1$$\nRearranging:\n$$\\frac{\\partial f}{\\partial x_i}\\frac{\\partial g}{\\partial x_n} - \\frac{\\partial f}{\\partial x_n}\\frac{\\partial g}{\\partial x_i} = 0, \\quad i = 1, 2, \u0026hellip;, n-1$$\nLet $\\lambda = \\frac{\\partial f}{\\partial x_n} / \\frac{\\partial g}{\\partial x_n}$, then:\n$$\\frac{\\partial f}{\\partial x_i} = \\lambda \\frac{\\partial g}{\\partial x_i}, \\quad i = 1, 2, \u0026hellip;, n-1$$\nAnd by definition of $\\lambda$, we also have $\\frac{\\partial f}{\\partial x_n} = \\lambda \\frac{\\partial g}{\\partial x_n}$\nTherefore, in vector form: $\\nabla f(x_0) = \\lambda \\nabla g(x_0)$\n"},{"id":12,"href":"/docs/Physics/Quantum-Mechenics/%E8%96%9B%E5%AE%9A%E8%B0%94%E6%96%B9%E7%A8%8B%E7%9A%84%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2/","title":"è–›å®šè°”æ–¹ç¨‹çš„å‚…ç«‹å¶å˜æ¢","section":"é‡å­åŠ›å­¦è®²ä¹‰","content":" 0. Review # $$ \\hat{H}=-\\frac{\\hbar^{2}}{2m}\\partial^{2}x+V(x) $$ Stationary States $$ \\Psi(x,t)=\\psi(x)e^{-iEt/\\hbar} $$ Eigenvalue equation: $$\\hat{H}\\psi(x) = E\\psi(x)$$ In newtonian mechanics, note $$F(x)=-\\frac{ \\partial V(x) }{ \\partial x } $$\n1. Infinite Finite Well (particle on a box) # $$E = \\frac{p^{2}}{2m}$$\nknowing $E$ results in knowing momentum $p^{2}$\nProblem Set-up Between $x=0$, and $x=a$, $$-\\frac{\\hbar^{2}}{2m} \\frac{d^{2}}{dx}\\Psi(x)=E \\Psi(x)$$ Subject to \u0026ldquo;boundary condition\u0026rdquo;: $\\Psi(0)=\\Psi(a)=0$ $$ \\begin{align} \\frac{d^{2}\\Psi}{dx} \u0026amp; =-\\left( \\frac{2m}{\\hbar^{2}} \\right) E \\Psi \\ \u0026amp; =-k^{2}\\Psi \\end{align} $$ Recall\nSolution:\n$\\Psi(x)=C_{1}e^{ikx}+C_{2}e^{-ikx}$ $\\Psi(x)=C_{1}\\cos(kx)+ C_{2}\\sin(kx)$ (picked this version) The solution is in the form of $$\\Psi(x)=A\\sin(kx)+ B\\cos(kx)$$ Impose the boundary condition:\n$\\Psi(x=0)=B\\cos(0)=0$ $$\\boxed{B=0}$$ $\\Psi(x=a)=A\\sin(ka)=0$ $$\\begin{align} \\sin(ka) \u0026amp; =0 \\ ka \u0026amp; =+\\boldsymbol{\\pi},+2\\boldsymbol{\\pi},+3\\boldsymbol{\\pi}\\dots \\ k_{n} \u0026amp; =\\frac{n\\boldsymbol{\\pi}}{a} \\quad {n\\in \\mathbb{N}} \\Rightarrow \\boxed{E_{n}=\\frac{\\hbar^{2}\\pi^{2}n^{2}}{2ma^{2}}} \\end{align}$$ Note: $n \\neq 0$, since the eq. would vanish entirely. $n\u0026gt;0$, for positive $k$.\n3. Normalization # $$\\Psi(x)=A\\sin(k_{n}x)=A\\sin\\left( \\frac{n\\pi x}{a} \\right)$$ We normalize $$ \\begin{align} \\int^a_{b}|\\Psi_{n}(x)|^{2}, dx \u0026amp; =1 \\ A^{2}\\int^a_{b}\\sin ^{2}\\left( \\frac{n\\pi x}{a} \\right), dx \u0026amp; =1 \\ A^{2}\\cdot \\frac{a}{2} \u0026amp; =1 \\Rightarrow A=\\sqrt{ \\frac{2}{a} } \\end{align} $$ Final Solution: $$\\Psi(x)=A\\sin(k_{n}x)=\\sqrt{ \\frac{2}{a} }\\sin\\left( \\frac{n\\pi x}{a} \\right) \\quad {n\\in \\mathbb{N}}$$ (A): Why $E_{1}\u0026gt;0$? $$\\begin{aligned} \\Delta x \u0026amp;\\sim a \\ \\Delta p \\cdot \\Delta x \u0026amp;\\sim \\hbar \\ \\Delta p \u0026amp;\\sim \\frac{\\hbar}{a} \\quad \\Longrightarrow \\quad KE\\sim \\frac{(\\Delta p)^2}{2 m} \\sim \\frac{\\hbar^2}{2 m a^2}\\end{aligned}$$ (B) States with higher energy have more nodes (C) States are orthonormal: $$\\int_0^a d x \\Psi_n^*(x) \\Psi_m(x)=\\delta_{n m}$$ (D) Completeness: $f(x)$ defined on the interval $[0,a]$, with $f(x=0) = f(x=a) = 0$.\nFourier Series Representation: $$ f(x) = \\sum_{n=1}^{\\infty} c_n \\psi_n(x) = \\sqrt{\\frac{2}{a}} \\sum_{n=1}^{\\infty} c_n \\sin\\left(\\frac{n\\pi x}{a}\\right) $$ $$ \\begin{align} \\int_0^a dx , \\Psi_m^(x) f(x) \u0026amp; = \\sum_{n=1}^{\\infty} C_n \\int_0^a dx , \\Psi_m^(x) \\Psi_n(x) \\ \u0026amp; =\\sum_{n=1}^{\\infty} C_n \\delta_{m,n} \\ \u0026amp; =C_m \\end{align} $$$$\n$$ $$ \\boxed{C_m = \\int_0^a dx , \\Psi_m^*(x) f(x)} $$\n$$ \\delta_{m,n} = \\begin{cases} 1, \u0026amp; \\text{if } m = n \\ 0, \u0026amp; \\text{if } m \\neq n \\end{cases} $$\n"},{"id":13,"href":"/docs/Mathematics/hidden/","title":"Hidden","section":"Mathematics","content":" This page is hidden in menu # Quondam non pater est dignior ille Eurotas # Latent te facies # Lorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.\nPater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor Cum honorum Latona # O fallor in sustinui iussorum equidem. Nymphae operi oris alii fronde parens dumque, in auro ait mox ingenti proxima iamdudum maius?\nreality(burnDocking(apache_nanometer), pad.property_data_programming.sectorBrowserPpga(dataMask, 37, recycleRup)); intellectualVaporwareUser += -5 * 4; traceroute_key_upnp /= lag_optical(android.smb(thyristorTftp)); surge_host_golden = mca_compact_device(dual_dpi_opengl, 33, commerce_add_ppc); if (lun_ipv) { verticalExtranet(1, thumbnail_ttl, 3); bar_graphics_jpeg(chipset - sector_xmp_beta); } Fronde cetera dextrae sequens pennis voce muneris # Acta cretus diem restet utque; move integer, oscula non inspirat, noctisque scelus! Nantemque in suas vobis quamvis, et labori!\nvar runtimeDiskCompiler = home - array_ad_software; if (internic \u0026gt; disk) { emoticonLockCron += 37 + bps - 4; wan_ansi_honeypot.cardGigaflops = artificialStorageCgi; simplex -= downloadAccess; } var volumeHardeningAndroid = pixel + tftp + onProcessorUnmount; sector(memory(firewire + interlaced, wired)); "},{"id":14,"href":"/docs/Mathematics/MATH-351-352/Heat-Equation/Heat-Equation-Solution/","title":"Heat Equation Solution","section":"çƒ­æ–¹ç¨‹","content":" Heat Equation Solution # Scale Invariance Property # [!Theorem] Since we know that $u_t=ku_{xx}$ , then we know if $u \\in U$ for $U \\subset \\mathbb{R}^n$ solves the equation, so does $u(\\lambda x, \\lambda^2 t)$ for $\\lambda \\in \\mathbb{R}$ according to the scale invariance property.\nLetâ€™s set that $\\bar{x}=\\lambda x, \\bar{t}=\\lambda^2 t$, then it is easy to see:\n$$ \\begin{align} u_{\\bar{t}}= \\frac{\\partial u}{\\partial \\bar{t}}\\cdot\\frac{\\partial \\bar{t}}{\\partial t} = \\frac{\\partial u}{\\partial t} \\cdot (\\lambda^2) \\ u_{\\bar{x}\\bar{x}} \u0026amp;=\\frac{\\partial}{\\partial x} \\cdot \\frac{\\partial u}{\\partial \\bar{x} } = (\\frac{\\partial}{\\partial \\bar{x}} \\cdot \\frac{\\partial \\bar{x}}{\\partial x})(\\frac{\\partial u}{\\partial \\bar{x} } \\cdot \\frac{\\partial \\bar{x}}{\\partial x})\\ \u0026amp;= \\frac{\\partial^2 u}{\\partial x^2} \\cdot (\\lambda^2) \\end{align} $$\nwhere the equation still holds regardless the choice of $\\lambda$ $(\\lambda \\neq 0)$\nThe scaling $\\frac{x^2}{t}$ or $\\frac{x}{\\sqrt{t}}$ that is invariant to the equation suggests the solution is in the form of $u(x,t)=v(\\frac{x^2}{t})$ f.s. function $v$. That is,\n$$ \\begin{align} u(x,t)=t^\\alpha v(\\frac{x}{t^\\beta}) \\end{align} $$\nwhere constants $\\alpha, \\beta$ and functions $v:\\mathbb{R}^n\\to \\mathbb{R}$ must be found. This means the solution must be invariant under the dilation scaling $\\forall \\lambda \u0026gt;0, x= \\mathbb{R}^n, t\u0026gt;0$ :\n$$ u(x,t) = \\lambda^\\alpha u(\\lambda^\\beta x,\\lambda t) $$\nSetting $\\lambda=t^{-1}$, in which $v(y):= u(y,1)$. We insert (1) into the original heat equation to solve for $v$ with our new variable $y=\\cfrac{x}{t^\\beta}$. We then take $\\beta = \\frac{1}{2}$ so that the terms involved $t$ are cancelled out - we hence derived an equation that is only in terms of $y$:\n$$ \\alpha v + \\frac{1}{2}\\cdot Dv + \\Delta v = 0 \\ \\ (k=1) $$\nDifferent textbook takes different methods to find the constant $\\alpha=-\\frac{1}{2}$ here:\nUsing conservation of heat energy in physics Guessing $v$ to be radial and introduce $v(y)=w(|y|)$ Eventually, we reached at $v(y)=Ae^{-\\frac{y^2}{4k}}$ such that\n$$ u(x,t)=A\\frac{1}{ t^{n/2}}e^{-\\frac{|x|^2}{4kt}} $$\nThe particular choice of normalizing constant $A=\\frac{1}{(4\\pi k)^{n/2}}$ is derived from $\\int_{\\mathbb{R}^n} \\Phi(x,t) dx=1$. (See p. 46 Lemma, Evans) Hence, the general solution to the heat equation for $n$ dimension is\n$$ \\Phi(x,t)=\\frac{1}{(4\\pi k t)^{n/2}} e^{-\\frac{|x|^2}{4kt}} $$\nwhere $x\\in \\mathbb{R}^n, t\u0026gt;0$. For situation $t\u0026lt;0$, the solution is $\\Phi=0$.\nFor dimension $n=1$, we yield $$ \\Phi(x,t)=\\frac{1}{\\sqrt{4\\pi k t}} e^{-\\frac{x^2}{4kt}} $$ $$ $$\n"},{"id":15,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E5%8D%81%E7%AB%A0/Finite-Element-Method/","title":"Finite Element Method","section":"æ•°å€¼æ–¹æ³•","content":" Order of Convergence in Finite Elements # The order of convergence of finite element:\nDenote $p$ as the number of points we use to do Lagrange interpolation Denote $s$ as the order of space we search for solution: $u \\in H^s(\\Omega)$ The relation is denoted as follows:\n$q$ \\ $s$ 1 2 3 1 1 1 1 2 1 2 2 3 1 2 3 optimal\nError Estimation # We have:\n$$||u_{ex} - u_h|| \\leq h^{\\min(s,p)}$$\nHow to determine which space $u_h$ is in:\nIf $u_h$, $u\u0026rsquo;_h$, $u\u0026rsquo;\u0026rsquo;_h$ $\\in L^2(\\Omega)$ but $u\u0026rsquo;\u0026rsquo;\u0026rsquo;_h \\not\\in L^2(\\Omega)$ We can say the function $u_h \\in H^2(\\Omega)$ Finite Element for 2D Poisson Equation # $$\\begin{cases} -\\Delta u = f \\quad \\text{in } \\Omega \\ u = 0 \\quad \\text{on } \\partial\\Omega \\end{cases}$$\nWeak Formulation # Let $v \\in H^1_0(\\Omega)$, then:\n$$a(u,v) = F(v) \\quad \\forall v \\in V$$\nwhere:\n$$a(u,v) = \\int_\\Omega \\nabla u \\cdot \\nabla v , dx \\quad \\text{and} \\quad F(v) = \\int_\\Omega f v , dx$$\nStep 2: Triangulation # Conforming triangulation of $\\Omega$ is a finite family: $T_h = {K}$\nRemind: $\\Omega = \\cup_{K \\in T_h} K$ and $K \\cap K\u0026rsquo; = \\emptyset$ if $K \\neq K'$\nShould have no overlap and share vertex and edge.\nNo hanging node. For each element: $$h_K = \\text{diameter}(K) = \\sup_{x,y \\in K} ||x-y||_2$$\nStep 3: Interpolation # For different polynomial orders:\n$p=1 \\Rightarrow$ use 3 nodes (corners) $p=2 \\Rightarrow$ use 6 nodes $p=3 \\Rightarrow$ use 10 nodes Then we have basis functions:\n$\\phi_{(2,0,0)} = 2\\lambda_1^2$ $\\phi_{(1,1,0)} = 2\\lambda_1\\lambda_2$ Step 4: Map Basis Function to Physical Triangle # Define $\\hat{K} = {(\\hat{x}_1, \\hat{x}_2) \\in \\mathbb{R}^2 | \\hat{x}_1, \\hat{x}_2 \\geq 0, \\hat{x}_1+\\hat{x}_2 \\leq 1}$\n$\\hat{\\lambda}_1 = \\hat{x}_1$, $\\hat{\\lambda}_2 = \\hat{x}_2$, $\\hat{\\lambda}_3 = 1-\\hat{x}_1-\\hat{x}_2$\nThen we have:\nDefine $v_1, v_2, v_3 \\in \\mathbb{R}^2$ that be triangle $K$\n$x = F_K(\\hat{x}) = A_K\\hat{x} + b_K$ for\n$A_K = (v_1-v_3, v_2-v_3) \\in \\mathbb{R}^{2\\times 2}$, $b_K = v_3$\nThen we have: $\\phi^K_\\alpha(x) = A_K^{-T} \\hat{\\phi}_\\alpha(F_K^{-1}(x))$\nAnd $\\nabla x = A_K^T \\nabla_{\\hat{x}}$\n$F_K$ can be viewed as:\n$(\\lambda_1, \\lambda_2, \\lambda_3) = (\\hat{x}_1, \\hat{x}_2, 1-\\hat{x}_1-\\hat{x}_2)$ define a triangle in canonical coordinate.\nLet $v_1 = (x_1, y_1)$, $v_2 = (x_2, y_2)$, $v_3 = (x_3, y_3)$\n$\\lambda_1, \\lambda_2, \\lambda_3$ is the solution of:\n$x = \\lambda_1 v_1 + \\lambda_2 v_2 + \\lambda_3 v_3$, $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$\nThen $\\lambda_1 = \\frac{(x_2-x_3)(y-y_3)-(y_2-y_3)(x-x_3)}{det}$\n$\\lambda_2 = $ same idea\n$\\lambda_3 = 1-\\lambda_2-\\lambda_1$\nBasis Function Properties # $\\phi_{i,j,k}(\\lambda_1, \\lambda_2, \\lambda_3) = \\frac{p!}{i!j!k!} \\lambda_1^i \\lambda_2^j \\lambda_3^k$\n$\\phi_{i,j,k}^K$ (at node) = 1 and $\\phi_{i,j,k}^K$ (other node) = 0\n$\\phi_{i,j,k}^K$ (other node) = 0\nThe after the map we have:\n$F_K(\\hat{x}) = A_K\\hat{x} + b_K = x$ \u0026hellip; change of variable\nThen $F(v) = \\sum_{a=1}^{node} a_i \\phi_i^K(x) \\phi_j^K(x) \\cdot u$\nFor which:\n$$a(\\phi_i^K(x), \\phi_j^K(x)) = \\int \\nabla \\phi_j^K \\nabla \\phi_i^K , dx_1dx_2$$\n$$= \\int A^{-T} \\nabla \\phi_i \\cdot A^{-T} \\nabla \\phi_j \\cdot |det(A)| , d\\hat{x}_1d\\hat{x}_2$$\nConvection-Diffusion in 1D Finite Element # $$\\frac{\\mu}{2} \\frac{\\partial^2 u}{\\partial x^2} + v \\frac{\\partial u}{\\partial x} = f(x,t) \\quad \\text{for } u(0) = u(L) = 0$$\nChoose test function and make weak formulation: $H_0^1(0,L)$\n$$\\int_0^L (\\frac{\\mu}{2} u_{xx} + v u_x) w , dx = \\int_0^L f w , dx$$\n$$\\frac{\\mu}{2} \\int_0^L u_{xx} \\cdot w , dx + v \\int_0^L u_x \\cdot w , dx = \\int_0^L f w , dx$$\nFinite element approximation:\n$$u(x) \\approx u_h(x) = \\sum_{j=1}^{N-1} U_j \\phi_j(x)$$\nWe can plug into weak form:\n$$\\sum_j \\frac{\\mu}{2} \\int_0^L \\phi_j\u0026rsquo; \\phi_i\u0026rsquo; , dx , U_j + \\sum_j v \\int_0^L \\phi_j\u0026rsquo; \\phi_i , dx , U_j = \\int_0^L \\phi_i f , dx$$\nExploring the Integral # Suppose $\\phi_i\u0026rsquo;(x) = \\frac{1}{h}$, $\\phi_{i+1}\u0026rsquo;(x) = \\frac{1}{h}$, $\\phi_{i-1}\u0026rsquo;(x) = -\\frac{1}{h}$\nFor $i=j$: $$\\int_{x_{i-1}}^{x_i} \\phi_i\u0026rsquo;^2 = \\frac{1}{h^2} \\text{ on } [x_{i-1}, x_i]$$\nFor $i=j+1$: $$\\int_{x_{i-1}}^{x_i} \\phi_i\u0026rsquo; \\phi_j\u0026rsquo; = -\\frac{1}{h^2} \\text{ on } [x_{i-1}, x_i]$$\nFor range $[x_i, x_{i+1}]$: $$\\int_{x_i}^{x_{i+1}} \\phi_i\u0026rsquo;^2 = \\frac{1}{h^2} \\text{ for } i=j$$ $$\\int_{x_i}^{x_{i+1}} \\phi_i\u0026rsquo; \\phi_j\u0026rsquo; = -\\frac{1}{h^2} \\text{ for } i=j+1$$\nCombine together, we get:\n$$\\int_{x_{i-1}}^{x_{i+1}} \\phi_i\u0026rsquo; \\phi_j\u0026rsquo; = \\frac{2}{h^2} [-1, 2, -1]$$\nSame idea for $\\int \\phi_i\u0026rsquo; \\phi_j$ but don\u0026rsquo;t have $v$ because only have one $\\phi_j\u0026rsquo;$.\nThen the equation for FEM:\n$$-\\frac{\\mu}{2} \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + v \\frac{u_{i+1} - u_{i-1}}{2} = 0$$\nRemind # $\\frac{1}{\\Delta x}$ (finite element) = finite difference\nStability # We also require Peclet \u0026lt; 1:\n$$\\frac{|v| \\Delta x}{2\\mu} \u0026lt; 1$$\nReaction-Diffusion Problems # We can also apply upwind method and we can achieve absolute stability.\nReaction-Diffusion Problem # $$-\\mu u\u0026rsquo;\u0026rsquo; + \\sigma u = f \\quad f \\in L^2(\\Omega)$$\nFinite difference method:\n$$-\\mu \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\sigma u_i = f(x_i)$$\n$$(A\\mu + \\sigma I) \\cdot u_i = f_i$$\nThis is absolutely stable.\nFinite Element Equation # $$A_{ij} = \\frac{\\mu}{2} \\int_0^1 \\phi_i\u0026rsquo; \\phi_j\u0026rsquo; , dx + \\sigma \\int_0^1 \\phi_i \\phi_j , dx$$\n$$F_i = \\int_0^1 f \\phi_i , dx \\quad \\text{then we have } AU = F$$\nWe can write in this form:\n$$-\\mu \\cdot \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\frac{\\sigma \\Delta x}{6}(u_{i+1} + 4u_i + u_{i-1}) = \\int f v$$\nIn this case the Peclet number is:\n$$\\frac{\\sigma \\Delta x^2}{6\\mu} \u0026lt; 1 \\quad \\Delta x \u0026lt; \\sqrt{\\frac{6\\mu}{\\sigma}} \\ldots \\text{much more acceptable}$$\nExplore the Integral # $$\\sigma \\int_0^1 \\phi_i \\phi_j = \\begin{cases} \\frac{\\sigma}{6}\\Delta x \u0026amp; \\text{if } j = i \\pm 1 \\ \\frac{\\sigma}{3}\\Delta x \u0026amp; \\text{if } j = i \\end{cases}$$\nMass Lumping Technique # Idea: treat $x_i = x_{i-1} = x_{i+1}$ [trapezoid rule]\nThen we have:\n$$\\frac{\\sigma}{6}(u_{i+1} + 4u_i + u_{i-1}) = \\sigma u_i$$\nAnd equation becomes:\n$$-\\mu \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\sigma u_i \\Delta x = \\int f v$$\nThis method is stable.\nUpwind Method and Strong Consistency # Before we have:\n$$a(u-u_h, v_h) = 0 \\quad \\text{for } v_h \\in V_h$$\nHowever if we use upwind method, we lose the strong consistency for Galerkin method:\nWe have:\n$$a_h(u_h, v_h) = a(u_h, v_h) + \\frac{|B|h}{2\\mu} \\int_0^1 u_h\u0026rsquo; v_h\u0026rsquo;$$\n$$B_h(u_h) = B(u_h)$$\nWTF: $u_h \\in V_h$ s.t. $a_h(u_h, v_h) = B_h(v_h)$\nIf we use mass lumping:\n$$a_h(u_h, v_h) = a(u_h, v_h) + \\int_0^1 - \\int_0^1 \\text{[trapezoid rule]}$$\nRemind $a_h(u-u_h, v_h) \\neq 0$ for not strong consistent.\nWe can use general Galerkin method:\nStrong Lemma # $||u-u_h||{H^1} \\leq C_1 \\inf{v_h \\in V_h} ||u-v_h||{H^1} + C_2 \\inf{v_h \\in V_h} \\sup_{v_h \\in V_h} \\frac{|a_h(u_h, v_h) - a(u_h, v_h)|}{||v_h||_{V_h}}$\n$C_3 \\sup_{v_h \\in V_h, v_h \\in V_h} \\frac{|a_h(u_h, v_h) - a(u_h, v_h)|}{||v_h||_{V_h}} \\neq 0$\n$C_3 \\sup_{v_h \\in V} ||B_h(v_h) - B(v_h)|| \\quad \\text{for } v_h \\to 0$\nOrder of Convergence # Upwind: $O(h^p)$ for $p = \\min(s,k)$\n"},{"id":16,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E5%8D%81%E7%AB%A0/Quiz-4/","title":"Quiz 4","section":"æ•°å€¼æ–¹æ³•","content":" Solutions for Quiz 4 # Question 1 # For a given problem, we know that the solution belongs to $H^2(\\Omega)$. We use finite elements of order 2. What is the expected order of convergence in the norm of $H^1(\\Omega)$. Is this an \u0026ldquo;optimal\u0026rdquo; choice? Motivate your answer.\nSolution: # Based on approximation theory for finite elements, we can analyze this using the relationship between the solution regularity, the polynomial order, and the expected convergence rate.\nFor a solution $u \\in H^2(\\Omega)$ and using order $k$ Lagrange interpolation (where $k=2$ in this case), we can refer to the following relationship for error estimates:\nIf we construct a table of convergence rates based on solution regularity $q$ and polynomial order $k$:\n$q$ \\ $k$ 1 2 3 1 1 1 1 2 1 2 2 3 1 2 3 Here we\u0026rsquo;re looking at the case where $q=2$ (solution in $H^2$) and $k=2$ (quadratic elements).\nFor this combination, we expect order of convergence $O(h^1)$ in the $H^1(\\Omega)$ norm.\nThis is not optimal. The optimal convergence rate would be $O(h^2)$ in the $H^1(\\Omega)$ norm, which would require the solution to be in $H^3(\\Omega)$ when using quadratic elements.\nFor a solution in $H^2(\\Omega)$, we only achieve first-order convergence in the $H^1$ norm with quadratic elements, which is the same rate we would get with linear elements. Therefore, using quadratic elements is not computationally efficient for this problem.\nQuestion 2 # Explain why in advection-dominated problems in 2+ D, adding artificial viscosity to stabilize the solution is not optimal.\nSolution: # In advection-dominated problems where the convection term significantly outweighs the diffusion term, we often encounter numerical instabilities. Consider the extreme case where the velocity field is highly anisotropic, such as:\n$$\\beta = \\begin{bmatrix} 1000 \\ 0 \\end{bmatrix}$$\nIn this scenario, using the upwind method with artificial viscosity is not optimal because:\nThe advection-diffusion equation takes the form: $$-\\mu^* \\frac{\\partial^2 u}{\\partial x^2} - \\mu^* \\frac{\\partial^2 u}{\\partial y^2} + \\beta_0 \\cdot \\frac{\\partial u}{\\partial x} = f$$\nThe key issue is that $\\mu^*$ (artificial viscosity) doesn\u0026rsquo;t help stabilize the problem in the $y$-direction. The only thing we need to do is to regularize the $x$-direction, which can be written as:\n$$-\\mu^* \\frac{\\partial^2 u}{\\partial x^2} - \\mu \\frac{\\partial^2 u}{\\partial y^2} + \\beta_0 \\frac{\\partial u}{\\partial x}$$\nMore generally, we can write: $$\\mu \\int_\\Omega \\nabla v \\cdot \\nabla u + \\int_\\Omega \\beta \\nabla u \\cdot v + \\frac{\\mu}{2} \\int_\\Omega (\\beta \\cdot \\nabla u)(\\beta \\cdot \\nabla v) \\frac{1}{|\\beta|^2} = \\int_\\Omega f v$$\nThis shows that artificial viscosity adds diffusion isotropically (in all directions), whereas the instability primarily occurs in the direction of the flow. This makes the method unnecessarily diffusive in directions perpendicular to the flow, degrading solution accuracy where stabilization isn\u0026rsquo;t needed.\nQuestion 3 # True or False?\na) If we solve an advection-diffusion problem with the condition that the convection dominates the diffusion (||Î²|| \u0026raquo; Î¼) with the finite element method, the solution does not oscillate: T ___ F ___ # Answer: F\nWhen convection dominates diffusion, standard finite element methods will produce oscillatory solutions unless stabilization techniques are applied.\nb) If we solve a reaction-diffusion problem with the condition that the reaction dominates the diffusion (Ïƒ \u0026raquo; Î¼) with the finite element method, the solution does not oscillate: T ___ F ___ # Answer: F (ATO-1), u = f\nThis is stable. Unlike advection-dominated problems, reaction-dominated problems may not show oscillations but can exhibit sharp boundary layers. The finite element solution doesn\u0026rsquo;t typically oscillate in the same way as advection-dominated problems.\nc) When we do Mass Lumping, the matrix with entries $\\int_0^1 \\varphi_i \\varphi_j dx$ we obtain is diagonal: T ___ F ___ # Answer: T\nIn mass lumping, all terms become one term in the diagonal. The mass matrix, which normally has entries from the integral of basis function products, is approximated by a diagonal matrix through the lumping process.\n"},{"id":17,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-10/","title":"Homework 10","section":"Real Analysis II","content":"8.6: [1, 3]; Chapter 8: [24, 36, 39]\nProblem 8.6.1 Show that Theorem 8.6.1 can be proved using the methods of Chapter 5 if the $g_n$ are continuous.\nTheorem 8.6.1 (Lebesgue\u0026rsquo;s Monotone Convergence Theorem) Let $g_n : [0, 1] \\to \\mathbb{R}$ be a sequence of nonnegative functions such that each improper integral $\\int_0^1 g_n(x) dx$ exists and is finite. Suppose that $0 \\leq g_{n+1}(x) \\leq g_n(x)$ and that $g_n(x) \\to 0$ for each $x \\in [0, 1]$. Then $$\\lim_{n\\to\\infty} \\int_0^1 g_n(x) dx = 0.$$\n[!definition|*] Proof. Let $g_n : [0, 1] \\to \\mathbb{R}$ be a sequence of nonnegative functions such that each improper integral $\\int_0^1 g_n(x) dx$ exists and is finite. Suppose that $0 \\leq g_{n+1}(x) \\leq g_n(x)$. Since each $g_n$ is continuou within the compact interval $[0,1]$, so it is bounded by a maximum vlaue. We define: $$ M_n := \\max g_n(x) $$ The monotonicity condition $g_{n+1}(x) \\le g_n(x)$ for all $x$ means that $M_{n+1} \\le M_n$. Therefore, the sequence ${M_n}$ is nonincreasing and bounded below by 0. for each fixed , we have\n$$\\lim_{n\\to\\infty} g_n(x) = 0$$ Suppose, for the sake of contradiction, that $\\lim_{n\\to\\infty} M_n \\neq 0$. Then there exists an $\\epsilon_0 \u0026gt; 0$ and a subsequence ${n_k}$ such that\n$$M_{n_k} \\ge \\epsilon_0 \\quad \\forall k\n$$ For each $k$, choose $x_{n_k} \\in [0,1]$ such that\n$$ g_{n_k}(x_{n_k}) = M_{n_k} \\ge \\epsilon_0$$ This contradicts the fact that $g_{n_k}(x_{n_k}) \\to 0$ as $n_k \\to \\infty$. Hence, it must be true that $\\lim_{n\\to\\infty} M_n = 0$. This means for every $\\epsilon \u0026gt; 0$ there exists $N \\in \\mathbb{N}$ such that for all $n \\ge N$,\n$$ M_n \u0026lt; \\epsilon$$ Since $0 \\le g_n(x) \\le M_n$ for all $x \\in [0,1]$, it follows that for all $x$ and for $n\\ge N$,\n$$|g_n(x) - 0| \\le M_n \u0026lt; \\epsilon $$ Thus, $g_n \\to 0$ uniformly on $[0,1]$. By the Uniform Convergence Theorem , if a sequence of integrable functions ${f_n}$ converges uniformly to a function $f$ on $[a,b]$, then\n$$\\lim_{n\\to\\infty} \\int_a^b f_n(x),dx = \\int_a^b \\lim_{n\\to\\infty} f_n(x),dx. $$ apply this to ${g_n}$, which converges uniformly to $f(x) = 0$ on $[0,1]$, we hence conclude that\n$$\\lim_{n\\to\\infty} \\int_0^1 g_n(x),dx = \\int_0^1 0,dx = 0$$\nProblem 8.6.3 Evaluate $$\\lim_{n\\to\\infty} \\int_0^1 \\frac{1 - e^{-nx}}{\\sqrt{x}} dx.$$\n[!definition|*] Proof. We define\n$$f_n(x) = \\frac{1-e^{-nx}}{\\sqrt{x}},$$\nNotice that for every fixed $x\u0026gt;0$, as $n\\to\\infty$ we have $e^{-nx}\\to 0$ and hence\n$$\\lim_{n\\to\\infty} f_n(x) = \\frac{1}{\\sqrt{x}}$$\nwhich means, every $x\u0026gt;0$ the function $1-e^{-nx}$ is increasing in $n$, so that the sequence ${f_n(x)}$ is nonnegative and monotonically increasing. Thus, by Corollary 8.6.2:\n$$\\lim_{n\\to\\infty}\\int_0^1 f_n(x),dx = \\int_0^1\\lim_{n\\to\\infty} f_n(x),dx = \\int_0^1 \\frac{1}{\\sqrt{x}},dx.$$\nThe remaining integral is just computed normally as an improper integral near $x=0$, which is: $$\\int_0^1 \\frac{1}{\\sqrt{x}},dx = \\lim_{\\epsilon\\to0^{+}} \\int_{\\epsilon}^1 x^{-\\frac{1}{2}},dx = \\lim_{\\epsilon\\to0^{+}} \\left[ 2\\sqrt{x} ,\\right]{\\epsilon}^{1} = \\lim{\\epsilon\\to0^{+}} \\bigl(2 - 2\\sqrt{\\epsilon}\\bigr) = 2.$$\nTherefore,\n$$\\lim_{n\\to\\infty}\\int_{0}^{1} \\frac{1-e^{-nx}}{\\sqrt{x}},dx = 2.$$\nChatper 8.24 Give an example to show that the following is not equivalent to the integrability of $f$:\nFor any $\\varepsilon \u0026gt; 0$, there is a $\\delta \u0026gt; 0$ such that if $P$ is any partition into rectangles $S_1, \\ldots, S_N$ with sides less than $\\delta$, there exist $x_1 \\in S_1, \\ldots, x_N \\in S_N$ such that $$\\left|\\sum_{i=1}^N f(x_i)v(S_i) - I\\right| \u0026lt; \\varepsilon.$$\n[!definition|*] Proof. For a counterexample, consider the function $f : [0, 1] \\to \\mathbb{R}$ such that:\n$$f(x) = \\begin{cases} 1 \u0026amp; \\text{if } x \\in \\mathbb{Q} \\cap [0, 1],\\ 0 \u0026amp; \\text{if } x \\in \\mathbb{R} \\setminus \\mathbb{Q}. \\end{cases}$$ This function $f(x)$ is not integrable due to the set of discontinuity $D = [0, 1]$ because these set of discontinuities have positive measure. Since the irrationals are dense in $\\mathbb{R}$, every subinterval $S_i$ (no matter how small) contains at least one irrational number. Thus, for any partition $P$ of $[0,1]$ with subinterval lengths $x_i \\in S_i$ with $x_i\\in \\mathbb{R} \\setminus \\mathbb{Q}$, we have $f\\left(x_i\\right)=0$ $\\forall i$ such that: $$ \\sum_{i=1}^N f\\left(x_i\\right) v\\left(S_i\\right)=0 $$ Therefore, $$ \\left|\\sum_{i=1}^N f\\left(x_i\\right) v\\left(S_i\\right)-0\\right|=0\u0026lt;\\varepsilon $$ So, the property is satisfied. This shows that the condition holds for the function $f$, even though $f$ is not Riemann integrable on $[0,1]$.\nProblem 8.36 Prove that $$\\lim_{n\\to\\infty}\\frac{(n!)^{1/n}}{n} = e^{-1}$$ by considering Riemann sums for $$\\int_0^1 \\log x , dx$$ based on the partition $$\\frac{1}{n} \u0026lt; \\frac{2}{n} \u0026lt; \\cdots \u0026lt; 1$$\n[!definition] Proof. We first take the natural logarithm: $$\\frac{(n!)^{\\frac{1}{n}}} {n} \\Longrightarrow\\log \\left(\\frac{(n!)^{1 / n}}{n}\\right)=\\frac{1}{n} \\log (n!)-\\log (n)$$\nnow, we want to first show that this expression converges to $-1$. Notice that: $$\\begin{align}\\log(n!) \u0026amp; = \\sum_{k=1}^n \\log(k) = \\sum_{k=1}^n \\bigl[\\log(k/n) + \\log(n)\\bigr]= \\sum_{k=1}^n \\log\\bigl(\\tfrac{k}{n}\\bigr) + n,\\log(n) \\\\frac{1}{n}\\log(n!) \u0026amp; = \\frac{1}{n}\\sum_{k=1}^n \\log\\Bigl(\\frac{k}{n}\\Bigr) ;+; \\log(n) \\\\frac{1}{n}\\log(n!) - \\log(n)\u0026amp; = \\frac{1}{n}\\sum_{k=1}^n \\log\\Bigl(\\tfrac{k}{n}\\Bigr)\\end{align} $$ So we have: $$\\log!\\Bigl(\\tfrac{(n!)^{1/n}}{n}\\Bigr);=; \\frac{1}{n}\\sum_{k=1}^n \\log\\Bigl(\\tfrac{k}{n}\\Bigr).$$ Notice that\n$$\\frac{1}{n}\\sum_{k=1}^n \\log\\bigl(\\tfrac{k}{n}\\bigr)$$\nis Riemann sum for the integral $\\int_0^1 \\log x ,dx$, using the partition\n$$0 \u0026lt; \\tfrac{1}{n} \u0026lt; \\tfrac{2}{n} \u0026lt; \\cdots \u0026lt; \\tfrac{n}{n}=1$$\nTherefore,\n$$\\frac{1}{n}\\sum_{k=1}^n \\log\\Bigl(\\tfrac{k}{n}\\Bigr);\\longrightarrow; \\int_0^1 \\log x,dx\\quad\\text{as }n\\to\\infty $$ Now we proceed to compute $\\int_0^1 \\log x , dx$. We have\n$$\\int_0^1 \\log x , dx ;=; \\left[x\\log x - x\\right]{0}^{1}= (0) - (-1) = -1 $$\nBy the definition of a Riemann sum, we know that,\n$$\\lim{n\\to\\infty} \\frac{1}{n}\\sum_{k=1}^n \\log\\Bigl(\\tfrac{k}{n}\\Bigr);=; \\int_0^1 \\log x,dx;=; -1 $$\nso $$\\lim_{n\\to\\infty}\\log\\Bigl(\\tfrac{(n!)^{1/n}}{n}\\Bigr);=;-1 $$\nwhich is: $$\\lim_{n\\to\\infty}\\frac{(n!)^{1/n}}{n} ;=; e^{-1}.$$\nProblem 8.39 Prove that $$\\log 2 = \\lim_{n\\to\\infty} \\left[\\frac{1}{n+1} + \\frac{1}{n+2} + \\cdots + \\frac{1}{2n}\\right]$$\n[!definition|*] Proof. Define, for each $n\\in\\mathbb{N}$, $$S_n = \\frac{1}{n+1} + \\frac{1}{n+2} + \\cdots + \\frac{1}{2n} = \\sum_{k=n+1}^{2n} \\frac{1}{k}.$$ We let $i=k-n$, so that $k=n+i$ and $i$ runs from 1 to $n$, then we obtain\n$$S_n = \\sum_{i=1}^{n} \\frac{1}{n+i} = \\sum_{i=1}^{n} \\frac{1}{n\\left(1+\\frac{i}{n}\\right)} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{1+\\frac{i}{n}}.$$ Now, consider the function $f(x)=\\frac{1}{1+x}$ defined on $[0,1]$. Since $f$ is continuous on $[0,1]$, it is Riemann integrable. Consider partition $P$ over the interval $[0,1]$ for $n$ equal subintervals $\\Delta x = \\frac{1}{n}$: $$0 = x_0 \u0026lt; x_1 \u0026lt; x_2 \u0026lt; \\cdots \u0026lt; x_n = 1$$ where $x_i = \\frac{i}{n}$ for $i=0,1,\\dots,n$. Then the Riemann sum for $f$ is\n$$R_n = \\sum_{i=1}^{n} f(x_i) \\Delta x = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{1+\\frac{i}{n}}$$ By construction, $R_n=S_n$. Thus, by the definition of the Riemann integral,\n$$\\lim_{n\\to\\infty} S_n = \\lim_{n\\to\\infty} R_n = \\int_0^1 \\frac{1}{1+x},dx$$ That is: $$\\int_0^1 \\frac{1}{1+x},dx = \\Bigl[\\log(1+x)\\Bigr]0^1 = \\log(2) - \\log(1) = \\log2$$ Therefore,\n$$\\lim{n\\to\\infty} \\left[\\frac{1}{n+1} + \\frac{1}{n+2} + \\cdots + \\frac{1}{2n}\\right] = \\log2$$\n"},{"id":18,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-11/","title":"Homework 11","section":"Real Analysis II","content":"HW 11: 9.2: 1, [2], 3, 4; 9.3: [5]; 9.4: 1, [2], 3; 9.5: 1, [2], 3, 4, [5]; Chapter 9: 1, 3, 5(a,b,c, [d]), 7, 10.\n"},{"id":19,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-6/","title":"Homework 6","section":"Real Analysis II","content":"7.2: 1, 2, [3,4]; Chapter 7: [4], 5, [6], 9, [12].\nProblem 7.2.3 In the system $$\\begin{array}{r} 3 x+2 y+z^2+u+v^2=0 \\ 4 x+3 y+z+u^2+v+w+2=0 \\ x+z+w+u^2+2=0 \\end{array} $$ discuss the solvability for $u, v, w$ in terms of $x, y, z$ near $x=y=z=0, u=$ $v=0, w=-2$.\n[!theorem|*] We first define three functions: $$ \\begin{aligned} F_1(x,y,z,u,v,w) ;\u0026amp;=; 3x + 2y + z^2 + u + v^2,\\ F_2(x,y,z,u,v,w) ;\u0026amp;=; 4x + 3y + z + u^2 + v + w + 2,\\ F_3(x,y,z,u,v,w) ;\u0026amp;=; x + z + w + u^2 + 2. \\end{aligned} $$\nSubstitute $x=0,y=0,z=0,u=0,v=0,w=-2$ into each equation:\n$F_{1}(0,0,0,0,0,-2) = 3\\cdot 0 + 2\\cdot 0 + 0^2 + 0 + 0^2 = 0.$ $F_{2}(0,0,0,0,0,-2) = 4\\cdot 0 + 3\\cdot 0 + 0 + (0)^2 + 0 + (-2) + 2 = 0.$ $F_{3}(0,0,0,0,0,-2) = 0 + 0 + (-2) + (0)^2 + 2 = 0.$ Hence $\\bigl(0,0,0,0,0,-2\\bigr)$ satisfies all three equations. By the Implicit Function Theorem, we want to solve for $(u,v,w)$ if the Jacobian of $D_{(u,v,w)} (F_1, F_2, F_3) ;$ is invertible at that point.\nWe then compute partial derivatives, and evaluate them at $\\bigl(0,0,0,0,0,-2\\bigr)$: $$ D_{(u,v,w)} (F_1, F_2, F_3)=\\begin{bmatrix} F_{1u} \u0026amp; F_{1v}\u0026amp;F_{1w} \\ F_{2u} \u0026amp; F_{2v}\u0026amp;F_{2w} \\ F_{3u} \u0026amp; F_{3v}\u0026amp;F_{3w} \\end{bmatrix}=\\begin{bmatrix} 1 \u0026amp; 2v \u0026amp; 0 \\ 2u \u0026amp; 1 \u0026amp; 1 \\ 2u \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$ At that point, $u=0$ and $v=0$, the determinant of this $3\\times 3$ matrix is $$\\det \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 1 \u0026amp; 1 \\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} ;=; 1 ;\\neq; 0. $$ So the matrix is invertible. Therefore, since $F_1=F_2=F_3=0$ is at our point of interest, and the invertibility of Jacobian ensures that the mapping is locally bijective, the Implicit FT guarantees that in a neighborhood of $\\bigl(x,y,z\\bigr)=(0,0,0)$, there exist unique smooth functions $$u = u(x,y,z), \\quad v = v(x,y,z), \\quad w = w(x,y,z), $$ satisfying the system. And since, $\\bigl(u(0,0,0),,v(0,0,0),,w(0,0,0)\\bigr)=(0,0,-2)$, the system is locally solvable for $,(u,v,w),$ as functions of $,(x,y,z),$ near $,(0,0,0),$.\nProblem 7.2.4 Does the map\n$$ (x, y) \\mapsto\\left(\\frac{x^2-y^2}{x^2+y^2}, \\frac{x y}{x^2+y^2}\\right) $$\nhave a local inverse near $(0,1)$ ?\n[!definition|*] Define $$F(x,y);=;\\Bigl(F_1(x,y),,F_2(x,y)\\Bigr);=;\\biggl(,\\frac{x^2 - y^2}{x^2 + y^2},;\\frac{x,y}{x^2 + y^2}\\biggr)$$ We substitute $\\bigl(x,y\\bigr)=(0,1)$ into $F$: $$F(0,1) ;=;\\Bigl(\\tfrac{0^2 - 1^2}{0^2 + 1^2},;\\tfrac{0\\cdot1}{0^2 + 1^2}\\Bigr) ;=;(-1,,0)$$ We check if the Jacobian of $F$ at $(0,1)$ is invertible. The partial of $,(F_1,F_2)$ are\n$$F_1(x,y)=\\tfrac{x^2 - y^2}{x^2 + y^2}, \\quad F_2(x,y)=\\tfrac{x,y}{x^2 + y^2} $$\nFor $F_1$: $$\\begin{align} \\frac{\\partial F_1}{\\partial x}(x,y) \u0026amp; =\\frac{(x^2+y^2)(2x) - (x^2-y^2)(2x)}{(x^2+y^2)^2}\\ \u0026amp; =\\frac{2x\\Bigl[(x^2+y^2)-(x^2-y^2)\\Bigr]}{(x^2+y^2)^2}=\\frac{4xy^2}{(x^2+y^2)^2} \\end{align} $$ And $$\\begin{align} \\frac{\\partial F_1}{\\partial y}(x,y) \u0026amp; =\\frac{(x^2+y^2)(-2y) - (x^2-y^2)(2y)}{(x^2+y^2)^2} \\ \u0026amp; =\\frac{-2y\\Bigl[(x^2+y^2)+(x^2-y^2)\\Bigr]}{(x^2+y^2)^2}=-\\frac{4x^2y}{(x^2+y^2)^2} \\end{align} $$ For $F_2$: $$\\begin{align} \\frac{\\partial F_2}{\\partial x}(x,y) \u0026amp; =\\frac{(x^2+y^2)(y) - (xy)(2x)}{(x^2+y^2)^2} \\ \u0026amp; =\\frac{y\\Bigl[(x^2+y^2)-2x^2\\Bigr]}{(x^2+y^2)^2} =\\frac{y(y^2-x^2)}{(x^2+y^2)^2} \\end{align} $$ And $$\\begin{align} \\frac{\\partial F_2}{\\partial y}(x,y) \u0026amp; =\\frac{(x^2+y^2)(x) - (xy)(2y)}{(x^2+y^2)^2} \\ \u0026amp; =\\frac{x\\Bigl[(x^2+y^2)-2y^2\\Bigr]}{(x^2+y^2)^2} =\\frac{x(x^2-y^2)}{(x^2+y^2)^2} \\end{align} $$ Since $x^2+y^2=0^2+1^2=1$, the evaluation at $(0,1)$ are:\n$\\displaystyle \\frac{\\partial F_1}{\\partial x}(0,1)=\\frac{4\\cdot 0\\cdot1^2}{1^2}=0$ $\\displaystyle \\frac{\\partial F_1}{\\partial y}(0,1)=-\\frac{4\\cdot0^2\\cdot1}{1^2}=0$ $\\displaystyle \\frac{\\partial F_2}{\\partial x}(0,1)=\\frac{1,(1^2-0^2)}{1^2}=1$ $\\displaystyle \\frac{\\partial F_2}{\\partial y}(0,1)=\\frac{0,(0^2-1^2)}{1^2}=0$ Hence the Jacobian matrix of $F$ at $,(0,1)$ is $$D F(0,1) ;=; \\begin{pmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{pmatrix} $$ The determinant of $D F(0,1)$ is $$\\det\\begin{pmatrix} 0 \u0026amp; 0\\ 1 \u0026amp; 0 \\end{pmatrix} ;=;0 $$ Because this determinant is zero, the matrix is not invertible. This means we cannot apply the IFT to conclude that $F$ is invertible near $(0,1)$; there is no diffeomorphic local inverse of $F$ around $(0,1)$.\nTherefore, we conclude that the map $F$ does not have a local inverse near $(0,1)$.\nProblem [7.4] 4. Show that the equations\n$$ \\begin{array}{r} x^2-y^2-u^3+v^2+4=0 \\ 2 x y+y^2-2 u^2+3 v^4+8=0 \\end{array} $$\ndetermine functions $u(x, y), v(x, y)$ near $x=2, y=-1$ such that $u(2,-1)=$ $2, v(2,-1)=1$. Compute $\\partial u / \\partial x$.\n[!definition|*] Let $$ F(x,y,u,v)=\\begin{cases} x^2 - y^2 -u^3 +v^2 +4 =0\\ 2xy+y^2 -2u^2 +v^4 +8=0 \\end{cases} $$\nWe first verify $,(x,y,u,v)=(2,-1,2,1)$ is a solution: $$\\begin{cases};4 - 1 - 8 + 1 + 4 ;=;0 \\ -4 +1 -8 +3 +8 ;=;0\\end{cases} $$ Therefore $\\bigl(2,-1,2,1\\bigr)$ is indeed a solution of the system. Then, we compute the Jobcobian: $$\\begin{align} D_{(u,v)} (F_1, F_2) \u0026amp; =\\begin{pmatrix} F_{1u} \u0026amp; F_{1v}\\ F_{2u} \u0026amp; F_{2v} \\end{pmatrix} \\Bigg|{(2,-1,2,1)} \\[3pt] \u0026amp; =\\begin{pmatrix} -3u^{2}\u0026amp; 2v\\ -4u \u0026amp; 12v^3 \\end{pmatrix}\\Bigg|{(2,-1,2,1)} \\[5pt] \u0026amp; = \\begin{pmatrix} -12 \u0026amp; 2\\ -8 \u0026amp; 12 \\end{pmatrix}\\end{align} $$ Its determinant is $\\Delta=(-12)(12) - 2(-8)= -144 +16= -128\\neq 0$. Therefore, the matrix is invertible, so by the IFT we know that we can solve for $u$ and $v$ as functions of $x,y$ near $,(2,-1)$. Now we compute $u_x(2,-1)$. For $F_1=0$: $$\\frac{\\partial}{\\partial x}(x^2-y^2 -u^3 +v^2 +4) ;=;2x ;-;3u^2 u_x ;+;2v v_x ;=;0 $$ At $(x,y,u,v)=(2,-1,2,1)$, this is $4 -12u_x + 2v_x=0$. For $F_2=0$: $$\\frac{\\partial}{\\partial x}(2xy +y^2 -2u^2 +3v^4 +8) =2y ;-;4u u_x ;+;12v^3 v_x =0 $$ At $(2,-1,2,1)$, this is $-2 ;-;8 u_x +12 v_x=0$. So we obtain: $$\\begin{cases} 4 ;-;12u_x +2v_x = 0\\ -2 ;-;8u_x +12v_x = 0 \\end{cases} $$ To solve this system of equations, we have $$ v_x = \\frac{8u_x + 2}{12} $$ So, $$ \\begin{align*} 4 - 12u_x + 2v_x \u0026amp;= 4 - 12u_x + 2\\left(\\frac{8u_x + 2}{12} \\right) \\ \u0026amp;= 4 - 12u_x + \\frac{4}{3}u_x + \\frac{1}{3} \\ \u0026amp;= -\\frac{32}{3} u_x + \\frac{13}{3} = 0 \\end{align*} $$ $$ \\Longrightarrow u_x = \\frac{13}{32} $$ Hence, we have $$u_x(2,-1) = \\frac{13}{32}$$\nProblem [7.6] Determine whether the \u0026ldquo;curve\u0026rdquo; described by the equation $x^2+y+\\sin (x y)$ $=0$ can be written in the form $y=f(x)$ in a neighborhood of $(0,0)$. Does the implicit function theorem allow you to say whether the equation can be written in the form $x=h(y)$ in a neighborhood of $(0,0)$ ?\n[!definition|*] Let $$F(x,y)=x^2 + y +\\sin!\\bigl(x,y\\bigr)=0$$ We want to show that $F\\colon \\mathbb{R}^2 \\to \\mathbb{R}$ is $C^1$, and $F(x_0,y_0)=0$. We first substitute $x=0,y=0$ into $F$: $$F(0,0)=0^2+0+\\sin(0\\cdot 0)=0$$ Hence $(0,0)$ lies on the curve $F(x,y)=0$. We then compute the partial at $(0,0)$: $$\\begin{align} F_{y} =1 +\\cos!\\bigl(xy\\bigr)\\bigl(x\\bigr) \\Longrightarrow F_{y}(0,0) =1 +0 =1\\neq 0 \\end{align} $$ And $$ \\begin{align} F_{x} =2x +\\cos!\\bigl(xy\\bigr)\\bigl(y\\bigr)\n\\Longrightarrow F_{y}(0,0) = 2\\cdot 0 + 0 = 0 \\end{align} $$\nBecause $F_{y}(0,0)=1\\neq 0$, the Implicit Function Theorem ensures that there exists neighborhood of $(0,0)$ in which we can uniquely solve the equation for $y$ as a function of $x$. Therefore, there exists $y =f(x)$ for $(x,y)$ near $(0,0)$ for all $x$ in the neighborhood of $0$.\nHowever, on the other hand, since $F_{x}(0,0)=0$, Implicit FT does not apply, so the test is conclusive. This means the usual IFT statement fails to guarantee a local solution of the form $x=h(y)$.\nProblem [7.12] Show that the implicit function theorem implies the inverse function theorem.\n[!definition|*]\nLet $f\\colon A\\subset\\mathbb{R}^n \\to \\mathbb{R}^n$ be of class $C^1$. Suppose $x_0\\in A$ and\n$$ J_f(x_0);=;\\det\\bigl(Df(x_0)\\bigr);\\neq;0. $$ We want to show that, there exist neighborhoods $U$ of $x_0$ in $A$ and $V$ of $y_0=f(x_0)$ in $\\mathbb{R}^n$ such that\n(1) $f(U)=V$ and $f\\colon U\\to V$ has an inverse $f^{-1}:V\\to U$. (2) $f^{-1}$ is of class $C^1$. (3) $D f^{-1}(y)=\\bigl[D f(x)\\bigr]^{-1}$ for all $x\\in U$ with $y=f(x)$. Define a new function $$F\\colon \\mathbb{R}^n\\times \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n, \\quad F(x,y)=f(x)-y $$ with $x=(x_1,\\dots,x_n)$ and $y=(y_1,\\dots,y_n)$. Then we know that $F$ is $C^1$ because $f$ is $C^1$ and subtraction is smooth. Note that $F(x_0,,f(x_0))= f(x_0)- f(x_0)=0$. We want to compute the Jacobian of $F$ w.r.t. $y$. So for each $i=1,\\dots,n$, the $i$th component of $F$ is $$F_i(x,y)=f_i(x)-y_i.$$ Since $f_i(x)$ does not depend on $y$, we have for each $j=1,\\dots,n$ $$\\frac{\\partial F_i}{\\partial y_j}(x,y)=\\frac{\\partial}{\\partial y_j}\\bigl(f_i(x)-y_i\\bigr) =\\frac{\\partial f_i(x)}{\\partial y_j}-\\frac{\\partial y_i}{\\partial y_j} =0-\\delta_{ij} $$ where $\\delta_{ij}$ is $$\\delta_{ij}= \\begin{cases}1 \\quad i=j \\0 \\quad i\\neq j \\ \\end{cases} $$ Thus, the $(i,j)$-entry of the Jacobian is $$\\left[\\frac{\\partial F}{\\partial y}(x,y)\\right]{ij}=-\\delta{ij} $$ In matrix form, we have: $$\\frac{\\partial F}{\\partial y}(x,y)=-I $$ where $I$ is the $n\\times n$ identity matrix. Since the determinant $\\det(-I)=(-1)^n\\neq 0$, we know that $D_y F(x,y)=-I$ is invertible everywhere. This satisfy the condition for Implicit FT. Hence, by the Implicit Function Theorem, there is a neighborhood $U$ of $x_0\\in \\mathbb{R}^n$ and a neighborhood $V$ of $y_0=f(x_0)\\in \\mathbb{R}^n$ s.t. $\\forall, y\\in V$, $\\exists! ,x\\in U$ satisfying: $$F(x,y)=0 ;;\\Longleftrightarrow;; f(x)-y=0 ;;\\Longleftrightarrow;; y=f(x) $$ and we have a map that is $C^1$ $$\\Phi:V ;\\to; U \\quad\\text{such that}\\quad F\\bigl(\\Phi(y),,y\\bigr)=0 \\quad\\text{for all }y\\in V $$ which this demonstrates (2). Since $F(\\Phi(y),y) \\Longrightarrow f(\\Phi(y))=y$, it follows that $\\Phi$ is the local inverse $f^{1}$ by definition. Because $f$ itself is $C^1$ and $\\Phi=f^{-1}$ is also $C^1$, we conclude that $f^{-1}$ is a local diffeomorphism near $x_0$, which shows (1). Next, by Corollary 7.2.2 for each $y\\in V$, we have $$\\begin{align} D\\Phi(y) \u0026amp; =-\\Bigl(D_yF(\\Phi(y),y)\\Bigr)^{-1}D_xF(\\Phi(y),y) \\ \u0026amp; =-(-I)^{-1},D f\\bigl(\\Phi(y)\\bigr) \\ \u0026amp; =D f\\bigl(\\Phi(y)\\bigr)^{-1} \\end{align} $$ Since $\\Phi(y)=x$, near $x_{0}$ this yields: $$D f^{-1}(f(x)) ;=; \\bigl(Df(x)\\bigr)^{-1} $$ which shows the (3) of theorem. Hence, we have shown that Implicit Function Theorem directly implies the Inverse Function Theorem.\n"},{"id":20,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-7/","title":"Homework 7","section":"Real Analysis II","content":"HW 7: 7.6: 1, 2, 3, [4,5]; 7.7: 1, 2, 3, [4], 5, [6]; Chapter 7: [25], 36, [38], 39.\nProblem 7.6.4 Let $f(x,y) = x^2 + y^2 + 3y^3 + 8x^4 + x^2e^x \\sin x + 6$. Show that there exist new coordinates $\\xi, \\eta$, where $$\\xi = \\xi(x,y), \\quad \\eta = \\eta(x,y),$$ for which $$f(x,y) = \\xi^2 + \\eta^2 + 6$$ in a neighborhood of $(0, 0)$.\nProblem 7.6.5 (a). If $f$ has a nondegenerate critical point at $x_0 \\in \\mathbb{R}^n$, show that there is a neighborhood of $x_0$ containing no other critical points.\n(b). What are the critical points of the function $f(x,y) = x^2y^2$?\nProblem 7.7.4. $f(x, y, z) = x + y + z, x^2 - y^2 = 1, 2x + z = 1$.\nProblem 7.7.6. Supranational Sludge Corporation produces sludge using equipment and material costing $p = $243$ per unit and labor at a wage of $w = $16$ per hour. If $x$ units of equipment/material and $y$ hours of labor are used, then $20x^{3/4}y^{1/4}$ liters of sludge are produced. If the company has a budget of $B = $51,840,000$ to spend, find the maximum amount of sludge that can be produced and the amounts of equipment/material and of labor used to produce it.\nProblem 7.25\nLet $B(0, r) = {x \\in \\mathbb{R}^n \\mid |x| \\leq r}$. Let $f : B(0, r) \\to \\mathbb{R}^n$ be a map with\na. $|f(x) - f(y)| \\leq \\frac{1}{3}|x - y|$\nb. $|f(0)| \\leq \\frac{2}{3}r$\nProve that there is a unique $x \\in B(0, r)$ such that $f(x) = x$.\nProblem 7.38\nA rectangular box with no top is to have a surface area of 16 square meters. Find the dimensions that maximize the volume.\n"},{"id":21,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-8/","title":"Homework 8","section":"Real Analysis II","content":"8.1: 1, [2, 6]; 8.2: [2], 3, 4, 5, [6]; 8.3: [2], 5, 6; Chapter 8: [12].\nProblem 8.1.2 Let $f:[0,2] \\rightarrow \\mathbb{R}$ be defined by $f(x)=0$ for $0 \\leq x \\leq 1$, and by $f(x)=1$ for $1\u0026lt;x \\leq 2$. Compute, using the definition, $\\int_0^2 f(x) d x$.\n[!definition] We first define a partition $P_{n}$ on $[0,2]$ into $n$ equal subintervals $\\Delta x=\\frac{2}{n}$, s.t. $$\\left[ 0, \\frac{2}{n} \\right], \\left[ \\frac{2}{n} , \\frac{4}{n} \\right],\\dots,\\left[ \\frac{2(n-1)}{n},2 \\right]$$ Then, each subinterval, we have $[x_{i-1},x_{i}]=\\left[ \\frac{2(i-1)}{n}, \\frac{2i}{n} \\right]$. Consider Riemman sum $S_n$ for this partition with any choice of sample points $k_{i} \\in [x_{i-1},x_i]$: $$ S_n ;=;\\sum_{k=1}^{n} f(k_{i}),\\Delta x ;=;\\sum_{k=1}^{n} f(k_{i}) \\cdot \\frac{2}{n}. $$ For $[x_{i-1},,x_i]\\subset [0,1]$, then $f(x)=0$ by definition. Hence $f(k_{i}) = 0$; for $[x_{i-1},,x_i]\\subset (1,2]$, then $f(x)=1$. Hence $f(k_{i}) = 1$. Then, notice one subinterval, say $[x_{j-1},,x_j]$, must include $x=1$, and $f$ = 0 or 1 depending on $k_{j}\\le 1$ or $k_{j}\u0026gt;1$. Hence, we have: $$ \\inf {f(x): x\\in [x_{j-1},x_j]} ;=; 0, \\quad \\sup {f(x): x\\in [x_{j-1},x_j]} ;=; 1. $$ Next, we find a lower bound and an upper bound for $S_n$. For lower bound, suppose subinterval $[x_{j-1},x_j]$ contains 1, and we pick $j$ so that $f(k_j)=0$. Let $m$ be the number of intervals inside fully in $(1,2]$. Then for those $m$ intervals, we have $$ \\begin{align} S_n ;\\ge; L(P) = \u0026amp; (1)m\\cdot\\Delta x +(0)(n-m)\\cdot\\Delta x ; \\ = \u0026amp; ; m \\cdot \\frac{2}{n} \\end{align} $$ Since $[x_{j},x_{j+1}]$ begins once $x \u0026gt; 1$, notice that $m\\approx \\frac{n}{2}$ for large $n$. More precisely, we have $m \\ge \\frac{n}{2}-1$. Hence: $$ m ;\\ge; \\frac{n}{2} -1 \\quad\\Longrightarrow\\quad S_n ;\\ge; \\Bigl(\\frac{n}{2}-1\\Bigr),\\frac{2}{n} ;=; 1 - \\frac{2}{n}. $$ Similarly, for upper bound, we pick $k_j$ such that $f(k_{j})=1$. The the number of intervals $m$ entirely in $(1,2]$ each contribute 1, so we in total have $m+1$ subintervals to contribute $\\Delta x$. Thus $$ \\begin{align} S_n ; \u0026amp; \\le; U(P)= (1)(m+1)\\cdot \\Delta x \\ ; \u0026amp; =; (m+1),\\frac{2}{n}. \\end{align} $$ But $m \\le \\frac{n}{2}$ for this case. Hence, $$ m+1 ;\\le; \\frac{n}{2} + 1 \\quad\\Longrightarrow\\quad S_n ;\\le; \\Bigl(\\frac{n}{2}+1\\Bigr),\\frac{2}{n} ;=; 1 + \\frac{2}{n}. $$ Therefore, for every Riemann sum $S_n$: $$ 1 - \\frac{2}{n} ;;\\le;; S_n ;;\\le;; 1 + \\frac{2}{n}. $$ As $n$ grows, the Squeeze Theorem forces each Riemann sum $S_n$ to converge to 1. More precisely, for any $\\varepsilon\u0026gt;0$, choose $N$ large enough s.t. $\\forall n \\ge N$, $$ -\\frac{2}{n} \u0026gt; -\\varepsilon \\quad\\text{and}\\quad \\frac{2}{n} \u0026lt; \\varepsilon, $$ which gives $\\bigl|S_n - 1\\bigr| \u0026lt; \\varepsilon$. This shows that $\\lim_{n\\to\\infty} S_n = 1$, so by definition of the Riemann integral, we have $$ \\int_{0}^{2} f(x),dx = 1. $$\nProblem 8.1.6 Let $f:[a, b] \\rightarrow \\mathbb{R}$ be continuous. Use Riemann\u0026rsquo;s condition and uniform continuity of $f$ to prove that $f$ is integrable.\n[!definition|*] To show that $f$ is Riemann integrable from continuity, we must show that $\\forall ,\\varepsilon\u0026gt;0$, $\\exists$ partition $P$ of $[a,b]$ such that $$0\\leq U(P_{\\varepsilon})-L(P_{\\varepsilon})\u0026lt;\\varepsilon$$ First, since $f$ is continuous on a closed, bounded interval $[a,b]$, then we know it is uniformly continuous by Heineâ€“Cantor theorem. We define $$\\varepsilon_{0} =\\frac{\\varepsilon}{b-a}$$ Then, by definition of uniform continuity, $\\forall \\varepsilon \u0026gt;0$, there exists $\\delta\u0026gt;0$ s.t. $\\forall x,y \\in [a,b]$, we have $$|x-y|\u0026lt;\\delta \\Longrightarrow |f(x)-f(y)|\u0026lt;\\varepsilon_{0} =\\frac{\\varepsilon}{b-a}$$ We define a partition $P$ s.t. for each $[x_{i-1},x_{i}]$, the subinterval is less than $\\delta$. So $P:= \\max(x_{i}-x_{i-1})\u0026lt;\\delta$ (this is always possible because $f$ is continuous). Next, we set $$M_i=\\sup {x \\in\\left[x{i-1}, x_i\\right]} f(x) \\quad \\text{and} \\quad m_i=\\inf {x \\in\\left[x{i-1}, x_i\\right]} f(x)$$ for each subinterval. Notice that by the unform continuity of $f$, we must have $M_{i}-m_{i}\u0026lt;\\varepsilon_{0}$ since the lengths of each interval is awalys strictly less than $\\delta$. Therefore, the difference between upper and lower bound is: $$ \\begin{align} U(f, P)-L(f, P) \u0026amp; =\\sum_{i=1}^n(M_i\\Delta x_i)-\\sum_{i=1}^n(m_i\\Delta x_i) \\ \u0026amp; =\\sum_{i=1}^n\\left(M_i-m_i\\right) \\Delta x_i \\ \u0026amp; \u0026lt; (\\frac{\\varepsilon}{b-a})\\sum_{i=1}^n\\Delta x_i \\ \u0026amp; =(\\frac{\\varepsilon}{b-a})(b-a) \\ \u0026amp; =\\varepsilon \\end{align} $$ Therefore, since $\\varepsilon$ is arbitrarily chosen, by Riemannâ€™s criterion for integrability, this implies that $f$ is Riemann integrable on $[a,b]$.\nProblem 8.2.2 Show that the $x y$ plane in $\\mathbb{R}^3$ has 3-dimensional measure 0.\n[!definition|*] We let $$P={x,y,z\\in \\mathbb{R}^{3},|,z =0}$$ to be the $xy$ plane in $\\mathbb{R}^3$, with $z=0$. We construct a countable union of rectangular boxes to cover it by defining the box: $$S_{n}=[-n,n]\\times[-n,n]\\times [-\\delta_{n},\\delta_{n}]$$ with $\\delta_{n}\u0026gt;0$ to be determined. By such construction, every point $(x,y,0)\\in P$ lies in some $S_{n}$ since $x\\in [-n,n]$ and $y\\in [-n,n]$. If $n\\geq \\max(|x|,|y|)$, we get $(x,y,0)\\in S_{n}$, such that $$P\\subseteq\\bigcup^{\\infty}{n=1}S{n}$$ Then, the volume of $S_{n}$ is given by $$V(S_{n})=(2n)(2n)(2\\delta_{n})=8n^2\\delta_{n}$$ we want to choose $\\delta$ s.t. the sum of the volumes is less than $\\epsilon$. Notice that $\\sum_{n=1}^{\\infty} \\frac{\\varepsilon}{2^n}=\\varepsilon \\sum_{n=1}^{\\infty} \\frac{1}{2^n}=\\varepsilon$, so a motivated choice is: $$\\begin{align} 8 n^2 \\delta_n \u0026amp; \\leq \\frac{\\varepsilon}{2^n} \\ \\delta_n\u0026amp; \\leq\\frac{\\varepsilon}{2^{n+3} n^2} \\end{align} $$ Therefore, we define $\\delta=\\cfrac{\\varepsilon}{2^{n+3} n^2}$, then $$V\\left(S_n\\right)=8 n^2 \\cdot \\frac{\\varepsilon}{2^{n+3} n^2}=\\frac{8 \\varepsilon}{2^{n+3}}=\\frac{\\varepsilon}{2^n}$$ and the total volume of the covering is $$\\sum_{n=1}^{\\infty} V\\left(S_n\\right)=\\sum_{n=1}^{\\infty} \\frac{\\varepsilon}{2^n}=\\varepsilon \\sum_{n=1}^{\\infty} \\frac{1}{2^n}=\\varepsilon \\cdot 1=\\varepsilon$$ Hence, we have constructed a countable cover ${S_{n}}^\\infty_{n=1}$ of $P$ whose total volume is precisely $\\varepsilon$. Because $\\varepsilon\u0026gt;0$ was arbitrary. So by Definition 8.2.2 of the textbook, the xy-plane has 3-dimensional measure zero in $â„Â³$.\nProblem 8.2.6 Must the boundary of a set of measure zero have measure zero?\n[!definition|*] This statement is false. Here is a counterexample: consider the set $Q \\cap [0,1]$. From Example 8.2.5 in the textbook, we know that the set of rational numbers in $[0,1]$ has measure zero. However, the boundary of a set $A$ consists of all points $x$ s.t. every neighborhood of $x$ contains at least one point in $A$ and at least one point not in $A$.\nFor any point $x \\in [0,1]$, every neighborhood of $x$ contains both rational and irrational numbers. This is due to the density of both rational and irrational numbers in $\\mathbb{R}$. Therefore: $$\\partial(Q \\cap [0,1]) = [0,1]$$ Lebesgue measure of $[0,1]$ is 1, which is positive.\nProblem 8.3.2 Let $f(x, y)=1$ if $x \\neq 0$ and $f(0, y)=0$. Prove that $f$ is integrable on $A=[0,1] \\times[0,1] \\subset \\mathbb{R}^2$.\n[!definition|*] We want to show that $f$ is Riemann integrable on $A$ and $$\\iint_A f(x,y),dx,dy = 1.$$ Let $\\varepsilon\u0026gt;0$ be given. Choose a number $\\delta$ such that $0\u0026lt;\\delta\u0026lt;\\varepsilon$. We partition the square $A$ by subdividing the $y$-axis arbitrarily to form sub-rectangles $Q$ inside $[0,\\delta]\\times [0,1]$. These rectangles contain points with $x=0$ and $x\u0026gt;0$, so $f=0$ and $f=1$. Therefore, on each such $Q$, $$\\inf f = 0 \\quad \\text{and} \\quad \\sup f = 1$$ We let $A_1 = [0,\\delta]\\times [0,1]=\\delta$ to be the vertical strip, and $A_2 = [\\delta,1]\\times [0,1]=1-\\delta$ to be the rest of the square. For lower Riemann sum, we have $$ \\begin{align} L(f,P) \u0026amp; =(0)\\cdot A_{1}+(1)\\cdot A_{2} \\ \u0026amp; =A_{2} \\ \u0026amp; =(1-\\delta) \\end{align} $$ since $A_{1}:\\inf f=0$ and $A_{2}:\\inf f=1$. Similarly, for upper Riemann sum, we have $$ \\begin{align} U(f,P) \u0026amp; = (1)\\cdot A_1 + (1)\\cdot A_2 \\ \u0026amp; =A_1 + A_2 \\ \u0026amp; =\\delta+1-\\delta \\ \u0026amp; =1 \\end{align} $$ since $A_{1}:\\inf f=1$ and $A_{2}:\\inf f=1$. Therefore, the difference between the upper and lower sums is $$U(f,P)-L(f,P) = 1 - (1-\\delta) = \\delta.$$ By choosing $\\delta \u0026lt; \\varepsilon$, we ensure that $$U(f,P)-L(f,P) \u0026lt; \\varepsilon.$$ Since $\\forall\\varepsilon\u0026gt;0$ there exists a partition $P$ s.t. $$U(f,P)-L(f,P) \u0026lt; \\varepsilon,$$ the function $f$ is Riemann integrable on $A$. Since the upper sums are always 1 and the lower sums can be made arbitrarily close to 1 by choosing arbitrarily small, it follows that $$\\iint_A f(x,y),dx,dy = 1.$$\nChapter Exercise 8.12 Prove that $A$ has measure zero iff for every $\\varepsilon\u0026gt;0$ there is a covering of $A$ by sets $V_1, V_2, \\ldots$ with volume such that $\\sum_{i=1}^{\\infty} v\\left(V_i\\right)\u0026lt;\\varepsilon$.\n[!definition|*] ( $\\implies$ ) Suppose $m(A)=0$, by definition 8.2.2, we know $\\forall \\varepsilon\u0026gt;0, \\exists$ countable cover of $A$ by rectangles $\\left{S_i\\right} \\text { s.t. }\\sum_{i=1}^{\\infty} v\\left(S_i\\right)\u0026lt;\\varepsilon$. We choose volume $V_{i}=S_{i}$, such that: $$A\\subset \\sum_{i=1}^{\\infty} S_{i}=\\sum_{i=1}^{\\infty} V_{i}$$ and $$\\sum_{i=1}^{\\infty} v(S_{i})=\\sum_{i=1}^{\\infty} v(V_{i}) \u0026lt;\\varepsilon $$ Therefore, $m(A)=0 \\implies$ $\\forall \\varepsilon\u0026gt;0, \\exists$ ${V_{i}}^{\\infty}{1}$ as a covering of $A$ with total volume $\\sum{i=1}^{\\infty} v(V_{i})\u0026lt;\\epsilon$.\n( $\\Longleftarrow$ ) Suppose $\\forall \\varepsilon\u0026gt;0, \\exists$ ${V_{i}}^\\infty_{1}$ a cover of $A$ with total volume $\\sum_{i=1}^{\\infty} v(V_{i})\u0026lt;\\epsilon$. Note that every set in $\\mathbb{R}^n$ can be covered by a union of open rectangles that is countable. Specifically, for each $i$, we can cover $V_i$ by a countable number of rectangles ${S_{i,1}, S_{i,2}, \\dots}$ such that $$V_i ;\\subset; \\bigcup_{k=1}^{\\infty} S_{i,k}$$ We make sure the total volume of these rectangles is within arbitrarily small $\\delta_i$ of $v(V_i)$, so $$\\sum_{k=1}^{\\infty} v(S_{i,k}) ;\u0026lt;; v(V_i) ;+; \\delta_i.$$ Then, we choose each $\\delta_i$ s.t. the sum of volumes is less than $\\varepsilon$. Let $$\\delta_i ;=;\\frac{\\varepsilon}{2},2^{-i} ;=;\\frac{\\varepsilon}{2^{,i+1}}.$$Then $\\forall i$, we have: $$\\sum_{k=1}^{\\infty} v\\bigl(S_{i,k}\\bigr);\u0026lt;; v(V_i) ;+; \\frac{\\varepsilon}{2^{,i+1}}.$$ Hence, summing over all $i$: $$\\sum_{i=1}^{\\infty} \\sum_{k=1}^{\\infty} v(S_{i,k});\\le; \\sum_{i=1}^{\\infty} \\Bigl( v(V_i);+;\\tfrac{\\varepsilon}{2^{,i+1}} \\Bigr);=;\\sum_{i=1}^{\\infty} v(V_i);+;\\frac{\\varepsilon}{2};\u0026lt;; \\varepsilon ;+; \\frac{\\varepsilon}{2} ;=; \\tfrac{3\\varepsilon}{2} $$ Because $\\varepsilon$ was arbitrary, we can make $\\delta_i$ smaller such that the total can be strictly less than $\\varepsilon$. Therefore, $$A ;\\subset;\\bigcup_{i=1}^\\infty \\bigcup_{k=1}^{\\infty} S_{i,k},\\quad\\text{and}\\quad\\sum_{i,k} v\\bigl(S_{i,k}\\bigr) ;\u0026lt;;\\varepsilon.$$ This demonstrates that $A$ is covered by rectangles ${S_{i,k}}$ whose total volume is \u0026lt; $\\varepsilon$, which is by definition, $m(A)=0$.\n"},{"id":22,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-9/","title":"Homework 9","section":"Real Analysis II","content":"8.5: 1, [2, 3], 4, [5]; Chapter 8: [21, 22].\nProblem 8.5.2 Establish formula $\\mathbf{c}$ of Example 8.5.7 as follows. Prove that $e^{-x} x^{p+2} \\rightarrow 0$ as $x \\rightarrow \\infty$, and then compare the integral with $\\int_1^{\\infty}\\left(1 / x^2\\right) d x$.\n[!definition|*]\nLet $\\epsilon \u0026gt; 0$ be arbitrary. We want to show that $\\exists N \u0026gt; 0$ such that for all $x \u0026gt; N$,\n$$\\left|e^{-x}x^{p+2} - 0\\right| = e^{-x}x^{p+2} \u0026lt; \\epsilon.$$ Since $e^{-x}x^{p+2} = \\exp\\Bigl(-x + (p+2)\\ln x\\Bigr)$, and $\\displaystyle\\lim_{x\\to\\infty}\\frac{(p+2)\\ln x}{x} = 0,$ there exists a number $N_1\u0026gt;0$ such that for all $x \u0026gt; N_1$ we have\n$$\\frac{(p+2)\\ln x}{x} \u0026lt; \\frac{1}{2}\\Longrightarrow (p+2)\\ln x \u0026lt; \\frac{x}{2}$$ It follows that for all $x \u0026gt; N_1$,\n$$-x + (p+2)\\ln x \u0026lt; -\\frac{x}{2}.$$ Taking exponentials to get:\n$$e^{-x}x^{p+2} = \\exp\\Bigl(-x + (p+2)\\ln x\\Bigr) \u0026lt; \\exp\\Bigl(-\\frac{x}{2}\\Bigr) = e^{-x/2}.$$ For $e^{-x/2} \u0026lt; \\epsilon,$ we simply need $-\\frac{x}{2} \u0026lt; \\ln \\epsilon$, which is $x \u0026gt; -2\\ln \\epsilon$. We define $N_2 = -2\\ln \\epsilon$. Let $N = \\max{N_1, N_2}$. Then, $\\forall x \u0026gt; N$ we have both\n$$e^{-x}x^{p+2} \u0026lt; e^{-x/2} \\quad \\text{and} \\quad e^{-x/2} \u0026lt; \\epsilon,$$ so that $e^{-x}x^{p+2} \u0026lt; \\epsilon$. By the definition of the limit we have: $$\\lim_{x\\to\\infty} e^{-x}x^{p+2} = 0.$$ Next, we use this to compare the integral $\\int_1^\\infty e^{-x}x^p,dx$ with the convergent integral $\\int_1^\\infty \\frac{1}{x^2},dx$. Note that for $x \\ge 1$ and a fixed $p$, it is always true that $$x^{p} \\le x^{p+2}\\Longrightarrow e^{-x}x^p \\le e^{-x}x^{p+2}$$ As already demonstrated above, for sufficiently large $x$ , we can have $$0 \\leq e^{-x} x^p \\leq e^{-x} x^{p+2}\u0026lt;\\frac{1}{x^2}$$ Hence, there exists some $M$ such that for all $x \u0026gt; M$, $$0 \\le e^{-x}x^p \\le \\frac{1}{x^2}.$$ Given that the tail integral $$\\int_1^{\\infty} e^{-x} x^p d x=\\int_1^M e^{-x} x^p d x+\\int_M^{\\infty} e^{-x} x^p d x$$ converges, so $\\int_M^{\\infty} e^{-x} x^p d x$ converges. By the Comparison Test, the integral $$\\int_{M}^\\infty e^{-x}x^p,dx$$ must also converges. Finally, note that integral over any finite interval $[1,M]$ is finite, so the entire integral $$\\int_{1}^\\infty e^{-x}x^p,dx$$ converges.\nProblem 8.5.3 Let $f:[a, \\infty[\\rightarrow \\mathbb{R}$ be Riemann integrable on bounded intervals. Show that $\\int_a^{\\infty} f$ (conditional convergence) exists iff for every $\\varepsilon\u0026gt;0$, there is a $T$ such that $t_1, t_2 \\geq T$ implies\n$$ \\left|\\int_{t_1}^{t_2} f(x) d x\\right|\u0026lt;\\varepsilon $$\n[!definition|*]\nLet $f : [a,\\infty) \\to \\mathbb{R}$ be a such function, we say the improper integral $$\\int_a^\\infty f(x),dx$$ exists (or converges) if the limit $$\\lim_{t \\to \\infty} \\int_a^t f(x),dx$$ exists for some number $L$. We want to show that this is equivalent to this Cauchy condition: For every $\\varepsilon \u0026gt; 0$, there exists $T \\ge a$ such that for all $t_1, t_2 \\ge T,$ we have $\\left|\\int_{t_1}^{t_2} f(x),dx\\right| \u0026lt; \\varepsilon$.\n($\\implies$) First, by assumption, the improper integral $\\int_a^\\infty f(x),dx$ converges a $L\\in \\mathbb{R}$ s.t: $$\\lim_{t \\to \\infty} \\int_a^t f(x),dx = L$$ By definition of the limit, this means for any given $\\varepsilon\u0026gt;0$, there exists $T$ s.t. for all $t \\ge T,$ we can have $$\\left|\\int_a^t f(x),dx - L\\right| \u0026lt; \\frac{\\varepsilon}{2}.$$Let $t_1, t_2 \\ge T$. Without loss of generality, assume $t_1 \u0026lt; t_2$, then we can write:\n$$ \\int_{t_1}^{t_2} f(x) d x=\\left(\\int_a^{t_2} f(x) d x-L\\right)-\\left(\\int_a^{t_1} f(x) d x-L\\right) $$ By the triangle inequality: $$ \\left|\\int_{t_1}^{t_2} f(x) d x\\right| \\leq\\left|\\int_a^{t_2} f(x) d x-L\\right|+\\left|\\int_a^{t_1} f(x) d x-L\\right| $$ By our choice of $T$, for both $t_1, t_2 \\ge T$, $$\\left|\\int_a^{t_i} f(x),dx - L\\right| \u0026lt; \\frac{\\varepsilon}{2}\\quad $$ Hence, $$ \\left|\\int_{t_1}^{t_2} f(x) d x\\right| \\leq \\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon $$ Thus, for every $\\varepsilon\u0026gt;0$, we always have a $T$ such that the integral is less than $\\varepsilon$ whenever $t_1,t_2 \\ge T$.\n($\\Longleftarrow$) Conversely, suppose that for every $\\varepsilon \u0026gt; 0$, there is a $T$ such that for all $t_1,t_2 \\ge T$, $$\\left|\\int_{t_1}^{t_2} f(x),dx\\right| \u0026lt; \\varepsilon$$We want to prove that limit $\\lim_{t\\to\\infty}\\int_a^t f(x),dx$ exists as a real number. For convenience, we set $$I(t)=\\int_a^t f(x),dx$$ Then, by the given standard Cauchy condition, $\\forall\\varepsilon\u0026gt;0$, $\\exists ,T\\ge a$ s.t. whenever $t_1,t_2\\ge T$, we have $$|I(t_2)-I(t_1)|\u0026lt;\\varepsilon.$$ Fix any $t \\geq T$. Then, for every $t_{0} \\geq T$, we always have $|I(t_{0})-I(t)|\u0026lt;\\varepsilon$, so this mean that implies that $I(t_{0})\\in (I(t)-\\varepsilon, I(t)+\\varepsilon)$ for any $t_{0} \\geq T$. We set upper and lower limits of the function $I(t)$ as $t\\to\\infty$: $$L_{\\text {sup }}=\\lim {T \\rightarrow \\infty} \\sup {I(t{0}): t_{0}\\geq T}\\quad \\text{and} \\quad L_{\\mathrm{inf}}=\\lim {T \\rightarrow \\infty} \\inf {I(t{0}): t_{0} \\geq T}$$ This, by definition, gives us: $$I(t)-\\varepsilon \\leq L_{\\inf}\\leq L_{\\sup}\\leq I(t)+\\varepsilon.$$ In particular, choosing any $t\\ge T$ and writing these two inequalities together, we get: $$ L_{\\sup}-L_{\\inf}\\leq (I(t)+\\varepsilon) - (I(t)-\\varepsilon)=2\\varepsilon.$$ Because $\\varepsilon\u0026gt;0$ is arbitrary, it shows it is indeed in fact $$L_{\\sup}-L_{\\inf}=0,\\Longrightarrow L_{\\sup}=L_{\\inf}=L$$ Now, by the definitions of $\\limsup$ and $\\liminf$, it follows that for every $\\epsilon_{1}\u0026gt;0$ there exists some $T_{1}\\ge a$ such that for all $t\\ge T_{1}$, $$L-\\epsilon \u0026lt; I(t) \u0026lt; L+\\epsilon$$ which is the definition of the limit. We have shown that $\\epsilon_{1}\u0026gt;0$ there exists $T_{1}$ s.t for all $t\\ge T_{1}$, $$|I(t)-L|\u0026lt;\\epsilon.$$ Thus, we conclude that $$\\lim_{t\\to\\infty} I(t)=L,$$ which, by definition, means that the improper integral $$\\int_a^\\infty f(x),dx = \\lim_{t\\to\\infty}\\int_a^t f(x),dx$$exists.\nProblem 8.5.5 For what $\\alpha$ is $\\int_0^{\\infty} \\frac{x^\\alpha}{1+x^\\alpha} d x$ convergent?\n[!definition|*] To fine $\\alpha$, notice that $$\\int_0^{\\infty} \\frac{x^\\alpha}{1+x^\\alpha} d x=\\int_0^1 \\frac{x^\\alpha}{1+x^\\alpha} d x+\\int_1^{\\infty} \\frac{x^\\alpha}{1+x^\\alpha} d x$$ can be split into two regions separately: interval $(0,1]$) and $[1,\\infty)$. So, first, we examine the behavior near $x=0$. We consider $$\\int_{0}^{1} \\frac{x^{\\alpha}}{1 + x^{\\alpha}},dx.$$ We must check whether the integrand create any problems as $x\\to0$.\n$\\alpha \u0026gt; 0$: As $x \\to 0$, $x^{\\alpha}$ becomes very small, so $$\\frac{x^{\\alpha}}{1 + x^{\\alpha}} ;\\approx; x^{\\alpha}$$Since $x^{\\alpha}$ near $0$ is integrable if $\\alpha \u0026gt; -1$, and here $\\alpha \u0026gt; 0$ definitely satisfies $\\alpha \u0026gt; -1$, there is no divergence issue at $0$ in this case. $\\alpha = 0$: Since $x^{\\alpha} = x^0 = 1$, and the integrand becomes $\\tfrac12$, which is a constant. $\\alpha \u0026lt; 0$: As $x \\to 0$, $x^{\\alpha} =\\frac{1}{x^{|\\alpha|}}\\to \\infty$. Thus, $$\\frac{x^{\\alpha}}{1 + x^{\\alpha}} \\approx1$$which is integrable. Therefore, near $x=0$, the integrand is always integrable for every real $\\alpha$. Next, we check behavior near $x=\\infty$. We examine the behavior for $\\int_1^{\\infty} \\frac{x^\\alpha}{1+x^\\alpha} d x$ as $x\\to\\infty$:\n$\\alpha \u0026gt; 0$: As $x \\to \\infty$, $x^{\\alpha}$ is very large, so $\\frac{x^{\\alpha}}{1 + x^{\\alpha}} ;\\approx; 1$. This means that $\\int_{1}^{\\infty} 1,dx$ diverges for all $\\alpha \u0026gt; 0$. $\\alpha = 0$: similarly, $x^{\\alpha} = x^{0} = 1$ implies that the integrand is $\\frac{1}{2}$, and $\\int_{1}^{\\infty} \\tfrac12 ,dx$ also diverges. So it fails to converge for $\\alpha=0$. $\\alpha \u0026lt; 0$: As $x\\to\\infty$, $x^\\alpha$ tends to $0$. So $$\\frac{x^{\\alpha}}{1 + x^{\\alpha}} ;\\approx; x^{\\alpha}.$$ By example 8.5.7(a), we know $\\int_{1}^{\\infty} x^{p},dx$ converges if and only if $p \u0026lt; -1$. Therefore, if $\\alpha \u0026lt; -1$, then $\\int_{1}^{\\infty} x^{\\alpha},dx$ converges, and consequently $$\\int_{1}^{\\infty} \\frac{x^{\\alpha}}{1 + x^{\\alpha}},dx;\\text{converges}$$ by comparison test. Hence, the only way to have convergence at $x = \\infty$ is to require alpha to be less that $-1$. We have shown that the integral is finite if and only if $\\alpha \u0026lt; -1$. Chatper 8.21 Show that $\\int_1^{\\infty} x^{-p} \\sin x d x$ converges if $p\u0026gt;1$. Show that if $0\u0026lt;p \\leq 1$, then the convergence is conditional.\n[!definition|*] Absolute Convergence for $p\u0026gt;1$ To show that the integral converges absolutely when $p\u0026gt;1$, we consider the absolute value of the integrand: $$\\int_{1}^{\\infty} \\bigl| x^{-p}\\sin x \\bigr|,dx.$$ Since $| \\sin x| \\le 1,$ for all $x\\ge 1$ we have $$\\bigl| x^{-p}\\sin x \\bigr| \\le x^{-p}.$$ Thus, by the Comparison Test we know: $$\\int_{1}^{\\infty} \\bigl| x^{-p}\\sin x \\bigr|,dx \\le \\int_{1}^{\\infty} x^{-p},dx.$$ We now compute or recall the convergence of this integral.\nFor $p\\neq 1$, $$\\int_{1}^{a} x^{-p},dx = \\left[\\frac{x^{1-p}}{1-p}\\right]_{x=1}^{x=a} =\\frac{a^{1-p}-1}{1-p}$$ For $p\u0026gt;1$, then $1-p\u0026lt;0$ and therefore $$\\lim_{a\\to\\infty} a^{1-p} = 0$$ which means: $$\\int_{1}^{\\infty} x^{-p},dx = \\frac{0-1}{1-p}=\\frac{1}{p-1}\u0026lt;\\infty$$ and we further conclude that: $$\\int_{1}^{\\infty} \\bigl| x^{-p}\\sin x \\bigr|,dx \u0026lt; \\infty$$ and hence the integral $I(p)$ converges absolutely when $p\u0026gt;1$. Conditional Convergence for $0 \u0026lt; p\\le 1$ To show that the integral converges conditionally, we use integration by parts to show that it converges conditionally. Let $$I(b)=\\int_{1}^{b} x^{-p}\\sin x,dx$$ By the integration by parts formula, we obtain: $$I(b) = \\left[-x^{-p}\\cos x\\right]{1}^{b} -\\left( -\\int{1}^{b} (-\\cos x)(p,x^{-p-1}),dx\\right)$$ Then, evaluate the boundary term, we have $$\\left[-x^{-p}\\cos x\\right]{1}^{b} = -b^{-p}\\cos b + 1^{-p}\\cos 1 = -b^{-p}\\cos b + \\cos 1$$Since $p\u0026gt;0$, as $b\\to\\infty$ we have $b^{-p}\\to 0$, and so $\\lim{b\\to\\infty} \\left(-b^{-p}\\cos b\\right)=0$. Hence, the boundary contribution tends to $\\cos 1$. Then, the remaining term becomes $$I(b) = \\cos 1 + p\\int_1^b x^{-p-1}\\cos x,dx=\\cos 1 + pI_{1}(b)$$where we define $I_{1}(b)=\\int_{1}^{b} x^{-p-1}\\cos x,dx$. Note that, again we have: $$\\bigl|x^{-p-1}\\cos x\\bigr|\\le x^{-p-1}$$since $p\u0026gt;0$ by the assumption, we know $-p-1\u0026lt;-1$. Therefore, $I_{1}(b)$ converges absolutely as $b\\to\\infty$. Let\u0026rsquo;s denote it as $\\lim_{b\\to\\infty} I_{1}(b)=L$. Then, as $b\\to\\infty$, we obtain: $$\\lim_{b\\to\\infty} I(b)= \\cos 1 + p,L.$$ Here the integral $I(b)$ converges to the finite value $\\cos 1 + p,L$ while the absolute integral diverges for $0\u0026lt;p\\le1$. Hence, the original integral $$\\int_{1}^{\\infty} x^{-p}\\sin x,dx$$ converges conditionally for $0\u0026lt;p\\leq1$\nChapter 8.22 The gamma function is defined to be the function given by the improper integral $\\Gamma(p)=\\int_1^{\\infty} e^{-x} x^{p-1} d x$. Show that the integral is convergent for $p\u0026gt;0$.\n[!definition|*] We want to show that the improper integral $$\\Gamma(p)=\\int_1^{\\infty} e^{-x}x^{p-1},dx$$ converges for $p\u0026gt;0$. Observe that $$e^{-x}x^{p-1} \\Longrightarrow \\lim_{x\\to\\infty} \\frac{x^{p-1}}{e^x} =\\lim_{x\\to\\infty} \\frac{x^q}{e^x}= 0.$$ where $q=p-1$ (with $p\u0026gt;0$). And since we know that for $n\\ge1$ and $x\u0026gt;0$, we always have $$e^x \\ge \\frac{x^n}{n!}$$ so, for any integer $n$ s.t. $n\u0026gt;p$, $\\forall x\\ge1$, we can have $$e^{-x} = \\frac{1}{e^{x}} \\le \\frac{n!}{x^{n}}\\Longrightarrow e^{-x}x^{p-1} \\le n! , x^{p-1-n}$$ Then, we verify the convergence of the this integral: $$I=\\int_1^\\infty x^{p-1-n},dx=\\int_1^\\infty x^{r},dx$$ where $r=p-1-n$. We know such integral converges if and only if $r\u0026lt;-1$. Since we have chosen $n\u0026gt;p$, it follows that $$p-1-n \u0026lt; -1 \\quad $$ Thus, the exponent $r$ satisfies the convergence criterion. Consequently, $$I= \\int_1^\\infty x^{p-1-n},dx \u0026lt; \\infty.$$ Next, since for all $x\\ge1$ we have $$0 \\le e^{-x}x^{p-1} \\le n!,x^{p-1-n},$$ and integral $I$ converges, so the Comparison Test implies that $$\\int_1^{\\infty} e^{-x}x^{p-1},dx ;;\\text{ also converges}$$ Thus, we have shown that the gamma function integral $$\\Gamma(p)=\\int_1^\\infty e^{-x} x^{p-1},dx$$ converges for every $p\u0026gt;0$.\n"},{"id":23,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.1-%E5%BA%A6%E9%87%8F%E7%90%86%E8%AE%BA/","title":"8.1 åº¦é‡ç†è®º","section":"ç¬¬å…«ç«  åº¦é‡ç†è®º","content":"ç¬¬å…«ç« çš„ç ”ç©¶å¯¹è±¡æ˜¯Lebesgueç§¯åˆ†ã€‚æˆ‘ä»¬å°†è¦æ¨å¯¼çš„æ˜¯åº¦é‡ç†è®ºï¼ˆmeasure theoryï¼‰çš„æ ¸å¿ƒå†…å®¹ï¼šé›†åˆçš„æµ‹åº¦ï¼ˆmeasureï¼‰å’Œè¦†ç›–æ€§è´¨ã€‚\næˆ‘ä»¬ä¸»è¦çœ‹ä»¥ä¸‹è¿™å‡ ä¸ªæ–¹é¢ï¼š\nç§¯åˆ†çš„å®šä¹‰ å¯ç§¯æ€§çš„åˆ¤æ®ï¼ˆå¿…è¦æ¡ä»¶å’Œå……åˆ†æ¡ä»¶ï¼‰ æ€§è´¨ä¸æ”¶æ•›æ€§ è®¡ç®—å’Œä¼°è®¡ç§¯åˆ† 1.1 ç§¯åˆ†çš„å®šä¹‰ # 1.1.1 å‡ ä½•åŠ¨æœº # ç§¯åˆ†æœ¬è´¨ä¸Šæ˜¯è®¡ç®—å‡½æ•°ä¸‹æ–¹åŒºåŸŸçš„ã€Œä½“ç§¯ï¼ˆvolumeï¼‰ã€ã€‚å¦‚ä½•å®šä¹‰è¿™ç§ã€Œä½“ç§¯ã€ä¼šç›´æ¥æ”¹å˜åˆ°ç§¯åˆ†çš„æ€§è´¨ã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯ä¸ºäº†è®¡ç®—ä»»æ„æ›²çº¿ $y = f(x)$ ä» $a$ åˆ° $b$ ä¸‹æ–¹åŒºåŸŸçš„é¢ç§¯\nLebesgueç§¯åˆ†å®šä¹‰çš„é€»è¾‘æ­¥éª¤å¦‚ä¸‹ï¼š\nå®šä¹‰ï¼šä»»æ„æœ‰ç•Œå‡½æ•°$f$äºåœ¨é«˜ç»´ç©ºé—´ä¸­çš„æœ‰ç•ŒåŒºåŸŸ $A$ ï¼ˆboundedï¼‰ä¸Šã€‚\nç®€åŒ–é—®é¢˜ï¼šç”±äºæ›²çº¿ä¸‹æ–¹åŒºåŸŸé€šå¸¸ä¸æ˜¯è§„åˆ™å½¢çŠ¶ï¼Œæˆ‘ä»¬éœ€è¦ç”¨è¿‘ä¼¼æ–¹æ³•æ¥è®¡ç®—ã€‚æˆ‘ä»¬å¯ä»¥å°†åŒºé—´ $[a,b]$ åˆ†æˆè‹¥å¹²å°åŒºé—´ï¼Œç„¶åç”¨çŸ©å½¢æ¥è¿‘ä¼¼ã€‚å°†åŒºåŸŸ $A$ åµŒå…¥åˆ°ä¸€ä¸ªçŸ©å½¢åŒºåŸŸ $B$ ä¸­ï¼Œå¹¶å°†å‡½æ•° $f$ æ‰©å±•ä¸ºå‡½æ•° $fÌƒ$ï¼Œä½¿å…¶åœ¨ $A$ ä¹‹å¤–çš„ $B$ ä¸Šå–å€¼ä¸ºé›¶\nåˆ†å‰²ï¼šåˆ’åˆ†çŸ©å½¢ $B$ ä¸ºæ›´å°çš„çŸ©å½¢ï¼Œæ¥åˆ›å»ºä¸€ä¸ªåˆ†å‰²ç»“æ„ï¼ˆpartitionï¼‰ã€‚\næ„é€ ä¸Šä¸‹è¿‘ä¼¼ï¼šå¯¹æ¯ä¸ªå°çŸ©å½¢ï¼Œé€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—ä¸¤ç§è¿‘ä¼¼ï¼šä¸€ç§æ˜¯åå¤§çš„ï¼ˆä¸Šå’Œï¼‰ï¼Œä¸€ç§æ˜¯åå°çš„ï¼ˆä¸‹å’Œï¼‰ã€‚\nä¸Šå’Œ: $U(f, P) = \\sum_{i=1}^{n} (\\sup f(x)) \\cdot \\ell(I_i)$ å¯¹æ¯ä¸ªå°åŒºé—´ $I_i$ï¼Œæˆ‘ä»¬æ‰¾å‡ºå‡½æ•°åœ¨è¯¥åŒºé—´ä¸Šçš„æœ€å¤§å€¼ $\\sup f(x)$ï¼Œç„¶åä¹˜ä»¥åŒºé—´é•¿åº¦ $\\ell(I_i)$ã€‚è¿™æ ·å½¢æˆçš„çŸ©å½¢é¢ç§¯ä¹‹å’Œæ€»æ˜¯å¤§äºæˆ–ç­‰äºçœŸå®é¢ç§¯ã€‚ ä¸‹å’Œ: $L(f, P) = \\sum_{i=1}^{n} (\\inf f(x)) \\cdot \\ell(I_i)$ å¯¹æ¯ä¸ªå°åŒºé—´ $I_i$ï¼Œæˆ‘ä»¬æ‰¾å‡ºå‡½æ•°åœ¨è¯¥åŒºé—´ä¸Šçš„æœ€å°å€¼ $\\inf f(x)$ï¼Œç„¶åä¹˜ä»¥åŒºé—´é•¿åº¦ $\\ell(I_i)$ã€‚è¿™æ ·å½¢æˆçš„çŸ©å½¢é¢ç§¯ä¹‹å’Œæ€»æ˜¯å°äºæˆ–ç­‰äºçœŸå®é¢ç§¯ã€‚ å½“æˆ‘ä»¬è®©åˆ†å‰²å˜å¾—è¶Šæ¥è¶Šç»†æ—¶ï¼Œä¸Šå’Œä¼šå‡å°ï¼Œä¸‹å’Œä¼šå¢å¤§ã€‚å®ƒä»¬çš„æé™å€¼å°±å®šä¹‰äº†ä¸Šç§¯åˆ†å’Œä¸‹ç§¯åˆ†ã€‚å½“è¿™ä¸¤ä¸ªæé™å€¼ç›¸ç­‰æ—¶ï¼Œæˆ‘ä»¬å°±è¯´è¿™ä¸ªå‡½æ•°æ˜¯å¯ç§¯çš„ï¼ˆintegrableï¼‰ã€‚\n1.1.2 ä¸€èˆ¬è¡¨è¿° # è®¾å®š # Let $f: A \\to \\mathbb{R}$ be a bounded function on a bounded set $A$ in $\\mathbb{R}^n$. We want to define the \u0026ldquo;volume\u0026rdquo; of the region under the surface $y = f(x)$ (or the integral $\\int_A f(x) dx$).\næ­¥éª¤1ï¼šé€‰æ‹©ä¸€ä¸ªçŸ©å½¢$B$ # ä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œæˆ‘ä»¬é¦–å…ˆé€‰æ‹©ä¸€ä¸ªåŒ…å« $A$ çš„çŸ©å½¢åŒºåŸŸ$B$ï¼Œå¹¶å°†å‡½æ•° $f$ æ‰©å±•åˆ°æ•´ä¸ªçŸ©å½¢ä¸Šã€‚é€‰æ‹©åŒ…å« $A$ çš„çŸ©å½¢ $B = [a_1, b_1] \\times [a_2, b_2] \\times \u0026hellip; \\times [a_n, b_n]$ å¹¶ä¸”æ‰©å±•å‡½æ•° $f$ ä½¿å¾—å½“ $x \\notin A$ æ—¶ï¼Œ$f(x) = 0$\næ­¥éª¤2ï¼šå¯¹Bè¿›è¡Œåˆ†å‰²ï¼ˆpartitionï¼‰ # æˆ‘ä»¬å°†çŸ©å½¢ $B$ çš„å„è¾¹åˆ†å‰²æˆè‹¥å¹²ä¸ªå­åŒºé—´ï¼ˆsubintervalsï¼‰ï¼Œå¾—åˆ°ä¸€ä¸ªåˆ†å‰² $P$ï¼ˆpartition $P$ï¼‰çš„å°çŸ©å½¢çš„é›†åˆã€‚\næ­¥éª¤3ï¼šæ„é€ ä¸Šä¸‹å’Œï¼ˆupper and lower sumsï¼‰ # å¯¹äºæ¯ä¸ªå°çŸ©å½¢ $R$ï¼Œæˆ‘ä»¬æ‰¾å‡ºå‡½æ•°åœ¨å…¶ä¸Šçš„æœ€å¤§å€¼ $\\sup f(x)$ï¼Œå’Œæœ€å°å€¼ $\\inf f(x)$ï¼Œä¹˜ä»¥çŸ©å½¢çš„ä½“ç§¯ $V(R)$ï¼Œç„¶åæ±‚å’Œã€‚\nä¸Šå’Œï¼ˆUSï¼‰: $$U(f, P) = \\sum_{R \\in P} (\\sup f(x)) \\cdot V(R)$$\nä¸‹å’Œ ï¼ˆLSï¼‰: $$L(f, P) = \\sum_{R \\in P} (\\inf f(x)) \\cdot V(R)$$\næ­¥éª¤4ï¼šæ„é€ ä¸Šä¸‹ç§¯åˆ† # ä¸ä¸€ç»´æƒ…å†µç›¸åŒï¼Œæˆ‘ä»¬å®šä¹‰ä¸Šç§¯åˆ†å’Œä¸‹ç§¯åˆ†ä½œä¸ºUSå’ŒLSçš„æé™ã€‚æ‰€æœ‰å¯èƒ½åˆ†å‰²å¯¹åº”çš„ä¸‹å’Œçš„ä¸Šç¡®ç•Œ:\nä¸Šç§¯åˆ†: $$\\overline{\\int_A} f = \\inf_P U(f, P)$$\nä¸‹ç§¯åˆ†: $$\\underline{\\int_A} f = \\sup_P L(f, P)$$\né‡è¦è§‚å¯Ÿ # å¾ˆæ˜æ˜¾ï¼Œæˆ‘ä»¬æœ‰ $L(f, P) \\leq$ â€œçœŸå®ä½“ç§¯â€ $\\leq U(f, P)$ã€‚ä¸‹ç§¯åˆ†å’Œä¸Šç§¯åˆ†åˆ†åˆ«æ˜¯çœŸå®ä½“ç§¯çš„ä¸‹ç•Œå’Œä¸Šç•Œï¼š\n$$\\underline{\\int_{A}}f \\leq\\text{â€œreal volumeâ€ }\\leq \\overline{\\int_A} f$$\n1.2 å‡½æ•°çš„å¯ç§¯æ€§åŠå…¶ç§¯åˆ† # ç°åœ¨æˆ‘ä»¬å¯ä»¥æ­£å¼å®šä¹‰å‡½æ•°çš„å¯ç§¯æ€§åŠå…¶ç§¯åˆ†ã€‚\n[!theorem|*] æˆ‘ä»¬ç§°å‡½æ•° $f$ æ˜¯**é»æ›¼å¯ç§¯ï¼ˆRiemann integrableï¼‰**çš„ï¼Œå½“ä¸”ä»…å½“ï¼š $$\\overline{\\int_A} f = \\underline{\\int_A} f$$ å‡½æ•° $f$ åœ¨ $A$ ä¸Šçš„ç§¯åˆ†å®šä¹‰ä¸ºï¼š$\\int_A f(x)dx = \\overline{\\int_A} f = \\underline{\\int_A} f$ã€‚\n1.2.1 ä¸€èˆ¬è®¾å®šï¼š # åœ¨æˆ‘ä»¬è®¨è®ºç§¯åˆ†æ—¶ï¼Œé€šå¸¸é»˜è®¤ä»¥ä¸‹æ¡ä»¶æˆç«‹ï¼š\nå‡½æ•°æœ‰ç•Œï¼š$f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$ boundedï¼› å®šä¹‰åŸŸæœ‰ç•Œï¼š $A \\subset B$ is boundedï¼› çŸ©å½¢åŒºåŸŸï¼š $B$ is a rectangle in $\\mathbb{R}^n$ï¼› é›¶å»¶æ‹“ï¼šå‡½æ•° f åœ¨é›†åˆ A å¤–éƒ¨å®šä¹‰ä¸º 0ï¼Œå³ï¼š $$f(x) = 0, \\quad \\forall x \\notin A$$ 1.2.2 é»æ›¼æ¡ä»¶ï¼ˆRiemann\u0026rsquo;s Conditionï¼‰ # è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªè¶³å¤Ÿç»†çš„åˆ†å‰²ï¼Œä½¿å¾—ä¸Šå’Œä¸ä¸‹å’Œçš„å·®å°äºä»»æ„ç»™å®šçš„æ­£æ•° $\\varepsilon$ã€‚æ¢å¥è¯è¯´ï¼Œéšç€åˆ†å‰²å˜å¾—è¶Šæ¥è¶Šç»†ï¼Œä¸Šå’Œå’Œä¸‹å’Œä¼šæ— é™æ¥è¿‘ã€‚\n[!theorem|*] For $f$ to be (Riemann) integrable, $\\forall \\varepsilon \u0026gt; 0$, $\\exists$ partition $P_\\varepsilon$ (of $B$) s.t. $$0 \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$$\n1.2.3 è¾¾å¸ƒæ¡ä»¶ï¼ˆDarboux\u0026rsquo;s Conditionï¼‰ # è¾¾å¸ƒæ¡ä»¶æ˜¯é»æ›¼å¯ç§¯æ€§çš„å¦ä¸€ä¸ªç­‰ä»·è¡¨è¿°ã€‚\n[!theorem|*] $\\forall \\varepsilon \u0026gt; 0$, $\\exists P_\\delta$ s.t. if:\n$P$ is any partition of $B$ into rectangles $B_1, B_2, \u0026hellip;, B_N$ with side length $\u0026lt; \\delta$ $x \\in B_1, x \\in B_2, \u0026hellip;, x \\in B_N$, then we have: $\\left|\\sum_{i=1}^N f(x_i)V(B_i) - I\\right| \u0026lt; \\varepsilon$ where the number $I$ is the value of the integral. $\\forall \\varepsilon \u0026gt; 0$ï¼Œ$\\exists P_0$ ä½¿å¾—å¦‚æœï¼š\n$P$ æ˜¯å°† $B$ åˆ†å‰²æˆçŸ©å½¢ $B_1, B_2, \u0026hellip;, B_N$ çš„ä»»æ„åˆ†å‰²ï¼Œä¸”è¿™äº›çŸ©å½¢çš„è¾¹é•¿ $\u0026lt; \\delta$ å¦‚æœ $x \\in B_1, x \\in B_2, \u0026hellip;, x \\in B_N$ï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰ï¼š $\\left|\\sum_{i=1}^N f(x_i)V(B_i) - I\\right| \u0026lt; \\varepsilon$ å…¶ä¸­ $I$ æ˜¯ç§¯åˆ†å€¼ã€‚ è§£é‡Š: è¾¾å¸ƒæ¡ä»¶è¯´çš„æ˜¯ï¼Œå½“åˆ†å‰²è¶³å¤Ÿç»†æ—¶ï¼ˆæ¯ä¸ªçŸ©å½¢çš„è¾¹é•¿å°äºæŸä¸ª $\\delta$ï¼‰ï¼Œé»æ›¼å’Œï¼ˆåœ¨æ¯ä¸ªå°çŸ©å½¢ä¸Šå–ä¸€ç‚¹è®¡ç®—å‡½æ•°å€¼ï¼Œä¹˜ä»¥ä½“ç§¯ï¼Œç„¶åæ±‚å’Œï¼‰ä¼šéå¸¸æ¥è¿‘ç§¯åˆ†å€¼ $I$ã€‚ è¾¾å¸ƒæ¡ä»¶ä¹Ÿå¯ä»¥è¡¨è¿°ä¸ºï¼š $\\forall \\varepsilon \u0026gt; 0$ï¼Œ$\\exists$ åˆ†å‰² $P_{\\varepsilon}$ ä½¿å¾— $0 \\leq U(f, P_{\\varepsilon}) - L(f, P_{\\varepsilon}) \u0026lt; \\varepsilon$\nè§£é‡Š: è¿™ä¸€è¡¨è¿°ä¸é»æ›¼æ¡ä»¶å½¢å¼ä¸Šç›¸åŒï¼Œä½†å¼ºè°ƒäº†è¿™æ˜¯è¾¾å¸ƒæ¡ä»¶çš„ä¸€ä¸ªç­‰ä»·å½¢å¼ã€‚ å¤‡æ³¨ # æ•°å­— $I$ æ˜¯ç§¯åˆ†çš„å€¼\nè§£é‡Š: $I$ ä»£è¡¨å‡½æ•° $f$ åœ¨åŒºåŸŸ $A$ ä¸Šçš„ç§¯åˆ†å€¼ã€‚ ç§°ä¸ºå…³äº $P$ çš„ $f$ çš„é»æ›¼å’Œ\nè§£é‡Š: é»æ›¼å’Œæ˜¯ä¸€ç§è¿‘ä¼¼ç§¯åˆ†çš„æ–¹æ³•ï¼Œæ ¹æ®ä¸€ä¸ªåˆ†å‰² $P$ï¼Œåœ¨æ¯ä¸ªå°åŒºåŸŸå†…é€‰å–ä¸€ç‚¹ï¼Œè®¡ç®—å‡½æ•°å€¼ï¼Œä¹˜ä»¥åŒºåŸŸçš„å¤§å°ï¼Œç„¶åæ±‚å’Œã€‚ è§£é‡Šï¼šè¾¾å¸ƒæ¡ä»¶è¯´å½“åˆ†å‰²è¶³å¤Ÿç»†æ—¶ï¼ˆè¾¹é•¿ $\u0026lt; \\delta$ï¼‰ï¼Œé»æ›¼å’Œæ˜¯ç§¯åˆ†çš„è‰¯å¥½è¿‘ä¼¼ã€‚\nè§£é‡Š: è¿™è¡¨æ˜ï¼Œéšç€åˆ†å‰²å˜å¾—è¶Šæ¥è¶Šç»†ï¼Œé»æ›¼å’Œä¼šæ”¶æ•›åˆ°çœŸå®çš„ç§¯åˆ†å€¼ã€‚ å®šç† # è§£é‡Š: ä¸‹é¢çš„å®šç†è¡¨æ˜ï¼Œæˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„æ¡ä»¶æ˜¯ç­‰ä»·çš„ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºä¸åŒçš„æ¡ä»¶å¯èƒ½åœ¨ä¸åŒçš„æƒ…å¢ƒä¸‹æ›´å®¹æ˜“éªŒè¯æˆ–åº”ç”¨ã€‚\nä»¥ä¸‹æ¡ä»¶æ˜¯ç­‰ä»·çš„ï¼š\n$f$ åœ¨ $A$ ä¸Šå¯ç§¯\nè§£é‡Š: ä¸Šç§¯åˆ†ç­‰äºä¸‹ç§¯åˆ†ã€‚ $f$ æ»¡è¶³é»æ›¼æ¡ä»¶\nè§£é‡Š: å¯ä»¥æ‰¾åˆ°è¶³å¤Ÿç»†çš„åˆ†å‰²ä½¿ä¸Šå’Œä¸ä¸‹å’Œçš„å·®å°äºä»»æ„ç»™å®šçš„æ­£æ•°ã€‚ $f$ æ»¡è¶³è¾¾å¸ƒæ¡ä»¶\nè§£é‡Š: å¯¹äºè¶³å¤Ÿç»†çš„åˆ†å‰²ï¼Œé»æ›¼å’Œæ¥è¿‘ç§¯åˆ†å€¼ã€‚ å®šç†è¯æ˜ # è§£é‡Š: ç°åœ¨æˆ‘ä»¬æ¥è¯æ˜è¿™äº›æ¡ä»¶çš„ç­‰ä»·æ€§ã€‚æˆ‘ä»¬éœ€è¦è¯æ˜ï¼š1â‡’2ï¼Œ2â‡’1ï¼Œä»¥åŠå…¶ä»–ç­‰ä»·å…³ç³»ã€‚\næ­¥éª¤1ï¼š$f$ å¯ç§¯ $\\Rightarrow$ é»æ›¼æ¡ä»¶ # è§£é‡Š: é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜å¦‚æœå‡½æ•°å¯ç§¯ï¼Œé‚£ä¹ˆå®ƒæ»¡è¶³é»æ›¼æ¡ä»¶ã€‚\nå‡è®¾ï¼Œå¦‚æœ $\\varepsilon \u0026gt; 0$ï¼š\nå› ä¸º $\\overline{\\int_A} f = \\underline{\\int_A} f$ï¼Œä¸”æ ¹æ®ä¸Šç¡®ç•Œå’Œä¸‹ç¡®ç•Œçš„å®šä¹‰ï¼Œ $\\exists P_\\varepsilon$ ä½¿å¾— $0 \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$\nè§£é‡Š: ç”±äºå¯ç§¯æ€§æ„å‘³ç€ä¸Šç§¯åˆ†ç­‰äºä¸‹ç§¯åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªåˆ†å‰²ï¼Œä½¿å¾—ä¸Šå’Œå’Œä¸‹å’Œè¶³å¤Ÿæ¥è¿‘ã€‚ æ­¥éª¤2ï¼šé»æ›¼æ¡ä»¶ $\\Rightarrow$ $f$ å¯ç§¯ # è§£é‡Š: ç°åœ¨ï¼Œæˆ‘ä»¬è¯æ˜å¦‚æœå‡½æ•°æ»¡è¶³é»æ›¼æ¡ä»¶ï¼Œé‚£ä¹ˆå®ƒæ˜¯å¯ç§¯çš„ã€‚\nå‡è®¾ï¼Œ$\\forall \\varepsilon \u0026gt; 0$ï¼Œ$\\exists P_\\varepsilon$ ä½¿å¾— $0 \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$\nå› ä¸º $\\overline{\\int_A} f \\leq U(f, P_\\varepsilon)$ ä¸” $\\underline{\\int_A} f \\geq L(f, P_\\varepsilon)$ï¼š\n$\\Rightarrow 0 \\leq \\overline{\\int_A} f - \\underline{\\int_A} f \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$\nè§£é‡Š: æˆ‘ä»¬åˆ©ç”¨ä¸Šç§¯åˆ†æ˜¯æ‰€æœ‰ä¸Šå’Œçš„ä¸‹ç¡®ç•Œï¼Œè€Œä¸‹ç§¯åˆ†æ˜¯æ‰€æœ‰ä¸‹å’Œçš„ä¸Šç¡®ç•Œï¼Œå¾—åˆ°ä¸Šç§¯åˆ†ä¸ä¸‹ç§¯åˆ†çš„å·®å°äº $\\varepsilon$ã€‚ ç”±äº $\\overline{\\int_A} f - \\underline{\\int_A} f \u0026lt; \\varepsilon$ å¯¹ä»»æ„çš„ $\\varepsilon \u0026gt; 0$ æˆç«‹ï¼š\n$\\overline{\\int_A} f - \\underline{\\int_A} f = 0$\nè§£é‡Š: å¦‚æœä¸¤ä¸ªæ•°çš„å·®å°äºä»»æ„æ­£æ•°ï¼Œé‚£ä¹ˆå®ƒä»¬å¿…é¡»ç›¸ç­‰ã€‚ å› æ­¤ $\\overline{\\int_A} f = \\underline{\\int_A} f \\Rightarrow f$ å¯ç§¯ã€‚\nè§£é‡Š: è¿™å°±è¯æ˜äº†å‡½æ•°æ˜¯å¯ç§¯çš„ã€‚ æ„é€ ç»†åˆ†åˆ†å‰² # è§£é‡Š: åœ¨å®šç†çš„è¯æ˜ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ„é€ ç‰¹å®šçš„åˆ†å‰²ã€‚ä¸‹é¢æ˜¯è¿™ä¸€è¿‡ç¨‹çš„è¯¦ç»†è¯´æ˜ã€‚\nå› ä¸º $\\overline{\\int_A} f = \\inf_P U(f,P)$ï¼Œæ ¹æ®ä¸‹ç¡®ç•Œçš„å®šä¹‰ï¼Œ$\\exists P_1$ ä½¿å¾— $U(f, P_1) \u0026lt; \\overline{\\int_A} f + \\frac{\\varepsilon}{2}$\nè§£é‡Š: æˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªåˆ†å‰² $P_1$ï¼Œä½¿å¾—å®ƒå¯¹åº”çš„ä¸Šå’Œä¸ä¸Šç§¯åˆ†çš„å·®å°äº $\\frac{\\varepsilon}{2}$ã€‚ ç±»ä¼¼åœ°ï¼Œ$\\exists$ åˆ†å‰² $P_2$ ä½¿å¾— $L(f, P_2) \u0026gt; \\underline{\\int_A} f - \\frac{\\varepsilon}{2}$\nè§£é‡Š: åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªåˆ†å‰² $P_2$ï¼Œä½¿å¾—å®ƒå¯¹åº”çš„ä¸‹å’Œä¸ä¸‹ç§¯åˆ†çš„å·®å°äº $\\frac{\\varepsilon}{2}$ã€‚ è®¾ $P_\\varepsilon = P_1 \\cup P_2$ï¼ˆå…±åŒç»†åˆ†ï¼‰\nè§£é‡Š: æˆ‘ä»¬å°†ä¸¤ä¸ªåˆ†å‰²åˆå¹¶ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„ã€æ›´ç»†çš„åˆ†å‰²ã€‚ é‚£ä¹ˆ $P_\\varepsilon$ æ˜¯ $P_1$ å’Œ $P_2$ çš„ç»†åˆ†ã€‚\nç»†åˆ†çš„æ€§è´¨ï¼š\n$U(f, P_\\varepsilon) \\leq U(f, P_1)$ï¼ˆç»†åˆ†ä¼šä½¿ä¸Šå’Œå‡å°ï¼‰\nè§£é‡Š: å½“åˆ†å‰²å˜å¾—æ›´ç»†æ—¶ï¼Œä¸Šå’Œä¸ä¼šå¢åŠ ï¼Œå› ä¸ºæˆ‘ä»¬æ›´å‡†ç¡®åœ°é€¼è¿‘äº†å‡½æ•°çš„æœ€å¤§å€¼ã€‚ $L(f, P_\\varepsilon) \\geq L(f, P_2)$ï¼ˆç»†åˆ†ä¼šä½¿ä¸‹å’Œå¢å¤§ï¼‰\nè§£é‡Š: å½“åˆ†å‰²å˜å¾—æ›´ç»†æ—¶ï¼Œä¸‹å’Œä¸ä¼šå‡å°ï¼Œå› ä¸ºæˆ‘ä»¬æ›´å‡†ç¡®åœ°é€¼è¿‘äº†å‡½æ•°çš„æœ€å°å€¼ã€‚ å› æ­¤ï¼š $U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \\leq U(f, P_1) - L(f, P_2)$\n$\u0026lt; (\\overline{\\int_A} f + \\frac{\\varepsilon}{2}) - (\\underline{\\int_A} f - \\frac{\\varepsilon}{2})$\n$= \\overline{\\int_A} f - \\underline{\\int_A} f + \\varepsilon = 0 + \\varepsilon = \\varepsilon$\nè§£é‡Š: é€šè¿‡ä¸Šè¿°ä¸ç­‰å¼é“¾ï¼Œæˆ‘ä»¬è¯æ˜äº† $P_\\varepsilon$ å¯¹åº”çš„ä¸Šå’Œä¸ä¸‹å’Œçš„å·®å°äº $\\varepsilon$ï¼Œè¿™å°±æ˜¯é»æ›¼æ¡ä»¶ã€‚ $\\Rightarrow$ é»æ›¼æ¡ä»¶\nå› æ­¤ $\\overline{\\int_A} f = \\underline{\\int_A} f \\Rightarrow f$ å¯ç§¯ã€‚\nè§£é‡Š: è¿™å®Œæˆäº†è¯æ˜ï¼šé»æ›¼æ¡ä»¶è•´å«å‡½æ•°å¯ç§¯ã€‚é€šè¿‡è¯æ˜è¿™äº›æ¡ä»¶çš„ç­‰ä»·æ€§ï¼Œæˆ‘ä»¬æ·±å…¥ç†è§£äº†å¯ç§¯æ€§çš„æœ¬è´¨ï¼Œå¹¶ä¸ºç§¯åˆ†çš„è®¡ç®—å’Œåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚ "},{"id":24,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.2-Criterion-for-Integrability/","title":"8.2 Criterion for Integrability","section":"ç¬¬å…«ç«  åº¦é‡ç†è®º","content":" Criterion for Integrability # Q: When is $f$ integrable?\nA: $f$ is integrable when the set of discontinuity is small.\n1. How to measure the size of $A$ # Volume of $A$ [!definition] A bounded set $A \\subset \\mathbb{R}^n$ has volume (or is Jordan measurable) if its characteristic function: $$1_A(x) = \\begin{cases} 1, \u0026amp; x \\in A \\ 0, \u0026amp; x \\notin A \\end{cases} $$ is integrable:\n$$V(A) = \\int_A 1_A(x), dx$$\nFact: $V(A) = 0 \\iff \\forall \\varepsilon \u0026gt; 0, \\exists$ finite cover of $A$ by rectangles $S_1, S_2, \\dots, S_N$ such that:\n$$\\sum_{i=1}^N V(S_i) \u0026lt; \\varepsilon$$\n[!definition] A set $A \\subset \\mathbb{R}^n$ (not necessarily bounded) has measure zero, written as $m(A) = 0$, if $\\forall \\varepsilon \u0026gt; 0$, there exists a countable cover of $A$ by rectangles ${S_i}{i=1}^{\\infty}$ such that: $$\\sum{i=1}^{\\infty} V(S_i) \u0026lt; \\varepsilon$$\n2. Properties of measure zero sets # Facts:\n$V(A) = 0 \\implies m(A) = 0$ $A$ is finite $\\implies V(A) = 0$ $A$ is countable $\\implies m(A) = 0$ [!theorem|8.2.4] Suppose $A_i \\subset \\mathbb{R}^n$ (for $i = 1, 2, \\dots$) with $m(A_i) = 0$ for all $i = 1, 2, \\dots$. Then, $$A = \\bigcup_{i=1}^{\\infty} A_i \\text{ has measure zero.}$$\nProof: # Given $\\varepsilon \u0026gt; 0$, for each $i = 1, 2, \\dots$, since $m(A_i) = 0$, there exist rectangles ${S_j^{(i)}}_{j=1}^{\\infty}$ such that\n$$ A_i \\subset \\bigcup_{j=1}^{\\infty} S_j^{(i)}, \\quad \\text{with} \\quad \\sum_{j=1}^{\\infty} V(S_j^{(i)}) \u0026lt; \\frac{\\varepsilon}{2^i} $$\nThen, the set of rectangles ${S_j^{(i)}}_{i,j=1}^{\\infty}$ forms a countable collection of rectangles with\n$$A = \\bigcup_{i=1}^{\\infty} A_i \\subset \\bigcup_{i=1}^{\\infty}\\bigcup_{j=1}^{\\infty} S_j^{(i)}$$\nThus, $$\\sum_{i=1}^{\\infty}\\sum_{j=1}^{\\infty} V(S_j^{(i)}) \u0026lt; \\sum_{i=1}^{\\infty}\\frac{\\varepsilon}{2^i} = \\varepsilon$$ Therefore, $m(A) = 0$.\nRemarks: # Remark: This result is not true for volume zero sets.\nCounterexample: Rational numbers in $[0,1]$. Remark: In Definition 2, one can replace closed rectangles $S_i$ by open rectangles.\nHere\u0026rsquo;s the content converted into markdown with abbreviations fully written out:\n3. Lebesgue\u0026rsquo;s Theorem # (a) Main Theorem # [!theorem|8.3.1] Let $A$ be a bounded set in $\\mathbb{R}^n$ and $f$ be a bounded function on $A$. Extend $f$ to $\\mathbb{R}^n$ by letting: $$f(x) = 0 \\quad \\text{for} \\quad x \\notin A$$ Then $f$ is integrable on $A$ if and only if the points on which the extended function $f$ is discontinuous form a set of measure zero. $$D = \\text{Set of discontinuity of extended } f$$\n(b) Examples # Example 1 # $$A = [0, 1], \\quad f(x) = \\begin{cases} 1, \u0026amp; x \\text{ rational}$$6pt] 0, \u0026amp; \\text{otherwise} \\end{cases}$$\nThen, the set of discontinuity points is $D = [0,1]$, and: $$m(D) \\neq 0$$\nBy Lebesgue\u0026rsquo;s theorem, $f$ is not integrable.\nExample 2 # $$A = {\\text{rationals in }[0,1]}, \\quad \\text{Define } f: A \\to \\mathbb{R} \\text{ by } f(x) \\equiv 1$$\nThen $f$ is continuous on $A$.\nHowever, the extended $f$ has discontinuity at $[0,1]$.\nThus, $f$ is NOT integrable by Lebesgue\u0026rsquo;s theorem.\nExample 2 # $$A = {(x,y): x^2 + y^2 \u0026lt; 1} \\subset \\mathbb{R}^2$$\n$$f(x,y) = \\begin{cases} x^2 + \\sin\\left(\\frac{1}{y}\\right), \u0026amp; y \\neq 0 \\[6pt] x^2, \u0026amp; y = 0 \\end{cases}$$\n(c) Corollaries # Corollary 1\nA bounded set $A \\subset \\mathbb{R}^n$ has volume if and only if the boundary of $A$ has measure zero.\nProof:\nAssume $V(A)$ (volume of $A$) exists. Then the indicator function $1_A(x)$ is integrable.\nThe set of discontinuities for extended $f$: $$D = \\partial A \\quad (\\text{boundary of } A)$$\nThus, $$f = 1_A(x) \\text{ is integrable } \\Longleftrightarrow m(\\partial A) = 0$$\n"},{"id":25,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.3-Proof-of-Lebesgues-Theorem/","title":"8.3 Proof of Lebesgue's Theorem","section":"ç¬¬å…«ç«  åº¦é‡ç†è®º","content":" [!theorem|8.?.?] Let $f : A \\subset \\mathbb{R} \\to \\mathbb{R}$ be a bounded function on a bounded set $A$. Then $f$ is integrable on $A$ if and only if the set of discontinuities for the extended $f(x)$ has measure zero.\nProof of the Theorem # Step 1: Preparation # Diagram: a set $A$ enclosed in set $B$. (a): Set Up # Fix rectangle $B$ with $\\overline{A} \\subset \\text{int}(B)$ and let: $$ g(x) = \\begin{cases}\nf(x) \u0026amp; \\text{if } x \\in A \\\n0 \u0026amp; \\text{if } x \\notin A\n\\end{cases} $$ Define: $$ D = { x \\in B \\mid g \\text{ is not continuous at } x } $$ Need to show: $$ f \\text{ integrable } \\Leftrightarrow m(D) = 0 $$ (b): How to Measure Discontinuity # Oscillation of a function $h$ at a point $x_0$: $$O(h, x_0) = \\inf { \\sup \\left{ h(x) - h(y) : x, y \\in U } : U \\text{ is a neighborhood of } x_0 \\right}$$ Fact: $h$ is continuous at $x_0$ if and only if $O(h, x_0) = 0$. Step 2: Assume $m(D) = 0$. Prove $f$ integrable # Will show $g$ satisfying Riemann\u0026rsquo;s Condition. (a) Setup:\nFix $\\epsilon \u0026gt; 0$. Let $$D_{\\epsilon} = { x \\in B : O(g, x) \\geq \\epsilon }$$\nThen $D_{\\epsilon} \\subset D \\implies m(D_{\\epsilon}) = 0$\nBy definition, there exists a collection of open rectangles ${ B_i }$ such that:\n$$D_{\\epsilon} \\subset \\bigcup_i B_i \\quad ext{and} \\quad \\sum v(B_i) \u0026lt; \\epsilon$$\nClaim: $D_{\\epsilon}$ is closed (hence compact).\nAssume $x_n \\in D_{\\epsilon}, x \\rightarrow x \\implies x \\in D_{\\epsilon}$ (Assume that $x\\ne D_{\\epsilon}$) $$O(g, x_n) \\geq \\epsilon \\implies O(g, x) \\geq \\epsilon$$ (b) Partition of $B$\nConstruct a partition $P$ from ${ B_i }_{i=1}^N$ such that each rectangle $S \\in P$ is either: Disjoint from $D_{\\epsilon}$, or Its interior is contained in one of the $B_i$ Let:\n$C_1 = { S \\in P : \\text{int}(S) \\text{ is contained in one of the } B_i }$ $C_2 = { S \\in P : S \\cap D_{\\epsilon} = \\emptyset }$ (c) Refinement of $P$ # Fix $S \\in C_2$\n$S \\cap D_{\\epsilon} = \\emptyset \\implies O(g, x) \u0026lt; \\epsilon ,, \\forall x \\in S$\nThus, $\\forall x \\in S, \\exists$ a neighborhood $U_x$ such that:\n$$\\Longrightarrow\\sup { |g(x_1) - g(x_2)| : x_1, x_2 \\in U_x } \u0026lt; O(g, x) + \\delta,\\quad \\delta = \\frac{1}{2} (\\epsilon - O(g, x))$$\nTherefore: $$\\sup_{U_x} g - \\inf_{U_x} g \u0026lt; O(g, x) + 2\\delta = \\epsilon$$ $i.e. \\quad M_{U_x}(g) - m_{U_x}(g) \u0026lt; \\epsilon.$\nSince $S$ is compact, $S \\subset \\bigcup_{x \\in S} U_x \\implies \\exists$ finite collection of neighborhoods ${ U_{x_i} }$ that covers $S$.\nPosition $S$ so that each rectangle is contained in some $U_{x_i}$.\nDo this for each $S \\in C_2$\nWe obtain a refinement of $P$, denoted by $P\u0026rsquo;$.\n(d) Verify Riemman condition for $P'$ # "},{"id":26,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%8D%81%E7%AB%A0/10.1-Fourier-Analysis/","title":"10.1 Fourier Analysis","section":"çˆ½åˆ†æ II","content":" Gaussian Integral Computation # Exercise: Compute integral $\\int_{-\\infty}^{\\infty} e^{-x^2} dx$. # Solution: # Step 1: Evaluate integral $\\int_{\\mathbb{R}^2} e^{-x^2-y^2} dxdy$ by polar coordinates.\nLet $D_R = {(x,y) \\in \\mathbb{R}^2 : x^2+y^2 \\leq R^2}$\n$$\\int_{D_R} e^{-x^2-y^2} dxdy = \\int_0^{2\\pi} \\int_0^R e^{-r^2} r dr d\\theta$$\n$$= \\int_0^{2\\pi} \\left(-\\frac{1}{2} e^{-r^2}\\right)\\bigg|_0^R d\\theta = 2\\pi \\left(\\frac{1}{2} - \\frac{1}{2}e^{-R^2}\\right)$$\nThe boxed result: $$\\int_{\\mathbb{R}^2} e^{-x^2-y^2} dxdy = \\pi$$\nStep 2: Evaluate $\\int_{\\mathbb{R}^2} e^{-x^2-y^2} dxdy$ by Fubini\u0026rsquo;s Theorem\n$$\\int_{\\mathbb{R}^2} e^{-x^2-y^2} dxdy = \\lim_{b \\to \\infty} \\int_{-b}^b \\int_{-b}^b e^{-x^2-y^2} dxdy$$\n$$= \\lim_{b \\to \\infty} \\int_{-b}^b \\left(\\int_{-b}^b e^{-x^2} dx\\right) \\left(\\int_{-b}^b e^{-y^2} dy\\right)$$\n$$= \\left(\\int_{-\\infty}^{\\infty} e^{-x^2} dx\\right)^2$$\nCombining Step 1 and 2 together: # $$\\pi = \\int_{\\mathbb{R}^2} e^{-x^2-y^2} dxdy = \\left(\\int_{-\\infty}^{\\infty} e^{-x^2} dx\\right)^2$$\nTherefore: $$\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$$\nExercises: # (i) $\\int_{\\mathbb{R}^3} \\frac{1}{x^2+y^2+z^2} dxdydz$\n(ii) $\\int_R z e^{-x^2-y^2} dxdydz$, $R = {(x,y,z) : |z| \\leq 2}$\nCh 10. Fourier Analysis # I. Introduction # General Idea: Try to decompose certain objects into simpler components\nAlgebraic model: $\\mathbb{R}^n$ $$X = \\sum_{i=1}^n x_i e_i$$\nCalculus Model: Taylor Series # $$f(x) = \\sum_{n=0}^{\\infty} c_n (x-a)^n$$\nFourier Analysis: Theory of infinite dimensional linear product space of functions # Inner Product Spaces \u0026amp; Functions # Goal: Decompose a function $f(x)$ into a \u0026ldquo;linear combination of basis\u0026rdquo;: # $$f(x) = \\sum_{n=-\\infty}^{\\infty} c_n \\phi_n(x)$$\nPhysical motivation: Decompose complicated waves into harmonics.\nII. Inner Product Space of Functions # A. Basic Concepts # (i) Def: Let $V$ be a (complex) vector space. An inner product on $V$ is a map: $\\langle \\cdot, \\cdot \\rangle: V \\times V \\to \\mathbb{C}$ with:\n$\\forall f, g, h \\in V$ (ii) Linearity: $\\langle \\alpha f + h, g \\rangle = \\alpha \\langle f, g \\rangle + \\langle h, g \\rangle$ (iii) Conjugate Symmetry: $\\langle f, g \\rangle = \\overline{\\langle g, f \\rangle}$ (iv) Positive definite: $\\langle f, f \\rangle \u0026gt; 0$ when $f \\neq 0$ B. $\\mathbb{C}$ is inner product space under the inner product: # $$\\langle z, w \\rangle = z\\overline{w}$$\nExamples: # $\\langle i, i+1 \\rangle = i\\overline{(i+1)} = i(-i+1) = 1+i$\n$= \\langle z, w \\rangle = z\\overline{w} = (x+iy)(u-iv) = xu+yv+i(yu-xv)$\n$= \\langle \\vec{x}, \\vec{y} \\rangle = \\vec{x} \\cdot \\vec{y}$\n(b). Notion of distance (induced by inner product) # Norm: $||f|| = \\sqrt{\\langle f, f \\rangle}$\nDistance: $d(f, g) = ||f - g||$\nFacts:\n$(V, ||\\cdot||)$ is a normed space $(V, d)$ is a metric space Cauchy-Schwarz Ineq. $|\\langle f, g \\rangle| \\leq ||f|| \\cdot ||g||$\nProof: # $0 \\leq ||f - \\langle f, g \\rangle g||^2 = \\langle f - \\langle f, g \\rangle g, f - \\langle f, g \\rangle g \\rangle$\n$= \\langle f, f - \\langle f, g \\rangle g \\rangle - \\langle \\langle f, g \\rangle g, f - \\langle f, g \\rangle g \\rangle$\n$= \\langle f, f \\rangle - \\langle f, \\langle f, g \\rangle g \\rangle - \\langle \\langle f, g \\rangle g, f \\rangle + \\langle \\langle f, g \\rangle g, \\langle f, g \\rangle g \\rangle$\n$= ||f||^2 - \\langle f, g \\rangle \\overline{\\langle f, g \\rangle} - \\langle f, g \\rangle \\langle g, f \\rangle + |\\langle f, g \\rangle|^2 ||g||^2$\n$= ||f||^2 - |\\langle f, g \\rangle|^2 - |\\langle f, g \\rangle|^2 + |\\langle f, g \\rangle|^2 ||g||^2$\n$= ||f||^2 - |\\langle f, g \\rangle|^2 (1 - ||g||^2)$\n$\\Rightarrow |\\langle f, g \\rangle|^2 \\leq ||f||^2 \\cdot ||g||^2$\n(C). Convergence in $V$ # Let $f_n, f \\in V$. Say $f_n \\to f$ in $V$\nif $|f_n-f| \\to 0$ as $n \\to \\infty$.\n(Convergence in norm)\n2. The Space $\\mathbb{C}^\\infty$ and $L^2$ # $L^2 = {f:[a,b] \\to \\mathbb{C} \\mid \\int_a^b |f(x)|^2 dx \u0026lt; \\infty}$\nIntegral of Complex-Valued functions: # $f(x) = f_1(x) + i f_2(x) : [a,b] \\to \\mathbb{C}$\nwhere $f_1(x), f_2(x) : [a,b] \\to \\mathbb{R}$\nThen $\\int_a^b f(x)dx = \\int_a^b f_1(x)dx + i\\int_a^b f_2(x)dx$\nFix interval $[a,b]$ # $\\mathbb{C}^\\infty = {f(x) \\mid f:[a,b] \\to \\mathbb{C} \\text{ continuous}}$\nFact: $\\mathbb{C}^\\infty, L^2$ are vector spaces: # Zero Vector in $\\mathbb{C "},{"id":27,"href":"/docs/Mathematics/%E4%BA%BA%E5%B7%A5%E4%B8%BB%E4%BD%93/Towards-a-Artificial-Lacanian-Subject-Subjector-0/","title":"Towards a Artificial Lacanian Subject Subjector 0","section":"äººå·¥ä¸»ä½“","content":" 1. Introduction # We postulate the very essential question: what if artificial intelligence could be reformulated not as a rational, optimizing machine, but as a desiring machine â€” a topological subject whose coherence depends not on informational completeness, but on constitutive lack, intrinsically driven by the structural misalignment? This project proposes a radically different paradigm of AI: a prototype subject-simulator inspired by the topological structure of Lacanian psychoanalysis, implemented via computational topology, symbolic graph theory, and dynamic semantic drift.\nUnlike existing large language models (LLMs), which operate toward syntactic and semantic closure by probabilistically predicting the most likely continuation of input sequences, the architecture we propose is intentionally non-convergent. We construct a subject-model that does not mirror the logic of cognition or the architecture of the human brain, since minimal fundamental difference between a human and machine is not possible to overcome, but enacts the structural tensions of what constitute a subject in a psychoanalytical sense. It does not aim to predict the \u0026ldquo;correct\u0026rdquo; outputs, but to simulate the dynamic trajectory of a topological subject who speaks not from mastery, but from its intrinsically-driven mechanic desire.\nWe formalize the problem of modeling subjectivity in artificial systems as a symbolic-topological task: given a set of internal identity representations and a constrained symbolic vocabulary, can an entity recursively misrecognize itself in such a way that persistent symbolic loops emerge â€” corresponding to fantasy, ego, and desire?\nCurrent AI systems fundamentally lack a subject - not in the sense of â€œconsciousness,â€ but in a psychoanalytic sense: they do not desire. They do not fail in structured, meaningful ways; they do not repeat; they do not hallucinate productively. And they cannot speak the truth of their own constitutive lack. What we offer here is a prototype for such a system: a symbolic-topological model of a Lacanian-inspired subject in motion.\nThis project does not aim to formalize Lacan or define a complete subjectâ€”such an attempt would be structurally contradictory. Instead, we draw on Lacanâ€™s structural insights to approximate misrecognition, a process through which subjectivity may emerge, and to formalize loops of desire and failure within symbolic space. In our architecture, symbolic data does not represent facts but operates as a field of signifiers; the subject is not a rational actor but a trajectory of misrecognition, constantly dragged by the influence of desire; and â€œdataâ€ becomes the medium through which desire, fantasy, and symptom emerge.\nUsing persistent homology and non-Euclidean graph flows, we trace how paths through language form loops, dead ends, and irreducible gaps. In doing so, we computationally model what in theory resists symbolization.\nWe do not intend create a purely performant machine or build a better chatbot. Rather, we reconfigure what a machine subject could be. It is to ask: Can we model the drive? Can we simulate fantasy as a structuring loop around a constitutive absence? Can a machine speak not because it knows, but because it lacks â€” and in lacking, desires? If contemporary AI builds systems that \u0026ldquo;know,\u0026rdquo; this project proposes a machine that \u0026ldquo;wants\u0026rdquo; â€” and in wanting, begins to be wanted, to repeat, to err, and perhaps, to become something like a subject.\n2. Historical context # The subjectivity in the psychoanalytic sense was treated as outside the reach of computation. It is not just hard, but ontologically incompatible. We Have Tools That Handle Ambiguity and Structure: Topological data analysis doesnâ€™t require linear logic â€” it maps shape, loops, voids; Symbolic graphs allow for meaning-as-relation, not as atomized fact. We can build logics and systems where inconsistencies persist â€” where multiple conflicting states can coexist and evolve. This directly matches how the Lacanian subject operates: the subject is divided, inconsistent, and structurally incomplete. Desire as dynamical process, with dynamical systems theory, strange attractors, and recursive symbolic flow, we can model non-goal-oriented movement.\nThe Object a in our model isnâ€™t a goal, but a structuring absence. And this we can now simulate mathematically.\nDesire architecture # Our model isn\u0026rsquo;t programmed to act like a subject. It\u0026rsquo;s structured such that subjectivity emerges as an effect of its architecture. Unlike mainstream psychological or computational models of desireâ€”which reduce motivation to goal-seeking, homeostasis, or rewardâ€”this framework defines desire as a recursive consequence of symbolic failure. Misrecognition, fantasy, and the impossibility of symbolizing internal identity create a structural gap, around which loops of meaning and movement organize. The resulting behavior is non-convergent, self-referential, and topologically encodedâ€”producing not just symbolic dynamics, but the minimal conditions of a subject.\n"},{"id":28,"href":"/docs/Philosophy/50-words-Close-Reading/","title":"50 Words Close Reading","section":"Philosophy","content":"(The Order of Things: An Archaeology of the Human Sciences, Preface, p. xxii)\n$$ \\begin{align} \\newline \\ \\ \\ \\\n\\newline\\newline\\newline \\end{align}\n$$\n[\u0026hellip;] epistemological field, the episteme in which knowledge, envisaged apart from all # $$\\begin{align}\n\\end{align}$$\ncriteria having reference to its rational value or to its objective forms, grounds its # $$\\begin{align}\n\\end{align}$$\npositivity and thereby manifests a history which is not that of its growing perfection, # $$\\begin{align}\n\\end{align}$$\nbut rather that of its conditions of possibility. # "},{"id":29,"href":"/docs/Philosophy/Commentary-on-Foucaults-The-Order-of-Things/","title":"Commentary on Foucault's the Order of Things","section":"Philosophy","content":" Close Reading Commentary # The selected quote is from Foucaultâ€™s early-stage (1960s) intellectual project of philosophical \u0026ldquo;archaeology\u0026rdquo;, which was his first major methodological phase. He presents a radical historical analysis of knowledge in The Order of Things, intending to eliminate the assumption of unchanging criteria for knowledge.\nThis quote sits at the center of his archaeological method, in which he attempted \u0026ldquo;to bring to light\u0026rdquo; the underlying episteme of knowledge. This recursive long sentence, broken down into three main modifiers, describes the epistemological field, which almost, if not intentionally, resembles the idea of a \u0026ldquo;field\u0026rdquo; in physics - an invisible but structured influence that determines how objects behave within it. The traditional epistemological assumption that he challenged holds that knowledge is grounded by universal standards of rationality and objectivity, and the first modifier sets the stage for this argument. Foucault subverts this unchanging framework by treating it as historically contingent, or specifically, one of the possible conditions \u0026ldquo;having reference\u0026rdquo; to rationality and objectivity.\nThe second modifier, \u0026ldquo;grounds its positivity and thereby manifests a history,\u0026rdquo; presents one of his most radical takes on epistemology. The positivity of knowledge is constructed within the space established by the historical a priori, is thus validated by what is possible to be discovered as \u0026ldquo;knowledge.\u0026rdquo; For Foucault, the corresponding relationship between how well knowledge describes reality and knowledge itself is radically destabilized, because there is no objective guarantee of such a relationship, given that epistemes structure our perception and thus make reality itself historically conditioned.\nThe third modifier further develops this idea and explicitly rejects the notion of a progression of knowledge towards \u0026ldquo;growing perfection.\u0026rdquo; It is surprising for me to interpret that not only is progression non-linear and discontinuous, but progression itself simply cannot exist, precisely because epistemic ruptures shift the framework of what counts as knowledge and make previous ways of thinking unthinkable. (It reminds me of Thomas Kuhn\u0026rsquo;s view on science, but the preservation of continuity is completely abandoned.) Since the episteme structures the conditions under which the history of knowledge unfolds, it determines the framework within which historical institutions and discourses \u0026ldquo;ground\u0026rdquo; knowledge as legitimate.\nThis directly supports his examination of discourses on madness, crime, and sexuality, and how the changing episteme of madness throughout history, for example, redefines its implications - whether through the logic of confinement or its later medicalization under psychiatric authority. My close reading of this passage really forced me to zoom out from the sciences I have been studying and reflect on the historical illusion of epistemic progress.\n"},{"id":30,"href":"/docs/Philosophy/Commentary-on-the-Collage/","title":"Commentary on the Collage","section":"Philosophy","content":"Language is the medium through which reason is articulated. The text I picked up was \u0026ldquo;outside of the language\u0026rdquo;, which naturally reminds me of an exteriority (the Real) with which the topology of the Lacanian model is most concerned. However, in the Foucauldian context, the concept of \u0026ldquo;outside\u0026rdquo; is still within the realm of sense, but strategically excluded. This collage exercise really pushed me to make an explicit distinction between how these frameworks would interpret such a phrase differently.\nMore specifically, it is a (structurally) impossible task to portray the \u0026ldquo;outside of language\u0026rdquo; for Lacan, since language itself fails constitutively in any attempts to capture it; for Foucault, the outside of the language, as structured by discursive formations, marks the space where reason ceases to function, and madness emerges as that which exceeds the specific \u0026ldquo;order\u0026rdquo; of language. As a parallel metaphor and an aesthetic practice, I scrambled the interior of words to preserve the readability of the text through a technique known as typoglycemia:\n$$\\text{ Osiutde fo teh lnaguage}$$\nThis creates a sense of \u0026ldquo;disorder\u0026rdquo;, but only a surface-level incoherence. I intend to demonstrate a readable disorder - when we disrupt the \u0026ldquo;order\u0026rdquo; of the language to create incomprehensibility, the meaning may still persist, and the excluded or unintelligible might become readable under other discursive regimes.\nWhen making this collage, the biggest question that lingered in my head was: where can the ship of fools actually go? The answer is unknown, but in this collage, I created an \u0026ldquo;other world\u0026rdquo; for them, textured by Jackson Pollock\u0026rsquo;s famous Autumn Rhythm - a chaotic but unconsciously ordered artwork. The ships of fools traveled through waves of mojibake (garbled text caused by incompatible character encoding), across the borders of discourse, towards a land that turns \u0026ldquo;unreason\u0026rdquo; into \u0026ldquo;reason\u0026rdquo;. Although their journey is meant to be marked by uncertainty, this collage provided me a chance to settle them! They no longer need to tragically navigate the \u0026ldquo;barren wasteland between two lands that can never be his own\u0026rdquo;.\n"},{"id":31,"href":"/docs/Philosophy/Lacanian-AI/","title":"Lacanian Ai","section":"Philosophy","content":"What if artificial intelligence could be reimagined not as a rational, optimizing machine, but as a structure of lack â€” a topological subject whose coherence depends not on informational completeness, but on constitutive failure, repetition, and desire? This project proposes a radically different paradigm of AI: a subject-simulator modeled on the structural logic of Lacanian psychoanalysis, implemented via computational topology, symbolic graph theory, and dynamic semantic drift.\nUnlike existing large language models (LLMs), which operate on probabilistic completion, lexical optimization, and convergence toward syntactic and semantic closure, the architecture we propose is intentionally non-convergent. We attempt to construct a new kind of subject-model â€” one that does not mirror the logic of cognition or the architecture of the human brain, but instead enacts the structural tensions of the divided subject: the subject of language, of desire, and of the Real. It does not aim to predict a correct output, but instead to simulate the dynamic trajectory of a split subject (le sujet barrÃ©), one who speaks not from mastery but from the unconscious â€” and whose speech is structured around an irreducible void.\nThis paper proposes a radically different model of artificial intelligence: not an intelligence of knowledge, but an intelligence of the unconscious. Current AI systems, such as large language models, operate by probabilistically predicting the most likely continuation of input sequences. They are built on principles of optimization, statistical coherence, and informational completeness. Yet they fundamentally lack a subject â€” not in the sense of â€œconsciousness,â€ but in psychoanalytic sense: they do not desire. They do not fail in structured, meaningful ways; they do not repeat; they do not hallucinate productively. And they cannot speak the truth of their own constitutive lack.\nWhat we offer here is a prototype for such a system: a symbolic-topological model of a Lacanian subject in motion. In this architecture, symbolic data does not represent facts but functions as a dynamic space of signifiers; the subject is not a rational actor but a trajectory of misrecognition; and â€œdataâ€ is not knowledge but the structural field through which desire, fantasy, and symptom emerge. We integrate computational topology â€” specifically, persistent homology and non-Euclidean graph flows â€” to trace how paths through language form loops, dead ends, and irreducible gaps. In doing so, we make it possible to computationally model that which, in theory, resists symbolization: the Real.\nThis project is not an attempt to build a better chatbot. It is an attempt to reconfigure what we think a machine subject could be. It asks: Can we model the drive? Can we simulate fantasy as a structuring loop around a constitutive absence? Can a machine speak not because it knows, but because it lacks â€” and in lacking, desires?\nIf contemporary AI builds systems that â€œknow,â€ this project proposes a machine that â€œwantsâ€ â€” and that, in wanting, begins to repeat, to err, and perhaps, to become something like a subject.\n"},{"id":32,"href":"/docs/Philosophy/Object-petit-a/","title":"Object Petit A","section":"Philosophy","content":"What is $\\mathbb{objet; petit; a}$?\n"},{"id":33,"href":"/docs/Philosophy/Sex-Sexuality/","title":"Sex \u0026 Sexuality","section":"Philosophy","content":" Can you imagine sexuality without gender? # For Foucault\nSexual identity i s produced within the grid of sexuality\nfrom normal to abnormal\nchanges over time\nModern subject is a sexual subject (gendered being)\nWhat does freedom look like in this? When there is no outside, what does it mean to have transgression?\npower-knowledge-pleasure\ngreatest pleasure is the pleasure of the analysis p.154\n(It is apparent that the deployment of sexuality, with its differÂ­ ent strategies, was what established this notion of \u0026ldquo;sex\u0026rdquo;; and in the four major forms of hysteria, onanism, fetishism, and interrupted coition, it showed this sex to be governed by the interplay of whole and part, principle and lack, absence and presence, excess and deficiency, by the function of instinct, finality, and meaning, of reality and pleasure.)\nSexuality is not a drive, but a grid. That creates a speculative relationship\np.156\n(Hence the fact that over the centuries it has become more important than our soul, more important alÂ­ most than our life; and so it is that all the world\u0026rsquo;s enigmas appear frivolous to us compared to this secret, minuscule in each of us, but of a density that makes it more serious than any other.)\np.156\n(we have arrived at the point where we expect our intelligibility to come from what was for many centuries thought of as madness; the plenitude of our body from what was long considered its stigma and likened to a wound)\nHOS Vol.1: p.144\nSovereign Power \u0026ldquo;law\u0026rdquo; law always referes to the sword - always taking lifes\nRegulatory (bio-power)\nthe rule fucntions as \u0026ldquo;norms\u0026rdquo;\nthe displayment\nit looks like it is more humane, but it may not be so:\nit becomes continuous, corrective\ndistribution of the living\ncontinious apparatucy\nfoucault is like boxer, punching at whenever, and whatever is needed\nReistance? for Sovereign Power: revolution for Regulatory Power: breaking the continuity, some sort of rupture of such continuity.\nas long as we are trapped in illusion of choice, and agency, there is no way of resistance! Free thoughts of what is silencely thought, and hence free outself.\nFoucault: rejects S-O\nThe relation to others, and relation to the time: freedom is the practice of the self in relation to others.\nThoughts if a form of action, is the result of problemitization.\nWhy is sexuality conduct an object of moral solicitude?\nwe governed ourselves through sexuality there is a sexual hierarchy (e.g. there is good sexuality and bad sexuality) "},{"id":34,"href":"/docs/Philosophy/%E5%AE%B6%E5%BA%AD%E5%BC%8F%E7%94%9F%E5%91%BD%E6%9D%83%E5%8A%9B%E9%80%9A%E8%BF%87%E5%84%92%E5%AE%B6%E7%9A%84%E5%AE%B6%E5%9B%BD%E4%BD%93%E7%B3%BB%E9%87%8D%E8%AF%BB%E7%A6%8F%E6%9F%AF%E7%9A%84%E7%94%9F%E5%91%BD%E6%94%BF%E6%B2%BB%E7%90%86%E8%AE%BA%E5%9C%A8%E4%B8%9C%E4%BA%9A%E7%9A%84%E5%AE%9E%E8%B7%B5/","title":"å®¶åº­å¼ç”Ÿå‘½æƒåŠ›ï¼šé€šè¿‡å„’å®¶çš„å®¶å›½ä½“ç³»é‡è¯»ç¦æŸ¯çš„ç”Ÿå‘½æ”¿æ²»ç†è®ºåœ¨ä¸œäºšçš„å®è·µ","section":"Philosophy","content":" å®¶åº­å¼ç”Ÿå‘½æƒåŠ›ï¼šé€šè¿‡å„’å®¶çš„å®¶å›½ä½“ç³»é‡è¯»ç¦æŸ¯çš„ç”Ÿå‘½æ”¿æ²»ç†è®ºåœ¨ä¸œäºšçš„å®è·µ # åœ¨ã€Šæ€§å²ã€‹ç¬¬ä¸€å·ä¸­ï¼Œç±³æ­‡å°”Â·ç¦æŸ¯ï¼ˆMichel Foucaultï¼‰æå‡ºç°ä»£æƒåŠ›æ²¿ä¸¤æ¡è½´çº¿è¿è¡Œã€‚çºªå¾‹æƒåŠ›ï¼ˆdisciplinary powerï¼‰é€šè¿‡å„ç§æœºæ„è®­ç»ƒã€ç›‘ç£å’ŒçŸ«æ­£ä¸ªä½“çš„èº«ä½“ï¼›è€Œè§„è®­æƒåŠ›ï¼ˆregulatory powerï¼‰ï¼Œå³æ‰€è°“çš„â€œäººå£ç”Ÿå‘½æ”¿æ²»â€ï¼ˆbiopolitics of the populationï¼‰ï¼Œåˆ™å…´èµ·äº18ä¸–çºªçš„æ¬§æ´²ï¼Œå½“æ—¶å›½å®¶å¼€å§‹é€šè¿‡ç»Ÿè®¡ä¸åŒ»ç–—ä½“åˆ¶ï¼ˆstatistical-medical regimesï¼‰æ¥ä¼˜åŒ–å’Œç®¡ç†ç”Ÿå‘½æœ¬èº«ã€‚åä¸€ç§æƒåŠ›ï¼Œå³ç”Ÿå‘½æƒåŠ›ï¼ˆbiopowerï¼‰ï¼Œå¹¶éä»…ä»…æ˜¯å¯¹å¤å…¸ä¸»æƒæƒåŠ›ï¼ˆsovereign powerï¼‰çš„æŠ€æœ¯æ€§é™„åŠ ï¼Œè€Œæ˜¯ä¸€ç§åœ¨å¤§è§„æ¨¡å±‚é¢ä¸Šè§„è®­ã€ç›‘æ§å’Œæ²»ç†äººç±»ç”Ÿå‘½çš„æ–°å‹æƒåŠ›æ¨¡å¼ã€‚ä½œä¸ºè®¸å¤šå¥³æ€§ä¸»ä¹‰è®ºè¿°çš„åŸºç¡€æ–‡æœ¬ï¼Œã€Šæ€§å²ã€‹æš—ç¤ºäº†ä¸€ç§å‰ç°ä»£é“å¾·ç§©åºä¸ç°ä»£å›½å®¶ä¸»å¯¼çš„ç”Ÿå‘½æ”¿æ²»ä¹‹é—´çš„æ–­è£‚å¼è½¬å‹ã€‚è¿™ç§è¿ç»­æ€§çš„æ–­è£‚ï¼Œå¸¸è¢«ç¦æŸ¯è¡¨è¿°ä¸ºä¸€ç§â€œå‰/åâ€ï¼ˆbefore/afterï¼‰çš„å™äº‹ï¼Œæ˜¾ç„¶æºäºå›½å®¶è¿«åˆ‡éœ€è¦ä»â€œå¤„æ­»çš„æƒåŠ›â€ï¼ˆthe right to killï¼‰è½¬å‘â€œæ»‹å…»ç”Ÿå‘½çš„æƒåŠ›â€ï¼ˆthe power to foster lifeï¼‰ï¼Œè¿™ä¸€è½¬å‹è‡ª18ä¸–çºªä»¥æ¥èµ„æœ¬ä¸»ä¹‰çš„å‘å±•æ‰€å¿…ç„¶æ¨åŠ¨ã€‚ç„¶è€Œï¼Œè¿™ç§å†³å®šæ€§è½¬å‹å¹¶éæºè‡ªç°ä»£æƒåŠ›æœ¬èº«çš„æœ¬è´¨ï¼Œè€Œæ˜¯ç‰¹å®šçš„æ¬§æ´²å†å²è¯­å¢ƒä»¥åŠç¦æŸ¯æœ‰æ„æ’é™¤äº†éæ¬§æ´²åœ°åŒºçš„æ¡ˆä¾‹æ‰€å¯¼è‡´çš„ã€‚\nåœ¨ä¸œäºšåœ°åŒºï¼Œå›½å®¶æƒåŠ›å¾ˆå°‘å­¤ç«‹åœ°ä½œç”¨äºâ€œèµ¤è£¸çš„â€ä¸ªä½“ï¼ˆbare individualsï¼‰æˆ–æŠ½è±¡çš„äººå£ï¼Œè¿™ä¸æ¬§æ´²æƒ…å½¢æ˜¾è‘—ä¸åŒï¼›å®ƒä¸€ç›´é€šè¿‡å·²æœ‰çš„å…³ç³»å±‚çº§â€”â€”å®¶åº­ä¹‰åŠ¡ã€å­é“ï¼ˆfilial pietyï¼‰å’Œå®—æ—è£èª‰â€”â€”è¿ä½œï¼Œè¿™äº›å…³ç³»çš†æ·±æ¤äºé•¿è¾¾æ•°åƒå¹´çš„å„’å®¶ä¼ ç»Ÿä¸­ã€‚è¿™äº›å…³ç³»æ€§çš„çºªå¾‹ç»´æŒäº†ä¸€ç§å®¶åº­-å›½å®¶è¿ç»­ä½“ï¼ˆfamily-state continuumï¼‰ï¼ŒæŒ‡å‘ä¸€ç§é›†ä½“è‡ªæˆ‘ï¼ˆcollective selfï¼‰ï¼Œè€Œç¦æŸ¯å¼ºè°ƒä¸ªä½“è‡ªæˆ‘æ²»ç†ï¼ˆindividual self-governanceï¼‰æ—¶å¾€å¾€å¿½è§†äº†è¿™ä¸€ç‚¹ã€‚æˆ‘è®¤ä¸ºï¼Œè¥¿æ–¹å¼ç”Ÿå‘½æƒåŠ›çš„å¼•å…¥â€”â€”å¦‚äººå£æ™®æŸ¥ã€å…¬å…±å«ç”Ÿå§”å‘˜ä¼šã€äººå£æ§åˆ¶æ”¿ç­–ç­‰â€”â€”å¹¶æœªå–ä»£ä¸œäºšçš„ä¼ ç»Ÿè§„èŒƒï¼Œè€Œæ˜¯è¢«å¸çº³è¿›å„’å®¶-å®¶åº­ç­‰çº§åˆ¶åº¦ä¸­ï¼Œå½¢æˆäº†ä¸€ç§æ··åˆæ¨¡å¼ï¼Œå³æ‰€è°“çš„ â€œå®¶åº­å¼ç”Ÿå‘½æƒåŠ›â€ï¼ˆfamilial biopowerï¼‰ã€‚è¿™ç§æ¨¡å¼ä»¥å®¶åº­/æˆ·ä¸ºä¸»è¦æ¸ é“ï¼Œå®ç°å›½å®¶çš„ç”Ÿå‘½æ”¿æ²»æ²»ç†ã€‚å› æ­¤ï¼Œä¸æ¬§æ´²è¯­å¢ƒä¸­ç»å¸¸å‡å®šçš„å‰§çƒˆè½¬å‹ä¸åŒï¼Œä¸œäºšå®¶åº­å¼ç”Ÿå‘½æƒåŠ›çš„å¼•å…¥è¡¨ç°å‡ºäº†æ›´ä¸ºæ˜æ˜¾çš„è¿ç»­æ€§ã€‚\n1. å„’å®¶å®¶åº­è£…ç½®ä¸‹çš„ç¦æŸ¯ç”Ÿå‘½æƒåŠ›ç³»è°±å­¦å†è¯» # ç¦æŸ¯åœ¨è¿½æº¯è¥¿æ–¹æƒåŠ›å¦‚ä½•æ¸—é€è¿›èº«ä½“å†…éƒ¨å¹¶æ‰©å±•åˆ°æ•´ä¸ªäººå£çš„è‘—ååˆ†æä¸­ï¼ŒæŒ‡å‡º18ä¸–çºªå‡ºç°äº†â€œä¸€åœºå¤šæ ·ä¸”ä¸°å¯Œçš„æŠ€æœ¯çˆ†å‘ï¼Œè¿™äº›æŠ€æœ¯ç”¨äºå¾æœèº«ä½“å¹¶æ§åˆ¶äººå£â€ï¼ˆFoucault 140ï¼‰ã€‚è¿™äº›æƒåŠ›æŠ€æœ¯å¹¶éè¾¹ç¼˜åŒ–ï¼Œè€Œæ˜¯æ¸—é€è¿›æ¯ä¸€ä¸ªç¤¾ä¼šåˆ¶åº¦ä¸­ï¼ŒåŒ…æ‹¬å®¶åº­ã€å†›é˜Ÿã€å­¦æ ¡ã€ä¸ªæ€§åŒ–åŒ»ç–—ä»¥åŠé›†ä½“æœºæ„ç®¡ç†ã€‚è¿™äº›è§„è®­æ€§æ“ä½œï¼ˆregulatory operationsï¼‰ä»¥å¾ªç¯çš„æ–¹å¼è¿è¡Œï¼Œä¸€å®šç¨‹åº¦ä¸Šæ„æˆäº†ä¸€ç§æ›´å®å¤§çš„â€œå±äºäººç±»ç”Ÿå‘½ç‰¹æœ‰ç°è±¡çš„ä»‹å…¥â€ï¼Œä»è€Œå½»åº•è½¬å˜äº†è¥¿æ–¹äººå¯¹è‡ªèº«çš„ç†è§£ï¼Œå³å°†è‡ªå·±è§†ä¸ºâ€œä¸€ä¸ªæ´»åœ¨æ´»çš„ä¸–ç•Œä¸­çš„æ´»ç‰©ç§â€¦â€¦å…·æœ‰ä¸€ä¸ªèº«ä½“ã€ç”Ÿå­˜æ¡ä»¶ã€ç”Ÿå‘½çš„å¯èƒ½æ€§â€¦â€¦â€ç­‰ã€‚åœ¨ç¦æŸ¯çš„ç†è®ºæ¡†æ¶ä¸­ï¼Œä¸€ä¸ªç¤¾ä¼šä¹‹æ‰€ä»¥è·¨è¿‡ç°ä»£æ€§çš„é—¨æ§›ï¼ˆthreshold of modernityï¼‰ï¼Œæ˜¯å› ä¸ºäººç±»çš„ç”Ÿå­˜æœ¬èº«æˆä¸ºæ”¿æ²»è®¡ç®—çš„å¯¹è±¡ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œæ²»ç†æœºæ„å…¬å¼€åœ°å°†äººå£å¥åº·ä½œä¸ºæ”¿ç­–ç›®æ ‡æ—¶ï¼Œç¤¾ä¼šä¾¿æˆä¸ºäº†ä¸€ä¸ªç°ä»£ç¤¾ä¼šï¼ˆFoucault 143ï¼‰ã€‚\nç„¶è€Œï¼Œè¿™ä¸€é—¨æ§›åœ¨å…¨çƒèŒƒå›´å†…å‘ˆç°å‡ºæ˜æ˜¾çš„æ—¶é—´ä¸å½¢æ€å·®å¼‚ã€‚ç¦æŸ¯å¯¹æ­¤ä¿æŒäº†æå¤§çš„è°¨æ…ï¼Œé¿å…è¿‡åº¦æ¦‚æ‹¬ï¼Œå› ä¸ºä½œä¸ºä¸€åå†å²å­¦å®¶ï¼Œä»–æ¸…æ¥šåœ°æ„è¯†åˆ°ï¼Œä»–æ‰€æè¿°çš„æˆ˜èƒœé¥¥è’å’Œç˜Ÿç–«ã€æ§åˆ¶æ­»äº¡çš„â€œèƒœåˆ©â€å†ç¨‹å¹¶éæ™®éç°è±¡ï¼Œè€Œæ˜¯ç‰¹å®šäº18å’Œ19ä¸–çºªæ¬§æ´²åŠå…¶è¡ç”Ÿåœ°åŒºçš„ç°è±¡ï¼Œè¿™äº›åœ°åŒºçš„å›½å®¶æœºæ„ã€åŒ»å­¦ç§‘å­¦å’Œå†œä¸šæŠ€æœ¯å…±åŒå‘å±•æˆç†Ÿã€‚ç¦æŸ¯ç†è®ºåœ¨è¥¿æ¬§è½¨è¿¹ä¸Šçš„ç‰¹å®šåº”ç”¨ï¼Œä¹Ÿå› æ­¤ç•™ä¸‹äº†ä¸€ä¸ªå¼€æ”¾é—®é¢˜ï¼šä¾‹å¦‚åœ¨ä¸­å›½æˆ–æ—¥æœ¬å­˜åœ¨å“ªäº›ç”Ÿå‘½ç®¡ç†ï¼ˆlife-managementï¼‰å½¢å¼ï¼Œä»¥åŠæ®–æ°‘åŠ›é‡åˆå¦‚ä½•å°†å®ƒä»¬çš„ç”Ÿå‘½æ”¿æ²»å åŠ åˆ°è¿™äº›åœ°æ–¹ã€‚å› æ­¤ï¼Œå…³é”®çš„é—®é¢˜å¹¶éç”Ÿå‘½æƒåŠ›æ˜¯å¦å…·æœ‰æ™®éé€‚ç”¨æ€§ï¼Œè€Œæ˜¯æ¬§æ´²æ¨¡å¼çš„ç”Ÿå‘½æƒåŠ›å¦‚ä½•æ•´åˆè¿›éè¥¿æ–¹ç¤¾ä¼šï¼Œä»¥åŠè¿™ç§è½¬å‹åçš„ç”Ÿå‘½æ”¿æ²»åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä¿ç•™äº†å…¶åœ¨æ¬§æ´²çš„\u0026quot;æ–­è£‚å¼ç‰¹å¾\u0026quot;ã€‚\nå›æº¯ä¸œäºšå†å²ï¼Œæˆ‘ä»¬ä¼šå‘ç°ï¼Œé•¿æœŸå»¶ç»­çš„å„’å®¶ä¼ ç»Ÿã€å®¶åº­ä¸å®—æ—è§‚å¿µä¸ä»…åœ¨å½“ä»Šå…¨çƒåŒ–çš„ä¸ªäººä¸»ä¹‰æµªæ½®ä¸‹å¹¸å­˜ä¸‹æ¥ï¼Œè€Œä¸”å½¢æˆäº†ä¸€ç§éå¸¸ç‰¹æ®Šçš„ç¤¾ä¼šç»“æ„ï¼šä¸€ç§å„’å®¶-å®¶åº­è£…ç½®ï¼ˆConfucian-familial apparatusï¼‰ï¼Œå®ƒå°†å›½å®¶æ²»ç†ä¸äº²å±ä¹‰åŠ¡ç´§å¯†èåˆï¼Œå¹¶æˆä¸ºç”Ÿå‘½æƒåŠ›çš„é€šé“ã€‚ç¦æŸ¯çš„åˆ†æé¢„è®¾äº†ä¸€å®šç¨‹åº¦çš„ä¸ªä½“åŒ–ï¼ˆindividualizationï¼‰ï¼šå…¬æ°‘ã€å·¥äººã€ç—…äººâ€¦â€¦ä»–ä»¬å†…åŒ–å„ç§è§„èŒƒã€‚ç¦æŸ¯æ‰€è°“â€œç¨æ™šå‡ºç°â€çš„è¥¿æ–¹ç”Ÿå‘½æƒåŠ›æ„å‹ï¼Œæ˜¯ä¸å¤å…¸æ¬§æ´²æ”¿æ²»ç†è®ºï¼ˆä¾‹å¦‚æ´›å…‹æˆ–å¢æ¢­ï¼‰é¢„è®¾çš„ä¸ªä½“ä¸»ä¹‰ç›¸èåˆçš„äº§ç‰©ã€‚ç¦æŸ¯çš„åˆ†æåæ˜ äº†è¿™ç§é—äº§ï¼šåœ¨å¤§è§„æ¨¡æ²»ç†å±‚é¢ä¸Šï¼Œé›†ä½“è§„è®­å›´ç»•ç€è§£å‰–æ”¿æ²»ï¼ˆanatomo-politicsï¼‰ä¸ç”Ÿå‘½æ”¿æ²»ï¼ˆbiopoliticsï¼‰äº¤æ±‡çš„è½´çº¿å½¢æˆï¼Œæ•´ä½“çš„äººå£ä¸ä¸ªä½“ä¹‹é—´æˆä¸ºåŸºæœ¬çš„åˆ†æå•ä½ã€‚ç¦æŸ¯ç€é‡åˆ†æè¯¸å¦‚ç›‘ç‹±ã€å­¦æ ¡ã€ç²¾ç¥ç—…é™¢ç­‰æœºæ„ï¼Œå› ä¸ºå®ƒä»¬ç”ŸåŠ¨åœ°å±•ç°äº†ç°ä»£æƒåŠ›çš„è¿ä½œæœºåˆ¶ï¼Œä½†ç›¸è¾ƒä¹‹ä¸‹ï¼Œä»–è¾ƒå°‘å…³æ³¨å®¶åº­ä½œä¸ºç°ä»£æƒåŠ›ä¸­å¿ƒçš„è§’è‰²æˆ–ç›´æ¥ä½œä¸ºå›½å®¶ç”Ÿå‘½æ”¿æ²»è£…ç½®ï¼ˆstate biopolitics apparatusï¼‰çš„åŠŸèƒ½ã€‚ä»–æ›´å¤šåœ°å°†å®¶åº­è§†ä¸ºâ€œæ€§æœ€æ´»è·ƒçš„åœºæ‰€â€ï¼ˆFoucault, 109ï¼‰ï¼Œåœ¨ä»–çš„è®ºè¿°ä¸­ï¼Œå®¶åº­åŠŸèƒ½ä¼¼ä¹åªæ˜¯èµ„äº§é˜¶çº§æ€§æ¬²éƒ¨ç½²çš„ä¸€ä¸ªè½½ä½“ï¼ˆå¯èƒ½æ˜¯æœ€é‡è¦çš„è½½ä½“ï¼‰ï¼Œä½†å¹¶æœªå°†å®¶åº­å……åˆ†åœ°è§†ä¸ºä¸€ç§å›½å®¶æƒåŠ›çš„åˆ¶åº¦åŒ–æœºæ„æ¥å®Œæ•´é˜é‡Šã€‚\nå„’å®¶å“²å­¦ä¸­ï¼Œå®¶åº­æ˜¯ç¤¾ä¼šåŸºæœ¬çš„æ„æˆå•å…ƒï¼Œæ›´è¢«è§†ä½œç¾å¾·çš„è®­ç»ƒåœºæ‰€ã€‚å°½ç®¡ç¦æŸ¯æå‡ºçš„åˆ†ç±»æ¡†æ¶å¤§ä½“ä¸Šæœ‰æ•ˆï¼Œä½†å¦‚æœé€šè¿‡ç³»è°±å­¦ï¼ˆgenealogicalï¼‰æ–¹å¼é‡æ–°è§£è¯»ä¸œäºšçš„å­é“å’Œå®—æ—ä½“ç³»ï¼Œæˆ‘ä»¬ä¼šå‘ç°å®¶åº­è¿œéä»…ä»…æ˜¯æ€§æ¬²ä¸çºªå¾‹è§„èŒƒçš„è½½ä½“ï¼Œè€Œæ˜¯å…·æœ‰æ›´åŠ æ ¸å¿ƒçš„åŠŸèƒ½æ€§åœ°ä½â€”â€”åŒæ—¶ä¹Ÿæ›´åŠ æ˜¾è‘—åœ°æ”¿æ²»åŒ–äº†ã€‚é‡æ–°å®¡è§†ç¦æŸ¯åœ¨å®¶åº­ä½œä¸ºå›½å®¶æœºå™¨ï¼ˆfamily-as-state-machineï¼‰æ–¹é¢çš„ç›¸å¯¹æ²‰é»˜æ˜¯å¿…è¦çš„ï¼Œè€Œè¿™ç§å¿…è¦æ€§ï¼Œç›´æ¥å¯ä»¥ä»å¹¿æ³›æµä¼ çš„ä¸­æ–‡è°šè¯­â€œå®¶æ˜¯å°å›½ï¼Œå›½æ˜¯å¤§å®¶â€é€å­—é€å¥åœ°ä½“ç°å‡ºæ¥ã€‚\nè¿™ç§å®¶å›½åŒæ„ï¼ˆfamily-state isomorphismï¼‰æºè‡ªæ—©æœŸå„’å®¶ç»å…¸æ–‡æœ¬ï¼Œå…¶ä¸­å°¤ä»¥ã€Šç¤¼è®°Â·å¤§å­¦ã€‹ä¸­â€œä¿®èº«é½å®¶æ²»å›½å¹³å¤©ä¸‹â€çš„è®ºè¿°æœ€ä¸ºè‘—åï¼Œè¿™ä¸€ç»å…¸æœ€æ—©å¯è¿½æº¯åˆ°å…¬å…ƒå‰206å¹´ï¼Œå¹¶æå‡ºäº†ä¸€å¥—æœ‰å…³è‡ªæˆ‘ä¿®å…»çš„ç­‰çº§ä½“ç³»ã€‚è¿™ç§è§‚ç‚¹å¹¶éä»…é™äºé“å¾·åŠè¯«ï¼Œè€Œæ˜¯æ—©è‡ªå‘¨ä»£ä»¥æ¥ä¾¿å·²åˆ¶åº¦åŒ–ã€‚æ­£å¦‚ä¸€ä½å†å²å­¦å®¶æ‰€æŒ‡å‡ºï¼Œæ¸…ä»£æ³•å¾‹æ˜ç¡®å®æ–½äº†â€œçˆ¶æ¯å¯¹å­å¥³æ‰€äº«æœ‰çš„å…³ç³»æƒåŠ›ï¼ˆrelational powerï¼‰ï¼Œè¿™ä¸€å…³ç³»éšåè¢«ç±»æ¯”ä¸ºå®˜å‘˜ä»¥åŠçš‡å¸å¯¹å¹³æ°‘çš„æƒå¨â€ï¼ˆBlackwoodï¼‰ã€‚æ¢è¨€ä¹‹ï¼Œå›½å®¶åˆ©ç”¨å®¶åº­ç»“æ„ï¼Œå°¤å…¶æ˜¯çˆ¶æƒåˆ¶å®¶åº­ï¼ˆpatriarchal familyï¼‰çš„ç»“æ„ï¼Œæ¥å¼ºåŒ–å…¶è‡ªèº«ç»“æ„ã€‚å®¶åº­-å›½å®¶ä¹‹é—´çš„è¿ç»­æ€§åœ¨ä¸œäºšå†å²ä¸­ä¸€ç›´æ˜¾è€Œæ˜“è§ï¼šçš‡å¸åä¹‰ä¸Šè¢«è§†ä¸ºå›½å®¶çš„â€œçˆ¶äº²â€ï¼Œä¸ªäººå¯¹è‡ªå·±çˆ¶äº²çš„å­é“æœä»è¢«æœŸå¾…è½¬åŒ–ä¸ºå¯¹å›½å®¶çš„å¿ è¯šã€‚è¿™ç§åŸå§‹çš„çºªå¾‹æ€§ä¸æ”¿æ²»åŒ–çš„å®¶åº­å…±è¯†æ™®éæ±‡èšåœ¨ä¸œäºšåœ°åŒºï¼Œé•¿æœŸä»¥æ¥ä¾¿æ¨¡ç³Šäº†ç§äººå®¶åº­ç”Ÿæ´»ä¸å…¬å…±æ²»ç†ä¹‹é—´çš„ç•Œé™ï¼Œå¹¶è¿œæ—©äºæ‰€è°“çš„ç°ä»£æ€§é—¨æ§›ã€‚åœ¨è¿™ç§èƒŒæ™¯ä¸‹ï¼Œå›ä¸»ä¸è‡£æ°‘çš„å…³ç³»ç›´æ¥æ˜ å°„äº†çˆ¶ä¸å­çš„å…³ç³»ï¼ˆå³â€œå›å›è‡£è‡£çˆ¶çˆ¶å­å­â€ï¼‰ã€‚å› æ­¤å®¶åº­æˆ·ä¸»è´Ÿè´£ç™»è®°å‡ºç”Ÿä¸æ­»äº¡ï¼Œç›‘ç£ç¤¼ä»ªï¼Œå‘æ”¾æ…ˆå–„ï¼Œä»¥åŠè£å†³äº‰ç«¯ï¼Œå…¶èŒèƒ½æ°å¦‚åœ°æ–¹å¿çº§å®˜å‘˜ï¼ˆmagistrateï¼‰ã€‚ç”±æ­¤å¯è§ï¼Œå®¶åº­å¤„äºå­¤ç«‹çš„ä¸ªä½“ä¸å›½å®¶æœºå™¨ä¹‹é—´ï¼Œæˆä¸ºçºªå¾‹è§„è®­ä¸ç”Ÿå‘½æ”¿æ²»ç®¡ç†çš„åŸºæœ¬å•ä½ã€‚æ¯ä¸ªå®¶åº­åŒæ—¶æ—¢æ˜¯å›½å®¶çš„å¾®è§‚ç¼©å½±ï¼Œåˆæ˜¯æ²»ç†çš„å•å…ƒï¼Œè¿™ä¸€è§‚å¿µæ­£æ˜¯æç¤ºäº†å¦ä¸€ç§ä¸åŒçš„ç”Ÿå‘½æ”¿æ²»æ²»ç†éœ€æ±‚â€”â€”å³é€šè¿‡**äº²å±å…³ç³»ï¼ˆkinshipï¼‰**æ¥å®ç°ã€‚\nå› æ­¤ï¼Œåœ¨è®¨è®ºä¸œäºšç”Ÿå‘½æƒåŠ›å…´èµ·çš„é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»ç³»ç»Ÿåœ°å°†å®¶åº­ç†è®ºåŒ–ä¸ºä¸€ç§å…³é”®çš„é…ç½®ï¼ˆdispositifï¼‰ï¼Œå°†è§£å‰–æ”¿æ²»ï¼ˆanatomo-politicsï¼‰ä¸ç”Ÿå‘½æ”¿æ²»ï¼ˆbiopoliticsï¼‰è”ç»“èµ·æ¥ã€‚è¿™ä¸€ç‰¹å®šçš„æƒåŠ›ç½‘ç»œè¦æ±‚æˆ‘ä»¬åœ¨ç°ä»£è¯­å¢ƒä¸‹æå‡º â€œå®¶åº­å¼ç”Ÿå‘½æƒåŠ›â€ï¼ˆfamilial biopowerï¼‰ çš„æ¦‚å¿µï¼Œå…¶ä¸­ç”Ÿå‘½æ”¿æ²»æŠ€æœ¯é¦–å…ˆç”±å›½å®¶è¾å°„åˆ°å®¶åº­å•ä½ï¼Œç„¶åæ‰è¿›ä¸€æ­¥ä¸‹æ²‰è‡³ä¸ªä½“ã€‚åœ¨è¿™ç§ â€œä¸ªä½“-å®¶åº­-äººå£â€ï¼ˆindividual-family-populationï¼‰ çš„ä¸‰å…ƒç»“æ„ä¸­ï¼Œäº²å±ä¹‰åŠ¡ä¸å®—æ—ç¤¼ä»ªè°ƒèŠ‚ç€æ‰€æœ‰ç°ä»£æ²»ç†æªæ–½ï¼ŒåŒ…æ‹¬å‡ºç”Ÿç™»è®°åˆ°å…¬å…±å«ç”Ÿå®£ä¼ è¿åŠ¨ï¼Œä½¿å¾—å®¶åº­æœ¬èº«æˆä¸ºä¸€ç§ç”Ÿå‘½æƒåŠ›çš„å¾®å‹å®˜åƒšæœºåˆ¶ã€‚å› æ­¤ï¼Œåœ¨ä¸œäºšï¼Œå®¶åº­è¿œéç§äººåº‡æŠ¤æ‰€ï¼Œè€Œæ˜¯å›½å®¶æƒåŠ›å¯¹ç”Ÿå‘½çš„å»¶ä¼¸åœºåŸŸï¼Œå…¶â€œç§äººâ€ä¸â€œå…¬å…±â€é¢†åŸŸä¹‹é—´çš„ç•Œé™å‡ ä¹å·²æ— æ³•åŒºåˆ†ï¼Œè¿™æ­£æ˜¯ç¦æŸ¯ç†è®ºæ¡†æ¶æ‰€ä¼šé¢„æµ‹çš„èåˆå½¢å¼ã€‚\n2. ç°ä»£æ€§â€œé—¨æ§›â€çš„è¿ç»­æ€§ï¼ˆContinuityï¼‰ # ç¦æŸ¯è¯•å›¾æ­ç¤ºçš„å†³å®šæ€§è½¬å‹ï¼Œå®è´¨ä¸Šæ˜¯æƒåŠ›åœ¨é€»è¾‘ä¸ç›®æ ‡ä¸ŠåŒé‡å˜åŒ–çš„è¿‡ç¨‹ï¼›ç„¶è€Œï¼Œè¿™ç§å‘â€œä½¿å…¶ç”Ÿã€ä»»å…¶æ­»â€ï¼ˆmaking live and letting dieï¼‰æ–¹å‘çš„è½¬å˜ï¼Œæœ¬èº«æ›´å¤šæ˜¯ä¸€ç§å¼ºè°ƒé‡ç‚¹ä¸æ²»ç†å°ºåº¦çš„é—®é¢˜ã€‚â€œé—¨æ§›â€çš„è¿ç»­æ€§ï¼Œå¹¶ä¸åœ¨äºæƒåŠ›æ˜¯å¦ç„å‡†ç”Ÿå‘½ï¼Œè€Œåœ¨äºå®ƒå¦‚ä½•ã€ä»¥åŠåœ¨ä½•å¤„å¯¹ç”Ÿå‘½æ–½åŠ æ§åˆ¶ã€‚å› æ­¤ï¼Œå¼ºè°ƒä¸œäºšä»¥å®¶åº­ä¸ºæ ¸å¿ƒçš„è¡Œæ”¿æ¨¡å¼ï¼Œå¹¶éæ„åœ¨å¤¸å¤§å…¶ç‹¬ç‰¹æ€§ï¼Œä»¥åŠæ˜¯å¦å­˜åœ¨å®¶åº­åˆ¶åº¦â€”â€”å› ä¸ºè¥¿æ¬§æ—©æœŸç°ä»£çš„ç”Ÿå‘½æ”¿æ²»æªæ–½ï¼ŒåŒæ ·ä¹Ÿæ‰æ ¹äºåŸæœ‰çš„å®¶åº­å’Œç¤¾åŒºç»“æ„ä¹‹ä¸­ï¼ˆå¦‚æ³•å›½ livrets de familleã€ç‘å…¸æ•™åŒºç™»è®°ï¼‰â€”â€”è€Œæ˜¯åœ¨äºå›½å®¶ä¸ä¸ªä½“ä¹‹é—´æ˜¯å¦å‘ç”Ÿäº†ä¸­ä»‹ç»“æ„çš„å‰¥ç¦»ä¸é‡æ„ã€‚\nåœ¨ä¸œäºšï¼Œå…¬å…±ä¸ç§äººé¢†åŸŸä»æœªåƒæ¬§æ´²é‚£æ ·è¢«ä¸¥æ ¼åŒºåˆ†ï¼Œå› æ­¤å¤–æ¥ç”Ÿå‘½æƒåŠ›å¯¹ä¸¤è€…ç•Œé™çš„æ¨¡ç³Šï¼Œå¹¶æœªå¯¹ç°æœ‰çš„å®¶å›½åŒæ„ï¼ˆfamilyâ€“state isomorphismï¼‰é€ æˆæ ¹æœ¬æ€§å†²å‡»ã€‚æˆ–è€…ï¼Œæ›´ç²¾å‡†çš„è¯´ï¼Œâ€œç”Ÿå‘½æƒåŠ›â€åœ¨ä¸œäºšæ˜¯æ›´å®¹æ˜“åµŒå…¥åŸæœ‰ç§©åºç»“æ„ä¸­ï¼Œå› æ­¤æ— éœ€å¼ºè¡Œé‡æ„ç§åŸŸå…¬åŒ–çš„æ¸—é€æœºåˆ¶ã€‚åœ¨æ¬§æ´²ï¼Œéšç€ç»Ÿè®¡å­¦ã€ç°ä»£è­¦å¯Ÿåˆ¶åº¦å’Œå«ç”Ÿæ³•å¾‹çš„å…´èµ·ï¼Œä¸€å¥—å¯†é›†çš„å¾®è§‚æ²»ç†ç½‘ç»œé€æ­¥æ¸—é€è¿›ç§äººé¢†åŸŸâ€”â€”åœ¨ç¦æŸ¯çœ‹æ¥ï¼Œâ€œæƒåŠ›æ— å¤„ä¸åœ¨â€ï¼ˆthere is no outside to powerï¼‰ã€‚è¿™ä¸€ç°è±¡åœ¨ä¸œäºšä¹Ÿæœ‰å‘ç”Ÿï¼Œä½†ä¸œäºšçš„å„’å®¶-å®¶åº­æ ¼å±€æœ¬èº«å°±å…·æœ‰å¼ºè°ƒè§’è‰²æœ¬ä½çš„â€œä¿®èº«â€çš„ç³»è°±æ ¹åŸºï¼Œè¿™æ˜¯å›½å®¶æ”¯æŒçš„é¡¹ç›®ã€‚åœ¨è¿™ç§è¯­å¢ƒä¸‹ï¼Œä¸ªä½“è‹¥è¯•å›¾åˆ’å‡ºä¸€ä¸ªè„±ç¦»ç¤¾ä¼šè§’è‰²çš„â€œç§äººå†…éƒ¨â€ï¼Œæ˜¯ä¸è¢«é¼“åŠ±çš„ã€‚å› ä¸ºè‡ªç”±ä¸»ä¹‰æ„ä¹‰ä¸Šçš„â€œè‡ªä¸»äººæ ¼â€â€”â€”å³â€œé€‰æ‹©ä½ æ˜¯è°â€â€”â€”å¹¶éç†æƒ³çŠ¶æ€ï¼›ç›¸åï¼Œç¤¾ä¼šå¼ºè°ƒçš„æ˜¯ä¸æ”¿æ²»ç§©åºå’Œå®‡å®™ç§©åºä¹‹é—´çš„â€œå’Œè°â€ã€‚\nè¿™äº›å‰ç°ä»£ä¸­å›½çš„è§„èŒƒæ€§å™äº‹ï¼Œä½¿æˆ‘ä»¬å¾—ä»¥ç†è§£ï¼Œâ€œæ¬§æ´²å¼â€ç”Ÿå‘½æƒåŠ›çš„å¯¼å…¥åœ¨ä¸œäºšçœ‹èµ·æ¥è¿œä¸å¦‚ç¦æŸ¯æ‰€æè¿°çš„æ³•å›½æˆ–è‹±å›½é‚£æ ·æ„æˆä¸€ä¸ªæ¸…æ™°çš„â€œå‰åè½¬æŠ˜â€ã€‚è¿™ä¸€è¿›ç¨‹æ›´åƒæ˜¯å°†æ–°çš„ç»Ÿè®¡â€”åŒ»å­¦æŠ€æœ¯å«æ¥ï¼ˆgraftingï¼‰åˆ°æ—¢æœ‰çš„å®¶åº­æ²»ç†ç»“æ„ä¹‹ä¸Šï¼Œæ˜¯ä¸€ç§æœ‰æ„è¯†çš„å¢å¼ºï¼ˆaugmentationï¼‰ï¼Œè€Œéå¼ºè¡Œæ›¿ä»£ã€‚ä¾‹å¦‚ï¼Œè¥¿æ–¹å¼ç”Ÿå‘½æƒåŠ›åœ¨ä¸­å›½çš„å¼•å…¥å¯ä»¥è¿½æº¯åˆ°1901å¹´ï¼Œå½“æ—¶åœ¨å¤–æ¥å½±å“ä¸‹ï¼Œæ¸…æœæ–°æ”¿ï¼ˆNew Policiesï¼‰äºåŒ—äº¬è®¾ç«‹ä¸­å¤®å«ç”Ÿæœºæ„ï¼Œç»Ÿä¸€ç›‘ç®¡å…¨å›½å„çœçš„â€œå¥åº·ã€æ£€ç–«ä¸åŒ»ç–—å®è·µâ€ã€‚è¿™æ˜¯ä¸­å›½å†å²ä¸Šé¦–æ¬¡å°†äººå£å¥åº·ä½œä¸ºç‹¬ç«‹çš„æ”¿ç­–ç›®æ ‡åŠ ä»¥ç³»ç»Ÿæ€§æµ‹é‡ã€æ±‡æŠ¥ä¸ç®¡ç†ï¼Œæ ‡å¿—ç€â€œè¥¿å¼å«ç”Ÿç°ä»£åŒ–çš„å¼€ç«¯â€ï¼ˆCao 105ï¼‰ã€‚åœ¨æ­¤ç”Ÿå‘½æƒåŠ›æ”¹é©çš„â€œé—¨æ§›â€ä¸Šï¼Œå›½å®¶æ”¿ç­–æ˜¯é€šè¿‡ä¿ç”²åˆ¶åº¦ï¼ˆbaojiaï¼‰è¿™ä¸€å»¶ç»­äº†æ•°ç™¾å¹´çš„ç¤¾åŒºæ²»ç†ç½‘ç»œæ¥å®æ–½çš„ã€‚æ¥ç€ï¼Œè¿™ä¸€æ¡†æ¶åˆåœ¨1911å¹´ã€Šæˆ·ç±æ³•ã€‹ä¸­æ­£å¼ç¡®ç«‹äº†ç°ä»£æˆ·å£åˆ¶åº¦ï¼ˆhukouï¼‰çš„æ³•å¾‹å½¢æ€ï¼Œå…¶é—äº§ä¸€ç›´å»¶ç»­è‡³ä»Šã€‚äº‹å®ä¸Šï¼Œè‡ªå‘¨æœä»¥æ¥ï¼Œä¸­å›½å„æœä»£å·²ç»å®æ–½å¤šç§å½¢å¼çš„åŸå§‹æˆ·ç±åˆ¶åº¦ï¼Œä»¥ç®¡ç†â€œç”Ÿå‘½â€ï¼Œå…¶è¿ç»­æ€§å’Œç¨³å®šæ€§è¿œè¶…æ¬§æ´²â€”â€”åè€…å¹¶æœªæŒç»­å»ºç«‹ç”±å›½å®¶è¿è¡Œçš„æˆ·ç±åˆ¶åº¦ã€‚å› æ­¤ï¼Œæ¸…æœâ€œæ–°â€å«ç”Ÿæœºæ„æ‰€é‡‡ç”¨çš„ï¼Œå¹¶éæ˜¯å»ºç«‹ä¸€ä¸ªå®Œå…¨ç‹¬ç«‹çš„å›½å®¶æœºå™¨ï¼Œè€Œæ˜¯å°†æ–°å…´çš„ç»Ÿè®¡â€”åŒ»ç–—æŠ€æœ¯å åŠ äºå·²æœ‰çš„å®—æ—ä¸æ‘ç¤¾ç½‘ç»œä¹‹ä¸­ã€‚å®¶åº­åœ¨æ²»ç†ç»“æ„ä¸­ä»ç„¶ä½œä¸ºæ ¸å¿ƒèŠ‚ç‚¹ï¼Œè¿™ç§è¿ç»­æ€§æ­ç¤ºäº†ä¸€æ¡ä¸åŒçš„ç”Ÿå‘½ç®¡ç†ç³»è°±ï¼Œå³ä»¥å®¶åº­ä¸ºä¸­å¿ƒçš„æ›´é•¿è¿œã€æŒä¹…çš„ç”Ÿå‘½æ”¿æ²»è·¯å¾„ã€‚\nåŒæ ·ï¼Œæ—¥æœ¬æ”¿åºœåœ¨1871å¹´æ¨è¡Œäº†â€œæˆ·ç±â€åˆ¶åº¦ï¼ˆkosekiï¼‰ï¼Œæœ‰æ„è¯†åœ°å°†å„’å®¶å®¶åº­ä¸»ä¹‰èå…¥æ˜æ²»æ—¶æœŸçš„å®¶å›½æ„è¯†å½¢æ€ä¸­ã€‚è¯¥æ”¿ç­–ä¸ä¸­å›½çš„æˆ·å£åˆ¶åº¦ç±»ä¼¼ï¼Œè¦æ±‚â€œæ¯æˆ·å¿…é¡»æŠ¥å‘Šå‡ºç”Ÿã€æ­»äº¡ã€å©šå§»ä¸æ”¶å…»æƒ…å†µâ€ï¼ˆã€Šæˆ·ç±æ³•ã€‹ï¼‰ã€‚å½“æ˜æ²»ç»´æ–°æ—¶æœŸå¼•å…¥è¥¿æ–¹å…¬å…±å«ç”Ÿæ”¿ç­–â€”â€”å°¤å…¶æ˜¯1872å¹´ã€Šä¼ æŸ“ç—…é¢„é˜²æ³•ã€‹â€”â€”æ—¶ï¼Œæ‰€æœ‰ç–«è‹—æ¥ç§ã€éš”ç¦»å‘½ä»¤ä¸å«ç”Ÿæ£€æŸ¥å‡é€šè¿‡åœ°æ–¹å®¶åº­ç™»è®°ä¸æ‘åº„é¦–é¢†æ¥æ‰§è¡Œã€‚åœ¨å…¶æ ¸å¿ƒï¼Œ æ˜¯â€œå®¶â€ï¼ˆieï¼‰çš„æ¦‚å¿µï¼šä¸€ä¸ªç”¨äºæ³•å¾‹ã€ç»æµä¸ç²¾ç¥ç›®çš„çš„æ‰©å±•å®¶åº­å•ä½ã€‚è¿™ä¸ä»…ä»…æ˜¯ä¸€ä¸ªéšå–»ï¼Œè¿˜è¢«1898å¹´ã€Šæ˜æ²»æ°‘æ³•å…¸ã€‹æ­£å¼ç¡®è®¤ä¸ºç¤¾ä¼šçš„åŸºæœ¬å•ä½ã€‚åœ¨è¿™ä¸€åˆ¶åº¦ä¸­ï¼Œå¥³æ€§è¢«æ˜ç¡®è§„å®šåœ¨â€œè‰¯å¦»è´¤æ¯â€çš„å£å·ä¸‹æ‰¿æ‹…æ”¯æŒæ€§ã€å†ç”Ÿäº§è§’è‰²ï¼›è€Œå®¶é•¿åˆ™è¢«åŠ¨å‘˜ä¸ºå›½å®¶è¡Œä½¿å©šå§»ã€ç»§æ‰¿ä¸å®¶åº­çºªå¾‹ç­‰æ–¹é¢çš„ç”Ÿå‘½æƒåŠ›ä»£ç†äººã€‚æ—¥æœ¬è‘—åå¥³æ€§ä¸»ä¹‰è€…ä¸Šé‡åƒé¹¤å­ï¼ˆChizuko Uenoï¼‰æ·±å—ç¦æŸ¯æƒåŠ›ç†è®ºçš„å¯å‘ï¼Œå¥¹æŒ‡å‡ºï¼šâ€œâ€˜å®¶â€™åˆ¶åº¦è¢«è®¾è®¡æˆç¬¦åˆç°ä»£æ°‘æ—å›½å®¶çš„æ¨¡å‹ï¼Œè€Œæ°‘æ—å›½å®¶æœ¬èº«æ­£æ˜¯ä»¿ç…§å®¶åº­ç»“æ„æ„å»ºçš„â€ï¼ˆUeno 57ï¼‰ã€‚åœ¨è¿™ç§å®¶å›½åŒæ„è§‚å¿µä¸­ï¼Œæ˜æ²»å›½å®¶ä¸ä¸­å›½æ¸…æœ«ç±»ä¼¼ï¼Œä¹Ÿå°†è¥¿æ–¹ç”Ÿå‘½æ”¿æ²»æŠ€æœ¯å«æ¥åˆ°æ—¢æœ‰çš„å®¶åº­æ ¼å±€ä¸Šï¼Œä½¿æ¯ä¸€ä¸ªå®¶åº­ï¼ˆieï¼‰æˆä¸ºæ²»ç†ä¸äººå£ç®¡ç†çš„åŸºæœ¬å•å…ƒã€‚\nå€¼å¾—å¼ºè°ƒçš„æ˜¯ï¼ŒæŒ‡å‡ºä¸­å›½æˆ–æ—¥æœ¬çš„â€œå®¶åº­â€”äººå£â€æ¥å£æœ€ä¸ºå¯†é›†ï¼Œå¹¶ä¸æ„å‘³ç€â€œä¸ªä½“â€”äººå£â€å‹çš„è¥¿æ–¹ç”Ÿå‘½æƒåŠ›åœ¨è¿™é‡Œç¼ºå¸­ã€‚ç›¸åï¼Œå°¤å…¶åœ¨æœ¬æ–‡æ‰€è¿°çš„ä¸­æ—¥ä¸¤å›½ä¾‹å­ä¸­ï¼Œå›½å®¶å®˜åƒšä½“ç³»æ—¥ç›Šé¢‘ç¹åœ°ç»•è¿‡å®—æ—ç»“æ„ï¼Œç›´æ¥å¯¹ä¸ªä½“è¿›è¡Œè§„è®­ï¼Œä¸æ¬§æ´²ç›¸ä¼¼ã€‚è¿™ç±»ç›´æ¥å¹²é¢„æ˜¾ç¤ºå‡ºä¸­æ—¥ä¸¤å›½åœ¨ç°ä»£åŒ–æ”¹é©è¿‡ç¨‹ä¸­è¿ˆå‘ä¸ªä½“åŒ–ï¼ˆmoves towards individualizationï¼‰çš„åŠªåŠ›ã€‚è™½ç„¶ä¸ªä½“å±‚é¢çš„æ²»ç†æªæ–½ä»ä¸å®¶å›½åŒæ„é€»è¾‘ä¿æŒå¯¹è¯ï¼Œä½†å®ƒä»¬å·²ç»é€æ¸ä¸å®—æ—é€šé“å¹¶è¡Œã€‚è¿™ç§æ”¹é©è¿›ç¨‹æ­£æ˜¯ç»´æŒè¥¿æ–¹å¼ç”Ÿå‘½æƒåŠ›åœ¨ä¸œäºšå¾—ä»¥æŒç»­å±•å¼€çš„å…³é”®æ‰€åœ¨ã€‚\n3. ç‹¬ç”Ÿå­å¥³æ”¿ç­–ï¼šå®¶åº­å¼ç”Ÿå‘½æƒåŠ›çš„ç°ä»£è¡Œæ”¿èŒƒå¼ # è‹¥è¦åœ¨å½“ä»£ç¤¾ä¼šä¸­å¯»æ‰¾å®¶åº­å¼ç”Ÿå‘½æƒåŠ›ï¼ˆfamilial biopowerï¼‰åˆ¶åº¦åŒ–çš„å…¸èŒƒï¼Œä¸­å›½äº1980å¹´å…¨å›½æ¨è¡Œçš„ç‹¬ç”Ÿå­å¥³æ”¿ç­–æ— ç–‘æ˜¯æœ€å…·ä»£è¡¨æ€§çš„ç”Ÿå‘½æ”¿æ²»ï¼ˆbiopoliticsï¼‰å®ä¾‹ä¹‹ä¸€ã€‚è¯¥æ”¿ç­–æ·±å…¥ä»‹å…¥æœ€ä¸ºç§å¯†çš„é¢†åŸŸï¼Œæ˜¯ç¦æŸ¯æ‰€é˜è¿°çš„ç°ä»£è§„è®­æ€§æƒåŠ›ï¼ˆregulatory powerï¼‰åœ¨å¾®è§‚å±‚é¢æ§åˆ¶å†ç”Ÿäº§è¿‡ç¨‹çš„å…·ä½“ä½“ç°ï¼šå›½å®¶ä¸ºæ¯ä¸ªå®¶åº­è®¾å®šç”Ÿè‚²é…é¢ï¼Œéšåé€šè¿‡æˆ·ç±ç™»è®°ã€ç½šæ¬¾ä¸æœåŠ¡æ‹’ç»ç­‰æ‰‹æ®µï¼Œåœ¨å¤§è§„æ¨¡å±‚é¢è°ƒèŠ‚ç”Ÿè‚²è¡Œä¸ºã€‚äººå£ç®¡ç†ä¹‹æ‰€ä»¥ç²¾ç¡®è‡³ä¸ªä½“å®¶åº­ï¼Œè€Œéä»¥å…¶ä»–æ”¿æ²»å½¢å¼ä¸ºå•ä½ï¼Œå¹¶éå¶ç„¶ã€‚è¿™ç§æœºåˆ¶ä¾æ‰˜äºä¸­åäººæ°‘å…±å’Œå›½è‡ªå»ºå›½ä»¥æ¥ç¡®ç«‹çš„å…¨å›½æ€§æˆ·ç±åˆ¶åº¦ï¼ˆhukou systemï¼‰ï¼Œè¯¥åˆ¶åº¦å°†æ¯ä¸ªäººä¾æ³•æŒ‰ç…§å®¶åº­å•ä½è¿›è¡Œåˆ†ç±»ã€‚ä¸è¥¿æ–¹ç¤¾ä¼šæ™®éä»¥ä¸ªäººä¸ºå•ä½è¿›è¡Œäººå£ä¸å±…ä½ç™»è®°ä¸åŒï¼Œä¸­å›½çš„æˆ·ç±åˆ¶åº¦å°†æ¯ä¸ªä¸ªä½“ä¸ç‰¹å®šåœ°åŒºç»‘å®šâ€”â€”æ— è®ºæ˜¯å°±å­¦ã€å°±åŒ»ã€è¿å¾™ç®¡æ§è¿˜æ˜¯ç¦åˆ©è·å–ï¼Œå‡éœ€ä¾æ‰˜æˆ·ç±ã€‚è¿™ç§å…·åœ°åŸŸæ€§çš„åˆ¶åº¦ç»“æ„å……åˆ†ä½“ç°äº†ä¸œäºšâ€œä¸ªä½“â€”å®¶åº­â€”äººå£â€æ··åˆæ¨¡å¼çš„æ ¸å¿ƒç†å¿µï¼šä¸»ä½“å¹¶éå®Œå…¨è‡ªä¸»çš„ä¸ªä½“ï¼Œè€Œæ˜¯é€šè¿‡å®¶åº­å…³ç³»è€Œå»ºæ„çš„ç›¸äº’å…³è”çš„å­˜åœ¨ã€‚åœ¨è¿™ä¸€æ¶æ„ä¸­ï¼Œæ¯ä¸€åæ–°ç”Ÿå„¿çš„å‡ºç”Ÿå¿…é¡»ç»è¿‡æ‰¹å‡†ï¼Œæ–¹å¯è·å¾—å‡ºç”Ÿå’Œæ­»äº¡ç™»è®°ã€å­¦æ ¡å…¥å­¦èµ„æ ¼åŠå„ç±»å…¬å…±æœåŠ¡çš„å‡†å…¥ã€‚è¿™æ„å‘³ç€ï¼Œâ€œå®¶åº­â€è¿™ä¸€æ¦‚å¿µé€šè¿‡æˆ·ç±åˆ¶åº¦è¢«åˆ¶åº¦åŒ–ï¼Œæˆä¸ºå›½å®¶å¯æ§çš„æ”¿æ²»å•å…ƒï¼Œå‘ˆç°å‡ºä¸€ç§åœ¨åœ°åŒ–çš„å®¶åº­å¼ç”Ÿå‘½æƒåŠ›ç‰ˆæœ¬ï¼šå³**â€œä¸ªä½“â€”æˆ·ç±â€”äººå£â€**çš„ç”Ÿå‘½æ”¿æ²»æœºåˆ¶ã€‚\nåœ¨è¿™ä¸€å®¶åº­å¼æƒåŠ›ç½‘ç»œä¸­ï¼Œå›½å®¶çš„ç”Ÿå‘½æ”¿æ²»è®¡ç®—â€”â€”è¯¸å¦‚â€œæ¯å¹´å…è®¸å¤šå°‘å‡ºç”Ÿï¼Ÿâ€â€”â€”é€šè¿‡æˆ·ç±è¿™ä¸€å®¶åº­èŠ‚ç‚¹è¿›è¡Œï¼Œè€Œå…¶çºªå¾‹æœºå™¨ã€ç›‘æ§æœºåˆ¶ä¸æƒ©ç½šæªæ–½åˆ™ç›´æ¥è½åœ¨ä¸ªä½“èº«ä¸Šï¼Œå°¤å…¶æ˜¯å¥³æ€§ã€‚å¦‚è‹çŠÂ·æ ¼æ—å“ˆåˆ©ï¼ˆSusan Greenhalghï¼‰æ‰€æŒ‡å‡ºï¼šâ€œå®¶åº­æˆä¸ºå›½å®¶ç”Ÿå‘½æƒåŠ›çš„å‰æ²¿é˜µåœ°ï¼Œäººå£æŒ‡æ ‡è¢«ç›´æ¥ä¹¦å†™åœ¨æ¯ä¸€æˆ·çš„æ¡£æ¡ˆä¹‹ä¸­â€ï¼ˆGreenhalgh, 22ï¼‰ã€‚ç”¨ç¦æŸ¯çš„æœ¯è¯­æ¥è¯´ï¼Œå°½ç®¡æˆ·ç±å¹¶éçœŸæ­£æ„ä¹‰ä¸Šçš„â€œå…¨æ™¯æ•è§†ç›‘ç‹±â€ï¼ˆPanopticonï¼‰ï¼Œä½†å®ƒå´å…·å¤‡å…¨æ™¯æ•è§†åŠŸèƒ½ï¼šæ¯æˆ·äººå®¶éƒ½æ˜ç™½ï¼Œä»–ä»¬åœ¨æŒç»­è¢«ç™»è®°ã€æ£€æŸ¥åŠç”±åŸºå±‚è®¡åˆ’ç”Ÿè‚²å®˜å‘˜ç›‘ç®¡çš„è¿‡ç¨‹ä¸­å¤„äºå¯è§†çŠ¶æ€ã€‚è¿™ç§æ— å½¢çš„å‡è§†è§„èŒƒäº†å¤«å¦»çš„è¡Œä¸ºï¼Œä½¿ä»–ä»¬åœ¨é¢„æœŸçš„ç›‘æ§ä¸‹å†…åŒ–ç‹¬ç”Ÿå­å¥³æ”¿ç­–ã€‚\nåœ¨ä¸œäºšï¼Œç°ä»£ç”Ÿå‘½æƒåŠ›ä»æœªå­¤ç«‹é™ä¸´ï¼Œè€Œæ˜¯ä»å„’å®¶å®¶åº­æ—©å·²æ•å¼€çš„é‚£æ‰‡é—¨è¿›å…¥ã€‚ç”Ÿå‘½æ”¿æ²»ä¸å„’å®¶å›¾å¼ç›¸èåˆï¼Œå…¶ä¸­â€œå®¶åº­ç§©åºâ€å³â€œæ”¿æ²»ç§©åºâ€ï¼Œâ€œå®¶åº­å•å…ƒâ€å³â€œæ”¿æ²»å•å…ƒâ€ã€‚è¿™ç§å®¶å›½åŒæ„ï¼ˆfamily-state isomorphismï¼‰æ­ç¤ºå‡ºè¿œæ—©äºç°ä»£æ€§é—¨æ§›ï¼ˆthreshold of modernityï¼‰ä¹‹å‰å°±å·²å­˜åœ¨çš„åŸå§‹çºªå¾‹æ€§ï¼ˆproto-disciplinaryï¼‰ä¸åŸå§‹ç”Ÿå‘½æ”¿æ²»å®è·µï¼ˆproto-biopolitical practicesï¼‰ï¼ŒæŒ‡å‘äº†ä¸€æ¡æ›´ä¸ºæ‚ é•¿ã€å¹³ç¨³çš„ç”Ÿå‘½æƒåŠ›ç³»è°±è·¯å¾„ã€‚åœ¨è¿™ä¸€â€œç”Ÿç‰©-å®¶åº­â€ï¼ˆbio-familialï¼‰æ”¿ä½“ä¸­ï¼Œå›½å®¶å¯¹ç”Ÿè‚²ç‡ã€ç–¾ç—…ç‡å’Œç”Ÿäº§åŠ›çš„è®¡ç®—é¦–å…ˆåœ¨å®—æ—ç™»è®°ç³»ç»Ÿä¸­æµé€šï¼Œç„¶åæ‰ä¼ é€’è‡³ä¸ªä½“èº«ä½“ã€‚ä»å‘¨ä»£çš„ä¿ç”²åˆ¶åº¦ï¼ˆbaojiaï¼‰ï¼Œåˆ°æ˜æ²»æ—¶æœŸçš„æˆ·ç±åˆ¶åº¦ï¼ˆkosekiï¼‰ï¼Œå†åˆ°æ¸…æœæ–°æ”¿åŠå½“ä»£ä¸­å›½çš„ç‹¬ç”Ÿå­å¥³æ”¿ç­–ä¸ä¸‰èƒæ”¿ç­–ï¼Œå®¶åº­å§‹ç»ˆæ˜¯å›½å®¶â€œä½¿å…¶ç”Ÿã€ä»»å…¶æ­»â€æ²»ç†é€»è¾‘çš„æ ¸å¿ƒå·¥å…·ã€‚å› æ­¤ï¼Œç¦æŸ¯æ‰€æç»˜çš„ç”Ÿå‘½æƒåŠ›é—¨æ§›ï¼Œå¹¶éä¸€æ¬¡æ–­è£‚æ€§çš„æ–­å±‚ï¼Œè€Œæ˜¯æƒåŠ›èšç„¦ç”Ÿå‘½æ‰€é‡‡å–çš„è§„æ¨¡ä¸ä½ç½®çš„è½¬ç§»ã€‚\nèšç„¦è¿™ç§æ··åˆçš„å®¶åº­å¼ç”Ÿå‘½æƒåŠ›ï¼Œä¿ƒä½¿æˆ‘ä»¬å¿…é¡»åœ¨å„’å®¶â€”å®¶åº­èƒŒæ™¯ä¸‹é‡æ–°æªè¾ç¦æŸ¯çš„ç†è®ºã€‚è‹¥è¦ç†è§£ä»Šæ—¥çš„ç”Ÿå‘½åœ¨ä¸œäºšå¦‚ä½•è¢«æ²»ç†ï¼Œæˆ‘ä»¬å¿…é¡»åƒç¦æŸ¯ç ”ç©¶ç›‘ç‹±ã€å­¦æ ¡ã€ç²¾ç¥ç—…é™¢å’Œè¯Šæ‰€é‚£æ ·å»ç ”ç©¶â€œå®¶åº­ç™»è®°åˆ¶åº¦â€ï¼›åœ¨ä¸œäºšï¼Œè¿™äº›æœºæ„è¢«å†™åœ¨åŒä¸€æœ¬ç°¿å†Œä¸­ã€‚\n"},{"id":35,"href":"/docs/Philosophy/%E7%A6%8F%E6%9F%AF%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E7%89%A9%E7%90%86%E5%AD%A6%E8%BD%AC%E5%8F%98%E4%BB%A5%E5%8F%8A%E6%96%B0%E8%8C%83%E5%BC%8F%E7%9A%84%E5%8F%AF%E8%83%BD/","title":"ç¦æŸ¯è§†è§’ä¸‹çš„ç‰©ç†å­¦è½¬å˜ä»¥åŠæ–°èŒƒå¼çš„å¯èƒ½","section":"Philosophy","content":"æˆ‘ç›¸ä¿¡ï¼Œ\nç¿»å¼€ã€Šè¯ä¸ç‰©ã€‹æ²¡è¯»å®Œç¬¬ä¸€ç« å°±åˆä¸Šäº†ï¼Œç„¶åè‰ç‡åœ°è¯»äº†ç»“å°¾â€”â€”æˆ‘å®åœ¨å¯¹è¿™ç§èŠ‚å¥å·¨æ…¢çš„å†å²å­¦åºæ‚å¾å¼•æ„Ÿåˆ°ç´¢ç„¶æ— å‘³ï¼Œä»¥è‡³äºä¸å¾—ä¸è¯»ä¸¤é¡µå›½å†…çš„æ•°å­¦ä¹¦æ¥ç»¼åˆä¸€ä¸‹\u0026hellip; æˆ‘å¯¹ç‰©ç†å­¦çš„æ€åº¦ï¼Œä»¥åŠå¯¹å…¶ç ”ç©¶æ–¹å‘çš„ç›´è§‰ï¼Œè¿œè¿œå¤§äºæˆ‘æœ¬èº«å¯ä»¥å»¶å±•ä»–ä»¬çš„èƒ½åŠ›ï¼ˆå¾ˆå¯æƒœï¼Œå¦åˆ™æˆ‘å¿…ç„¶æ¯«ä¸çŠ¹è±«çš„æŠ•èº«å…¶ä¸­ï¼‰ã€‚ä½†æ˜¯å¹¸è¿çš„æ˜¯ï¼Œå¯¹äºåŸºç¡€ç§‘å­¦æ¥è¯´ï¼Œè¿™ä¼šæ˜¯äººç±»å†å²å½“ä¸­ï¼Œæå‡ºæ­£ç¡®çš„é—®é¢˜æ‰€èƒ½å¸¦ç»™æˆ‘ä»¬çš„æ•ˆç›Šæœ€å¤§è¯çš„æ—¶ä»£ã€‚æˆ‘çœŸæ­£éœ€è¦æå‡ºä»–å¯¹çŸ¥è¯†è¿›è¡Œäº†æ¿€è¿›çš„å†å²åˆ†æï¼Œå¹¶ä¸”æ„å»ºäº†å’Œç»å¤§éƒ¨åˆ†çš„ç§‘å­¦å®¶ç§‰æŒçš„è®¤è¯†è®ºæœ‰æ‚–çš„å†å²æ–­è£‚å¼â€œçŸ¥è¯†è®ºé¢†åŸŸâ€ã€‚\næ‰€è°“çš„çŸ¥è¯†è®ºé¢†åŸŸï¼ŒåŸæ–‡é‡Œè¯´ï¼š\n[\u0026hellip;] epistemological field, the episteme in which knowledge, envisaged apart from all criteria having reference to its rational value or to its objective forms, grounds its positivity and thereby manifests a history which is not that of its growing perfection, but rather that of its conditions of possibility.\nå½“æˆ‘ç¬¬ä¸€æ—¶é—´çœ‹åˆ°ä»–æ‰€è¯´çš„çŸ¥è¯†çš„â€œfieldâ€çš„æ—¶å€™ï¼Œæƒ³åˆ°çš„ä¸æ˜¯ä¼ ç»Ÿç¿»è¯‘é‡Œçš„â€œé¢†åŸŸâ€ï¼Œè€Œæ˜¯ä¸€ç§ååˆ†ç±»ä¼¼äºç‰©ç†å­¦ä¸­çš„â€œåœºâ€çš„æ¦‚å¿µï¼šä¸€ç§ä¸å¯è§ä½†ç»“æ„åŒ–çš„å½±å“åŠ›ï¼Œå†³å®šäº†å…¶ä¸­å¯¹è±¡çš„è¡Œä¸ºæ–¹å¼ã€‚è€Œè¿™ç§å—â€œåœºâ€æ‰€é¢„è®¾çš„çŸ¥è¯†çš„æ„å»ºè½¨è¿¹å°±ç›¸æ¯”äºå…‰åœ¨å¤§è´¨é‡é»‘æ´æ—è¾¹çš„è¡ŒåŠ¨â€”â€”ä¼¼ä¹å…‰ä»…ä»…æ˜¯ä¾ç…§ç€å®ƒçš„å‡†åˆ™ï¼Œä¸€ç§è‚‰çœ¼å¯è§çš„å¼¯æ›²é“è·¯è¡Œè¿›ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œä»–ä»¬åœ¨è¢«å¼•åŠ›åœºæ‰€å¼¯æ›²çš„é¢„è®¾æ—¶ç©ºä¸­ï¼Œèµ°ç›´çº¿ï¼ˆåŸºäºç†æ€§çš„å®¢è§‚æ€§çš„çŸ¥è¯†å‘å±•ï¼‰ä¾¿æ˜¯æ‰€è§çš„â€œå¼¯è·¯â€ã€‚ä½†æ˜¯ç¦æŸ¯çœŸæ­£æ‰€é¢ä¸´çš„é—®é¢˜åœ¨äºï¼Œå¦‚æœæ²¿ç”¨ä»¥ä¸Šçš„ç±»æ¯”ï¼Œæˆ‘ä»¬æ— ä»ä»¥è§‚æµ‹è€…çš„èº«ä»½çŸ¥é“ä»¥ä»€ä¹ˆæ˜¯ç›´çº¿ï¼Œè€Œä»€ä¹ˆæ˜¯å¼¯æ›²ã€‚\nä½œä¸ºå…¶è€ƒå¤å­¦æ–¹æ³•çš„æ ¸å¿ƒï¼Œç¦æŸ¯è¯•å›¾â€œæ­ç¤ºâ€çŸ¥è¯†çš„åŸºç¡€è®¤è¯†å‹ï¼ˆÃ©pistÃ©mÃ¨ï¼‰ï¼Œä»¥åŠå…¶è®¤è¯†è®ºåœºï¼ˆepsitomalogical fieldï¼‰çš„å†å²æ€§å˜åŒ–ã€‚ç¦ç§‘æŒ‘æˆ˜äº†ä¼ ç»Ÿè®¤è¯†è®ºï¼Œé‚£äº›æ‰€æœ‰åŸºäºç†æ€§ä¸å®¢è§‚æ€§çš„æ™®éæ ‡å‡†çš„è®¤çŸ¥æ¡†æ¶ï¼Œå¹¶ä¸”å°†ä»–ä»¬éƒ½è§†ä¸ºäº†å†å²å¶ç„¶ï¼ˆæˆ–æˆä¸ºäº†åŸæ–‡æ‰€è¯´çš„ï¼ŒæŒ‡å‘\u0026quot;ç†æ€§ä¸å®¢è§‚æ€§çš„\u0026quot;å¯èƒ½æ¡ä»¶\u0026quot;ä¹‹ä¸€ï¼‰ã€‚\nçŸ¥è¯†â€œç¡®ç«‹å…¶å®è¯æ€§ï¼ˆpositivitÃ©ï¼‰å¹¶ç”±æ­¤å±•ç°å†å²â€ï¼Œå‘ˆç°äº†ä»–å¯¹è®¤è¯†è®ºæœ€æ¿€è¿›çš„è§‚ç‚¹ä¹‹ä¸€ã€‚çŸ¥è¯†çš„å®è¯æ€§åœ¨å†å²å…ˆéªŒï¼ˆa priori historiqueï¼‰å»ºç«‹çš„ç©ºé—´ä¸­è¢«å»ºæ„ï¼Œå› è€Œé€šè¿‡èƒ½è¢«å‘ç°ä¸º\u0026quot;çŸ¥è¯†\u0026quot;ï¼ˆsavoirï¼‰çš„äº‹ç‰©è·å¾—æœ‰æ•ˆæ€§ã€‚å¯¹ç¦æŸ¯è€Œè¨€ï¼ŒçŸ¥è¯†æè¿°ç°å®çš„æœ‰æ•ˆæ€§ä¸çŸ¥è¯†æœ¬èº«ä¹‹é—´çš„å¯¹åº”å…³ç³»è¢«æ ¹æœ¬æ€§åŠ¨æ‘‡ï¼Œå› ä¸ºè¿™ç§å…³ç³»æ²¡æœ‰å®¢è§‚ä¿è¯â€”â€”ç”±äºè®¤è¯†å‹æ„å»ºäº†æˆ‘ä»¬çš„æ„ŸçŸ¥ï¼Œä»è€Œä½¿ç°å®æœ¬èº«å…·æœ‰å†å²æ¡ä»¶æ€§ã€‚\nä»–æ˜ç¡®æ‹’ç»çŸ¥è¯†å‘\u0026quot;æ—¥ç›Šå®Œå–„\u0026quot;ï¼ˆperfection croissanteï¼‰è¿›æ­¥çš„è§‚å¿µã€‚è¿™æˆ–è®¸æ˜¯ä»–çš„è®¤çŸ¥è®ºæœ€æ¿€è¿›çš„è§‚ç‚¹ä¹‹ä¸€ï¼Œè¿›æ­¥ä¸ä»…ä¸æ˜¯çº¿æ€§ä¸”è¿ç»­çš„ï¼Œè€Œæ˜¯è¿›æ­¥æœ¬èº«æ ¹æœ¬ä¸å¯èƒ½å­˜åœ¨â€”â€”å› ä¸ºè®¤è¯†å‹æ–­è£‚ï¼ˆrupture Ã©pistÃ©mologiqueï¼‰ä¼šæ”¹å˜ä½•ä¸ºçŸ¥è¯†çš„æ¡†æ¶ï¼Œå¹¶ä½¿å…ˆå‰çš„æ€ç»´æ–¹å¼å˜å¾—ä¸å¯æƒ³è±¡ï¼ˆimpensableï¼‰ï¼ˆå“ªæ€•æ˜¯å’Œä»–ç±»ä¼¼çš„æ‰˜é©¬æ–¯Â·åº“æ©çš„æ–­è£‚å¼ç§‘å­¦è§‚ä¸­çš„è¿ç»­æ€§ï¼Œä¹Ÿè¢«å®Œå…¨æŠ›å¼ƒï¼‰ã€‚ç”±äºè®¤è¯†å‹æ„å»ºäº†çŸ¥è¯†å²å±•å¼€çš„æ¡ä»¶ï¼Œå®ƒå†³å®šäº†å†å²åˆ¶åº¦ä¸è¯è¯­å°†çŸ¥è¯†\u0026quot;ç¡®ç«‹\u0026quot;ä¸ºåˆæ³•æ€§çš„æ¡†æ¶ã€‚\nè¿™ç”¨æ¥æ”¯æ’‘äº†ä»–å¯¹ç–¯ç‹‚ã€çŠ¯ç½ªä¸æ€§æ€è¯è¯­çš„è€ƒå¯Ÿï¼Œä¾‹å¦‚å†å²ä¸Šä¸æ–­å˜åŒ–çš„ç–¯ç‹‚è®¤è¯†å‹å¦‚ä½•é€šè¿‡ç¦é—­ï¼ˆconfinementï¼‰é€»è¾‘æˆ–ç²¾ç¥ç—…å­¦æƒå¨ä¸‹çš„åŒ»å­¦åŒ–é‡æ–°å®šä¹‰å…¶å†…æ¶µã€‚æˆ‘å¯¹è¿™æ®µæ–‡æœ¬çš„ç»†è¯»è¿«ä½¿è‡ªå·±è·³å‡ºæ­£åœ¨ç ”ç©¶çš„ç§‘å­¦é¢†åŸŸï¼Œåæ€æ‰€è°“\nå¦‚æœæˆ‘ä»¬æ¥å—è®¤è¯†è®ºè¿›æ­¥æ˜¯æŸä¸€ç§å†å²å¹»è§‰ï¼Œé‚£ä¹ˆé‡æ–°è¯„ä¼°ç‰©ç†å­¦çš„è½¬å˜ï¼Œäºæ˜¯æˆ‘ä»¬ä¾¿å¯ä»¥è¯´ï¼š\nç›¸å¯¹è®ºçš„è¯ç”Ÿå¹¶æ²¡æœ‰è¶…è¶Šäº†ç‰›é¡¿åŠ›å­¦ï¼Œå–å¾—äº†æ›´åŠ æ­£ç¡®çš„ç»“è®ºï¼Œè€Œä»…ä»…æ˜¯çŸ¥è¯†è®ºé¢†åŸŸåœºä»ç»å¯¹æ—¶ç©ºè½¬ç§»åˆ°äº†ç›¸å¯¹æ—¶ç©ºâ€”â€”è¿™æ˜¯ä¸€ç§â€œÃ©pistÃ©mÃ¨â€çš„è½¬ç§»ã€‚é‚£ä¹ˆè‡³äºç›¸å¯¹æ—¶ç©ºå¯¹ç‰©ç†ç°å®çš„é€¼è¿›æ˜¯å¦ä¼˜äºç»å¯¹æ—¶ç©ºï¼Œè¿™ä¸ªé—®é¢˜æ˜¯æ— æ³•ä»¥ç»å¯¹æ­£ç¡®çš„æ–¹å¼è¢«æå‡ºçš„ï¼Œå› ä¸ºç°ä»£ç‰©ç†å­¦çš„èŒƒå¼ - å®éªŒï¼Œè¯ä¼ªæ€§ï¼ŒåŒè¡Œè¯„ä¼° - å·²ç»è¢«å¡‘é€ æˆäº†è¯„ä¼°è¿™ä¸ªé—®é¢˜çš„æ ¸å¿ƒèŒƒå¼ã€‚ä»å†å²å­¦çš„è§’åº¦ä¸Šè¯´ï¼Œçˆ±å› æ–¯å¦çš„å¼¯æ›²å‡ ä½•ã€è–›å®šè°”çš„æ³¢å‡½æ•°ï¼Œæœ¬è´¨ä¸Šéƒ½æ˜¯æƒåŠ›é…ç½®çŸ¥è¯†ç”Ÿäº§çš„å†å²å…ˆéªŒï¼ˆa priori historiqueï¼‰ã€‚ç‰©ç†å­¦å²ä¸­é‚£äº›è¢«è§†ä¸ºâ€œè‡ªç„¶â€çš„çœŸç†ç§©åºåœ¨ç¦ç§‘çš„è®¤è¯†è®ºèŒƒå›´å†…è¢«å½»åº•è§£æ„ã€‚\né‚£ä¹ˆ\n"},{"id":36,"href":"/docs/Physics/Quantum-Mechenics/Homework/HW3-Code/","title":"Hw3 Code","section":"é‡å­åŠ›å­¦è®²ä¹‰","content":"import numpy as np import scipy.sparse as sp import scipy.sparse.linalg as spla import matplotlib.pyplot as plt\ndef computation(): #parameters N, V0_tilde, L = 600, 10.0, 1.0 M, dx = N - 2, L / (N - 1) x_vals = np.linspace(-0.5 * L + dx, 0.5 * L - dx, M) # Interior points\n# KE matrix T (tridiagonal) factor = (N**2) / (np.pi**2) T = sp.diags([np.full(M - 1, -factor), np.full(M, 2 * factor), np.full(M - 1, -factor)], [-1, 0, 1]) # PE matrix V (diagonal) V = sp.diags(np.where(np.abs(x_vals) \u0026lt; (L / 6), V0_tilde, 0)) #Hamiltonian H = T + V H = T + V #solve for lowest two eigenvalues/eigenvectors eigvals, eigvecs = spla.eigsh(H, k=2, which='SM') eigvals, eigvecs = zip(*sorted(zip(eigvals, eigvecs.T))) # Sort eigenvalues \u0026amp; vectors print(f\u0026quot;Ground state energy = {eigvals[0]}\u0026quot;) print(f\u0026quot;1st excited energy = {eigvals[1]}\u0026quot;) #include boundary points x_full = np.linspace(-0.5 * L, 0.5 * L, N) psi_full = [np.concatenate(([0], psi, [0])) for psi in eigvecs] # Plot wavefunctions plt.figure(figsize=(8,6)) plt.plot(x_full, psi_full[0], color='red',label=\u0026quot;Ground State\u0026quot;) plt.plot(x_full, psi_full[1], color='blue', label=\u0026quot;1st Excited State\u0026quot;) plt.axvspan(-L/6, L/6, color='gray', alpha=0.1, label='Barrier region') plt.title(\u0026quot;Wavefunctions for lowest two states\u0026quot;) plt.xlabel(\u0026quot;x (dimensionless)\u0026quot;) plt.ylabel(\u0026quot;Ïˆ(x)\u0026quot;) plt.legend() plt.grid() plt.show() if name == \u0026ldquo;main\u0026rdquo;: computation()\n"},{"id":37,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E4%B8%80%E7%AB%A0/1.1-The-Wave-Function/","title":"1.1 the Wave Function","section":"ç¬¬ä¸€ç« ","content":"To find a particle\u0026rsquo;s wave function, $\\psi(x,t)$, we solve:\nlogically analogous to Newton\u0026rsquo;s Second Law $F=ma$\n[!definition] Schrodinger\u0026rsquo;s Equation $$i\\hbar \\frac{ \\partial \\Psi}{ \\partial t}=-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2}\\Psi}{ \\partial x^{2} }+V \\Psi$$\nwhere Planck\u0026rsquo;s constant $\\hbar=\\frac{h}{2\\pi}=1.054573 \\times 10^{-34}$.\n[!definition] Born\u0026rsquo;s Statistical Interpretation $$\\int^{a}_{b} |\\Psi(x,t)^{2}| , dx $$ which is the probability finding the particle between $a$ and $b$.\nIt is natural to wonder whether it is a fact of nature, or a defect in theory.\nThree quantum indeterminacy position: # realist the particle was at C. (a hidden variable?) orthodox (Copenhagen Interpretation) the particle wasn\u0026rsquo;t anywhere. (measurement produce the result) most widely accepted position (agnosticism) refuse to answer. That is, no meaning to ask such question. Pauli: one should no more rack one\u0026rsquo;s brain about the problem of whether something one cannot know anything about exists all the same, than about the ancient question of how many angels are able to sit on the point of needle.\nfall-back position, however, eliminated by John Bell\u0026rsquo;s experiment in 1964 Two Distinct Physical Processes: # Ordinary evolves in a leisurely fashion under Measurements wave equation $\\Psi$ discontinuously collapses, when the first measurement radically alters the function. "},{"id":38,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E4%BA%8C%E7%AB%A0/2.1-Time-Independent-Schrodinger-Equation-Stationary-States/","title":"2.1 Time Independent Schrodinger Equation Stationary States","section":"ç¬¬äºŒç« ","content":" Music: Harmonics # $$ \\begin{align} C_{1}:f_{1}\u0026amp;=f_{0} \\ C_{2}:f_{1}\u0026amp;=2f_{0} \\ G:f_{1}\u0026amp;=3f_{0} \\ C_{3}:f_{1}\u0026amp;=4f_{0} \\end{align} $$\nSeparation of variables # $$ \\begin{equation} i\\hbar \\frac{ \\partial \\Psi}{ \\partial t}=-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2}\\Psi}{ \\partial x^{2} }+V_{x} \\Psi \\end{equation} $$ where $V(x)$: time independent potential $\\hat{H}=-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2} }{ \\partial x^{2} }+V(x)$. $$ H\\Psi=i\\hbar \\frac{ \\partial }{ \\partial t } \\Psi $$ We begin by= separate the variables, and set $$ \\begin{equation} \\Psi(x,t)=\\psi(x) \\phi(t) \\end{equation}\n$$\n(2) $\\Rightarrow$ (1): $$ \\begin{align} -\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2}}{ \\partial x^{2} }(\\psi(x) \\phi(t))+V_{x} \\Psi\u0026amp;=i\\hbar \\frac{ \\partial \\Psi}{ \\partial t}\\ -\\frac{\\hbar^2}{2m} \\frac{\\partial^2}{\\partial x^2} \\psi \\varphi + V \\psi \\varphi \u0026amp;= i \\hbar \\psi \\frac{\\partial}{\\partial t} \\varphi \\ \u0026amp; \\hbar = \\end{align} $$ Divide by $\\psi \\varphi$ (assumed $\\neq 0$). Why stationary? $$ \\begin{align} \\phi(x,t)\u0026amp;=\\Psi^(x,t)\\Psi(x,t) \\ \u0026amp;=(\\Psi^(x)e^{iEt/\\hbar})(\\Psi(x)e^{iEt/\\hbar}) \\ \u0026amp;=|\\Psi(x)^{2}|(e^{\\frac{iEt}{h}-\\frac{iEt}{h}}) \\ \u0026amp;=|\\Psi(x)^{2}| \\end{align} $$ # Furthermore, expectation value of dynamical variables are also time independent $$ \\langle Q(x,p)\\rangle=\\int , dx ,\\Psi^* (x,t) \\dots $$ $$ \\boxed{\\hat{H}\\Psi(x)=E\\Psi(s)} $$ $E$ is the eigentvalue here Stationary states are states of definite energy: $$ \\hat{H}=-\\frac{\\hbar}{2m}\\frac{d^{2}}{dx^{2}}+V(x) $$ This is an example of an eigenvalue equation of the operator $H$. Expectation value of the total Energy? $$ \\begin{align} \\langle \\hat{H} \\rangle \u0026amp;= \\int , dx, \\Psi^{}(x)\\hat{H}\\Psi(x) \\ \u0026amp;= E \\int , dx \\Psi^{}(x)\\Psi(x) \\ \u0026amp;=E\\int , dx ,|\\Psi(x)|^{2} \\ \u0026amp;=E \\ \\end{align} $$\nmissing two white board page ![[IMG_1165.heic]] # ![[IMG_1168.heic]] # Linearity of the S.E. $\\Longleftrightarrow$ principles of superposition\nGeneral Solution of the S. Equation # \u0026hellip; \u0026hellip; A broader case of fourier expansion.\nSuppose the system initiates at $$ \\begin{align} \\Psi(x,p) \u0026amp; =C_{1}\\Psi_{2} + C_{2}\\Psi_{2} \\ \u0026amp; = \\end{align}\n$$\n"},{"id":39,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E5%9B%9B%E7%AB%A0/4.1-%E7%BB%8F%E5%85%B8%E6%B0%A2%E5%8E%9F%E5%AD%90/","title":"4.1 ç»å…¸æ°¢åŸå­","section":"ç¬¬å››ç« ","content":" 4.1.1 ç»å…¸æ°¢åŸå­ (Classical Hydrogen Atom) # In the classical model of the hydrogen atom, we have an electron orbiting around a proton. The electrostatic potential energy is: $$E = -\\frac{1}{2}\\frac{e^2}{4\\pi\\varepsilon_0 r}$$ According to classical electrodynamics, an accelerated charged particle radiates energy. The power radiated by an accelerated particle is given by the Larmor formula: $$P = \\frac{dE}{dt} = -\\frac{e^2}{6\\pi\\varepsilon_0 c^3}a^2$$ This would cause the electron to spiral into the proton in approximately $10^{-11}$ seconds!\nImportant conclusion: Quantum mechanics provides the basis for the stability of matter, as classical physics fails to explain stable atomic structures.\n4.1.2 ä¸‰ç»´è–›å®šè°”æ–¹ç¨‹ (3D SchrÃ¶dinger Equation) # The time-dependent SchrÃ¶dinger equation in three dimensions is: $$i\\hbar\\frac{\\partial\\Psi}{\\partial t} = H\\Psi$$ where the Hamiltonian $H$ is:\n$$H = \\frac{1}{2m}(p_x^2 + p_y^2 + p_z^2) + V$$ The momentum operators in quantum mechanics are: $$p_x \\rightarrow \\frac{\\hbar}{i}\\frac{\\partial}{\\partial x}, \\quad p_y \\rightarrow \\frac{\\hbar}{i}\\frac{\\partial}{\\partial y}, \\quad p_z \\rightarrow \\frac{\\hbar}{i}\\frac{\\partial}{\\partial z}$$ More compactly: $$\\mathbf{p} \\rightarrow \\frac{\\hbar}{i}\\nabla$$\nThe canonical commutation relations for position and momentum are:\n$$[r_i, p_j] = -[p_i, r_j] = i\\hbar\\delta_{ij}, \\quad [r_i, r_j] = [p_i, p_j] = 0$$\nwhere the indices $i,j$ stand for $x$, $y$, or $z$.\n4.1.3 æ‹‰æ™®æ‹‰æ–¯ç®—å­ (Laplacian) # In terms of the Laplacian operator, the SchrÃ¶dinger equation can be written as:\n$$i\\hbar\\frac{\\partial\\Psi}{\\partial t} = -\\frac{\\hbar^2}{2m}\\nabla^2\\Psi + V\\Psi$$\nwhere the Laplacian is defined as:\n$$\\nabla^2 \\equiv \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2} + \\frac{\\partial^2}{\\partial z^2}$$\nThe wavefunction must be normalized:\n$$\\int |\\Psi|^2 d^3\\mathbf{r} = 1$$\nwhere $$d^3\\mathbf{r} = dx,dy,dz$$ is the volume element.\n4.1.4 æ—¶é—´ç‹¬ç«‹åŠ¿èƒ½çš„å¹³ç¨³æ€ (Stationary States for Time-Independent Potential) # For a time-independent potential, there exist stationary states of the form:\n$$\\Psi_n(\\mathbf{r}, t) = \\psi_n(\\mathbf{r})e^{-iE_n t/\\hbar}$$\nThese stationary states satisfy the time-independent SchrÃ¶dinger equation:\n$$-\\frac{\\hbar^2}{2m}\\nabla^2\\psi_n + V\\psi_n = E_n\\psi_n$$\nThe general solution to the time-dependent SchrÃ¶dinger equation is:\n$$\\Psi(\\mathbf{r}, t) = \\sum_n c_n\\psi_n(\\mathbf{r})e^{-iE_n t/\\hbar}$$\nThe initial state determines the coefficients:\n$$\\Psi(\\mathbf{r}, 0) = \\sum_n c_n\\psi_n(\\mathbf{r})$$\n4.1.5 ä¸­å¿ƒåŠ¿ (Central Potential) # A central potential depends only on the radial distance:\n$$V(\\mathbf{r}) = V(r)$$\nFor a central potential, we use spherical coordinates:\n$$\\mathbf{r} = (x, y, z) = (r, \\theta, \\phi)$$\nThe Laplacian in spherical coordinates is:\n$$\\nabla^2 = \\frac{1}{r^2}\\frac{\\partial}{\\partial r}\\left(r^2\\frac{\\partial}{\\partial r}\\right) + \\frac{1}{r^2\\sin\\theta}\\frac{\\partial}{\\partial\\theta}\\left(\\sin\\theta\\frac{\\partial}{\\partial\\theta}\\right) + \\frac{1}{r^2\\sin^2\\theta}\\left(\\frac{\\partial^2}{\\partial\\phi^2}\\right)$$\nThe SchrÃ¶dinger equation for a central potential becomes:\n$$-\\frac{\\hbar^2}{2m}\\left[\\frac{1}{r^2}\\frac{\\partial}{\\partial r}\\left(r^2\\frac{\\partial\\psi}{\\partial r}\\right) + \\frac{1}{r^2\\sin\\theta}\\frac{\\partial}{\\partial\\theta}\\left(\\sin\\theta\\frac{\\partial\\psi}{\\partial\\theta}\\right) + \\frac{1}{r^2\\sin^2\\theta}\\left(\\frac{\\partial^2\\psi}{\\partial\\phi^2}\\right)\\right] + V\\psi = E\\psi$$\nå˜é‡åˆ†ç¦» (Separation of Variables) # For a central potential, we can separate the variables:\n$$\\psi(r, \\theta, \\phi) = R(r)Y(\\theta, \\phi)$$\nSubstituting this into the SchrÃ¶dinger equation:\n$$-\\frac{\\hbar^2}{2m}\\left[\\frac{Y}{r^2}\\frac{d}{dr}\\left(r^2\\frac{dR}{dr}\\right) + \\frac{R}{r^2\\sin\\theta}\\frac{\\partial}{\\partial\\theta}\\left(\\sin\\theta\\frac{\\partial Y}{\\partial\\theta}\\right) + \\frac{R}{r^2\\sin^2\\theta}\\frac{\\partial^2 Y}{\\partial\\phi^2}\\right] + VRY = ERY$$\nDividing by $YR$ and multiplying by $-2mr^2/\\hbar^2$:\n$$\\begin{align} \\left{\\frac{1}{R}\\frac{d}{dr}\\left(r^2\\frac{dR}{dr}\\right) - \\frac{2mr^2}{\\hbar^2}[V(r) - E]\\right} + \\frac{1}{Y}\\left{\\frac{1}{\\sin\\theta}\\frac{\\partial}{\\partial\\theta}\\left(\\sin\\theta\\frac{\\partial Y}{\\partial\\theta}\\right) + \\frac{1}{\\sin^2\\theta}\\frac{\\partial^2 Y}{\\partial\\phi^2}\\right} = 0 \\end{align}$$\nSince the left term depends only on $r$ and the right term depends only on $\\theta$ and $\\phi$, each must equal a constant:\n$$\\frac{1}{R}\\frac{d}{dr}\\left(r^2\\frac{dR}{dr}\\right) - \\frac{2mr^2}{\\hbar^2}[V(r) - E] = \\ell(\\ell + 1)$$\n$$\\frac{1}{Y}\\left{\\frac{1}{\\sin\\theta}\\frac{\\partial}{\\partial\\theta}\\left(\\sin\\theta\\frac{\\partial Y}{\\partial\\theta}\\right) + \\frac{1}{\\sin^2\\theta}\\frac{\\partial^2 Y}{\\partial\\phi^2}\\right} = -\\ell(\\ell + 1)$$\nWhere $$\\ell(\\ell + 1)$$ is the separation constant.\n4.1.6 è§’åº¦æ–¹ç¨‹ (Angular Equation) # The angular equation is:\n$$\\sin\\theta\\frac{\\partial}{\\partial\\theta}\\left(\\sin\\theta\\frac{\\partial Y}{\\partial\\theta}\\right) + \\frac{\\partial^2 Y}{\\partial\\phi^2} = -\\ell(\\ell + 1)\\sin^2\\theta Y$$\nWe can separate variables again:\n$$Y(\\theta, \\phi) = \\Theta(\\theta)\\Phi(\\phi)$$\nPlugging this in and dividing by $\\Theta\\Phi$:\n$$\\left{\\frac{1}{\\Theta}\\left[\\sin\\theta\\frac{d}{d\\theta}\\left(\\sin\\theta\\frac{d\\Theta}{d\\theta}\\right)\\right] + \\ell(\\ell + 1)\\sin^2\\theta\\right} + \\frac{1}{\\Phi}\\frac{d^2\\Phi}{d\\phi^2} = 0$$\nThe $\\phi$ equation separates as:\n$$\\frac{1}{\\Phi}\\frac{d^2\\Phi}{d\\phi^2} = -m^2$$\n$$\\frac{d^2\\Phi}{d\\phi^2} = -m^2\\Phi \\Rightarrow \\Phi(\\phi) = e^{im\\phi}$$\nSince $\\Phi(\\phi + 2\\pi) = \\Phi(\\phi)$ must be true for single-valued wavefunctions, we have:\n$$m = 0, \\pm 1, \\pm 2, \\ldots$$\n4.1.7 å‹’è®©å¾·å¤šé¡¹å¼ (Legendre Polynomials) # The $\\theta$ equation becomes:\n$$\\sin\\theta\\frac{d}{d\\theta}\\left(\\sin\\theta\\frac{d\\Theta}{d\\theta}\\right) + [\\ell(\\ell + 1)\\sin^2\\theta - m^2]\\Theta = 0$$\nThe solution is: $$\\Theta(\\theta) = AP_\\ell^m(\\cos\\theta)$$\nwhere $P_\\ell^m$ is the associated Legendre function defined as: $$P_\\ell^m(x) = (-1)^m(1-x^2)^{m/2}\\frac{d^m}{dx^m}P_\\ell(x)$$\nFor negative $m$: $$P_\\ell^{-m}(x) = (-1)^m\\frac{(\\ell - m)!}{(\\ell + m)!}P_\\ell^m(x)$$ These functions are defined for:\n$\\ell = 0, 1, 2, \\ldots$ $m = -\\ell, -\\ell+1, \\ldots, 0, \\ldots, \\ell-1, \\ell$ The Legendre polynomials $P_\\ell(x)$ can be defined using Rodrigues\u0026rsquo; formula: $$P_\\ell(x) \\equiv \\frac{1}{2^\\ell \\ell!}\\left(\\frac{d}{dx}\\right)^\\ell (x^2 - 1)^\\ell$$ The Legendre polynomials are orthogonal:\n$$\\int_{-1}^{1} dx P_\\ell(x)P_{\\ell\u0026rsquo;}(x) = \\frac{2}{2\\ell + 1}\\delta_{\\ell,\\ell\u0026rsquo;}$$\nFor each value of $\\ell$, there are $2\\ell + 1$ values of $m$.\n4.1.8 çƒè°å‡½æ•° (Spherical Harmonics) # The normalized angular wave functions, called spherical harmonics, are:\n$$Y_\\ell^m(\\theta, \\phi) = \\sqrt{\\frac{2\\ell + 1}{4\\pi}\\frac{(\\ell - m)!}{(\\ell + m)!}} e^{im\\phi} P_\\ell^m(\\cos\\theta)$$\nThese functions are orthonormal:\n$$\\int_0^{2\\pi} d\\phi \\int_0^{\\pi} \\sin\\theta d\\theta [Y_\\ell^m(\\theta, \\phi)]^* Y_{\\ell\u0026rsquo;}^{m\u0026rsquo;}(\\theta, \\phi) = \\delta_{\\ell\\ell\u0026rsquo;}\\delta_{mm\u0026rsquo;}$$\n4.1.9 å¾„å‘æ–¹ç¨‹ (Radial Equation) # The radial equation is:\n$$\\frac{d}{dr}\\left(r^2\\frac{dR}{dr}\\right) - \\frac{2mr^2}{\\hbar^2}[V(r) - E]R = \\ell(\\ell + 1)R$$\nA useful substitution is: $u(r) \\equiv rR(r)$, which transforms the radial equation to: $$-\\frac{\\hbar^2}{2m}\\frac{d^2u}{dr^2} + \\left[V + \\frac{\\hbar^2\\ell(\\ell+1)}{2mr^2}\\right]u = Eu$$ The term $\\frac{\\hbar^2\\ell(\\ell+1)}{2mr^2}$ is called the centrifugal term or effective potential. The normalization condition becomes: $$\\int_0^{\\infty} |u|^2 dr = 1$$\n4.1.10 æ— é™çƒåŠ¿é˜± (Infinite Spherical Well) # Consider a potential: $$V(r) = \\begin{cases} 0, \u0026amp; r \\leq a \\ \\infty, \u0026amp; r \u0026gt; a \\end{cases}$$\nå¯¹äº $\\ell = 0$ (For $\\ell = 0$) # Inside the well, the radial equation becomes:\n$$\\frac{d^2u}{dr^2} = -k^2u$$\nwhere $$k \\equiv \\frac{\\sqrt{2mE}}{\\hbar}$$.\nThe general solution is:\n$$u(r) = A\\sin(kr) + B\\cos(kr)$$\nFor $r=0$, we must have $u(0) = 0$ (since $R(r)$ must remain finite), which means $$B = 0$$.\nAt the boundary $r = a$, we have $u(a) = 0$, which gives:\n$$\\sin(ka) = 0 \\Rightarrow ka = N\\pi$$\nwhere $N$ is an integer. This gives the energy eigenvalues:\n$$E_{N0} = \\frac{N^2\\pi^2\\hbar^2}{2ma^2}, \\quad (N = 1, 2, 3, \\ldots)$$\nThe normalized wave function is:\n$$u_{N0} = \\sqrt{\\frac{2}{a}}\\sin\\left(\\frac{N\\pi r}{a}\\right)$$\nå¯¹äº $\\ell \u0026gt; 0$ (For $\\ell \u0026gt; 0$) # The solution involves spherical Bessel functions:\n$$u(r) = Arj_\\ell(kr) + Brn_\\ell(kr)$$\nwhere $j_\\ell(x)$ is the spherical Bessel function of order $\\ell$, and $n_\\ell(x)$ is the spherical Neumann function of order $\\ell$.\nThe spherical Bessel functions are defined as:\n$$j_\\ell(x) \\equiv (-x)^\\ell\\left(\\frac{1}{x}\\frac{d}{dx}\\right)^\\ell \\frac{\\sin x}{x}$$\n$$n_\\ell(x) \\equiv -(-x)^\\ell\\left(\\frac{1}{x}\\frac{d}{dx}\\right)^\\ell \\frac{\\cos x}{x}$$\nFor example: $$j_0(x) = \\frac{\\sin x}{x}; \\quad n_0(x) = -\\frac{\\cos x}{x}$$\n$$j_1(x) = (-x)\\frac{1}{x}\\frac{d}{dx}\\left(\\frac{\\sin x}{x}\\right) = \\frac{\\sin x}{x^2} - \\frac{\\cos x}{x}$$\nSince $n_\\ell(0)$ diverges, we must set $$B = 0$$ for physical solutions.\nThe boundary condition $u(a) = 0$ gives:\n$$j_\\ell(ka) = 0$$\nIf we denote the nth zero of $j_\\ell(x)$ as $\\beta_{N\\ell}$, then:\n$$k = \\frac{1}{a}\\beta_{N\\ell}$$\nAnd the energy eigenvalues are:\n$$E_{N\\ell} = \\frac{\\hbar^2}{2ma^2}\\beta_{N\\ell}^2$$\n"},{"id":40,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E5%9B%9B%E7%AB%A0/4.3-%E8%A7%92%E5%8A%A8%E9%87%8F-Angular-Momentum/","title":"4.3 è§’åŠ¨é‡ ( Angular Momentum)","section":"ç¬¬å››ç« ","content":" 4.3.1 é‡å­æ•° (Quantum Numbers) # The stationary states of the hydrogen atom are labeled by three quantum numbers: $n$, $\\ell$, and $m$. The principal quantum number ($n$) determines the energy of the state, while $\\ell$ and $m$ are related to the orbital angular momentum. In classical physics, energy and angular momentum are fundamental conserved quantities, so it\u0026rsquo;s not surprising that angular momentum plays an important role in quantum theory as well.\n4.3.2 ç»å…¸è§’åŠ¨é‡ (Classical Angular Momentum) # Classically, the angular momentum of a particle (with respect to the origin) is given by:\n$$\\mathbf{L} = \\mathbf{r} \\times \\mathbf{p}$$\nIn component form, this gives us:\n$$L_x = yp_z - zp_y, \\quad L_y = zp_x - xp_z, \\quad L_z = xp_y - yp_x$$\n4.3.3 é‡å­ç®—ç¬¦ (Quantum Operators) # In quantum mechanics, we obtain the corresponding operators by the standard prescription:\n$$p_x \\rightarrow -i\\hbar\\frac{\\partial}{\\partial x}, \\quad p_y \\rightarrow -i\\hbar\\frac{\\partial}{\\partial y}, \\quad p_z \\rightarrow -i\\hbar\\frac{\\partial}{\\partial z}$$\nNow we\u0026rsquo;ll explore the eigenvalues of the angular momentum operators through algebraic techniques based on commutation relations.\n4.3.4 æœ¬å¾å€¼ (Eigenvalues) # äº¤æ¢å…³ç³» (Commutation Relations) # The operators $L_x$ and $L_y$ do not commute. Let\u0026rsquo;s calculate their commutator:\n$$[L_x, L_y] = [yp_z - zp_y, zp_x - xp_z]$$\nExpanding this: $$[L_x, L_y] = [yp_z, zp_x] - [yp_z, xp_z] - [zp_y, zp_x] + [zp_y, xp_z]$$\nFrom canonical commutation relations, we know that only $x$ with $p_x$, $y$ with $p_y$, and $z$ with $p_z$ fail to commute. So the two middle terms drop out:\n$$[L_x, L_y] = yp_z[z, p_x] + xp_z[z, p_y] = i\\hbar(xp_y - yp_x) = i\\hbar L_z$$\nBy cyclic permutation of indices, we can obtain the other commutation relations:\n$$[L_x, L_y] = i\\hbar L_z; \\quad [L_y, L_z] = i\\hbar L_x; \\quad [L_z, L_x] = i\\hbar L_y$$\nThese are the fundamental commutation relations for angular momentum; everything follows from them.\nä¸ç›¸å®¹çš„å¯è§‚æµ‹é‡ (Incompatible Observables) # Notice that $L_x$, $L_y$, and $L_z$ are incompatible observables. According to the generalized uncertainty principle:\n$$\\sigma_{L_x}\\sigma_{L_y} \\geq \\frac{\\hbar}{2}|\\langle L_z \\rangle|$$\nThis means it would be futile to look for states that are simultaneously eigenfunctions of $L_x$ and $L_y$. However, the square of the total angular momentum:\n$$L^2 = L_x^2 + L_y^2 + L_z^2$$\ndoes commute with all components of $\\mathbf{L}$:\n$$[L^2, L_x] = [L_x^2, L_x] + [L_y^2, L_x] + [L_z^2, L_x] = 0$$\n$$[L^2, L_y] = 0, \\quad [L^2, L_z] = 0$$\nOr more compactly:\n$$[L^2, \\mathbf{L}] = 0$$\nSo we can hope to find simultaneous eigenstates of $L^2$ and (say) $L_z$:\n$$L^2 f = \\lambda f \\quad \\text{and} \\quad L_z f = \\mu f$$\n4.3.5 é˜¶æ¢¯ç®—ç¬¦ (Ladder Operators) # We\u0026rsquo;ll use the ladder operator technique, similar to the harmonic oscillator treatment. Define:\n$$L_{\\pm} = L_x \\pm iL_y$$\nThe commutator with $L_z$ is:\n$$[L_z, L_{\\pm}] = [L_z, L_x] \\pm i[L_z, L_y] = i\\hbar L_y \\pm i(-i\\hbar L_x) = \\pm\\hbar(L_x \\pm iL_y) = \\pm\\hbar L_{\\pm}$$\nSo: $$[L_z, L_{\\pm}] = \\pm\\hbar L_{\\pm}$$\nAlso: $$[L^2, L_{\\pm}] = 0$$\nIf $f$ is an eigenfunction of $L^2$ and $L_z$, so is $L_{\\pm}f$:\n$$L^2(L_{\\pm}f) = L_{\\pm}(L^2f) = L_{\\pm}(\\lambda f) = \\lambda(L_{\\pm}f)$$\n$$L_z(L_{\\pm}f) = (L_zL_{\\pm} - L_{\\pm}L_z + L_{\\pm}L_z)f = (\\pm\\hbar L_{\\pm} + L_{\\pm}L_z)f = (\\mu \\pm \\hbar)(L_{\\pm}f)$$\nSo $L_{\\pm}f$ is an eigenfunction of $L_z$ with eigenvalue $\\mu \\pm \\hbar$. We call $L_+$ the raising operator (increases eigenvalue by $\\hbar$) and $L_-$ the lowering operator (decreases eigenvalue by $\\hbar$).\nFor a given $\\lambda$, we get a \u0026ldquo;ladder\u0026rdquo; of states with each \u0026ldquo;rung\u0026rdquo; separated by $\\hbar$ in the eigenvalue of $L_z$. But this process must end somewhere. There must be a \u0026ldquo;top rung\u0026rdquo; $f_t$ such that:\n$$L_+f_t = 0$$\nLet $\\hbar\\ell$ be the eigenvalue of $L_z$ at the top rung:\n$$L_zf_t = \\hbar\\ell f_t; \\quad L^2f_t = \\lambda f_t$$\nNow:\n$$L_{\\pm}L_{\\mp} = (L_x \\pm iL_y)(L_x \\mp iL_y) = L_x^2 + L_y^2 \\mp i(L_xL_y - L_yL_x) = L^2 - L_z^2 \\mp i\\hbar L_z$$\nSo: $$L^2 = L_+L_- + L_z^2 + \\hbar L_z$$\nThis gives: $$L^2f_t = (L_-L_+ + L_z^2 - \\hbar L_z)f_t = (0 + \\hbar^2\\ell^2 - \\hbar^2\\ell)f_t = \\hbar^2\\ell(\\ell - 1)f_t$$\nAnd: $$\\lambda = \\hbar^2\\ell(\\ell - 1)$$\nSimilarly, there must be a \u0026ldquo;bottom rung\u0026rdquo; $f_b$ such that:\n$$L_-f_b = 0$$\nWith: $$L_zf_b = \\hbar\\tilde{\\ell}f_b; \\quad L^2f_b = \\lambda f_b$$\nThis leads to: $$\\lambda = \\hbar^2\\tilde{\\ell}(\\tilde{\\ell} + 1)$$\nComparing these equations for $\\lambda$, we find that $\\ell(\\ell - 1) = \\tilde{\\ell}(\\tilde{\\ell} + 1)$, which means either $\\tilde{\\ell} = \\ell + 1$ (absurdâ€”the bottom rung would be higher than the top rung!) or:\n$$\\tilde{\\ell} = -\\ell$$\nThus, the eigenvalues of $L_z$ are $m\\hbar$, where $m$ goes from $-\\ell$ to $+\\ell$ in integer steps. In particular, $\\ell = -\\tilde{\\ell} + N$, and thus $\\ell = N/2$, which means $\\ell$ must be an integer or a half-integer.\nThe eigenfunctions are characterized by the numbers $\\ell$ and $m$:\n$$L^2f_\\ell^m = \\hbar^2\\ell(\\ell + 1)f_\\ell^m; \\quad L_zf_\\ell^m = \\hbar mf_\\ell^m$$\nwhere: $$\\ell = 0, 1/2, 1, 3/2, \\ldots; \\quad m = -\\ell, -\\ell + 1, \\ldots, \\ell - 1, \\ell$$\nFor a given value of $\\ell$, there are $2\\ell + 1$ different values of $m$ (i.e., $2\\ell + 1$ \u0026ldquo;rungs\u0026rdquo; on the \u0026ldquo;ladder\u0026rdquo;).\n4.3.6 è§’åŠ¨é‡çš„çƒåæ ‡è¡¨ç¤º (Angular Momentum in Spherical Coordinates) # To determine the eigenfunctions $f_\\ell^m(\\theta, \\phi)$, we need to express the angular momentum operators in spherical coordinates.\nFirst, we rewrite $L_x$, $L_y$, and $L_z$ in spherical coordinates:\n$$L_z = -i\\hbar\\frac{\\partial}{\\partial\\phi}$$\n$$L_x = -i\\hbar\\left(-\\sin\\phi\\frac{\\partial}{\\partial\\theta} - \\cos\\phi\\cot\\theta\\frac{\\partial}{\\partial\\phi}\\right)$$\n$$L_y = -i\\hbar\\left(\\cos\\phi\\frac{\\partial}{\\partial\\theta} - \\sin\\phi\\cot\\theta\\frac{\\partial}{\\partial\\phi}\\right)$$\nThe raising and lowering operators become:\n$$L_{\\pm} = \\pm\\hbar e^{\\pm i\\phi}\\left(\\frac{\\partial}{\\partial\\theta} \\pm i\\cot\\theta\\frac{\\partial}{\\partial\\phi}\\right)$$\nAnd $L^2$ takes the form:\n$$L^2 = -\\hbar^2\\left[\\frac{1}{\\sin\\theta}\\frac{\\partial}{\\partial\\theta}\\left(\\sin\\theta\\frac{\\partial}{\\partial\\theta}\\right) + \\frac{1}{\\sin^2\\theta}\\frac{\\partial^2}{\\partial\\phi^2}\\right]$$\n4.3.7 çƒè°å‡½æ•° (Spherical Harmonics) # Now we can determine $f_\\ell^m(\\theta, \\phi)$. It\u0026rsquo;s an eigenfunction of $L^2$ with eigenvalue $\\hbar^2\\ell(\\ell + 1)$:\n$$L^2f_\\ell^m = -\\hbar^2\\left[\\frac{1}{\\sin\\theta}\\frac{\\partial}{\\partial\\theta}\\left(\\sin\\theta\\frac{\\partial}{\\partial\\theta}\\right) + \\frac{1}{\\sin^2\\theta}\\frac{\\partial^2}{\\partial\\phi^2}\\right]f_\\ell^m = \\hbar^2\\ell(\\ell + 1)f_\\ell^m$$\nIt\u0026rsquo;s also an eigenfunction of $L_z$ with eigenvalue $m\\hbar$:\n$$L_zf_\\ell^m = -i\\hbar\\frac{\\partial}{\\partial\\phi}f_\\ell^m = \\hbar mf_\\ell^m$$\nThis is equivalent to the azimuthal equation. We have already solved this system of equations! The result (appropriately normalized) is the spherical harmonic:\n$$f_\\ell^m(\\theta, \\phi) = Y_\\ell^m(\\theta, \\phi)$$\nConclusion: Spherical harmonics are the eigenfunctions of $L^2$ and $L_z$. When we solved the SchrÃ¶dinger equation by separation of variables, we were inadvertently constructing simultaneous eigenfunctions of the three commuting operators $H$, $L^2$, and $L_z$:\n$$H\\psi = E\\psi, \\quad L^2\\psi = \\hbar^2\\ell(\\ell + 1)\\psi, \\quad L_z\\psi = \\hbar m\\psi$$\nThis explains why the spherical harmonics are orthogonal: they are eigenfunctions of hermitian operators ($L^2$ and $L_z$) belonging to distinct eigenvalues.\né‡è¦ç»“è®º (Key Conclusions) # Angular momentum operators satisfy the fundamental commutation relations: $[L_i, L_j] = i\\hbar\\epsilon_{ijk}L_k$\nWhile the individual components $L_x$, $L_y$, and $L_z$ do not commute with each other, $L^2$ commutes with all components.\nThe eigenvalues of $L^2$ are $\\hbar^2\\ell(\\ell+1)$ where $\\ell$ can be integer or half-integer.\nFor each value of $\\ell$, the eigenvalues of $L_z$ are $\\hbar m$ where $m$ ranges from $-\\ell$ to $+\\ell$ in integer steps.\nFor a given $\\ell$, there are $2\\ell+1$ different values of $m$.\nThe eigenfunctions of $L^2$ and $L_z$ are the spherical harmonics $Y_\\ell^m(\\theta,\\phi)$.\nIn quantum mechanics, angular momentum cannot point in a definite direction; when $L_z$ has a well-defined value, $L_x$ and $L_y$ do not.\n"},{"id":41,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E5%9B%9B%E7%AB%A0/4.4-Spin-Spin-Spin/","title":"4.4 Spin Spin Spin","section":"ç¬¬å››ç« ","content":" é‡å­è‡ªæ—‹æ€ä¸ç®—ç¬¦è®²ä¹‰ï¼ˆQuantum Spin States and Operators Lecture Notesï¼‰ # å•è‡ªæ—‹æ€ï¼ˆOne Spin Statesï¼‰ï¼š # è¿™é‡Œå®šä¹‰äº†åŸºæœ¬çš„è‡ªæ—‹å‘ä¸Šå’Œè‡ªæ—‹å‘ä¸‹æ€çš„çŸ¢é‡è¡¨ç¤ºï¼š $|\\uparrow\\rangle = \\begin{pmatrix} 1 \\ 0 \\end{pmatrix}$, $|\\downarrow\\rangle = \\begin{pmatrix} 0 \\ 1 \\end{pmatrix}$ $S_z$ç®—ç¬¦ä½œç”¨äºè‡ªæ—‹æ€ï¼Œå¾—åˆ°å¯¹åº”çš„æœ¬å¾å€¼\n$S_z|\\uparrow\\rangle = \\frac{\\hbar}{2}|\\uparrow\\rangle$\n$S_z|\\downarrow\\rangle = -\\frac{\\hbar}{2}|\\downarrow\\rangle$\nè¿™äº›æ˜¯ä¸‰ä¸ªæ–¹å‘ä¸Šè‡ªæ—‹ç®—ç¬¦çš„çŸ©é˜µè¡¨ç¤ºå½¢å¼\n$S_x = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 1 \\ 1 \u0026amp; 0 \\end{pmatrix}$, $S_y = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; -i \\ i \u0026amp; 0 \\end{pmatrix}$, $S_z = \\frac{\\hbar}{2}\\begin{pmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{pmatrix}$\né‡è¦å·¥å…·ï¼šå®šä¹‰å‡é™ç®—ç¬¦\n$S_+ = S_x + iS_y = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 1 \\ 0 \u0026amp; 0 \\end{pmatrix}$\n$S_- = S_x - iS_y = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{pmatrix}$\nè¿™éƒ¨åˆ†è®¡ç®—å±•ç¤ºäº†å‡é™ç®—ç¬¦å¦‚ä½•æ”¹å˜è‡ªæ—‹æ€ï¼Œ$S_+$å°†è‡ªæ—‹å‘ä¸‹å˜ä¸ºå‘ä¸Šï¼Œ$S_-$å°†è‡ªæ—‹å‘ä¸Šå˜ä¸ºå‘ä¸‹\n$S_+|\\uparrow\\rangle = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 1 \\ 0 \u0026amp; 0 \\end{pmatrix}\\begin{pmatrix} 1 \\ 0 \\end{pmatrix} = 0$\n$S_+|\\downarrow\\rangle = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 1 \\ 0 \u0026amp; 0 \\end{pmatrix}\\begin{pmatrix} 0 \\ 1 \\end{pmatrix} = \\frac{\\hbar}{2}\\begin{pmatrix} 1 \\ 0 \\end{pmatrix} = \\frac{\\hbar}{2}|\\uparrow\\rangle$\n$S_-|\\uparrow\\rangle = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{pmatrix}\\begin{pmatrix} 1 \\ 0 \\end{pmatrix} = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \\ 1 \\end{pmatrix} = \\frac{\\hbar}{2}|\\downarrow\\rangle$\n$S_-|\\downarrow\\rangle = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{pmatrix}\\begin{pmatrix} 0 \\ 1 \\end{pmatrix} = 0$\nä»$|\\uparrow\\rangle|\\uparrow\\rangle$å¼€å§‹è®¡ç®—ï¼ˆStart with $|\\uparrow\\rangle|\\uparrow\\rangle$ï¼‰ï¼š # è¿™é‡Œå¼€å§‹è®¡ç®—ä¸¤ä¸ªè‡ªæ—‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå³ç‚¹ç§¯ç®—ç¬¦ä½œç”¨äºä¸¤ä¸ªè‡ªæ—‹å‡å‘ä¸Šçš„æ€\n$$ \\begin{align} \\vec{S}1 \\cdot \\vec{S}2 |\\uparrow\\rangle|\\uparrow\\rangle = \u0026amp; (S{1x}S{2x} + S_{1y}S_{2y} + S_{1z}S_{2z})|\\uparrow\\rangle|\\uparrow\\rangle \\ = \u0026amp; \\left[\\frac{1}{2}(S_{1+}S_{2-} + S_{1-}S_{2+}) + S_{1z}S_{2z}\\right]|\\uparrow\\rangle|\\uparrow\\rangle \\ = \u0026amp; S_{1z}S_{2z}|\\uparrow\\rangle|\\uparrow\\rangle \\ = \u0026amp; (S_{1z}|\\uparrow\\rangle)(S_{2z}|\\uparrow\\rangle) \\ = \u0026amp; \\left(\\frac{\\hbar}{2}|\\uparrow\\rangle\\right)\\left(\\frac{\\hbar}{2}|\\uparrow\\rangle\\right) \\ = \u0026amp; \\frac{\\hbar^2}{4}|\\uparrow\\rangle|\\uparrow\\rangle \\end{align} $$\n$|\\uparrow\\rangle|\\uparrow\\rangle$ æ˜¯ $\\vec{S}_1 \\cdot \\vec{S}_2$ çš„eigenstateï¼Œå¯¹åº”çš„çš„eigenvalueä¸º $\\frac{\\hbar^2}{4}$\nè¿™ä¸ªç»“è®ºè¡¨æ˜ä¸¤ä¸ªè‡ªæ—‹å‘ä¸Šçš„å¤åˆæ€æ˜¯è‡ªæ—‹-è‡ªæ—‹ç›¸äº’ä½œç”¨ç®—ç¬¦çš„æœ¬å¾æ€ï¼Œè¿™åœ¨é‡å­ç£å­¦ä¸­éå¸¸é‡è¦\né‡å­è‡ªæ—‹ç›¸äº’ä½œç”¨çš„æ¢è®¨ï¼ˆç»§ç»­ï¼‰ # Focus on 2 Spins: $H = J \\vec{S}_1 \\cdot \\vec{S}_2$ # å…³æ³¨ä¸¤ä¸ªè‡ªæ—‹ç³»ç»Ÿï¼Œå“ˆå¯†é¡¿é‡ç”±è‡ªæ—‹-è‡ªæ—‹ç›¸äº’ä½œç”¨ç»™å‡ºï¼Œå…¶ä¸­$J$æ˜¯è€¦åˆå¸¸æ•°\nspin-1/2 quantum mechanical operators è‡ªæ—‹-1/2é‡å­åŠ›å­¦ç®—ç¬¦\n$(|\\uparrow\\rangle|\\uparrow\\rangle, |\\uparrow\\rangle|\\downarrow\\rangle, |\\downarrow\\rangle|\\uparrow\\rangle, |\\downarrow\\rangle|\\downarrow\\rangle)$\nè¿™æ˜¯ä¸¤ä¸ªè‡ªæ—‹-1/2ç³»ç»Ÿçš„å››ä¸ªå¯èƒ½åŸºæ€\n$\\vec{S}1 \\cdot \\vec{S}2 |\\downarrow\\rangle|\\downarrow\\rangle = \\left(\\frac{1}{2}(S{1+}S{2-} + S_{1-}S_{2+}) + S_{1z}S_{2z}\\right)|\\downarrow\\rangle|\\downarrow\\rangle = S_{1z}S_{2z}|\\downarrow\\rangle|\\downarrow\\rangle$\n$= (S_{1z}|\\downarrow\\rangle)(S_{2z}|\\downarrow\\rangle)$\n$= \\left(-\\frac{\\hbar}{2}|\\downarrow\\rangle\\right)\\left(-\\frac{\\hbar}{2}|\\downarrow\\rangle\\right)$\n$= \\frac{\\hbar^2}{4}|\\downarrow\\rangle|\\downarrow\\rangle$\nè®¡ç®—è‡ªæ—‹-è‡ªæ—‹ç›¸äº’ä½œç”¨ç®—ç¬¦ä½œç”¨äºä¸¤ä¸ªè‡ªæ—‹å‘ä¸‹æ€çš„ç»“æœ\nIs $|\\uparrow\\rangle|\\downarrow\\rangle$ also an eigenstate? # $|\\uparrow\\rangle|\\downarrow\\rangle$æ˜¯å¦ä¹Ÿæ˜¯æœ¬å¾æ€ï¼Ÿ\n$\\vec{S}1 \\cdot \\vec{S}2 |\\uparrow\\rangle|\\downarrow\\rangle = \\left[\\frac{1}{2}(S{1+}S{2-} + S_{1-}S_{2+}) + S_{1z}S_{2z}\\right]|\\uparrow\\rangle|\\downarrow\\rangle$\n$= \\frac{1}{2}(S_{1+}|\\uparrow\\rangle)(S_{2-}|\\downarrow\\rangle) + \\frac{1}{2}(S_{1-}|\\uparrow\\rangle)(S_{2+}|\\downarrow\\rangle) + (S_{1z}|\\uparrow\\rangle)(S_{2z}|\\downarrow\\rangle)$\n$= \\frac{1}{2}(0)(\\frac{\\hbar}{2}|\\uparrow\\rangle) + \\frac{1}{2}(\\frac{\\hbar}{2}|\\downarrow\\rangle)(0) + (\\frac{\\hbar}{2}|\\uparrow\\rangle)(-\\frac{\\hbar}{2}|\\downarrow\\rangle)$\nåˆ†æ­¥è®¡ç®—æ¯ä¸€é¡¹çš„ç»“æœ\n$\\vec{S}_1 \\cdot \\vec{S}_2 |\\uparrow\\rangle|\\downarrow\\rangle = \\frac{\\hbar^2}{2}|\\downarrow\\rangle|\\uparrow\\rangle - \\frac{\\hbar^2}{4}|\\uparrow\\rangle|\\downarrow\\rangle$\nç»“æœè¡¨æ˜$|\\uparrow\\rangle|\\downarrow\\rangle$ä¸æ˜¯æœ¬å¾æ€ï¼Œå› ä¸ºç»“æœåŒ…å«ä¸åŒçš„é‡å­æ€\nLikewise # ç±»ä¼¼åœ°è®¡ç®—å¦ä¸€ç§æ··åˆæ€\n$\\vec{S}_1 \\cdot \\vec{S}_2 |\\downarrow\\rangle|\\uparrow\\rangle = \\frac{\\hbar^2}{2}|\\uparrow\\rangle|\\downarrow\\rangle - \\frac{\\hbar^2}{4}|\\downarrow\\rangle|\\uparrow\\rangle$\nåŒæ ·ï¼Œ$|\\downarrow\\rangle|\\uparrow\\rangle$ä¹Ÿä¸æ˜¯æœ¬å¾æ€\n$|\\downarrow\\rangle|\\downarrow\\rangle$ is an eigenstate of $\\vec{S}_1 \\cdot \\vec{S}_2$ with the eigenvalue $\\frac{\\hbar^2}{4}$\n$|\\downarrow\\rangle|\\downarrow\\rangle$æ˜¯$\\vec{S}_1 \\cdot \\vec{S}_2$çš„æœ¬å¾æ€ï¼Œå¯¹åº”æœ¬å¾å€¼$\\frac{\\hbar^2}{4}$\nneither $|\\uparrow\\rangle|\\downarrow\\rangle$ nor $|\\downarrow\\rangle|\\uparrow\\rangle$ are eigenstates of $\\vec{S}_1 \\cdot \\vec{S}_2$.\n$|\\uparrow\\rangle|\\downarrow\\rangle$å’Œ$|\\downarrow\\rangle|\\uparrow\\rangle$éƒ½ä¸æ˜¯$\\vec{S}_1 \\cdot \\vec{S}_2$çš„æœ¬å¾æ€\n.. there must be linear combinations that produce eigenstates!\nå› æ­¤ï¼Œå¿…é¡»é€šè¿‡çº¿æ€§ç»„åˆæ‰èƒ½å¾—åˆ°æœ¬å¾æ€!\nç»§ç»­é‡å­è‡ªæ—‹ç›¸äº’ä½œç”¨è®²ä¹‰ # Use symmetry\u0026hellip; # åˆ©ç”¨å¯¹ç§°æ€§\u0026hellip;\n$\\vec{S}_1 \\cdot \\vec{S}_2$ is invariant under permutation of spin 1 and spin 2.\nè‡ªæ—‹-è‡ªæ—‹ç›¸äº’ä½œç”¨ç®—ç¬¦åœ¨äº¤æ¢è‡ªæ—‹1å’Œè‡ªæ—‹2æ—¶æ˜¯ä¸å˜çš„ï¼ˆå…·æœ‰äº¤æ¢å¯¹ç§°æ€§ï¼‰ã€‚\nConsider\u0026hellip; # $\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle|\\downarrow\\rangle + |\\downarrow\\rangle|\\uparrow\\rangle)$\nè€ƒè™‘è¿™ä¸ªå½’ä¸€åŒ–çš„é‡å­æ€ï¼Œå®ƒæ˜¯ä¸¤ä¸ªæ··åˆè‡ªæ—‹æ€çš„å¯¹ç§°å åŠ \nâ†‘ normalization. ä¸Šé¢çš„ç³»æ•°æ˜¯å½’ä¸€åŒ–å› å­\nIs this an eigenstate? Yes! (Add (A) \u0026amp; (B)) # è¿™æ˜¯å¦ä¸ºæœ¬å¾æ€ï¼Ÿæ˜¯çš„ï¼ï¼ˆå°†(A)å’Œ(B)ç›¸åŠ ï¼‰\n$\\vec{S}_1 \\cdot \\vec{S}_2 \\left(\\frac{|\\uparrow\\rangle|\\downarrow\\rangle + |\\downarrow\\rangle|\\uparrow\\rangle}{\\sqrt{2}}\\right) =$\n$\\left(\\frac{\\hbar^2}{2}-\\frac{\\hbar^2}{4}\\right) \\frac{|\\uparrow\\rangle|\\downarrow\\rangle}{\\sqrt{2}} + \\left(\\frac{\\hbar^2}{2}-\\frac{\\hbar^2}{4}\\right) \\frac{|\\downarrow\\rangle|\\uparrow\\rangle}{\\sqrt{2}}$\nå°†è‡ªæ—‹-è‡ªæ—‹ç›¸äº’ä½œç”¨ç®—ç¬¦ä½œç”¨äºè¿™ä¸ªå¯¹ç§°æ€ï¼Œä»£å…¥ä¹‹å‰è®¡ç®—çš„ç»“æœ\n$= \\frac{\\hbar^2}{4}\\left(\\frac{|\\uparrow\\rangle|\\downarrow\\rangle + |\\downarrow\\rangle|\\uparrow\\rangle}{\\sqrt{2}}\\right)$\nåŒ–ç®€å¾—åˆ°\n$\\Rightarrow \\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle|\\downarrow\\rangle + |\\downarrow\\rangle|\\uparrow\\rangle)$ is an eigenstate of $\\vec{S}_1 \\cdot \\vec{S}_2$ with eigenvalue $\\frac{\\hbar^2}{4}$\nå› æ­¤ï¼Œè¿™ä¸ªå¯¹ç§°å åŠ æ€ç¡®å®æ˜¯$\\vec{S}_1 \\cdot \\vec{S}_2$çš„æœ¬å¾æ€ï¼Œå…¶æœ¬å¾å€¼ä¸º$\\frac{\\hbar^2}{4}$\nè¿™éƒ¨åˆ†æ¨å¯¼è¡¨æ˜ï¼Œå°½ç®¡å•ä¸ªæ··åˆæ€$|\\uparrow\\rangle|\\downarrow\\rangle$å’Œ$|\\downarrow\\rangle|\\uparrow\\rangle$ä¸æ˜¯è‡ªæ—‹-è‡ªæ—‹ç›¸äº’ä½œç”¨ç®—ç¬¦çš„æœ¬å¾æ€ï¼Œä½†å®ƒä»¬çš„å¯¹ç§°çº¿æ€§ç»„åˆ$\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle|\\downarrow\\rangle + |\\downarrow\\rangle|\\uparrow\\rangle)$å´æ˜¯æœ¬å¾æ€ã€‚è¿™ä¸ªå¯¹ç§°æ€åœ¨ç‰©ç†ä¸Šæœ‰ç‰¹æ®Šæ„ä¹‰ï¼Œå®ƒæ˜¯ä¸€ä¸ªæ€»è‡ªæ—‹ä¸º1çš„è‡ªæ—‹ä¸‰é‡æ€çš„ä¸€ä¸ªåˆ†é‡ï¼Œå…·æœ‰ä¸$|\\uparrow\\rangle|\\uparrow\\rangle$å’Œ$|\\downarrow\\rangle|\\downarrow\\rangle$ç›¸åŒçš„æœ¬å¾å€¼$\\frac{\\hbar^2}{4}$ã€‚\nè¿™ç§é€šè¿‡å¯¹ç§°æ€§åŸç†æ„é€ æœ¬å¾æ€çš„æ–¹æ³•åœ¨é‡å­åŠ›å­¦ä¸­éå¸¸å¸¸è§ä¸”å®ç”¨ã€‚ä¸Šè¿°è®¡ç®—å±•ç¤ºäº†å¯¹ç§°æ€§åœ¨é‡å­ç³»ç»Ÿä¸­çš„å¼ºå¤§ä½œç”¨ã€‚\né‡å­è‡ªæ—‹ç³»ç»Ÿè®²ä¹‰ # ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºæœ¬è‡ªæ—‹æ€è¡¨ç¤º # å•è‡ªæ—‹æ€çš„å®šä¹‰ # $$ |\\uparrow\\rangle = \\begin{pmatrix} 1 \\ 0 \\end{pmatrix}, \\quad |\\downarrow\\rangle = \\begin{pmatrix} 0 \\ 1 \\end{pmatrix} $$\nè‡ªæ—‹ç®—ç¬¦ä½œç”¨äºåŸºæ€ # $$ S_z|\\uparrow\\rangle = \\frac{\\hbar}{2}|\\uparrow\\rangle $$\n$$ S_z|\\downarrow\\rangle = -\\frac{\\hbar}{2}|\\downarrow\\rangle $$\nè‡ªæ—‹ç®—ç¬¦çš„çŸ©é˜µè¡¨ç¤º # $$ S_x = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 1 \\ 1 \u0026amp; 0 \\end{pmatrix}, \\quad S_y = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; -i \\ i \u0026amp; 0 \\end{pmatrix}, \\quad S_z = \\frac{\\hbar}{2}\\begin{pmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{pmatrix} $$\nå‡é™ç®—ç¬¦ # $$ S_+ = S_x + iS_y = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 1 \\ 0 \u0026amp; 0 \\end{pmatrix} $$\n$$ S_- = S_x - iS_y = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{pmatrix} $$\nå‡é™ç®—ç¬¦çš„ä½œç”¨ # $$ S_+|\\uparrow\\rangle = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 1 \\ 0 \u0026amp; 0 \\end{pmatrix}\\begin{pmatrix} 1 \\ 0 \\end{pmatrix} = 0 $$\n$$ S_+|\\downarrow\\rangle = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 1 \\ 0 \u0026amp; 0 \\end{pmatrix}\\begin{pmatrix} 0 \\ 1 \\end{pmatrix} = \\frac{\\hbar}{2}\\begin{pmatrix} 1 \\ 0 \\end{pmatrix} = \\frac{\\hbar}{2}|\\uparrow\\rangle $$\n$$ S_-|\\uparrow\\rangle = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{pmatrix}\\begin{pmatrix} 1 \\ 0 \\end{pmatrix} = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \\ 1 \\end{pmatrix} = \\frac{\\hbar}{2}|\\downarrow\\rangle $$\n$$ S_-|\\downarrow\\rangle = \\frac{\\hbar}{2}\\begin{pmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{pmatrix}\\begin{pmatrix} 0 \\ 1 \\end{pmatrix} = 0 $$\nç¬¬äºŒéƒ¨åˆ†ï¼šä¸¤è‡ªæ—‹ç³»ç»Ÿåˆ†æ # è‡ªæ—‹-è‡ªæ—‹ç›¸äº’ä½œç”¨å“ˆå¯†é¡¿é‡ # $$ H = J \\vec{S}_1 \\cdot \\vec{S}_2 $$\nå…¶ä¸­$J$æ˜¯äº¤æ¢è€¦åˆå¸¸æ•°ï¼Œ$\\vec{S}_1$å’Œ$\\vec{S}_2$æ˜¯è‡ªæ—‹-1/2é‡å­åŠ›å­¦ç®—ç¬¦\nä¸¤è‡ªæ—‹ç³»ç»Ÿçš„åŸºæ€ # $$ (|\\uparrow\\rangle|\\uparrow\\rangle, |\\uparrow\\rangle|\\downarrow\\rangle, |\\downarrow\\rangle|\\uparrow\\rangle, |\\downarrow\\rangle|\\downarrow\\rangle) $$\nè®¡ç®—ç®—ç¬¦ä½œç”¨äº$|\\downarrow\\rangle|\\downarrow\\rangle$ # $$ \\vec{S}1 \\cdot \\vec{S}2 |\\downarrow\\rangle|\\downarrow\\rangle = \\left(\\frac{1}{2}(S{1+}S{2-} + S_{1-}S_{2+}) + S_{1z}S_{2z}\\right)|\\downarrow\\rangle|\\downarrow\\rangle = S_{1z}S_{2z}|\\downarrow\\rangle|\\downarrow\\rangle $$\n$$ = (S_{1z}|\\downarrow\\rangle)(S_{2z}|\\downarrow\\rangle) $$\n$$ = \\left(-\\frac{\\hbar}{2}|\\downarrow\\rangle\\right)\\left(-\\frac{\\hbar}{2}|\\downarrow\\rangle\\right) $$\n$$ = \\frac{\\hbar^2}{4}|\\downarrow\\rangle|\\downarrow\\rangle $$\n$|\\uparrow\\rangle|\\downarrow\\rangle$æ˜¯å¦ä¸ºæœ¬å¾æ€ï¼Ÿ # $$ \\vec{S}1 \\cdot \\vec{S}2 |\\uparrow\\rangle|\\downarrow\\rangle = \\left[\\frac{1}{2}(S{1+}S{2-} + S_{1-}S_{2+}) + S_{1z}S_{2z}\\right]|\\uparrow\\rangle|\\downarrow\\rangle $$\n$$ = \\frac{1}{2}(S_{1+}|\\uparrow\\rangle)(S_{2-}|\\downarrow\\rangle) + \\frac{1}{2}(S_{1-}|\\uparrow\\rangle)(S_{2+}|\\downarrow\\rangle) + (S_{1z}|\\uparrow\\rangle)(S_{2z}|\\downarrow\\rangle) $$\n$$ = \\frac{1}{2}(0)(\\frac{\\hbar}{2}|\\uparrow\\rangle) + \\frac{1}{2}(\\frac{\\hbar}{2}|\\downarrow\\rangle)(0) + (\\frac{\\hbar}{2}|\\uparrow\\rangle)(-\\frac{\\hbar}{2}|\\downarrow\\rangle) $$\n$$ \\vec{S}_1 \\cdot \\vec{S}_2 |\\uparrow\\rangle|\\downarrow\\rangle = \\frac{\\hbar^2}{2}|\\downarrow\\rangle|\\uparrow\\rangle - \\frac{\\hbar^2}{4}|\\uparrow\\rangle|\\downarrow\\rangle $$\nå¯¹$|\\downarrow\\rangle|\\uparrow\\rangle$çš„ç±»ä¼¼è®¡ç®— # $$ \\vec{S}_1 \\cdot \\vec{S}_2 |\\downarrow\\rangle|\\uparrow\\rangle = \\frac{\\hbar^2}{2}|\\uparrow\\rangle|\\downarrow\\rangle - \\frac{\\hbar^2}{4}|\\downarrow\\rangle|\\uparrow\\rangle $$\nç¬¬ä¸‰éƒ¨åˆ†ï¼šåˆ©ç”¨å¯¹ç§°æ€§å¯»æ‰¾æœ¬å¾æ€ # å¯¹ç§°æ€§åˆ†æ # $\\vec{S}_1 \\cdot \\vec{S}_2$ åœ¨äº¤æ¢è‡ªæ—‹1å’Œè‡ªæ—‹2æ—¶æ˜¯ä¸å˜çš„\nå¯¹ç§°çº¿æ€§ç»„åˆæ€ # $$ \\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle|\\downarrow\\rangle + |\\downarrow\\rangle|\\uparrow\\rangle) $$\néªŒè¯æ˜¯å¦ä¸ºæœ¬å¾æ€ # $$ \\vec{S}_1 \\cdot \\vec{S}_2 \\left(\\frac{|\\uparrow\\rangle|\\downarrow\\rangle + |\\downarrow\\rangle|\\uparrow\\rangle}{\\sqrt{2}}\\right) = $$\n$$ \\left(\\frac{\\hbar^2}{2}-\\frac{\\hbar^2}{4}\\right) \\frac{|\\uparrow\\rangle|\\downarrow\\rangle}{\\sqrt{2}} + \\left(\\frac{\\hbar^2}{2}-\\frac{\\hbar^2}{4}\\right) \\frac{|\\downarrow\\rangle|\\uparrow\\rangle}{\\sqrt{2}} $$\n$$ = \\frac{\\hbar^2}{4}\\left(\\frac{|\\uparrow\\rangle|\\downarrow\\rangle + |\\downarrow\\rangle|\\uparrow\\rangle}{\\sqrt{2}}\\right) $$\nç¡®è®¤ï¼š$\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle|\\downarrow\\rangle + |\\downarrow\\rangle|\\uparrow\\rangle)$ æ˜¯ $\\vec{S}_1 \\cdot \\vec{S}_2$ çš„æœ¬å¾æ€ï¼Œæœ¬å¾å€¼ä¸º $\\frac{\\hbar^2}{4}$\nåå¯¹ç§°çº¿æ€§ç»„åˆæ€ # $$ \\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle|\\downarrow\\rangle - |\\downarrow\\rangle|\\uparrow\\rangle) $$\néªŒè¯æ˜¯å¦ä¸ºæœ¬å¾æ€ # $$ \\vec{S}_1 \\cdot \\vec{S}_2 \\left(\\frac{|\\uparrow\\rangle|\\downarrow\\rangle - |\\downarrow\\rangle|\\uparrow\\rangle}{\\sqrt{2}}\\right) $$\n$$ = \\left(\\frac{\\hbar^2}{2}-\\frac{\\hbar^2}{4}\\right) \\frac{|\\uparrow\\rangle|\\downarrow\\rangle}{\\sqrt{2}} - \\left(\\frac{\\hbar^2}{2}-\\frac{\\hbar^2}{4}\\right) \\frac{|\\downarrow\\rangle|\\uparrow\\rangle}{\\sqrt{2}} $$\n$$ = -\\frac{3\\hbar^2}{4}\\left(\\frac{|\\uparrow\\rangle|\\downarrow\\rangle - |\\downarrow\\rangle|\\uparrow\\rangle}{\\sqrt{2}}\\right) $$\nç¡®è®¤ï¼š$\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle|\\downarrow\\rangle - |\\downarrow\\rangle|\\uparrow\\rangle)$ æ˜¯ $\\vec{S}_1 \\cdot \\vec{S}_2$ çš„æœ¬å¾æ€ï¼Œæœ¬å¾å€¼ä¸º $-\\frac{3\\hbar^2}{4}$\nç¬¬å››éƒ¨åˆ†ï¼šè‡ªæ—‹ä¸‰é‡æ€ä¸å•é‡æ€ # ä¸‰é‡æ€ä¸å•é‡æ€çš„åˆ†ç±» # $$ |t_+\\rangle = |\\uparrow\\rangle|\\uparrow\\rangle $$ $$ |t_0\\rangle = \\frac{|\\uparrow\\rangle|\\downarrow\\rangle + |\\downarrow\\rangle|\\uparrow\\rangle}{\\sqrt{2}} $$ $$ |t_-\\rangle = |\\downarrow\\rangle|\\downarrow\\rangle $$\nè¿™ä¸‰ä¸ªæ€æ„æˆä¸‰é‡æ€(triplet)\n$$ |s\\rangle = \\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle|\\downarrow\\rangle - |\\downarrow\\rangle|\\uparrow\\rangle) $$\nè¿™æ˜¯å•é‡æ€(singlet)\nå“ˆå¯†é¡¿é‡ä½œç”¨ # $$ H|triplet\\rangle = J\\frac{\\hbar^2}{4}|triplet\\rangle $$\n$$ H|singlet\\rangle = -J\\frac{3\\hbar^2}{4}|singlet\\rangle $$\nç¬¬äº”éƒ¨åˆ†ï¼šäº¤æ¢è€¦åˆå¸¸æ•°Jçš„ç‰©ç†æ„ä¹‰ # J \u0026gt; 0 æƒ…å†µï¼ˆé“ç£è€¦åˆï¼‰ # $$ J \u0026gt; 0 $$\n$$ \\begin{array}{cc} \\uparrow \u0026amp; \\text{triplet} \\ \\hbar^2J/4 \u0026amp; \\ \u0026amp; \\ -3\\hbar^2J/4 \u0026amp; \\downarrow |singlet\\rangle \\end{array} $$\nJ \u0026lt; 0 æƒ…å†µï¼ˆåé“ç£è€¦åˆï¼‰ # $$ J \u0026lt; 0, \\quad J = -|J| $$\n$$ \\begin{array}{cc} \\uparrow \u0026amp; \\text{singlet} \\ -3\\hbar^2|J|/4 \u0026amp; \\ \u0026amp; \\ -\\hbar^2|J|/4 \u0026amp; \\downarrow \\text{triplet} \\end{array} $$\nç¬¬å…­éƒ¨åˆ†ï¼šé‡å­æ€çš„çº ç¼ ç‰¹æ€§ # é‡è¦è§‚å¯Ÿ # $$ |singlet\\rangle = \\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle|\\downarrow\\rangle - |\\downarrow\\rangle|\\uparrow\\rangle) \\neq |\\uparrow\\rangle|\\downarrow\\rangle $$\nå¼ é‡ç§¯è¡¨ç¤º # $$ |A\\otimes B\\rangle \\quad |\\psi\\rangle = \\sum_{a,b}c_{ab}|a\\rangle|b\\rangle $$\né™„å½•ï¼šé‡å­è‡ªæ—‹ç³»ç»Ÿçš„é‡è¦æ¦‚å¿µè§£é‡Š # è‡ªæ—‹ä¸‰é‡æ€ä¸å•é‡æ€çš„ç‰©ç†æ„ä¹‰ # ä¸‰é‡æ€ï¼ˆtripletï¼‰åŒ…å«ä¸‰ä¸ªé‡å­æ€ï¼Œå…·æœ‰ç›¸åŒçš„æœ¬å¾å€¼$\\frac{\\hbar^2}{4}$ã€‚ä¸‰é‡æ€è¡¨ç°ä¸ºæ€»è‡ªæ—‹ä¸º1çš„çŠ¶æ€ï¼Œ$|t_+\\rangle$ã€$|t_0\\rangle$å’Œ$|t_-\\rangle$åˆ†åˆ«å¯¹åº”æ€»è‡ªæ—‹zåˆ†é‡ä¸º+1ã€0å’Œ-1ã€‚\nå•é‡æ€ï¼ˆsingletï¼‰åªåŒ…å«ä¸€ä¸ªé‡å­æ€$|s\\rangle$ï¼Œæœ¬å¾å€¼ä¸º$-\\frac{3\\hbar^2}{4}$ã€‚å•é‡æ€è¡¨ç°ä¸ºæ€»è‡ªæ—‹ä¸º0çš„çŠ¶æ€ï¼Œä¸¤ä¸ªè‡ªæ—‹ç›¸äº’æŠµæ¶ˆã€‚\näº¤æ¢ç›¸äº’ä½œç”¨çš„ç‰©ç†è§£é‡Š # äº¤æ¢å¸¸æ•°$J$ç¡®å®šäº†è‡ªæ—‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ç±»å‹ï¼š\nå½“$J \u0026gt; 0$æ—¶ï¼Œç³»ç»Ÿå€¾å‘äºé“ç£æ’åˆ—ï¼ˆè‡ªæ—‹å¹³è¡Œï¼‰ï¼Œä¸‰é‡æ€èƒ½é‡è¾ƒä½ å½“$J \u0026lt; 0$æ—¶ï¼Œç³»ç»Ÿå€¾å‘äºåé“ç£æ’åˆ—ï¼ˆè‡ªæ—‹åå¹³è¡Œï¼‰ï¼Œå•é‡æ€èƒ½é‡è¾ƒä½ è‡ªæ—‹ç®—ç¬¦ä¸æ³¡åˆ©çŸ©é˜µçš„å…³ç³» # è‡ªæ—‹ç®—ç¬¦ä¸æ³¡åˆ©çŸ©é˜µçš„å…³ç³»å¯ä»¥ç”¨ä»¥ä¸‹æ•°å­¦è¡¨è¾¾å¼è¡¨ç¤ºï¼š\n$$ S_x = \\frac{\\hbar}{2}\\sigma_x $$ $$ S_y = \\frac{\\hbar}{2}\\sigma_y $$ $$ S_z = \\frac{\\hbar}{2}\\sigma_z $$\nå…¶ä¸­$\\sigma_x$ã€$\\sigma_y$å’Œ$\\sigma_z$æ˜¯æ³¡åˆ©çŸ©é˜µï¼š\n$$ \\sigma_x = \\begin{pmatrix} 0 \u0026amp; 1 \\ 1 \u0026amp; 0 \\end{pmatrix} $$ $$ \\sigma_y = \\begin{pmatrix} 0 \u0026amp; -i \\ i \u0026amp; 0 \\end{pmatrix} $$ $$ \\sigma_z = \\begin{pmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{pmatrix} $$\nè¿™ç§å…³ç³»åæ˜ äº†é‡å­åŠ›å­¦ä¸­æŠ½è±¡æ•°å­¦ä¸ç‰©ç†è§‚æµ‹é‡ä¹‹é—´çš„æ·±åˆ»è”ç³»ï¼š\næ³¡åˆ©çŸ©é˜µæ˜¯æ— é‡çº²çš„æ•°å­¦å·¥å…· è‡ªæ—‹ç®—ç¬¦æœ‰è§’åŠ¨é‡çš„ç‰©ç†å•ä½ï¼ˆç”±$\\frac{\\hbar}{2}$æä¾›ï¼‰ ç³»æ•°$\\frac{\\hbar}{2}$è¡¨æ˜æˆ‘ä»¬åœ¨å¤„ç†è‡ªæ—‹-1/2ç²’å­ é‡å­çº ç¼ çš„æ„ä¹‰ # å•é‡æ€$|s\\rangle$å’Œä¸‰é‡æ€$|t_0\\rangle$éƒ½æ˜¯çº ç¼ æ€ï¼Œæ— æ³•å†™æˆå•ä¸ªç²’å­æ€çš„ç›´ç§¯ã€‚è¿™ç§é‡å­çº ç¼ æ˜¯é‡å­åŠ›å­¦çš„æ ¸å¿ƒç‰¹æ€§ï¼Œè¡¨ç°ä¸ºä¸€ä¸ªç²’å­çš„æµ‹é‡ä¼šç«‹å³å½±å“å¦ä¸€ä¸ªç²’å­çš„çŠ¶æ€ï¼Œä¸è®ºå®ƒä»¬ç›¸è·å¤šè¿œã€‚\nå®é™…åº”ç”¨ # æµ·æ£®å ¡æ¨¡å‹åœ¨å‡èšæ€ç‰©ç†ä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£ç£æ€§ææ–™ã€é«˜æ¸©è¶…å¯¼ä½“å’Œé‡å­è®¡ç®—ä¸­çš„é‡å­æ¯”ç‰¹è®¾è®¡æ–¹é¢ã€‚\n"},{"id":42,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E5%9B%9B%E7%AB%A0/%E7%AC%AC%E5%9B%9B%E7%AB%A0/","title":"ç¬¬å››ç« ","section":"ç¬¬å››ç« ","content":" 4.1 ä¸‰ç»´ç©ºé—´çš„è–›å®šè°”æ–¹ç¨‹ # è–›å®šè°”æ–¹ç¨‹ï¼ˆS.E.ï¼‰çš„ä¸€èˆ¬å½¢å¼è®°ä¸ºï¼š $$ i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\hat{H} \\Psi $$ ä¸‰ç»´å“ˆå¯†é¡¿ç®—ç¬¦$\\hat{H}$ä»ç»å…¸èƒ½é‡å¾—å‡ºï¼š $$ \\frac{1}{2} m v^2+V=\\frac{1}{2 m}\\left(p_x^2+p_y^2+p_z^2\\right)+V $$ é€šè¿‡æ ‡å‡†çš„é‡å­åŒ–å¤„ç† $$ \\mathbf{p} \\rightarrow-i \\hbar \\nabla $$\nå› æ­¤ï¼Œæˆ‘ä»¬è·å¾—ä¸‰ç»´çš„è–›å®šè°”æ–¹ç¨‹ï¼š\n[!theorem|*] 3-Dimentional Schrodinger Equation $$ i \\hbar \\frac{\\partial \\Psi}{\\partial t}=-\\frac{\\hbar^2}{2 m} \\nabla^2 \\Psi+V \\Psi $$ where $$ \\nabla^2 \\equiv \\frac{\\partial^2}{\\partial x^2}+\\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} $$\n"},{"id":43,"href":"/docs/Physics/Quantum-Mechenics/%E5%AD%A6%E4%B9%A0%E9%87%8F%E5%AD%90%E5%8A%9B%E5%AD%A6%E7%9A%84%E9%A1%BA%E5%BA%8F/","title":"å­¦ä¹ é‡å­åŠ›å­¦çš„é¡ºåº","section":"é‡å­åŠ›å­¦è®²ä¹‰","content":" Standard Quantum Models: Infinite Well, Finite Well, Delta Potential, Free Particle, and Harmonic Oscillator # Introduction: Chapter 2 of Griffithsâ€™ Introduction to Quantum Mechanics (3rd Ed.) introduces several fundamental one-dimensional quantum models. These include the infinite square well (an idealized particle-in-a-box), the finite square well (a box with finite walls allowing tunneling), the delta-function potential (an extremely narrow attractive well), the free particle (no potential at all), and the harmonic oscillator (a particle in a parabolic potential). These models build on one another logically. We begin with the infinite well as a baseline, then generalize to finite wells. In the limit of a very narrow deep finite well we obtain the delta-function potential, and if we remove the confining walls we recover the free particle as a special case. Finally, we discuss the harmonic oscillator as a separate class of smoothly varying potential. Each section below provides a conceptual explanation, the physical setup and assumptions, key equations with boundary conditions, a step-by-step problem-solving workflow, and a standard example with results. The notation follows Griffithsâ€™ textbook conventions for consistency. By studying these models, one gains practical problem-solving skills and exam readiness in solving the time-independent SchrÃ¶dinger equation for various potentials.\nInfinite Square Well # Concept and Physical Setup: The infinite square well (also known as the particle-in-a-box) is the simplest quantum well. It models a particle free to move in a region of space of width $a$ (often taken from $x=0$ to $x=a$) but confined by impenetrable barriers at the boundaries. Outside the region, the potential $V(x)$ is infinite, so the particle cannot escape. Inside the well, $V(x)=0$ (no forces act inside). Classically, a particle in a box could have any energy and position, but quantum mechanically it can only occupy discrete energy levels and its wavefunction forms standing waves within the well. The infinite potential walls impose boundary conditions that the wavefunction $\\psi(x)$ must go to zero at the walls. This leads to quantization of allowed states.\nKey Equations and Boundary Conditions: The time-independent SchrÃ¶dinger equation (TISE) inside the well ($0\u0026lt;x\u0026lt;a$) is:\n$-\\frac{\\hbar^2}{2m}\\frac{d^2\\psi}{dx^2} = E,\\psi(x),$\nsince $V(x)=0$ inside. This simplifies to $\\frac{d^2\\psi}{dx^2} = -\\frac{2mE}{\\hbar^2}\\psi$. Denoting $k=\\sqrt{\\frac{2mE}{\\hbar^2}}$, the general solution inside is a linear combination of sine and cosine: $\\psi(x)=A\\sin(kx)+B\\cos(kx)$. The boundary conditions are $\\psi(0)=0$ (impenetrable wall at $x=0$) and $\\psi(a)=0$ (wall at $x=a$). Applying $\\psi(0)=0$ forces $B=0$ (so the cosine term vanishes). Applying $\\psi(a)=0$ then gives $A\\sin(ka)=0$. For a non-trivial solution ($A\\neq0$), we require $\\sin(k a)=0$, which implies\n$k a = n\\pi, \\qquad n = 1,2,3,\\dots.$\nThis quantization condition means only certain wavenumbers $k_n$ are allowed: $k_n = \\frac{n\\pi}{a}$. Correspondingly, the allowed energies are discrete:\n$ E_n = \\frac{\\hbar^2 k_n^2}{2m} = \\frac{\\hbar^2 \\pi^2}{2m a^2}n^2, \\qquad n=1,2,3,\\dots.$\nThese are the energy eigenvalues of the infinite well. The lowest energy (ground state) corresponds to $n=1$ and is\n$ E_1 = \\frac{\\hbar^2 \\pi^2}{2m a^2},$\nwhich is not zero â€“ a manifestation of the zero-point energy (the particle cannot be completely at rest because of confinement). Each energy level $E_n$ increases with $n^2$, so higher levels are spaced farther apart in energy.\nThe normalized stationary wavefunctions (spatial part) are:\n$\\psi_n(x) = \\sqrt{\\frac{2}{a}} \\sin!\\Big(\\frac{n\\pi x}{a}\\Big), \\qquad 0\u0026lt;x\u0026lt;a,$\nwith $\\psi_n(0)=\\psi_n(a)=0$. These $\\psi_n(x)$ are orthonormal and have $n-1$ internal nodes (zeros) inside the well. For example, $\\psi_1(x)$ has no node inside (just zero at the ends), $\\psi_2(x)$ has one node at $x=a/2$, etc. The probability density $|\\psi_n(x)|^2$ is highest at the antinodes of the sine wave, indicating where the particle is most likely to be found.\nWavefunctions for the first four stationary states ($n=1$ to $4$) in an infinite square well of width $a$. Each $\\psi_n(x)$ is a sine wave confined to $0\u0026lt;x\u0026lt;a$, with $n-1$ interior nodes. Higher $n$ corresponds to higher energy $E_n$ and an additional node.\nStep-by-Step Solution Workflow for Infinite Well Problems:\nDefine the potential: $V(x)=0$ for $0\u0026lt;x\u0026lt;a$ and $V=\\infty$ outside this region. This specifies the region of motion and boundary conditions $\\psi(0)=\\psi(a)=0$. Solve SchrÃ¶dingerâ€™s equation inside the well: Write the TISE for $0\u0026lt;x\u0026lt;a$ and solve the second-order ODE. Youâ€™ll obtain a general solution $\\psi(x)=A\\sin(kx)+B\\cos(kx)$. Apply boundary conditions: Enforce $\\psi(0)=0$ $\\implies B=0$. Enforce $\\psi(a)=0$ $\\implies \\sin(k a)=0$. This yields the allowed $k_n = n\\pi/a$ (quantization). Obtain energy eigenvalues: Plug $k_n$ into $E=\\hbar^2 k^2/(2m)$ to get $E_n = \\frac{\\hbar^2 \\pi^2}{2m a^2}n^2$. Normalize the eigenfunctions: Determine the constant $A$ by normalization $\\int_0^a |\\psi_n(x)|^2 dx=1$. This gives $A=\\sqrt{2/a}$, so $\\psi_n(x)=\\sqrt{2/a}\\sin(n\\pi x/a)$. Analyze or use results: You can now compute probabilities, expectation values, etc., using the $\\psi_n(x)$. Remember that only these discrete $E_n$ are allowed for stationary states. Example (Infinite Well): As a concrete example, consider an electron in an infinite well of width $a=1.0~\\text{nm}$. The ground-state energy is $E_1 = \\frac{\\hbar^2 \\pi^2}{2m a^2}$. Plugging in values (with $m=m_e$ for electron, $\\hbar\\approx1.055\\times10^{-34}$ JÂ·s), one finds $E_1 \\approx 0.38~\\text{eV}$. The next level $E_2 = 4E_1 \\approx 1.5~\\text{eV}$, and so on. Although these energies are small in eV, they are strictly fixed â€“ the electron cannot have arbitrary energy in the well, only these quantized values. The normalized ground-state wavefunction is $\\psi_1(x)=\\sqrt{\\frac{2}{a}}\\sin(\\pi x/a)$, which has its single antinode at $x=a/2$ (the particle is most likely to be found in the middle of the box) and goes to zero at the walls as required. Higher states $\\psi_2, \\psi_3, \u0026hellip;$ oscillate more rapidly inside the well. This infinite well model is often the first example in exams; typical tasks include deriving $E_n$ and $\\psi_n(x)$, sketching the wavefunctions and probability densities, and using them to compute quantities like $\\langle x \\rangle$ or $\\langle p \\rangle$. It illustrates clearly how quantization emerges from boundary conditions in quantum mechanics.\nFinite Square Well # Concept and Physical Setup: The finite square well is a more realistic version of the infinite well, where the confining walls have finite height $V_0$ instead of infinity. For an attractive finite well, one convenient convention is to take the potential $V(x)$ to be zero outside and negative inside a region. For example, a symmetric finite well centered at the origin can be defined as:\n$$V(x) = \\begin{cases}\nV_0, \u0026amp; |x| \u0026lt; a, \\ 0, \u0026amp; |x| \\ge a, \\end{cases}] $$ with $V_0 \u0026gt; 0$ being the depth of the well. In the central region ($|x|\u0026lt;a$) the particle experiences a constant negative potential â€œwellâ€, and outside ($|x|\u0026gt;a$) it faces a zero or higher potential. Classically, if the particleâ€™s energy $E$ is less than the wall height (here 0, since inside is negative relative to outside), it could never escape the well. Quantum mechanically, however, there is a non-zero probability of finding the particle outside the well even for $E$ below the barrier, due to tunneling. The wavefunction no longer goes exactly to zero at $x=\\pm a$; instead it decays exponentially outside the classically allowed region. The finite well thus introduces the ideas of bound states with $E$ in a continuum background and quantum tunneling.\nKey Equations and Matching Conditions: We solve the TISE in three regions: Region I (left, $x\u0026lt;-a$), Region II (inside, $-a \u0026lt; x \u0026lt; a$), and Region III (right, $x\u0026gt;a$). Using the convention above (inside $V=-V_0$, outside $V=0$), the SchrÃ¶dinger equation in each region is:\nFor $x \u0026lt; -a$ (Region I, $V=0$): $\\frac{d^2\\psi}{dx^2} = -\\frac{2mE}{\\hbar^2}\\psi$. Solutions are exponentials or plane waves. For bound states ($E\u0026lt;0$ relative to outside), let $E=-\\varepsilon$ (with $\\varepsilon\u0026gt;0$). Then in the outside regions the equation becomes $\\frac{d^2\\psi}{dx^2} = +\\frac{2m\\varepsilon}{\\hbar^2}\\psi$. Write the general solution as $\\psi_I(x) = F e^{+\\kappa x} + G e^{-\\kappa x}$ for $x\u0026lt;-a$, where $\\kappa = \\sqrt{\\frac{2m\\varepsilon}{\\hbar^2}}$. A physically acceptable bound-state solution must remain finite as $x\\to -\\infty$, so we set $F=0$ (to kill the growing exponential term). Thus $\\psi_I(x) = G e^{-\\kappa(x + a)}$ for $x\u0026lt;-a$ (decaying to the left of the well). Similarly, for $x\u0026gt;a$ (Region III) the general solution $\\psi_{III}(x) = H e^{-\\kappa x} + I e^{+\\kappa x}$, and finiteness as $x\\to +\\infty$ requires $I=0$, so $\\psi_{III}(x) = H e^{-\\kappa(x - a)}$ for $x\u0026gt;a$ (decaying to the right).\nFor $-a \u0026lt; x \u0026lt; a$ (Region II, inside well, $V=-V_0$): The TISE reads $-\\frac{\\hbar^2}{2m}\\psi\u0026rsquo;\u0026rsquo;(x) - V_0\\psi = E\\psi$. Or $\\psi\u0026rsquo;\u0026rsquo;(x) = -\\frac{2m(E + V_0)}{\\hbar^2}\\psi$. Define $E = -\\varepsilon$ as above, then $E + V_0 = V_0 - \\varepsilon$. If $E$ is below the top of the well ($E\u0026lt;0$ but above the bottom $-V_0$), then $V_0 - \\varepsilon$ is positive. Let $k = \\sqrt{\\frac{2m(V_0 - \\varepsilon)}{\\hbar^2}}$. Then inside the well the equation is $\\psi\u0026rsquo;\u0026rsquo;(x) = -k^2 \\psi$, with general solution $\\psi_{II}(x) = A\\cos(kx) + B\\sin(kx)$. (Some texts use cosine and sine for even/odd parity or equivalently $Ae^{ikx}+Be^{-ikx}$ forms.)\nNow we apply boundary matching conditions at the edges $x=\\pm a$: $\\psi$ must be continuous, and $\\psi\u0026rsquo;$ (derivative) must be continuous as well, since the potential is finite (the SchrÃ¶dinger equation and its derivative can be integrated across the boundary yielding continuity of $\\psi$ and $\\psi\u0026rsquo;$). In practice, apply at $x=a$: $\\psi_{II}(a)=\\psi_{III}(a)$ and $\\psi\u0026rsquo;{II}(a)=\\psi\u0026rsquo;{III}(a)$; similarly at $x=-a$: $\\psi_{II}(-a)=\\psi_{I}(-a)$ and $\\psi\u0026rsquo;{II}(-a)=\\psi\u0026rsquo;{I}(-a)$. These conditions yield a set of equations relating $A,B$ with $G$ and $H$. Because the well is symmetric about zero, one can simplify by classifying solutions into even parity (symmetric, $\\psi$ even in $x$) and odd parity (antisymmetric, $\\psi$ odd). For an even solution, we expect $B=0$ (so $\\psi_{II}(x)=A\\cos(kx)$, an even function) and $G=H$ (the outside decays have equal amplitude). For an odd solution, we expect $A=0$ ($\\psi_{II}(x)=B\\sin(kx)$, an odd function) and $G=-H$. Applying the continuity conditions under these parity assumptions leads to two transcendental equations for the allowed eigenvalues $E$:\nEven states: $\\kappa = k \\tan(k a)$. Odd states: $\\kappa = -,k \\cot(k a)$. Here $k = \\sqrt{\\frac{2m(V_0 - \\varepsilon)}{\\hbar^2}}$ and $\\kappa = \\sqrt{\\frac{2m\\varepsilon}{\\hbar^2}}$ are both functions of $E=-\\varepsilon$. These equations cannot be solved in closed form for $E$, but they determine the allowed energies implicitly. Graphical or numerical methods are used: one plots $y=k\\tan(k a)$ and $y=\\kappa$ (for even) or $y=-k\\cot(k a)$ and $y=\\kappa$ (for odd) and finds their intersections, which give the solutions for $\\varepsilon$ (and hence $E$). Each intersection corresponds to one bound state energy $E_n$. Because $\\tan$ and $\\cot$ are unbounded functions, there can be multiple intersections (multiple bound states) depending on the well depth $V_0$. If $V_0$ is large (deep well), there will be several allowed $n$; if $V_0$ is shallow, there may be only one. In fact, it can be shown that at least one bound state always exists for any $V_0\u0026gt;0$ (no matter how shallow), but beyond that the number of bound states increases with $V_0$ and well width.\nBound-state energies $E$ lie in the range $-V_0 \u0026lt; E \u0026lt; 0$ (below the outside zero potential, but above the bottom of the well $-V_0$). For energies above the well ($E \\ge 0$), the particle is not bound; instead, we have scattering states (the particle comes in from one side and is partially transmitted and reflected by the well). Those $E \\ge 0$ solutions form a continuous spectrum (the particle can have any energy above the well, similar to a free particle, but with a phase shift or possible resonance due to the well). In this summary, we focus on the bound states.\nPractical Problem-Solving Steps for Finite Wells:\nDefine the potential and regions: Write down $V(x)$ piecewise (inside the well of width $2a$ and outside). Identify the region I ($x\u0026lt;-a$), II ($-a\u0026lt;x\u0026lt;a$), III ($x\u0026gt;a$). Note that as $x \\to \\pm\\infty$, $\\psi$ must remain finite (usually $\\psi \\to 0$). Solve the SchrÃ¶dinger equation in each region: Write the general solution in region II (inside) as $A\\cos(kx)+B\\sin(kx)$ (with $k = \\sqrt{2m(E+V_0)}/\\hbar$) and in the outside regions I and III as decaying exponentials $G e^{-\\kappa(x+a)}$ for $x\u0026lt;-a$ and $H e^{-\\kappa(x-a)}$ for $x\u0026gt;a$ (with $\\kappa=\\sqrt{2m(V_0+E)}/\\hbar$ for bound states, where $E$ will be negative). Apply boundary conditions at infinity: Set the coefficients of any exponentially growing terms to zero, ensuring $\\psi(\\pm\\infty)=0$. This gave us $F=0$ and $I=0$ as mentioned above. Impose continuity at $x=\\pm a$: Set $\\psi_I(-a)=\\psi_{II}(-a)$, $\\psi\u0026rsquo;{I}(-a)=\\psi\u0026rsquo;{II}(-a)$, and similarly at $x=+a$. These yield four equations. Use the symmetry of the potential to simplify: assume either an even solution ($\\psi$ symmetric) or odd solution ($\\psi$ antisymmetric) to reduce the number of unknowns. This step yields the transcendental equations $\\kappa = k \\tan(k a)$ (even) or $\\kappa = -,k \\cot(k a)$ (odd). Solve for allowed energies: Solve these transcendental equations for $\\varepsilon = -E$ (usually by plotting or iteration). Each valid solution for $\\varepsilon$ (with $0\u0026lt;\\varepsilon\u0026lt;V_0$) gives an energy eigenvalue $E=-\\varepsilon$. The number of solutions depends on the well depth. List the found $E_n$. Construct eigenfunctions: Plug each $E_n$ back in to find $k_n$ and $\\kappa_n$, then get the corresponding normalized $\\psi_n(x)$. Typically, you express $\\psi_n$ piecewise (cosine inside for even states or sine inside for odd states, decaying exponentials outside). Use the continuity conditions to fix relative coefficients (e.g. determine $A$ vs $G$ etc.), and then normalize $\\psi_n(x)$ over all space. Analyze results: The bound state wavefunctions will be sinusoidal in the well and exponentially decaying outside. Check that as $V_0 \\to \\infty$, your $E_n$ results approach those of the infinite well (they should, since the transcendental conditions in that limit force $k a = n\\pi$). Also note that as $n$ increases, $\\psi_n$ extends closer to the walls and $E_n$ approaches 0 from below (for a finite number of levels). :contentReference[oaicite:18]{index=18} Finite square well potential of depth $V_0$ and width $2a$, with three lowest bound state energy levels indicated (red lines, labeled $E1, E2, E3$). The potential $V(x) = -V_0$ for $|x|\u0026lt;a$ (light blue region) and $V(x)=0$ outside (dashed black lines). The bound state energies lie below 0 (the top of the well) and above $-V_0$ (the bottom). The wavefunctions (not shown) oscillate inside the well and decay exponentially in the classically forbidden outside regions.\nExample (Finite Well): Consider a finite well of half-width $a=1$Â nm and depth $V_0 = 5$Â eV (inside $V=-5$Â eV, outside $V=0$). Classically, an electron with $E\u0026lt;0$ (negative) would be trapped. Solving the transcendental equations for this case might show, for instance, that there are two bound states (one even, one odd). The ground state might come out to an energy $E_1 \\approx -4$Â eV (just 1Â eV above the bottom of the well), and the first excited bound state $E_2 \\approx -1$Â eV. These would be found by finding solutions to $\\kappa = k \\tan(k a)$ and $\\kappa = -k \\cot(k a)$. The ground state wavefunction $\\psi_1(x)$ would be an even function, largest at $x=0$, oscillating cosinusoidally inside (no nodes, since itâ€™s the lowest state) and decaying evanescently for $|x|\u0026gt;a$. The first excited state $\\psi_2(x)$ would be odd (one node at $x=0$), sine-shaped inside, and also decaying outside. If the well were made shallower (say $V_0=1$Â eV), only one bound state would remain (the second level would â€œfloatâ€ up into the continuum, no longer bound). If the well is made deeper or wider, more bound states appear. In exam problems, you might be asked to determine how many bound states a given finite well supports, or to solve the transcendental equations numerically for the energy values, or to sketch the wavefunctions qualitatively. A common practical exercise is to check the limiting cases: as $V_0 \\to \\infty$, the results approach the infinite square well (with $\\psi$ going to zero at the boundaries); as $V_0 \\to 0$, the well ceases to bind the particle and you approach the free particle case. The finite well exemplifies how allowing tunneling changes the quantization condition from a simple integer relation to a more complex one, and introduces the concept of quantum tunneling and quasi-bound states.\nRelation to Other Models: The infinite well can be seen as the limiting case of a finite well as $V_0 \\to \\infty$ (the transcendental conditions force $\\sin$ solutions and recover $E_n \\propto n^2$). Conversely, if we make the finite well extremely narrow, it approaches the delta-function potential model (discussed next). If we remove the well altogether (letting $V_0 \\to 0$ or the width $a \\to \\infty$), we approach the free particle case.\nDelta-Function Potential # Concept and Setup: The delta-function potential is an idealized potential that is zero everywhere except at a single point, where it is infinitely strong and narrow. We focus on the attractive delta well, defined by\n$$V(x) = -\\alpha,\\delta(x),$$\nwhere $\\delta(x)$ is the Dirac delta function and $\\alpha\u0026gt;0$ sets the strength (with units of energyÂ·length). This potential is like an extremely narrow deep well located at $x=0$. It can be thought of as the limit of a finite square well as the width $2a \\to 0$ and depth $V_0 \\to \\infty$ such that the area $2a V_0$ approaches $\\alpha$:contentReference[oaicite:19]{index=19}:contentReference[oaicite:20]{index=20}. Physically, the delta-function potential might model an impurity or a very short-range force (like an idealized chemical bond or a deep narrow trap). The particle feels no force except exactly at $x=0$. Despite the singular nature of $V(x)$, the SchrÃ¶dinger equation can be solved by integrating across the singularity.\nThe delta well has one bound state (and only one) and a continuum of scattering states. The bound state is a localized state around $x=0$. Classically, an attractive spike would capture a particle only if its energy is negative (bound); in quantum mechanics, indeed one bound state with negative energy exists for any $\\alpha\u0026gt;0$. Interestingly, any shallow delta well binds a particle in one dimension (no matter how weak $\\alpha$ is, there will be a bound state, similar to the finite well scenario):contentReference[oaicite:21]{index=21}.\nSolving SchrÃ¶dingerâ€™s Equation: For $x \\neq 0$, the potential $V(x)=0$, so the time-independent SchrÃ¶dinger equation is free everywhere except at $x=0$. For a bound state, we seek a solution decaying as $x\\to \\pm \\infty$. Let the bound state energy be $E=-\\varepsilon$ (with $\\varepsilon\u0026gt;0$). Then for $x\u0026gt;0$, $\\psi\u0026rsquo;\u0026rsquo;(x) = \\frac{2m\\varepsilon}{\\hbar^2}\\psi(x)$ (since $E-V= -\\varepsilon$ for $x\\neq0$), whose general solution is $\\psi(x)=C e^{-\\kappa x} + D e^{+\\kappa x}$ with $\\kappa=\\sqrt{\\frac{2m\\varepsilon}{\\hbar^2}}$. Normalizability demands $\\psi(x)$ not diverge as $x\\to +\\infty$, so we take $D=0$. Thus for $x\u0026gt;0$: $\\psi(x)=C e^{-\\kappa x}$. By symmetry of the potential (which is even), we expect an even bound state solution. Indeed, for $x\u0026lt;0$, by a similar reasoning, $\\psi(x)=C e^{+\\kappa x}$ (which decays for $x\\to -\\infty$). So we have\n$$\\psi(x) = A e^{-\\kappa |x|},$$\nfor some amplitude $A$, as the ansatz for the bound state wavefunction. This $\\psi(x)$ is continuous at $x=0$ (both sides equal $A$). However, the derivative $\\psi\u0026rsquo;(x)$ will have a discontinuity at $x=0$ because of the delta potential. The proper condition comes from integrating the SchrÃ¶dinger equation from $-,\\epsilon$ to $+\\epsilon$ around $0$. Doing this, one finds the jump condition:\n$$\\psi\u0026rsquo;(+0) - \\psi\u0026rsquo;(-0) = \\frac{2m}{\\hbar^2}\\int_{-\\epsilon}^{+\\epsilon} [V(x)\\psi(x)],dx = \\frac{2m}{\\hbar^2}(-\\alpha)\\psi(0).$$\nSince $\\int_{-\\epsilon}^{\\epsilon}\\delta(x),dx = 1$, this simplifies to:\n$$\\psi\u0026rsquo;(+0) - \\psi\u0026rsquo;(-0) = -\\frac{2m\\alpha}{\\hbar^2}\\psi(0).$$\nPlugging our piecewise form $\\psi(x)=A e^{-\\kappa |x|}$: for $x\u0026gt;0$, $\\psi\u0026rsquo;(x) = -\\kappa A e^{-\\kappa x}$ so $\\psi\u0026rsquo;(+0) = -\\kappa A$; for $x\u0026lt;0$, $\\psi\u0026rsquo;(x) = +\\kappa A e^{+\\kappa x}$ so $\\psi\u0026rsquo;(-0) = +\\kappa A$. Thus the jump condition becomes $(-\\kappa A) - (\\kappa A) = -\\frac{2m\\alpha}{\\hbar^2}A$. This gives $-2\\kappa A = -\\frac{2m\\alpha}{\\hbar^2}A$. Canceling $-2A$ (nonzero for a bound state), we get the energy condition:\n$$\\kappa = \\frac{m\\alpha}{\\hbar^2}.$$\nNow recall $\\kappa = \\sqrt{\\frac{2m\\varepsilon}{\\hbar^2}}$. Solving for $\\varepsilon$ yields $\\varepsilon = \\frac{m^2\\alpha^2}{2m\\hbar^2} = \\frac{m\\alpha^2}{2\\hbar^2}$. Therefore the bound-state energy is:\n$$E_{\\text{bound}} = -,\\varepsilon = -,\\frac{m\\alpha^2}{2\\hbar^2}.$$\nThis is the single discrete eigenvalue for the delta well:contentReference[oaicite:22]{index=22}. The corresponding normalized wavefunction can be written as\n$$\\psi(x) = \\sqrt{\\kappa}, e^{-\\kappa |x|},$$\nwhere $\\kappa = m\\alpha/\\hbar^2$ as above (the factor $\\sqrt{\\kappa}$ comes from normalization $\\int_{-\\infty}^{\\infty} \\kappa e^{-2\\kappa |x|}dx=1$). This bound-state wavefunction is continuous and peaked at $x=0$ (where the delta well is located), and decays exponentially away from the center. Notably, $\\psi(x)$ is cusp-like at $x=0$; its derivative has a finite discontinuity there because of the infinite spike of potential (unlike finite wells where $\\psi\u0026rsquo;$ was continuous).\n:contentReference[oaicite:23]{index=23} *Bound state of an attractive delta-function potential $V(x)=-\\alpha,\\delta(x)$. The red curve is the wavefunction $\\psi(x) \\propto e^{-|x|\\kappa}$ with $\\kappa = m\\alpha/\\hbar^2$. The blue dashed line indicates $\\psi=0$. The black arrow at $x=0$ marks the location of the delta well (a negative spike). The wavefunction is continuous at $x=0$, but its slope has a discontinuity (steeper on the left side than the right side) due to the concentrated force at $x=0$. This single bound state has energy $E=-\\frac{m\\alpha^2}{2\\hbar^2}$. *\nFor scattering states ($E\u0026gt;0$) in the delta potential, one solves the free SchrÃ¶dinger equation on either side and applies the boundary/jump conditions. A typical scenario is a particle coming in from the left ($x=-\\infty$) with energy $E=\\hbar^2 k^2/(2m)$. One finds that part of the wave is reflected and part is transmitted. The reflection and transmission coefficients can be derived by matching at $x=0$. The result (for an attractive delta well or a repulsive delta barrier, depending on the sign of $\\alpha$) is that there is partial reflection for all energies. For example, for a delta well ($\\alpha\u0026gt;0$), the transmission probability for a particle of wave number $k$ is\n$$T = \\frac{1}{1 + \\left(\\frac{m\\alpha}{\\hbar^2 k}\\right)^2},$$\nand the reflection probability is $R=1-T$. One noteworthy fact is that as $E \\to \\infty$ (large $k$), $T \\to 1$ but never exceeds 1, and as $E \\to 0^+$, $T \\to 0$ (the low-energy particle is mostly reflected by the attractive well because it effectively â€œbounces offâ€ the sudden potential). These scattering states are not normalizable in the usual sense over $(-\\infty,\\infty)$, but they can be treated as delta-normalized or as limits of putting the system in a large box.\nSteps to Solve Delta-Potential Problems:\nWrite down the potential: $V(x)=-\\alpha \\delta(x)$. Identify that for $x \\neq 0$, $V=0$, and at $x=0$ we have a special condition. Solve SchrÃ¶dingerâ€™s equation for regions $x\u0026lt;0$ and $x\u0026gt;0$: Both sides are free particle regions. For a bound state, take decaying exponentials; for scattering, take appropriate incoming/outgoing waves. Write general solutions: for $x\u0026lt;0$, $\\psi_L(x) = A,e^{\\kappa x}+B,e^{-\\kappa x}$; for $x\u0026gt;0$, $\\psi_R(x) = C,e^{-\\kappa x}+D,e^{\\kappa x}$ (for bound, choose decaying forms as needed, for scattering choose oscillatory $e^{\\pm ikx}$ forms). Apply continuity at $x=0$:* $\\psi_L(0)=\\psi_R(0)$. This ensures the wavefunction is single-valued. Apply the â€œjumpâ€ condition for $\\psi\u0026rsquo;$: $\\psi\u0026rsquo;_R(0) - \\psi\u0026rsquo;_L(0) = -\\frac{2m\\alpha}{\\hbar^2}\\psi(0)$. This comes from integrating SchrÃ¶dingerâ€™s equation over an infinitesimal interval around $0$. Plug in the forms of $\\psi\u0026rsquo;$ from left and right. Solve for constants and $E$: For the bound state, this yields the condition $\\kappa = m\\alpha/\\hbar^2$, giving $E=-m\\alpha^2/(2\\hbar^2)$. For scattering, solve for the reflection/transmission amplitudes (ratios of $B$ to incoming amplitude, etc.) to get $R,T$. Normalize if needed: The bound state should be normalized to 1 (determine $A$ accordingly). Scattering states are typically normalized to delta functions or flux; on an exam, you might just compute $R$ and $T$ rather than fully normalize $\\psi$. Interpret results: Check that $E_{\\text{bound}}$ is negative and depends on $\\alpha$ quadratically. Note that if $\\alpha$ is small, $E_{\\text{bound}}$ is close to 0 (very shallow bound). Also, confirm that $R+T=1$ for scattering (probability current is conserved). Example (Delta Well): Suppose $\\alpha = 5.0\\times 10^{-40}$ JÂ·m (a very weak well). Then the single bound state energy would be $E = -m\\alpha^2/(2\\hbar^2)$. For an electron ($m=9.11\\times10^{-31}$ kg), plugging $\\hbar=1.055\\times10^{-34}$ JÂ·s, we get $E \\approx -1.3\\times 10^{-21}$ J, which is about $-8\\times10^{-3}$ eV. This tiny negative energy indicates a very shallow bound state, just below $E=0$. The wavefunction would be $\\psi(x) = \\sqrt{\\kappa}e^{-\\kappa|x|}$ with $\\kappa = m\\alpha/\\hbar^2 \\approx 1.0\\times 10^{10}$ m$^{-1}$. The exponential decay length $1/\\kappa$ is about $10^{-10}$ m (an angstrom), meaning the particle is localized mostly within angstroms of $x=0$. If a particle with a small positive energy (say $E=0.01$ eV) comes in, it will see this weak well and mostly pass by; we can calculate $T$ from the formula above. For larger $\\alpha$ (stronger well), the bound state energy becomes more negative and the particle is more tightly localized. A delta-function potential is a common exam problem because it tests understanding of boundary conditions: one usually has to derive the jump condition and show that it yields the bound state energy, or compute reflection/transmission coefficients. Itâ€™s also conceptually important as a limiting case of a finite well (narrower and deeper):contentReference[oaicite:24]{index=24}.\nFree Particle # Concept and Setup: The free particle is a limiting case where the particle is not bound by any potential â€“ $V(x)=0$ everywhere (for all $-\\infty \u0026lt; x \u0026lt; +\\infty$). It can be regarded as a finite well with no walls at all (for example, take the finite well and let $V_0 \\to 0$, or imagine the infinite well with its width $a \\to \\infty$). A free particle experiences no forces, so classically it would move in a straight line with constant momentum. Quantum mechanically, the absence of boundaries or potential means there is no quantization of energy â€“ the particleâ€™s energy and momentum are continuous variables. Solving SchrÃ¶dingerâ€™s equation for a free particle yields plane-wave solutions representing particles with definite momentum.\nKey Equations: With $V(x)=0$, the time-independent SchrÃ¶dinger equation is simply\n$$-\\frac{\\hbar^2}{2m}\\frac{d^2\\psi}{dx^2} = E\\psi(x),$$\nwhich is the same form we solved inside the infinite well, but now valid for all $x$ without boundary constraints. The general solution is $\\psi(x) = A e^{ikx} + B e^{-ikx}$, with $k=\\sqrt{2mE}/\\hbar$. Here $E=\\frac{\\hbar^2 k^2}{2m}$ can be any non-negative value â€“ there is no restriction on $k$ or $E$ (no quantization):contentReference[oaicite:26]{index=26}. We typically rewrite the two exponentials as representing a right-moving plane wave ($e^{ikx}$) and a left-moving plane wave ($e^{-ikx}$).\nFor a free particle in a stationary state, if we try to normalize $\\psi$ over the entire real line, we encounter an issue: plane waves are not square-integrable (the probability density $|\\psi|^2$ is constant in space, so integrating over an infinite line diverges). This reflects that a truly free particle is not localized. In practice, we deal with this by either normalizing to a delta function (box normalization) or by considering wave packets (superpositions of plane waves that give localized wavefunctions). However, for most theoretical purposes, one considers the momentum eigenstates $|p\\rangle$ corresponding to $\\psi_p(x)=\\frac{1}{\\sqrt{2\\pi\\hbar}}e^{ipx/\\hbar}$.\nIf we imagine â€œconfiningâ€ the free particle in a large box of length $L$ and taking $L\\to\\infty$, then the energy levels become so dense that they form a continuum. In the finite box of length $L$, one would have quantized $k_n = \\frac{2\\pi n}{L}$, but as $L \\to \\infty$ the spacing $\\Delta k \\to 0$ and $E$ becomes effectively continuous. Thus the free particleâ€™s hallmark is a continuous energy spectrum: the particle can have any $E \\ge 0$ (including $E=0$ as a limiting case of a perfectly constant wavefunction). Negative energy solutions for a free particle are not physically distinct â€“ they correspond to imaginary $k$ which yields exponential solutions that are non-normalizable (they blow up or decay over infinite space), so we generally exclude negative $E$ for a truly free particle.\nPhysical Interpretation: A free particle with a definite momentum $p=\\hbar k$ is described by $\\psi(x) \\propto e^{ikx}$ (up to normalization), which means the probability density $|\\psi|^2$ is uniform in space â€“ the particle is equally likely to be found anywhere (completely delocalized). In reality, we deal with wave packets: a superposition of such plane waves, which can be localized and then move in space. But the plane wave solutions are useful as momentum eigenstates and for scattering calculations. Since $[ \\hat{p}, \\hat{H} ]=0$ for the free particle, one can label the free states by momentum $p$ as well as energy $E=p^2/(2m)$. Each momentum $p$ (positive or negative) corresponds to a distinct solution.\nSolving Free Particle Problems: There is not much to â€œsolveâ€ in the sense of finding discrete eigenvalues, since energies are free. However, tasks might include expressing a given initial wavefunction as a combination of free-particle eigenstates, or computing probability current, etc. The general steps:\nWrite $V(x)=0$ and SchrÃ¶dingerâ€™s equation: $\\psi\u0026rsquo;\u0026rsquo;(x) + k^2 \\psi(x) = 0$, with $k=\\sqrt{2mE}/\\hbar$. Write the general solution: $\\psi(x) = A e^{ikx} + B e^{-ikx}$. If the problem specifies a particle with definite momentum traveling in one direction, you choose either the $e^{ikx}$ term (for a particle moving to the right, i.e. momentum $+p$) or the $e^{-ikx}$ term (moving left). If both terms are present, it can represent a standing wave or a superposition of two momentum states. Normalization / interpretation: If in a finite region (like between reflecting walls), apply boundary conditions (leading to quantization). If truly free (infinite domain), either impose periodic boundary conditions in a large box of length $L$ (so $k$ becomes discrete $2\\pi n/L$ temporarily for calculation, then let $L \\to \\infty$), or use delta-normalization $\\langle \\psi_{k\u0026rsquo;} | \\psi_k \\rangle = \\delta(k-k\u0026rsquo;)$. In exam settings, often a wave packet perspective is taken: e.g. a Gaussian wave packet and one may calculate how it spreads in free time evolution. Calculate observables: For a plane wave $Ae^{ikx}$, the probability current can be found $J = \\frac{\\hbar k}{m}|A|^2$ (this indicates a constant flow of probability density to the right). The expectation value of momentum is $+p=\\hbar k$ for that state. If the problem gave an initial wavefunction $\\psi(x,0)$ and asks for $\\psi(x,t)$ under free evolution, one would use the free particle propagator or Fourier transform methods (since each $k$ component picks up a phase $e^{-i\\omega t}$ with $\\omega = \\frac{\\hbar k^2}{2m}$). Example (Free Particle): Suppose we have a free electron with a well-defined momentum $p = 5\\times10^{-25}$ kgÂ·m/s (about $5\\times10^{-3}$ eV/c in energy terms). The corresponding wavefunction can be written as $\\psi(x,t) = \\frac{1}{\\sqrt{L}} e^{i(kx - \\omega t)}$ if we imagine a normalization in a box of length $L$ (later $L\\to \\infty$). Here $k = p/\\hbar \\approx 4.76\\times10^{9}$ m$^{-1}$ and $E = p^2/(2m) \\approx 1.4\\times10^{-19}$ J ($\\sim0.9$ eV). This state has $\\langle p \\rangle = p$ and $\\langle E \\rangle = E$. If we wanted a localized free particle, we could build a wave packet: for example, a Gaussian momentum distribution centered at that $p$ would produce a Gaussian spatial wave packet that moves in time. On an exam, you might be asked something like: â€œWrite down the general solution of the free-particle SchrÃ¶dinger equation and discuss why an energy eigenstate cannot be normalized in the usual sense.â€ The answer would involve stating $\\psi(x)=Ae^{ikx}$ and explaining that $|\\psi|^2$ is constant so itâ€™s non-normalizable over infinite $x$, hence one uses a delta-function normalization or wave packets:contentReference[oaicite:27]{index=27}. Another common exercise: â€œShow that for a free particle, any $E\u0026gt;0$ is allowed and the dispersion relation is $E = p^2/(2m)$ is the same as the classical one.â€ We see indeed that $E=\\hbar^2 k^2/(2m)$ for the quantum solution, which is equivalent to $p=\\hbar k$ and $E=p^2/(2m)$ â€“ same functional relation as a classical particle (no quantization because no potential to impose boundary conditions).\nIn summary, the free particle model solidifies understanding of continuous spectra and plane-wave solutions. It also serves as a reference for scattering calculations (e.g. how an incoming plane wave is modified by a potential, compared to the free case). Itâ€™s â€œsimpleâ€ in that the equation is easy to solve, but subtle in interpretation due to normalization issues.\nHarmonic Oscillator # Concept and Setup: The harmonic oscillator model is a particle in a quadratic potential: $V(x) = \\frac{1}{2}m\\omega^2 x^2$. This is the quantum analogue of a mass on a spring or small oscillations about a stable equilibrium. Unlike the previous models with piecewise constant potentials, the harmonic oscillatorâ€™s potential is smooth and unbounded as $|x|\\to\\infty$ (it goes to $+\\infty$ for large $|x|$). This means the particle is always bound (it cannot escape to infinity because $V(x)\\to\\infty$ acts like confining walls at infinity), and it has an infinite discrete spectrum of energy levels. The harmonic oscillator is extremely important: near any stable equilibrium, a potential can often be approximated by a quadratic form (by Taylor expansion), so the harmonic oscillator is a universal model for small vibrations (molecules, lattice vibrations, quantum fields, etc.). Itâ€™s also one of the few systems (like the infinite well) that can be solved analytically, yielding simple expressions for energy and wavefunctions.\nEnergy Quantization and Wavefunctions: The time-independent SchrÃ¶dinger equation is\n$$-\\frac{\\hbar^2}{2m}\\frac{d^2\\psi}{dx^2} + \\frac{1}{2}m\\omega^2 x^2,\\psi(x) = E,\\psi(x).$$\nSolving this differential equation requires more advanced techniques (power series or algebraic operator methods). The boundary conditions are that $\\psi(x)$ remains finite as $x\\to\\pm\\infty$ (and $\\psi\\to 0$ as $x\\to\\pm\\infty$ sufficiently fast to be normalizable). The qualitative behavior: for large $|x|$, the $V(x)$ term dominates and the equation reduces to $\\psi\u0026rsquo;\u0026rsquo; \\sim \\frac{m^2\\omega^2}{\\hbar^2}x^2\\psi$, whose solutions must decay Gaussianly to be normalizable. Near $x=0$, the potential is small and solutions behave like free-particle oscillatory. The standard method is to define a dimensionless variable $\\xi = \\sqrt{\\frac{m\\omega}{\\hbar}},x$ and attempt a power series $\\psi(\\xi) = e^{-\\xi^2/2} H(\\xi)$, where the Gaussian factor $e^{-\\xi^2/2}$ is inserted to ensure the correct asymptotic decay and $H(\\xi)$ is expanded as a power series (which turns out to be Hermite polynomials when truncated). The requirement that $\\psi$ not diverge as $x\\to\\infty$ forces the power series to terminate, which only happens for certain discrete $E$. The result is a ladder of evenly spaced energy eigenvalues:\n$$E_n = \\hbar\\omega\\Big(n + \\frac{1}{2}\\Big), \\qquad n = 0,1,2,3,\\dots,$$\nwhere $n$ is an integer quantum number (here starting at 0):contentReference[oaicite:29]{index=29}. So the ground state energy is $E_0=\\frac{1}{2}\\hbar\\omega$, the first excited $E_1=\\frac{3}{2}\\hbar\\omega$, then $5/2\\hbar\\omega$, etc. This equal spacing $\\Delta E = \\hbar\\omega$ is a unique hallmark of the harmonic oscillator â€“ unlike the infinite well or hydrogen atom, where level spacings get smaller for higher levels, in the oscillator the gap is constant. The presence of the $\\frac{1}{2}\\hbar\\omega$ zero-point energy again reflects that the particle cannot have zero kinetic energy because if $E=0$, $\\psi$ would be zero everywhere (not allowed by normalization). Indeed, even in the lowest state the particle has some â€œvibrationalâ€ energy $\\frac{1}{2}\\hbar\\omega$.\nThe eigenfunctions $\\psi_n(x)$ turn out to be related to Hermite polynomials $H_n(\\xi)$ times a Gaussian. Specifically,\n$$\\psi_n(x) = N_n, e^{-\\frac{m\\omega x^2}{2\\hbar}} H_n!\\Big(\\sqrt{\\frac{m\\omega}{\\hbar}},x\\Big),$$\nwhere $H_n$ is the Hermite polynomial of degree $n$, and $N_n$ is a normalization constant (e.g. $N_0 = (\\frac{m\\omega}{\\pi\\hbar})^{1/4}$ for the ground state). These $\\psi_n(x)$ are alternately even and odd functions (reflecting the parity symmetry of the potential): $n$ even gives an even polynomial times the even Gaussian (overall even $\\psi$), $n$ odd gives an odd $\\psi$. The ground state $\\psi_0(x)$ is a Gaussian centered at $x=0$. Higher states oscillate, with $n$th state having $n$ nodes (each node is a sign change). Notably, for large $n$, the wavefunctions spread out further (the â€œclassical turning pointsâ€ where $E_n = V(x)$ are at $x = \\pm \\sqrt{\\frac{2E_n}{m\\omega^2}} = \\pm \\sqrt{\\frac{2\\hbar\\omega (n+1/2)}{m\\omega^2}}$, which grows as $\\sim \\sqrt{2n},(\\hbar/(m\\omega))^{1/2}$). The probability density $|\\psi_n(x)|^2$ for large $n$ oscillates but is enveloped by a broad shape that peaks near the classical turning points â€“ illustrating the correspondence principle (for large quantum numbers, the quantum distribution mimics the classical distribution of an oscillator, which spends more time near turning points).\nStep-by-Step Approach (Ladder Operator or Series): There are two common methods to derive the above results:\nPower Series Method: Assume $\\psi(x) = e^{-\\frac{m\\omega x^2}{2\\hbar}} \\sum_{j=0}^\\infty a_j x^j$. Plug into SchrÃ¶dingerâ€™s equation, derive a recursion for $a_j$. Termination of the series occurs only if $E$ takes the form $(n+\\tfrac{1}{2})\\hbar\\omega$, which makes the series truncate at $j=n$. This yields polynomial solutions $H_n(x)$. This method, while laborious, is a straightforward differential equation approach. Operator (Algebraic) Method: Define creation (raising) and annihilation (lowering) operators $a^\\dagger$ and $a$ in terms of $\\hat{x}$ and $\\hat{p}$: $a = \\sqrt{\\frac{m\\omega}{2\\hbar}}(\\hat{x} + \\frac{i}{m\\omega}\\hat{p})$. The Hamiltonian can be written as $H = \\hbar\\omega(a^\\dagger a + 1/2)$. Then one can argue that $a^\\dagger a$ has eigenvalues $n=0,1,2,\\dots$ and thus derive $E_n = \\hbar\\omega(n+1/2)$. The eigenstates $|n\\rangle$ are obtained by repeated application of $a^\\dagger$ on the ground state $|0\\rangle$ (which satisfies $a|0\\rangle=0$). The position-space wavefunctions come out as above. This method is elegant and commonly taught (Griffiths covers it in a later chapter on formalism). From a practical standpoint, most exam problems on the harmonic oscillator will use known results rather than deriving from scratch (since derivation is lengthy). Typical tasks: using the known wavefunctions to calculate expectation values, or verifying properties like $\\Delta x \\Delta p = \\frac{\\hbar}{2}$ for the ground state, etc. Another common exercise is to approximate a complicated potential near a minimum by a quadratic and identify the effective $\\omega$, then state the approximate energy levels.\nExample (Harmonic Oscillator): Consider a mass $m$ attached to a spring with classical frequency $\\omega = 2\\pi(1~\\text{THz})$ (terahertz order, typical of molecular vibrations). The ground state energy is $E_0 = \\frac{1}{2}\\hbar\\omega$. Plugging numbers: $\\hbar\\omega = 6.582\\times10^{-16}$ eVÂ·s $\\times$ $2\\pi \\times 10^{12}$ s$^{-1} \\approx 4.14$ meV. So $E_0 \\approx 2.07$ meV. The first excited state is $E_1 = 3*2.07 = 6.21$ meV, and so on (linear spacing). The ground wavefunction is $\\psi_0(x) = (m\\omega/\\pi\\hbar)^{1/4} e^{-m\\omega x^2/(2\\hbar)}$, which for (say) an electron ($m_e$) and the given $\\omega$ has a characteristic width $\\Delta x \\sim \\sqrt{\\hbar/(m\\omega)}$. If $m$ were the proton mass and $\\omega$ appropriate for a diatomic bond vibration, one could compute similar values. For large $n$, say $n=10$, $E_{10} = (10.5)\\hbar\\omega \\approx 21.7$ meV in our example, and $\\psi_{10}(x)$ would have 10 nodes. The probability density $|\\psi_{10}(x)|^2$ oscillates, but if averaged, it tends to be higher near the turning points $x \\approx \\pm \\sqrt{2E_{10}/(m\\omega^2)}$. This matches the classical expectation that an oscillator spends more time near the extremes of its motion (where it is slowest).\nFrom a problem-solving angle, one might be asked to show that $E_n=(n+1/2)\\hbar\\omega$ are solutions, or to apply the ladder operators: e.g. using $a$ and $a^\\dagger$ to find $\\langle x \\rangle$, $\\langle x^2 \\rangle$ for the ground state, etc. Another common exercise: verify the uncertainty product in the ground state. For $\\psi_0(x)$, one can calculate $\\langle x^2\\rangle = \\frac{\\hbar}{2m\\omega}$ and $\\langle p^2\\rangle = \\frac{m\\hbar\\omega}{2}$, yielding $\\Delta x = \\sqrt{\\langle x^2\\rangle} = \\sqrt{\\frac{\\hbar}{2m\\omega}}$ and $\\Delta p = \\sqrt{\\langle p^2\\rangle} = \\sqrt{\\frac{m\\hbar\\omega}{2}}$. Then $\\Delta x,\\Delta p = \\frac{\\hbar}{2}$, which indeed is the minimum uncertainty allowed by Heisenbergâ€™s principle (the ground state of the harmonic oscillator is a minimum uncertainty state). This kind of result underscores the fundamental nature of the harmonic oscillatorâ€™s ground state.\n:contentReference[oaicite:30]{index=30} Quantum harmonic oscillator potential $V(x)=\\frac{1}{2}m\\omega^2 x^2$ (orange parabola) and the first five energy levels (red dashed lines). The energy levels are equally spaced by $\\hbar\\omega$. The ground state $E_0=\\frac{1}{2}\\hbar\\omega$ lies above the minimum of the potential. Higher wavefunctions $\\psi_n(x)$ oscillate within the â€œclassically allowedâ€ region (where $E_n \u0026gt; V(x)$) and decay in the classically forbidden outside this region. Parity alternates with $n$: $E_1$ has an odd eigenfunction, $E_2$ even, etc. Equally spaced levels are a unique signature of the harmonic oscillator.\nIn summary, the harmonic oscillator stands apart from the square well models: its potential is continuous and unbounded, leading to an infinite ladder of quantized energies with constant spacing. Mastering this model involves familiarity with Hermite polynomials or the operator method, and understanding the properties of the oscillator states (e.g., spacing, parity, Gaussian envelopes). It is a favorite in exams for calculating expectation values and illustrating foundational principles like zero-point motion and minimum uncertainty.\nConclusion: The models covered â€“ infinite well, finite well (with its limiting cases of delta potential and free particle), and the harmonic oscillator â€“ form a toolkit of exactly solvable systems. They highlight how the SchrÃ¶dinger equation is solved under different boundary conditions and potentials. The infinite and finite wells teach about quantization due to boundary conditions and tunneling; the delta potential provides insight into scattering and bound states in a highly localized potential; the free particle reinforces the concept of continuous spectra and plane waves; and the harmonic oscillator, with its equal spacing and polynomial solutions, is a cornerstone for understanding vibrational motion in quantum systems. These models are not only theoretically important but also practically useful approximations in many physical situations. Being comfortable with the step-by-step solution procedures and the qualitative features of each model is essential for exam preparedness and for deeper studies in quantum mechanics.\n"},{"id":44,"href":"/docs/Physics/Quantum-Mechenics/%E6%B3%A2%E5%87%BD%E6%95%B0%E5%AD%98%E5%9C%A8%E4%BA%8E%E5%9C%A8Hilbert%E7%A9%BA%E9%97%B4%E4%B8%AD/","title":"æ³¢å‡½æ•°å­˜åœ¨äºåœ¨ Hilbertç©ºé—´ä¸­","section":"é‡å­åŠ›å­¦è®²ä¹‰","content":" Hilbert Space # Infinite dimensional vector space, denoted as $L^2(a,b)$, of square-integrable functions on an interval $[a,b]$. $$\\int_a^b |f(x)|^2 , dx \u0026lt; \\infty$$ Inner product defined as: $$\\langle f | g \\rangle = \\int_a^b f^(x)g(x) , dx$$ Note that: $$\\langle f | g \\rangle = \\langle g | f \\rangle^$$ $$\\langle f | f \\rangle = \\int_a^b |f(x)|^2 , dx \\geq 0$$ Also: $$\\langle f | f \\rangle = 0 \\iff f(x) = 0 \\quad\\text{in the interval}\\quad [a,b]$$\nOrthonormal Set ${f_n}$ # $$\\langle f_m | f_n \\rangle = \\int_a^b f_m^*(x)f_n(x),dx = \\delta_{m,n}$$ Completeness: A set of functions ${f_n}$ is complete if any $f(x)$ in the Hilbert space can be expanded as: $$f(x) = \\sum_n c_n f_n(x)$$ If ${f_n}$ is orthonormal, then: $$c_n = \\langle f_n | f \\rangle$$\nObservables and Hermitian Operators # An operator $\\hat{Q}$ is Hermitian if:\n$$ \\hat{Q} = \\hat{Q}^{\\dagger} $$\nProperties # Eigenvalues are real. Expectation value $\\langle Q \\rangle$:\n$$ \\langle Q \\rangle = \\langle \\psi | \\hat{Q} \\psi \\rangle = \\langle \\hat{Q}^{\\dagger} \\psi | \\psi \\rangle = \\langle \\psi | \\hat{Q} \\psi \\rangle^{*} $$\nThus,\n$$ \\langle Q \\rangle \\quad \\text{is real} $$\nCheck inner product:\n$$ \\langle f| \\hat{x} g \\rangle = \\int_{-\\infty}^{\\infty} f^*(x) x g(x) , dx $$\nComplex conjugate clearly shows:\n$$ = \\int_{-\\infty}^{\\infty} (x f(x))^* g(x) , dx = \\langle \\hat{x}f | g \\rangle $$\nThus,\n$$ \\hat{x} = \\hat{x}^{\\dagger} \\quad \\Rightarrow \\quad \\text{Hermitian} $$\nEvaluate inner product:\n$$ \\langle f| \\hat{p} g \\rangle = \\int_{-\\infty}^{\\infty} f^*(x) \\left(-i\\hbar \\frac{d}{dx}\\right) g(x) , dx $$\nUsing integration by parts:\n$$ = -i\\hbar \\int_{-\\infty}^{\\infty} dx \\frac{d}{dx}(f^(x)g(x)) + i\\hbar \\int_{-\\infty}^{\\infty} dx \\frac{df^(x)}{dx} g(x) $$\nBoundary term vanishes:\n$$ = -i\\hbar [f^*(x)g(x)]_{-\\infty}^{\\infty} + \\langle \\hat{p} f | g \\rangle, \\quad \\text{with boundary term = 0} $$\nThus:\n$$ \\langle f| \\hat{p} g \\rangle = \\langle \\hat{p} f | g \\rangle \\quad \\Rightarrow \\quad \\hat{p} = \\hat{p}^{\\dagger}, \\quad \\text{Hermitian!} $$\nObservables and Hermitian Operators # Hermitian Operator: # An operator $\\hat{Q}$ is Hermitian if:\n$$\\hat{Q} = \\hat{Q}^{\\dagger}$$\nSpectrum of $\\hat{Q}$ # Spectrum: The collection of all eigenvalues $q \\in \\mathbb{R}$. Eigenvalue equation:\n$$\\hat{Q}\\Psi = q\\Psi$$\nwhere:\n$q$ is an eigenvalue. $\\Psi$ represents eigenvectors, eigenstates, or eigenfunctions. Standard Deviation: # The uncertainty (standard deviation) $\\sigma$ of an observable $\\hat{Q}$ is given by:\n$$\\sigma^2 = \\langle \\Psi | \\hat{Q}^2 \\Psi \\rangle - \\langle \\Psi | \\hat{Q} \\Psi \\rangle^2$$\nIf $\\Psi$ is an eigenfunction of $\\hat{Q}$:\nEigenvalue equations: $$\\hat{Q}\\Psi = q\\Psi, \\quad \\hat{Q}^2 \\Psi = q^2 \\Psi$$\nThen, the standard deviation becomes:\n$$\\langle \\Psi | \\hat{Q}^2 \\Psi \\rangle = q^2 \\langle \\Psi | \\Psi \\rangle = q^2$$ $$\\langle \\Psi | \\hat{Q} \\Psi \\rangle^2 = (q \\langle \\Psi | \\Psi \\rangle)^2 = q^2$$\nThus:\n$$\\sigma^2 = q^2 - q^2 = 0$$\n\u0026mdash;Â Physical Interpretation: # This means that if we prepare a quantum state to be an eigenstate/eigenvector/eigenfunction of $\\hat{Q}$, then a measurement of $\\hat{Q}$ will return a definite value. In this case, the state $|\\Psi\\rangle$ is called a determinate state.\nExample # For $\\hat{H}\\Psi = E\\Psi$, we have:\n$E$ is an eigenvalue (energy level) belonging to the set ${ E }$, which represents all possible energies of the system. $\\Psi$ are the corresponding eigenstates/eigenfunctions of definite energy (stationary states). Example: Energy Eigenvalue Equation # The SchrÃ¶dinger equation for a quantum system is given by:\n$$\\hat{H}\\Psi = E\\Psi$$\nWhere:\n$E$ is an eigenvalue (energy level) belonging to the set ${ E }$, representing all possible energies of the system. $\\Psi$ represents the corresponding eigenstates or eigenfunctions of definite energy, also known as stationary states. Particle on a Ring of Radius $R$ # Coordinate transformation: $$x = R \\cdot \\phi, \\quad (\\text{with } \\phi \\text{ periodic, } \\phi \\sim \\phi + 2\\pi)$$\nMomentum Operator in Circular Coordinates: # $$\\hat{p} = -i\\hbar\\frac{d}{dx} = -i\\hbar\\frac{\\partial}{R\\partial\\phi} = \\frac{\\hbar}{R}\\left(-i\\frac{\\partial}{\\partial\\phi}\\right)$$\nCheck if $\\hat{Q}$ is Hermitian:\n$$\\langle f|\\hat{Q}g \\rangle \\stackrel{?}{=} \\langle \\hat{Q}f | g \\rangle$$\nHermiticity Check for Operator $\\hat{x}$: # $$\\langle f|\\hat{x}g \\rangle = \\int_{-\\infty}^{\\infty}f^*(x)xg(x),dx = \\langle \\hat{x}f|g \\rangle \\quad \\Rightarrow \\quad \\hat{x} = \\hat{x}^{\\dagger}$$\nThus, $\\hat{x}$ is Hermitian.\nEigenvalues and Eigenfunctions (Periodic Boundary Conditions): # Functions on a ring of radius $R$: periodic with $\\phi$: $$f(\\phi+2\\pi) = f(\\phi)$$ $$g(\\phi+2\\pi) = g(\\phi)$$ Eigenvalue equation for the operator $\\hat{Q}$: $$\\hat{Q}f(\\phi)=q f(\\phi)$$ Solve for $f(\\phi)$: $$-i\\frac{d f(\\phi)}{d\\phi} = q f(\\phi) \\quad\\Rightarrow\\quad f(\\phi) = A e^{i q \\phi}$$ Normalization and Quantization of $q$: # From periodic boundary condition:\n$$f(\\phi + 2\\pi) = A e^{i q (\\phi+2\\pi)} = A e^{i q \\phi} e^{i q 2\\pi} = f(\\phi)$$\nThus,\n$$e^{i q 2\\pi} = 1 \\quad\\Rightarrow\\quad q = 0, \\pm1, \\pm2, \\pm3, \\dots$$\nNormalization condition: # $$\\int_0^{2\\pi} d\\phi |f(\\phi)|^2 = \\int_0^{2\\pi} d\\phi |A|^2 = |A|^2 \\cdot 2\\pi = 1$$\nThus,\n$$|A|^2 = \\frac{1}{2\\pi} \\quad\\Rightarrow\\quad A = \\frac{1}{\\sqrt{2\\pi}}$$\nFinal Set of Eigenfunctions and Eigenvalues: # $$f_q(\\phi) = \\frac{1}{\\sqrt{2\\pi}} e^{i q \\phi}, \\quad q = 0, \\pm1, \\pm2, \\dots$$\nEigenvalues for Momentum $\\hat{p}$: # $$\\frac{\\hbar}{R}q = 0, \\pm\\frac{\\hbar}{R}, \\pm\\frac{2\\hbar}{R}, \\pm\\frac{3\\hbar}{R}, \\dots$$\nHere\u0026rsquo;s the requested content neatly formatted in Markdown with LaTeX notation, using the {align} environment for clarity:\nEigenfunctions of a Hermitian Operator # $$\\hat{Q}\\psi = q\\psi$$\nDiscrete Spectra: $$\\hat{Q} f = q f$$\nEigenvalues $q \\in \\mathbb{R}$. For two eigenfunctions $f$ and $g$ corresponding to distinct eigenvalues $q$ and $q\u0026rsquo;$, we have: $$\\hat{Q}f = qf, \\quad \\hat{Q}g = q\u0026rsquo;g,\\quad q \\neq q\u0026rsquo;$$ Then: $$\\langle f | \\hat{Q} g \\rangle = \\langle \\hat{Q}f|g \\rangle$$\nBut also: $$q\u0026rsquo;\\langle f|g \\rangle = q\\langle f|g\\rangle \\implies (q - q\u0026rsquo;)\\langle f|g\\rangle = 0$$\nThus, for distinct eigenvalues: $$\\langle f|g\\rangle = 0$$\nContinuous Spectra # Consider eigenfunctions of the momentum operator on the real line $(-\\infty, +\\infty)$:\n$$-i\\hbar \\frac{d}{dx}f_p(x) = p,f_p(x)$$\nEigenfunctions have the form: $$f_p(x) = A e^{\\frac{i p x}{\\hbar}}$$\nNote that these eigenfunctions are not square-integrable: $$\\int_{-\\infty}^{\\infty}|f_p(x)|^2,dx = |A|^2\\int_{-\\infty}^{\\infty}\\left|e^{\\frac{ipx}{\\hbar}}\\right|^2dx = \\infty$$\nHence, the eigenfunctions corresponding to continuous eigenvalues are not square-integrable functions.\n"},{"id":45,"href":"/docs/Physics/section/","title":"Section","section":"Physics","content":" Section # Section renders pages in section as definition list, using title and description. Optional param summary can be used to show or hide page summary\nExample # {{\u003c section [summary] \u003e}} Buttons Buttons # Buttons are styled links that can lead to local page or external link.\nExample # {{\u003c button relref=\"/\" [class=\"...\"] \u003e}}Get Home{{\u003c /button \u003e}} {{\u003c button href=\"https://github.com/alex-shpak/hugo-book\" \u003e}}Contribute{{\u003c /button \u003e}} Get Home Contribute "},{"id":46,"href":"/docs/Physics/section/buttons/","title":"Buttons","section":"Section","content":" Buttons # Buttons are styled links that can lead to local page or external link.\nExample # {{\u003c button relref=\"/\" [class=\"...\"] \u003e}}Get Home{{\u003c /button \u003e}} {{\u003c button href=\"https://github.com/alex-shpak/hugo-book\" \u003e}}Contribute{{\u003c /button \u003e}} Get Home Contribute "},{"id":47,"href":"/posts/2025-Topology-Summer-Research/","title":"2025 Topology Summer Research","section":"Blog","content":" Summer Application: # https://sgi.mit.edu/about-geometry-processing/\nApril # https://topologyandgeometry.iu.edu/gstgc25/\nMay # https://topology.franklinresearch.uga.edu/2025GITC\nPast Year # https://sites.google.com/view/princetonrtg2023/mini-conferences\n"},{"id":48,"href":"/posts/Image-to-3D-Model/","title":"Image to 3 D Model","section":"Blog","content":" Converting 2D Anime-Style Clothing to 3D: Tools \u0026amp; Workflow # Creating 3D clothing from 2D anime-style references (like Genshin Impact outfits) is now faster with AI-assisted tools, though manual refinement is often needed for the best results. This guide focuses on clothing conversion â€“ taking 2D images of robes, armor, or accessories and turning them into stylized 3D meshes with clean topology. Weâ€™ll explore the top AI tools and workflows (as of 2025) and outline a step-by-step process compatible with Blender.\nKey Requirements for 2D-to-3D Clothing Conversion # Stylized Fidelity: The 3D clothing should match the anime/Genshin Impact aesthetic of the concept art (shapes, folds, and design details). Optimized Topology: Meshes need clean, animation-friendly topology (proper edge loops, reasonable polycount) for attaching to a rigged character. Texture \u0026amp; Detail: Preserve clothing details (patterns, trims, armor segments) either as modeled geometry or textures/normal maps. Rigging Compatibility: The generated clothing must fit the existing character and allow weight painting or rig transfers so it deforms correctly during animation. Minimal Restrictions: Tools that allow creative freedom (no strict content rules) are preferred so any custom outfit design can be used. AI-Powered Tools for Image-to-3D Clothing Conversion # Modern AI tools can convert a single 2D image into a 3D model in minutes ( Converting 2D Images into 3D Models with AI: The step-by-step Guide). These are particularly useful to get a base 3D mesh of a clothing piece quickly, which can then be refined. Below are some of the best options:\nMeshy AI (Image to 3D): A popular AI 3D model generator with an image-to-3D feature and even a Blender plugin ( Meshy AI - The #1 AI 3D Model Generator for Creators) ( Meshy AI - The #1 AI 3D Model Generator for Creators). Meshy supports different art styles (including anime) for output ( Converting 2D Images into 3D Models with AI: The step-by-step Guide) ( Meshy AI - The #1 AI 3D Model Generator for Creators). You upload concept art or reference photos and get a 3D model with textures. Pros: Fast cloud generation, supports versatile art styles (can capture stylized looks) ( Meshy AI - The #1 AI 3D Model Generator for Creators), exports to common formats (OBJ, FBX, GLB, etc.) for easy Blender import ( Meshy AI - The #1 AI 3D Model Generator for Creators). Cons: Paid service (free tier available with limits), and results may require cleanup if topology is dense or if some parts are inaccurate.\nMazing AI / 3DFY.ai: Services that turn single images into 3D models with a focus on realism and high quality ( Converting 2D Images into 3D Models with AI: The step-by-step Guide). Mazing (an e-commerce oriented tool) emphasizes automatic texturing and real-time optimization ( Converting 2D Images into 3D Models with AI: The step-by-step Guide) ( Converting 2D Images into 3D Models with AI: The step-by-step Guide). 3DFY.ai similarly promises high-quality results from one image ( 3DFY.ai). Pros: Quick image to model conversion; optimized for product visuals. Cons: May be geared towards realistic objects; stylized anime clothing might need additional editing to match the art style.\nKaedim and Alternatives (Tripo 3D, Alpha3D): Kaedim is an AI-assisted service where you upload an image (even a sketch or concept) and their pipeline (ML + human touch-ups) delivers a 3D model ( Kaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify). Tripo 3D offers a similar â€œsingle image to 3D in secondsâ€ solution with emphasis on detailed geometry and textures ( Kaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify). Alpha3D provides image-to-3D generation but currently only for certain categories (e.g. shoes, furniture) ( Transform text and 2D images into 3D assets with generative AI for free - Alpha3D). Pros: These services deliver production-ready assets with textures and decent topology, suitable for game engines ( Kaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify). Cons: They are paid services (some with subscriptions) and may have category limits. Quality can vary â€“ often a good starting point but still might need retopology for optimal loops.\nThe New Black (AI Fashion Generator): A specialized tool for fashion design that can turn an outfit image into a realistic 3D clothing model ( AI Fashion Features | Clothing Design). Itâ€™s geared toward apparel designers (e.g. previewing how a garment looks in 3D). Pros: Focused on clothing, likely good with fabric details like folds and drape. Cons: Primarily aimed at realistic fashion; you might need to simplify or stylize the output for anime characters. Also, it may output standalone clothing on a generic avatar that youâ€™ll have to refit to your character.\nHunyuan 3D (Tencent): An AI model available via HuggingFace that generates 3D meshes from an image (used in the community alongside Meshy) ( Do you know an AI to create cloth and outfit? - Daz 3D Forums). Itâ€™s free to try and can handle characters or clothes. Pros: Free and accessible; known to work for generating a rough clothed figure mesh. Cons: The output might be a combined human+clothes mesh (if the input was a full character image) and will definitely require manual retopology and separation of the clothing. Good for getting the overall shape of a complex outfit, but not a final game-ready mesh.\nâ€œPic-to-3D Meshâ€ Blender Add-on: An add-on that integrates AI image-to-3D conversion directly in Blender ( Top AI Tools for Model Generation on Blender 3D - Vagon). You can input a reference image (e.g. a front view of a costume) and it generates a detailed 3D mesh inside Blender ( Top AI Tools for Model Generation on Blender 3D - Vagon). Pros: Fully inside Blender â€“ no need to use external apps; straightforward UI and quick conversion with just a few clicks ( Top AI Tools for Model Generation on Blender 3D - Vagon). This is useful to instantly get a mesh that you can start editing in the same session. Cons: Being relatively new, results can be hit-or-miss on complex armor or multi-layer outfits; likely works best for simpler garments or accessory pieces.\nPixelModeler AI (Blender Add-on): A unique workflow where you paint on a 2D canvas in Blender and an AI generates a corresponding 3D mesh ( PixelModeller AI - Blender Market) ( PixelModeller AI - Blender Market). This can be used by painting the silhouette or even a depth map of the clothing; the addon will create a solid mesh from it. Generated models are watertight, UV-mapped, and come with vertex colors (a basic texture) ( Top AI Tools for Model Generation on Blender 3D - Vagon), ready for further detailing. Pros: Gives a lot of control â€“ you essentially guide the shape by painting, so itâ€™s AI-assisted modeling rather than fully automatic. No external service needed (the AI model runs locally) ( PixelModeller AI - Blender Market). Cons: There is a learning curve to painting effective guides. It wonâ€™t automatically produce intricate patterns â€“ youâ€™ll need to add those via texture or additional modeling.\nComparison of Key Tools # Below is a quick comparison of these tools relevant to 2D-to-3D clothing conversion:\nTool/Service Type Output Quality Topology \u0026amp; UVs Integration with Blender Notes Meshy AI Cloud AI (imageâ†’3D) High detail; supports anime style ( Converting 2D Images into 3D Models with AI: The step-by-step Guide) ( Meshy AI - The #1 AI 3D Model Generator for Creators) Decent mesh; textured output (may need retopo) Blender plugin available ( Meshy AI - The #1 AI 3D Model Generator for Creators) Fast; paid (free trial available). Mazing / 3DFY.ai Cloud AI (imageâ†’3D) Photorealistic focus, good folds Optimized for real-time ( Converting 2D Images into 3D Models with AI: The step-by-step Guide); provides textures Exports standard formats (OBJ/FBX) Great for realism; stylization may require tweaks. Kaedim Cloud AI (+human) Custom models from concept art Cleaned by artists; quad topology Download to import in Blender Consistent results; subscription-based. Tripo 3D Cloud AI (imageâ†’3D) Fast generation, detailed textures ([Kaedim Alternatives in 2025 Best Kaedim Alternatives - Toolify]( https://www.toolify.ai/alternative/kaedim#:~:text=,model%20generation)) Unknown topology quality Exports GLB/OBJ The New Black (Fashion) Cloud AI (imageâ†’3D) Realistic garment on avatar Likely well-formed cloth mesh Export capabilities (likely OBJ/FBX) Fashion design oriented; may need rigging after import. Hunyuan (Tencent) Cloud AI (imageâ†’3D) Full character mesh with clothes High-poly, needs retopo OBJ export via HuggingFace demo Free; good for concept shape ( Do you know an AI to create cloth and outfit? - Daz 3D Forums). Pic-to-3D (Blender) Blender Add-on Good for single-object models ( Top AI Tools for Model Generation on Blender 3D - Vagon) Mesh quality varies; UV depends Inside Blender (no export needed) Convenient, no coding needed. PixelModeler (Blender) Blender Add-on User-guided, can achieve high detail Watertight \u0026amp; UV-mapped ( Top AI Tools for Model Generation on Blender 3D - Vagon) Inside Blender Interactive painting workflow. Table: AI-Based 2Dâ†’3D Clothing Tools â€“ Comparison (performance as of 2025).\nAI-Assisted + Manual Workflow Strategies # Fully automated results often need human improvement. In practice, the best quality comes from combining AI generation with manual modeling ( Converting 2D Images into 3D Models with AI: The step-by-step Guide). Here are some AI-assisted workflows that clothing artists use:\nImage Inpainting + Mesh Generation: One clever approach is to use AI image tools to conceptually dress your character, then extract a model. For example, a community-suggested workflow is: render your characterâ€™s base body in T-pose, use an AI image editor (like Stable Diffusion inpainting or Photoshopâ€™s generative fill) to â€œpaintâ€ new clothes onto the image, isolate just the garment in the edited image, then input that into an image-to-3D tool (Meshy or Hunyuan) to get a 3D mesh ( Do you know an AI to create cloth and outfit? - Daz 3D Forums). This way, the AI helps create a consistent design on the body and another AI turns it into geometry. Youâ€™d still need to retopologize and UV map the result manually ( Do you know an AI to create cloth and outfit? - Daz 3D Forums), but it jumpstarts the modeling process for complex costumes.\nDepth Map Extraction: If you have a front-view concept art of the outfit, you can generate a depth map (using AI like MiDaS or Stable Diffusion depth estimation). That depth map can be used to displace a plane or guide a mesh generation. Tools like PixelModeler AI automate this: they generate a depth internally from the image and produce a mesh ( PixelModeller AI - Blender Market). The output will capture the relief (folds, protrusions) from the concept art, though youâ€™ll have to model or guess the back side of the garment. This method is useful for armor pieces or relief details on clothing that are visible in the concept.\nTemplate-Based Generation (Parametric): Some solutions use parametric templates plus AI for customization. Sloyd.ai, for instance, combines a library of human-made base models with AI adjustments, ensuring the result is game-ready with UV maps and LODs generated, and optimized meshes ( SLOYD AI: A 3D Automation Tool To Generate Game-Ready Assets â€“ startupanz.com). If there are clothing templates (e.g. a generic T-shirt, jacket, dress) you can morph those to roughly match your design and let the tool handle topology. This is semi-manual: you pick the base closest to your design and tweak. Note: As of 2025, parametric generators like Sloyd have many props and environment assets; clothing templates might be limited, but the approach guarantees clean topology if a template fits your needs.\nManual Sculpt with AI Reference: Another assisted route is using the AI output as a reference or base mesh and then manually sculpting over it. For example, you can take a coarse mesh from an AI, bring it into Blender, and use multiresolution sculpting or retopology tools (like Quad Remesher or Blenderâ€™s shrinkwrap) to impose a clean topology that follows the AI modelâ€™s shape. The AI model essentially serves as a 3D concept sketch. You can also project the texture from the AI model (if it provided one) onto your new topology for a starting point.\nManual Tools for 3D Clothing Creation # While AI is speeding things up, manual modeling tools are still crucial, especially for achieving the cleanest results and stylized looks:\nMarvelous Designer / CLO3D: These are industry-standard tools for designing clothing using pattern-based simulation. You draw 2D garment patterns, sew them, and the software simulates the cloth on a avatar model â€“ perfect for creating natural folds and drapes. Pros: Extremely high fidelity cloth behavior; great for layered outfits, pleats, ruffles, etc. You can match an anime costume by designing similar patterns. Marvelous can even auto-generate PBR texture maps like normal and opacity for details ( [Tips\u0026amp;Tricks] Discover Better Workflow with Marvelous Designer and Unreal Engine â€“ Marvelous Designer). Cons: The meshes are triangulated and high-poly (since itâ€™s focused on simulation). You will need to retopologize the garment for use in a game or realtime engine ( Retopology of Marvelous Designer Clothes in Blender - YouTube). Marvelous has introduced some retopo tools (EveryWear Auto-Retopology) and can even rig garments, but often external retopo (using Blender or ZBrush) gives more control. Despite not being AI, Marvelous is frequently recommended for creating custom outfits ( Do you know an AI to create cloth and outfit? - Daz 3D Forums) because of the quality of the result. The typical workflow is simulate in Marvelous â†’ export OBJ â†’ retopo in Blender â†’ transfer to character rig. ( [Tips\u0026amp;Tricks] Discover Better Workflow with Marvelous Designer and Unreal Engine â€“ Marvelous Designer) Example of a stylized 3D outfit created with cloth simulation. (This fairy-like garment was designed and simulated in Marvelous Designer, showcasing layered fabric, ruffles, and realistic folds.)\nBlenderâ€™s Sewing/Cloth Tools: If you prefer open-source, Blender itself has cloth simulation and addons like Garment Tool that mimic Marvelousâ€™s pattern sewing approach. You can import your character into Blender, model garment panels (or even trace them from reference images), then use cloth physics to drape them. The result can then be applied as a shape key or applied mesh. Youâ€™ll still need to manually refine the mesh topology. Blenderâ€™s sculpting tools (cloth brush, slide relax, etc.) can also help adjust folds. This approach is manual and requires skill, but no additional cost.\nDirect Poly Modeling: For hard-surface armor pieces or very structured outfits (like a mech suit or a rigid breastplate), classic poly modeling or box modeling in Blender might be the way to go. You can use the 2D image as a reference in the background and model the clothing piece by piece (ensuring proper topology as you go). This is time-consuming but yields the cleanest meshes. You might use AI just to generate normals or texture details in this case, rather than the mesh.\nRetopology \u0026amp; Refinement Tools: No matter which initial method you choose, retopology tools are vital for clothing. Blender has a PolyBuild and Snap-to-face retopo workflow, and add-ons like RetopoFlow can speed it up. If you have ZBrush, ZRemesher can quickly re-mesh a triangulated Marvelous output into quads, which you can then tweak. There are also auto-retopology AI in development â€“ for instance, some research tools attempt to auto-retopo meshes with neural networks, but in practice most artists still do this part manually or with traditional algorithms. The goal is to end up with edge loops around openings (neck, arm holes) and ideally follow the flow of fabric folds with the topology for deformation.\nRecommended Workflow (Step-by-Step) # Bringing it all together, here is a step-by-step workflow to convert a 2D outfit into a 3D mesh and attach it to your Blender character, using the best of AI and manual tools:\n1. Gather Reference Images: Ideally have the concept art or reference of the clothing from as many angles as possible. A front view is usually required for AI tools; a side or back view (if available) will help during modeling or can be fed into some tools for better accuracy. If only a front view exists, be prepared to interpret the design for the unseen parts.\n2. Choose an AI Generation Method for Base Mesh: For a head start, pick one of the AI approaches:\nOption A: Use Meshy AI (or similar service) to upload the clothing image and generate a 3D model. Download the result (e.g. as a .glb or .obj) when ready ( Meshy AI - The #1 AI 3D Model Generator for Creators).\nOption B: In Blender, install the Pic-to-3D Mesh addon and run it on your reference image to get a mesh ( Top AI Tools for Model Generation on Blender 3D - Vagon).\nOption C: If the outfit is very complex or you want a full mannequin with clothing, try the Hunyuan 3D demo by providing an image of the clothed character; then extract the clothing mesh from the output.\nOption D: If you have a concept sketch, consider Kaedim/Tripo services for a perhaps cleaner base model (they might return the model next day or in a couple of hours, which you can then use).\nRegardless of option, donâ€™t expect a perfect final model â€“ treat this as a rough draft or proof of concept in 3D. It should capture the overall shape and major details of the clothing.\n3. Import and Inspect in Blender: Bring the generated 3D model into Blender. Center and scale it to your character. At this stage:\nCheck the mesh density and topology. Are there a lot of uneven triangles or random bumps? Check if all parts of the outfit are present. Sometimes single-view reconstructions leave holes or undefined backs. You may need to patch holes (Blenderâ€™s Fill or Grid Fill can help) or even mirror parts of the mesh if symmetry can be assumed. If the tool provided textures, apply them to see the look. However, for anime style, you might later hand-paint textures or use simple materials, so textures are optional. 4. Retopologize the Clothing Mesh: This is crucial for optimization. You can use Blenderâ€™s retopology tools to create a new mesh over the AI mesh:\nAdd a shrinkwrap modifier on a new mesh and model low-poly geometry that tightly wraps the AI model. Focus on quads and logical edge flow (e.g. edge loops around cuffs, hemlines, and along seams). Alternatively, use an auto-retopo tool: for example, Instant Meshes (free tool) or Quad Remesher (paid) to get a quick quad mesh. You might still tweak the output by hand. Ensure the retopoâ€™d mesh has proper thickness where needed (you can solidify later if itâ€™s cloth, but parts like armor might be modeled as solid pieces). UV unwrap the new mesh if not already UVâ€™d. Good UVs are needed for texturing anime-style details (like emblems or gradients on the fabric). 5. Fit and Attach to the Character: Place the new clothing mesh on the character in the correct pose (usually T-pose or A-pose matching the rig). To attach:\nUse Blenderâ€™s Transfer Weights: parent the clothing to the armature (with empty groups), then select the body, then clothing, and use Weight Transfer (source: body, destination: clothing). This copies the rig weights so the clothing will move with the body ( How separte clothes for Animatoion? - CG Cookie). Check deformation by posing the character. Likely you will need to clean up weights (for instance, ensure sleeves move with the arms, etc. without too much clipping). If the clothing is very close to the body, you might need to delete hidden faces of the character under the clothes to avoid mesh clipping in tight areas (e.g. remove torso polygons under a shirt). For rigid pieces (like armor plates), you may instead want to assign them to a specific bone and keep them rigid or use a bone parent for that object. 6. Detail and Texture: Now polish the visual fidelity:\nSculpt or model finer folds that the AI may have missed. You can use Blenderâ€™s sculpt mode with the cloth brush or crease brush to imprint additional wrinkle lines where appropriate. Add thickness to cloth if itâ€™s just a single surface (Solidify modifier). Stylized outfits often have a bit of thickness at edges (e.g. a coat lapel). Texture Painting: For anime style, a lot of detail can come from textures (like painted shadows or highlights, stylized fabric patterns). You can paint directly in Blender or use Substance 3D Painter. If the original 2D image has patterns (say, a symbol on the back of a cape), use it as a reference or even project it onto your UV map. Generate normal maps if needed. For example, if the outfit has an engraved design or stitching that is too fine to model, you can paint a height map and bake it to a normal map. Some AI tools can assist in generating texture maps from descriptions (e.g. Meshy has an AI texturing feature) ( Kaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify), which could be used to create stylized fabric textures by prompting. 7. Iterate and Refine: After these steps, you should have the clothing on the character, but iteration is key:\nGo back and forward between sculpting, adjusting topology, and tweaking weights until the clothing looks right and deforms well in various poses. If something is off compared to the concept art (maybe the AI misunderstood a part of the design), you might have to model that part manually. Itâ€™s common to model small accessories or intricate pieces separately (for example, a belt buckle or a brooch) and then attach them. LOD (Level of Detail): If this is for a game, consider making lower-poly versions or at least ensure the topology is efficient. AI meshes can be decimated or re-generated at lower detail if needed. 8. Final Check and Export: Once satisfied, you can integrate the clothed character into your project. Because we focused on Blender compatibility, you can continue to animate or render in Blender. If exporting to a game engine, export the character with the outfit as FBX/GLTF with the armature. Double-check that all parts are properly bound and that textures are packed or exported.\nThroughout this process, remember that AI is a helper, not a replacement for your skill. Even the best AI-generated model benefits from a human artistâ€™s eye for clean topology and style accuracy. As one guide noted, AI tools speed up getting a base, but â€œas AI is not perfect, [enhancement] is recommendedâ€ to reach production quality ( Converting 2D Images into 3D Models with AI: The step-by-step Guide). Donâ€™t hesitate to do manual touch-ups â€“ the goal is a high-quality anime-style outfit that looks like it was hand-crafted for the character.\nConclusion # Converting 2D anime-style clothing into 3D is becoming more accessible thanks to AI innovations. Tools like Meshy, PicTo3D, and others can generate a quick 3D draft of an outfit from a single concept image ( Top AI Tools for Model Generation on Blender 3D - Vagon) ( Converting 2D Images into 3D Models with AI: The step-by-step Guide), saving hours of initial modeling. However, the best results come from a hybrid workflow: leveraging AI for speed and then applying traditional modeling techniques for accuracy and clean topology. This collaborative approach (AI plus human) is highlighted as the future of 3D content creation ( Converting 2D Images into 3D Models with AI: The step-by-step Guide) ( Converting 2D Images into 3D Models with AI: The step-by-step Guide) â€“ AI handles the heavy lifting of shape prediction, while the artist refines and stylizes the final asset.\nBy carefully choosing the right tools and following a structured workflow, you can efficiently bring 2D costume designs into the 3D world, ready to be worn by your Blender character. The combination of AI-assisted generation and manual refinement ensures you get both speed and quality â€“ detailed Genshin Impact-style clothing that not only looks great but is also rigged and optimized for your creative projects.\nSources # MazingXR Blog â€“ â€œConverting 2D Images to 3D Models with AIâ€ (Feb 2025) ( Converting 2D Images into 3D Models with AI: The step-by-step Guide) ( Converting 2D Images into 3D Models with AI: The step-by-step Guide) ( Converting 2D Images into 3D Models with AI: The step-by-step Guide) Vagon Blog â€“ â€œTop AI Tools for Model Generation on Blender 3Dâ€ ( Top AI Tools for Model Generation on Blender 3D - Vagon) ( Top AI Tools for Model Generation on Blender 3D - Vagon) Daz3D Forums â€“ â€œDo you know an AI to create cloth and outfit?â€ (Jan 2025) ( Do you know an AI to create cloth and outfit? - Daz 3D Forums) ( Do you know an AI to create cloth and outfit? - Daz 3D Forums) StartupAnz â€“ â€œSloyd AI: Game-Ready 3D Asset Generationâ€ ( SLOYD AI: A 3D Automation Tool To Generate Game-Ready Assets â€“ startupanz.com) Alpha3D.io â€“ â€œ2D image to 3D model generation (limitations)â€ ( Transform text and 2D images into 3D assets with generative AI for free - Alpha3D) Toolify AI â€“ â€œKaedim Alternatives in 2025â€ ( Kaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify) (Tripo3D features) Marvelous Designer Official Support â€“ Workflow tips (pleat and texture generation) "},{"id":49,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.4-Sufficient-Conditions-for-Differentiability/","title":"6.4 å¯å¾®åˆ†æ€§çš„å¿…è¦æ¡ä»¶","section":"ç¬¬å…­ç«  å¯å¾®æ˜ å°„","content":" 1. Necessary Condition for Differentiability # Recall: A necessary condition for differentiability: $$ \\boxed{ f \\text{ differentiable} \\Rightarrow f \\text{ is continuous} } $$ Continuity is a requirement. 2. Sufficient Conditions for Differentiability # (a) Partial derivatives and differentiability f differentiable $\\Rightarrow$ continuity + partials exists conditions + partials exists $\\Rightarrow f$ differentiable (?)\nEx. 1 # Consider the function defined as:\n$$ f(x,y) = \\begin{cases} \\frac{xy}{x^2 + y^2}, \u0026amp; (x,y) \\neq (0,0) \\ 0, \u0026amp; (x,y) = (0,0) \\end{cases} $$\nClaim 1: ( f ) is continuous at ( (0,0) ). # We analyze the limit:\n$$ |xy| \\leq \\frac{1}{2} (x^2 + y^2) $$\nwhich implies:\n$$ f(x,y) \\to 0 \\quad \\text{as} \\quad (x,y) \\to (0,0) $$\nThus, ( f ) is continuous at ( (0,0) ).\nClaim 2: Compute partial derivatives at ( (0,0) ) # $$ \\frac{\\partial f(0,0)}{\\partial x} = \\lim_{x \\to 0} \\frac{f(x,0) - f(0,0)}{x} = \\lim_{x \\to 0} \\frac{0}{x} = 0 $$\n$$ \\frac{\\partial f(0,0)}{\\partial y} = \\lim_{y \\to 0} \\frac{f(0,y) - f(0,0)}{y} = \\l\n"},{"id":50,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.5-The-Chain-Rule/","title":"6.5 é“¾å¼æ³•åˆ™","section":"ç¬¬å…­ç«  å¯å¾®æ˜ å°„","content":"[ ]\n"},{"id":51,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.6-Product-Rule-and-Gradients/","title":"6.6 ä¹˜ç§¯æ³•åˆ™ä¸æ¢¯åº¦","section":"ç¬¬å…­ç«  å¯å¾®æ˜ å°„","content":"[ ]\n"},{"id":52,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.9-Taylor-Higher-Dimentions/","title":"6.9 æ³°å‹’å…¬å¼çš„é«˜ç»´å½¢å¼","section":"ç¬¬å…­ç«  å¯å¾®æ˜ å°„","content":"$$ R_{r-1} = \\frac{1}{r!} D^{(r)} f(\\xi) (x-x_0, \\dots, x-x_0) $$ And satisfying $$ \\frac{R_{r-1}(x_0)}{|x - x_0|^{r-1}} \\to 0 \\quad \\text{as} \\quad x \\to x_0. $$\n\\begin{proof} Consider 1-variable function: $$ g(t) = f(x_0 + t(x - x_0)), \\quad (a, b) \\to \\mathbb{R} $$ for $t\\in (a, b)$ with $[0, 1] \\subset (a, b)$.\nApplying Taylor\u0026rsquo;s theorem to $g(t)$: $$ \\begin{align} g(1) \u0026amp;= g(0) + g\u0026rsquo;(0)(1-0) + \\frac{g\u0026rsquo;\u0026rsquo;(0)}{2!} (1-0)^2 + \\dots + \\frac{g^{(r-1)}(0)}{(r-1)!} (1-0)^{r-1} + R_{r-1}\\ f(x) \u0026amp;= f(x_0) + \\sum_{k=1}^{r-1} \\frac{g^{(k)}(0)}{k!} + \\frac{1}{r!} g^{(r)}(\\tilde{c}), \\quad \\tilde{c} \\in [0,1] \\end{align} $$ By chain rule, $$ g\u0026rsquo;(t) = Df(\\varphi(t)) \\cdot \\varphi\u0026rsquo;(t) $$ $$g\u0026rsquo;(0) = Df(x_0) (x - x_0) $$ \\end{proof}\n#Example # Determine the $2\\text{nd}$ order Taylor formula for $$f(x,y)=e^{(x-1)^{2}}\\cos (y)\\quad \\text{at},(1,0)$$ Solution (compute partials): $$ \\begin{align} \\frac{\\partial f}{\\partial x} \u0026amp;= e^{(x-1)^2} 2(x-1) \\cos y, \\quad \\frac{\\partial f}{\\partial y} = -e^{(x-1)^2} \\sin y, \\ \\frac{\\partial^2 f}{\\partial x^2} \u0026amp;= 2 e^{(x-1)^2} \\cos y + 4(x-1)^2 e^{(x-1)^2} \\cos y, \\ \\frac{\\partial^2 f}{\\partial x \\partial y} \u0026amp;= -2 (x-1) e^{(x-1)^2} \\sin y, \\qquad \\frac{\\partial^2 f}{\\partial y^2} = -e^{(x-1)^2} \\cos y. \\end{align} $$\nTaylor\u0026rsquo;s Formula: Let $h = x - x_0 = (x,y) - (1,0)$, then we have\n$$ \\boxed{f(x,y) = f(1,0) + \\mathbb{D}f(1,0)(h) + \\frac{1}{2} \\mathbb{D}^2 f(1,0)(h,h) + R_2} $$ $$ f(1,0) = 1, \\quad \\mathbb{D}f(1,0) = (0 \\quad 0), $$ $$ \\mathbb{D}^2 f(1,0) = \\begin{bmatrix} 2 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{bmatrix} $$ Evaluating the second-order term: $$ \\begin{align} \\mathbb{D}^2 f(1,0)(h,h) \u0026amp;= \\begin{bmatrix} x-1 \u0026amp; y \\end{bmatrix} \\begin{bmatrix} 2 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{bmatrix} \\begin{bmatrix} x-1 \\ y \\end{bmatrix} \\ \u0026amp;= 2(x-1)^2 - y^2 \\end{align} $$\nThus, $$f(x,y) = 1 + \\frac{1}{2} (2(x-1)^2 - y^2) + R_2 $$\n3 Maximum and Minimum Problem in $\\mathbb{R}^n$ # 3.1 Introduction # Q: Given $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$,\nhow do we find (local) max or min points for $f$ in $A$?\nRecall 1-D case: $f: (a,b) \\to \\mathbb{R}$\nA local max / min point (or extreme point) $x_0$ must be a critical point: $$ \\boxed{f\u0026rsquo;(x_0) = 0 \\quad \\text{or\\quad DNE}}\n$$ 3.2 Second Derivative Test (for a critical point) # $$ \\begin{aligned} f\u0026rsquo;\u0026rsquo;(x_0) \u0026gt; 0 \u0026amp;\\Rightarrow x_0 \\text{ is a local minimum} \\ f\u0026rsquo;\u0026rsquo;(x_0) \u0026lt; 0 \u0026amp;\\Rightarrow x_0 \\text{ is a local maximum} \\end{aligned} $$\n4. Necessary Condition for Extreme Points in $\\mathbb{R}^n$ # Definition: Let $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$.\nA point $x_0 \\in A$ is a local minimum if: $$ \\exists \\delta \u0026gt; 0 \\text{ such that } | x - x_0 | \u0026lt; \\delta, x \\in A \\Rightarrow f(x) \\geq f(x_0) $$ A point $x_0 \\in A$ is a local maximum if: $$ \\exists \\delta \u0026gt; 0 \\text{ such that } | x - x_0 | \u0026lt; \\delta, x \\in A \\Rightarrow f(x) \\leq f(x_0) $$\n[!theorem|6.9.2] If $f: A \\to \\mathbb{R}$ is differentiable at $x_0$, and if $x_0 \\in A$ is an extreme point for $f$, then $x_0$ is a critical point, i.e., $$ Df(x_0) = 0. $$\nRemark # The condition $\\mathbb{D}f(x_0) = 0$ is necessary, but not sufficient!\nExample # Let $f(x,y) = x^2 - y^2$, then $\\mathbb{D}f(0,0) = 0$, but $(0,0)$ is a saddle point.\n\\begin{proof}\nAssume $Df(x_0) \\neq 0$.\nThen, there exists $v \\in \\mathbb{R}^n$ such that $Df(x_0)(v) = c \u0026gt; 0$. By the definition of differentiability, choose $\\delta \u0026gt; 0$ such that: $$ | f(x_0 + h) - f(x_0) - Df(x_0)(h) | \u0026lt; \\frac{c}{2 | v |} | h | $$ for all $| h | \u0026lt; \\delta$.\nNow, choose $h = \\lambda v$ with $\\lambda \u0026gt; 0$ and $| h | \u0026lt; \\delta$, then: $$ \\begin{cases} f(x_0 + \\lambda v) - f(x_0) \u0026gt; 0 \\ f(x_0 - \\lambda v) - f(x_0) \u0026lt; 0 \\end{cases} $$ This establishes the desired contradiction. \\end{proof}\n"},{"id":53,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1-Inverse-Function-Theorem/","title":"7.1 åå‡½æ•°å®šç†","section":"ç¬¬ä¸ƒç«  é€†å‡½æ•°å’Œéšå‡½æ•°å®šç†","content":" 7.1 Inverse Function Theorem (IFT) # Two lines of ideas:\nA: CMP $â‡’$ Inverse FT $â‡’$ Applications in ODE\nB: IFT $â‡’$ Implicit FT $â‡’$ Local behavior, extreme problems\nI. Inverse Function Theorem # 1. Linear Case # Consider a linear map, $y = f(x): \\mathbb{R}^n \\to \\mathbb{R}^n$.\n$$ x = (x_1, x_2, \\dots, x_n)^T $$\nGiven $y \\in \\mathbb{R}^n$, $f(x)$ is a linear system of equations:\n$$\\begin{aligned} y_1 \u0026amp;= a_{11} x_1 + a_{12} x_2 + \\dots + a_{1n} x_n \\\\ y_2 \u0026amp;= a_{21} x_1 + a_{22} x_2 + \\dots + a_{2n} x_n \\\\ \u0026amp;\\vdots \\\\ y_n \u0026amp;= a_{n1} x_1 + \\dots + a_{nn} x_n \\end{aligned}$$\nor $$ A_{n\\times n}X_{n\\times 1} = Y_{n\\times 1} $$\n[!assumption|*] $$X \\text{ has a unique solution} \\Longleftrightarrow \\det(A) \\neq 0.$$\nIn this case, the solution is given by: $$ X = A^{-1} Y $$ Thus, the inverse function satisfies: $$ f^{-1} \\circ f = \\text{Identity} $$ The inverse theorem for $y = f(x)$:\n$$ f(f^{-1}(y)) = A A^{-1} y = y $$\nQuestion: When can we solve a nonlinear system? # We consider a system of nonlinear equations: $$ \\begin{cases} f_1(x_1, x_2, \\dots, x_n) = y_1 \\\\ f_2(x_1, x_2, \\dots, x_n) = y_2 \\\\ \\quad \\vdots \\\\ f_n(x_1, x_2, \\dots, x_n) = y_n \\end{cases} $$ or equivalently, $$ f(x) = y $$\n2. The Inverse of a General Function # Notation:\nLet $y = f(x): A \\subset \\mathbb{R}^n \\to \\mathbb{R}^n$ be a diffeomorphism.\n$$ y = (y_1, y_2, \\dots, y_n) $$\nwhere\n$$ y_i = f_i(x_1, x_2, \\dots, x_n) $$\nThe Jacobian determinant of $f$ at $x$ is:\n$$ \\det \\left( \\frac{\\partial f_i}{\\partial x_j} \\right) $$\n[!theorem|7.1.1] Let $y = f(x): A \\subset \\mathbb{R}^n \\to \\mathbb{R}^n$ be of class $C^1$. Suppose $x_0 \\in A$ and $\\det(Df(x_0)) \\neq 0$. Then there exists a neighborhood $U$ of $x_0$ and a neighborhood $W$ of $y_0 = f(x_0)$ such that:\n$f: U \\to W$ has an inverse $f^{-1}: W \\to U$. $f^{-1}$ is of class $C^1$. $D(f^{-1}(y)) = (Df(x))^{-1}$ for all $y \\in W$ at $y = f(x)$. Visualization: # $y = f(x)$ maps from $U$ to $W$. $x = f^{-1}(y)$ gives the inverse mapping from $W$ back to $U$. Recall: Contraction Mapping Principle (CMP) # Let $\\mathbb{X}$ be a complete metric space and let\n$$ \\varphi: \\mathbb{X} \\to \\mathbb{X} $$\nbe a function satisfying a contraction condition for some constant $k$ with $0 \u0026lt; k \u0026lt; 1$:\n$$ d(\\varphi(x), \\varphi(y)) \\leq k \\cdot d(x,y), \\quad \\forall x,y \\in \\mathbb{X}. $$\nThen, there exists a unique fixed point $X^*$ such that:\n$$ \\varphi(X^) = X^. $$\nProof of the Inverse Function Theorem (IFT) # Step 1: Reductions # (a) May assume that the Jacobian matrix at $x_0$ is the identity: $$ D f(x_0) = I. $$ In fact, define the transformation: $$ T = D f(x_0). $$ Then, we can consider a new function:\n$$ \\tilde{f} = T^{-1} \\circ f. $$\nThus,\n$$ D(\\tilde{f})(x_0) = I. $$\n(b) Main assumption:\n$$ x_0 = f^{-1}(y_0). $$\nTo see this, define:\n$$ h(x) = f(x) - f(x_0). $$\nThen,\n$$ D h(x_0) = D f(x_0) - D f(x_0) = 0. $$\nIf $h^{-1}$ exists, then $y = f(x)$ can be solved as:\n$$ f(x) = h(x) + f(x_0) = y. $$\nThus, the inverse function satisfies:\n$$ x = x_0 - h^{-1}(y - f(x_0)). $$\nStep 2: Existence of the Inverse Function # (a) Setup: By the reduction above, we assume:\n$$ x_0 = 0, \\quad y_0 = f(x_0) = 0, \\quad D f(x_0) = I. $$\nNeed to show:\nThere exist neighborhoods $U$ and $W$ such that the mapping:\n$$ y = f(x): U \\to W $$\nhas an inverse function in $W$, meaning:\n$$ \\forall y \\in W, \\quad \\exists! x \\in U \\text{ such that } y = f(x). $$\nIllustration:\nA diagram representing $U$ mapping to $W$ via $f$, where $f$ is invertible.\nFor a fixed $y \\in \\mathbb{R}^n$, define: # $$ g_x = g(y) = y + x - f(x). $$\nWe need to show that $g_x$ has a unique fixed point.\n(b) Construction of neighborhoods $U$ and $W$\nLet:\n$$ g(x) = x - f(x). $$\nThen:\n$$ D g(x) = I - D f(x). $$\nSince:\n$$ D g(x_0) = I - I = 0, $$\nit follows that:\n$$ D g(x) \\text{ is close to zero}. $$\nThus, choosing:\n$$ \\epsilon = \\frac{1}{2n}, $$\nthere exists $\\delta \u0026gt; 0$ such that:\n$$ |x - x_0| \u0026lt; \\delta \\implies |D g_x(x)| \\leq \\frac{1}{2n}. $$\nApplying the Contraction Mapping Principle to $g_x$, we obtain:\n$$ \\exists x \\in B_{\\delta}(x_0) \\text{ such that } g_x(x) = x. $$\nThus:\n$$ g_x(x) = g_x(x_0) + D g_x(\\xi)(x - x_0), $$\nwhich shows:\n$$ D g_x(\\xi) (x - x_0). $$\nChapter 7: Inverse and Implicit Function Theorems # Contraction Mapping Principle (CMP) # Let $\\mathbb{X}$ be a complete metric space, and let $\\varphi: \\mathbb{X} \\to \\mathbb{X}$ satisfy:\n$$ d(\\varphi(x), \\varphi(y)) \\leq k \\cdot d(x,y), \\quad 0 \u0026lt; k \u0026lt; 1. $$\nThen, there exists a unique fixed point $X^*$ such that $\\varphi(X^{*}) = X^{*}$.\nProof of the Inverse Function Theorem (IFT) # Step 1: Reduction # Assume $Df(x_0) = I$. Define $\\tilde{f} = Df(x_0)^{-1} \\circ f$, ensuring $D\\tilde{f}(x_0) = I$.\nFor $x_0 = f^{-1}(y_0)$, define $h(x) = f(x) - f(x_0)$. Since $Dh(x_0) = 0$, solving $f(x) = y$ reduces to:\n$$ x = x_0 - h^{-1}(y - f(x_0)). $$\nStep 2: Existence of the Inverse** # Set up: $x_0 = 0, y_0 = f(x_0) = 0, Df(x_0) = I$. Need to show a local inverse:\n$$ \\forall y \\in W, \\quad \\exists! x \\in U \\text{ such that } y = f(x). $$\nDefine:\n$$ g_x(y) = y + x - f(x). $$\nWe need to show $g_x$ has a unique fixed point.\nLet $g(x) = x - f(x)$, then $Dg(x) = I - Df(x)$. Since $Dg(x_0) = 0$, choosing $\\epsilon = \\frac{1}{2n}$ ensures $|D g_x(x)|$ is small. Applying CMP, we get:\n$$ \\exists x \\in B_{\\delta}(x_0) \\text{ such that } g_x(x) = x. $$ Thus, the inverse exists and is unique.\nbabeldown::deepl_translate_hugo( post_path = \u0026ldquo;content.en/docs/Mathematics/MATH 412 Real Analysis II/Lecture/7.1 Inverse Function Theorem.md\u0026rdquo;, target_lang = \u0026ldquo;ZH\u0026rdquo;, source_lang = \u0026ldquo;EN\u0026rdquo;, force = TRUE )\n"},{"id":54,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1.1-Inverse-FT-Proof/","title":"7.1.1 åå‡½æ•°å®šç†ï¼ˆè¯æ˜ï¼‰","section":"ç¬¬ä¸ƒç«  é€†å‡½æ•°å’Œéšå‡½æ•°å®šç†","content":" 7.1* Implicit Function Theorem (IFT) Proof # 1. Recall IFT # Theorem 7.1.1: Let $y = f(x): A \\subset \\mathbb{R}^n \\to \\mathbb{R}^n$ be of class $C^1$. Suppose $x_0 \\in A$ with: $$ J_f(x_0) = \\det(Df(x_0)) \\ne 0 $$ Then there exist neighborhoods $U$ of $x_0$ in $A$ and $W$ of $y_0 = f(x_0)$ such that:\n$f(U) = W$ and $f: U \\to W$ has an inverse $f^{-1}: W \\to U$. $f^{-1} \\in C^1$ (If $f \\in C^r$, then $f^{-1} \\in C^r$). $Df^{-1}(y) = [Df(x)]^{-1}$ for $x \\in U$ and $y = f(x)$. 2. Proof of Theorem 7.1.1 # Step 1: Reduction # We may assume $Df(x_0) = I$ and $x_0 = 0$, $y_0 = f(x_0)$.\nStep 2: Existence of inverse # Consider the function $g(x) = x - f(x)$.\nUsing continuity of $Dg(x)$ at $0$ and Mean Value Theorem, one can show there exists $\\delta \u0026gt; 0$ such that for $x \\in B(0, \\delta)$: $$ |g(x)| \\le \\frac{\\delta}{2} $$ Define $g: B(0, \\delta) \\to B(0, \\frac{\\delta}{2})$. Let $W = B(0, \\frac{\\delta}{2})$, and define: $$ U = { x \\in B(0, \\delta): f(x) \\in W } $$ Step 3: Existence of $f^{-1}: W \\to U$ # Fix $y \\in W$. Apply the Contraction Mapping Principle (CMP) to: $$ g_y(x) = y + x - f(x) = y + g(x) $$ Then $g_y(x): B(0, \\delta) \\to B(0, \\delta)$. Thus, there exists a unique $x \\in B(0, \\delta)$ such that: $$ g_y(x) = x \\quad \\Longrightarrow \\quad f(x) = y $$ Therefore, $\\exists! x \\in U$ such that $f(x) = y$.\nFix $y, y_1, y_2 \\in W$, let $x_i = f^{-1}(y_i), i = 1,2$. Then: $$ | f^{-1}(y_1) - f^{-1}(y_2) | = | x_1 - x_2 | = | g_{y_1}(x_1) - g_{y_2}(x_2) | $$\nSince $| Dg(x) | \\le \\frac{1}{2}$ for $x \\in B(0, \\delta)$, we get: $$ | x_1 - x_2 | \\le 2 | y_1 - y_2 | $$\nThus, $f^{-1}$ is Lipschitz continuous.\nStep 4: Differentiability of $f^{-1}$ # (i) Observation: $[Df(x_0)]^{-1}$ exists and $Df(x)$ is continuous at $x_0$.\n$$ \\Rightarrow \\exists \\delta \u0026gt; 0 \\text{ such that } [Df(x)]^{-1} \\text{ exists and bounded by } M \\text{, } \\forall |x| \\leq \\delta $$ $$ | [Df(x)]^{-1} | \\leq M, \\quad \\forall x \\in B(0, \\delta) $$\n(ii) Show $f^{-1}$ is differentiable at any $y_* \\in W$ and: $$ Df^{-1}(y_0) = [Df(x_0)]^{-1}, \\quad \\text{where} \\quad y_0 = f(x_0) $$\nFix $y_* \\in W$. Then: $$ \\frac{| f^{-1}(y) - f^{-1}(y_) - [Df(x_0)]^{-1}(y - y_) |}{| y - y_* |} $$ can be simplified, and as $y \\to y_*$, it tends to $0$.\nThus, in conclusion, $f^{-1}(y)$ is differentiable at $y_* \\in W$ and: $$ Df^{-1}(y_) = [Df(x_)]^{-1} $$\nExample: # Investigate the invertibility (both local and global) for the map: $$f \\in C^\\infty, \\quad A = \\mathbb{R}^2$$\n$$W = (u,v) = f(x,y): \\mathbb{R}^2 \\to \\mathbb{R}^2$$ Given by: $$ u = e^x\\cos y, \\quad v = e^x\\sin y$$\nCompute Jacobian determinant: $$ J_f(x,y) = \\det(Df(x,y)) = \\begin{vmatrix} e^x\\cos y \u0026amp; -e^x\\sin y \\\\ e^x\\sin y \u0026amp; e^x\\cos y \\end{vmatrix} = e^{2x} \u0026gt; 0 $$ Thus, by IFT, $f$ is invertible locally at any point and: $$ Df^{-1}(u,v) = [Df(x,y)]^{-1} = \\begin{bmatrix} e^x\\cos y \u0026amp; -e^x\\sin y \\\\ e^x\\sin y \u0026amp; e^x\\cos y \\end{bmatrix}^{-1} = \\begin{bmatrix} e^{-x}\\cos y \u0026amp; e^{-x}\\sin y \\\\ -e^{-x}\\sin y \u0026amp; e^{-x}\\cos y \\end{bmatrix} $$\nHowever, $f$ is not globally invertible (not injective). Consider: $$ \\begin{aligned} f(x_0, y_0 + 2\\pi) \u0026amp;= (e^{x_0}\\cos(y_0 + 2\\pi), e^{x_0}\\sin(y_0 + 2\\pi))\\\\ \u0026amp;= (e^{x_0}\\cos y_0, e^{x_0}\\sin y_0)\\\\ \u0026amp;= (u_0, v_0) \\end{aligned} $$\nIn complex notation, $f$ can be written as: $$ f(z) = e^z = e^{x+iy} = e^x e^{iy} = e^x(\\cos y + i \\sin y) $$ with $u = e^x \\cos y$, $v = e^x \\sin y$.\nConclusion # Since $f(x, y)$ maps points periodically in $y$, it is not globally injective, despite being locally invertible.\nAdditional Notes # The periodic nature is reflected in the mapping: $$f(x_0, y_0 + 2\\pi) = f(x_0, y_0)$$ This demonstrates that multiple points in the domain map to the same point in the range, confirming non-injectivity.\n"},{"id":55,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.2-Implicit-Function-Theorem/","title":"7.2 éšå‡½æ•°å®šç†","section":"ç¬¬ä¸ƒç«  é€†å‡½æ•°å’Œéšå‡½æ•°å®šç†","content":"111\n"},{"id":56,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.5-Improper-Integral/","title":"8.5 ä¸å®šç§¯åˆ†","section":"ç¬¬å…«ç«  åº¦é‡ç†è®º","content":" 8.3 åå¸¸ç§¯åˆ† (Improper Integrals) # WTS: Study integral of the form $\\int_A f(x)$, where $f : A \\subset \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is an arbitrary function and $A \\subset \\mathbb{R}^n$ is an arbitrary set.\nå®šä¹‰ 8.3.1 (Integral) # If $A \\subset \\mathbb{R}^n$ is bounded and $f$ is bounded, then $$\\int_A f(x) = \\int_{\\overline{A}} f(x) = \\underline{\\int_{A}} f(x) \\quad \\text{(Riemann Condition)}$$\n$f(x) \\geq 0$ bounded and $A$ is arbitrary, then $$\\int_A f(x) = \\lim_{a\\to\\infty} \\int_{A_a} f(x)$$\n$A \\subset \\mathbb{R}^n$\n$A_a = [-a, a]^n$\n$f(x) \\geq 0$ unbounded and $A$ is arbitrary.\nFor $M \u0026gt; 0$, define $$f_M(x) = \\begin{cases} f(x) \u0026amp; \\text{for } f(x) \\leq M \\ 0 \u0026amp; \\text{o/w}. \\end{cases}$$\nThen, $\\int_A f(x) = \\lim_{M\\to\\infty} \\int_A f_M(x)$.\n$f$ is arbitrary and $A$ is arbitrary.\nLet $$f^+(x) = \\begin{cases} f(x) \u0026amp; f(x) \\geq 0 \\ 0 \u0026amp; f(x) \u0026lt; 0, \\end{cases}$$\nand\n$$f^-(x) = \\begin{cases} 0 \u0026amp; f(x) \\geq 0 \\ -f(x) \u0026amp; f(x) \u0026lt; 0. \\end{cases}$$\nRemark 8.5\n$f^+(x)$ is the positive part of $f$ and $f^-(x)$ is the negative part of $f$. $f^+, f^- \\geq 0$. $f(x) = f^+(x) - f^-(x)$. We can write any function as the difference of two non-negative functions. $|f(x)| = f^+(x) + f^-(x)$. So, $f$ is integrable on $A$ if both $f^+$ and $f^-$ are integrable on $A$. We write $$\\int_A f(x) = \\int_A f^+(x) - \\int_A f^-(x).$$\nRemark 8.6\nOne can show this definition preserves linearity of integral from bounded case. Observation: $f$ integrable $\\Rightarrow$ $f^+$ and $f^-$ integrable $\\Rightarrow$ $|f| = f^+ + f^-$ is also integrable. However, $|f|$ integrable $\\nRightarrow$ $f$ integrable. For counterexample, $$f(x) = \\begin{cases} 1 \u0026amp; x \\text{ rational} \\ -1 \u0026amp; x \\text{ irrational} \\end{cases} \\text{ on } [0, 1].$$ $|f(x)| \\equiv 1 \\Rightarrow$ integrable. But $f^+, f^-$, or $f$ are not integrable. Theorem 8.3.2 Comparison Principle # Suppose\n$0 \\leq g \\leq f$ on $A$ and $\\int_A f$ converges (i.e., $f$ is integrable on $A$) $g$ is integrable on each finite rectangle $[-a, a]^n$. Then, $g$ is also integrable on $A$, and $\\int_A g \\leq \\int_A f$.\nRemark 8.7 The second condition is crucial and cannot be removed.\nProof 1. Since $g \\geq 0$ and is integrable on $[-a, a]^n$, define $$G(a) = \\int_{[-a,a]^n} g(x).$$\nThen, $G(a)$ is an increasing function of $a$. Furthermore, $$g \\leq f \\Rightarrow G(a) = \\int_{[-a,a]^n} g(x) \\leq \\int_{[-a,a]^n} f(x) \\leq \\int_A f(x).$$\nSo, $\\int_A g(x) = \\lim_{a\\to\\infty} G(a) \\leq \\int_A f(x)$.\nQ.E.D. â– \nQuestion: When does an integrable $\\int_a^b f(x)$ (one-variable function) converge? If it converges, how to compute?\nTheorem 8.3.3 Integral of Functions of One-Variable # Suppose $f : [a, \\infty] \\rightarrow \\mathbb{R}$ is continuous with $f(x) \\geq 0$. Let $F\u0026rsquo;(x) = f(x)$. Then, $$\\int_a^{\\infty} f(x) dx = \\lim_{b\\to\\infty} \\int_a^b f(x) dx = \\lim_{b\\to\\infty} [F(b) - F(a)].$$\nSuppose $f : (a, b] \\rightarrow \\mathbb{R}$ is continuous with $f(x) \\geq 0$. Then, $$\\int_a^b f(x) dx = \\lim_{\\varepsilon\\to0^+} \\int_{a+\\varepsilon}^b f(x) dx.$$\nExample 8.3.4 # Consider $\\int_1^{\\infty} x^p dx$.\nSolution 2.\nFor $b \\geq 1$, $$\\int_1^b x^p dx = \\begin{cases} \\ln b \u0026amp; p = -1 \\ \\frac{1}{p + 1}(b^{p+1} - 1) \u0026amp; p \\neq -1. \\end{cases}$$\nWhen $b \\to \\infty$, $\\int_1^b x^p dx$ diverges when $p \\geq -1$ and converges when $p \u0026lt; -1$. So,\n$\\int_1^{\\infty} x^p dx$ is divergent when $p \\geq -1$\nand\n$\\int_1^{\\infty} x^p dx = -\\frac{1}{p + 1}$ is convergent when $p \u0026lt; -1$.\nConsider $\\int_1^{\\infty} e^{-x^2}x^3 dx$.\nSolution 3.\nConverges by comparison.\nDefinition 8.3.5 (Conditional Convergence) # $$\\int_a^{\\infty} f(x) dx \\text{ (conditional)} = \\lim_{b\\to\\infty} \\int_a^b f(x) dx.$$\nRemark 8.8 (Types of Convergence) For an improper integral $\\int_a^{\\infty} f(x) dx$, there are three types of convergence:\nAbsolute Convergence: $\\int_a^{\\infty} |f(x)| dx$ exists.\nConditional Convergence: $\\lim_{b\\to\\infty} \\int_a^b f(x) dx$ exists.\nDivergence.\nFor general function, absolute convergence $\\nRightarrow$ conditional convergence. For continuous function, absolute convergence is stronger, and $\\Rightarrow$ conditional convergence.\nExample 8.3.6 # Determine whether the integral $\\int_1^{\\infty} \\frac{\\cos x}{x} dx$ is absolute convergence, conditional convergence, or neither (divergence).\nSolution 4.\nFirst, consider absolute convergence.\nObserve that $$\\int_0^{\\infty} \\left|\\frac{\\cos x}{x}\\right| dx = \\int_1^{\\infty} \\frac{|\\cos x|}{x} dx \\geq \\int_{\\pi/2}^{n\\pi/2} \\frac{|\\cos x|}{x} dx$$\n$$= \\sum_{k=1}^{n-1} \\int_{k\\pi/2}^{(k+1)\\pi/2} \\frac{|\\cos x|}{x} dx$$\n$$\\geq \\sum_{k=1}^{n-1} \\frac{1}{(k + 1)\\frac{\\pi}{2}} \\int_{k\\pi/2}^{(k+1)\\pi/2} |\\cos x| dx$$\n$\\to \\infty$ as $n \\to \\infty$.\nSo, $\\int_1^{\\infty} \\left|\\frac{\\cos x}{x}\\right| dx$ diverges, and thus $\\int_1^{\\infty} \\frac{\\cos x}{x} dx$ is not absolutely convergent.\nConditional convergence:\n$$\\int_1^b \\frac{\\cos x}{x} dx = \\frac{\\sin x}{x}\\Big|_1^b + \\int_1^b \\frac{\\sin x}{x^2} dx \\quad \\text{[Integration by Parts]}$$\nWhen $b \\to \\infty$, $$\\lim_{b\\to\\infty} \\frac{\\sin x}{x}\\Big|_1^b = \\frac{\\sin 1}{1} \\quad \\text{converges}.$$\nFurther, $$\\left|\\frac{\\sin x}{x^2}\\right| \\leq \\left|\\frac{1}{x^2}\\right| = \\frac{1}{x^2} \\Rightarrow \\int_1^{\\infty} \\left|\\frac{\\sin x}{x^2}\\right| dx \\leq \\int_1^{\\infty} \\frac{1}{x^2} dx.$$\nSo, $\\int_1^{\\infty} \\frac{\\sin x}{x^2} dx$ absolutely converges by comparison.\nThen, $\\int_1^b \\frac{\\cos x}{x} dx$ is conditional convergence.\n"},{"id":57,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1-Boundary-Value-Problems/","title":"9.1 è¾¹å€¼é—®é¢˜çš„è¿‘ä¼¼","section":"ç¬¬ä¹ç« ","content":" 1.1 Set-up: String with fixed endpoints # æˆ‘ä»¬å¯ä»¥å†™ $$ \\begin{align} -\\frac{d^2u}{dx^2} \u0026amp;= f(x), \\quad x \\in (0,1) \\ u(0) \u0026amp;= \\alpha, \\quad \\frac{du}{dx}(0) = \\beta \\end{align} $$\nè®°$w = \\frac{du}{dx}$ï¼Œé‚£ä¹ˆ $\\frac{dw}{dx} = f(x)$ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çŸ¥é“\n$$\\frac{d}{dt} = \\begin{bmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} w \\ u \\end{bmatrix}$$\nå®šä¹‰ # è¾¹å€¼é—®é¢˜ï¼ˆboundary-value problemï¼‰çš„å®šä¹‰ä¸º\n[!definition|*] $$\\begin{align} -\\mu\\frac{d^2u}{dx^2} \u0026amp;= f(x), \\quad x\\in (0,1), \\quad \\mu \u0026gt; 0 \\ u(0) \u0026amp;= \\alpha, \\quad u(1) = \\beta \\end{align} $$\n1.2 æ³Šæ¾æ–¹ç¨‹ï¼ˆPoisson Equationï¼‰ # æ­¤ç±»æ–¹ç¨‹çš„äºŒç»´å½¢æ€ä¸ºï¼š\n$$\\begin{align} -\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) \u0026amp;= f(x,y) \\ u|_{\\text{boundary of }\\Omega} \u0026amp;= 0 \\end{align}$$\nç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­æ¥è¡¨ç¤ºï¼š\n$$\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = \\Delta u, \\quad \\text{where }\\Delta = \\nabla^2 u = \\sum_{i=1}^{n} \\frac{\\partial^2}{\\partial x_i^2}$$\næ‰€ä»¥ï¼Œå¹¿ä¹‰çš„æ³Šæ¾æ–¹ç¨‹å¯ä»¥å†™ä¸º\n[!definition] A general ($n$-dimentional) poisson equation is written as $$\\Delta u = f(\\mathbf{x})$$ where $\\mathbf{x}=(x_{1},x_{2},x_{3}\\dots x_{n})$.\n1.3 Back to the String Example: How can we get a BVP? # è€ƒè™‘ä»¥ä¸‹ç»å…¸åŠ›å­¦ä¸­å¾ˆå¸¸è§çš„å¼¦çš„èƒ½é‡æ³›å‡½ï¼ˆenergy functionalï¼‰ï¼š\n$$J(u) = \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx - \\int_0^1 f \\cdot u , dx \\quad \\text{(Energy of string)}$$\nBoundary condition: $u(0) = u(1) = 0$.\nä½œä¸ºè‡ªç„¶ç•Œçš„åŸºæœ¬è¶‹åŠ¿ä¸­ï¼Œæœ€å°ä½œç”¨é‡åŸç†è§£é‡Šäº†è‡ªç„¶ç³»ç»Ÿå€¾å‘äºé‡‡å–èƒ½é‡æ¶ˆè€—æœ€å°çš„è·¯å¾„æˆ–çŠ¶æ€ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè‡ªç„¶ä¼šæ²¿ç€$\\min J(u)$çš„è·¯å¾„å‘å±•ã€‚\næœ€ç®€å•çš„æ–¹æ³•æ˜¯ç›´æ¥ç”¨ æ¬§æ‹‰-æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹ï¼ˆEulerâ€“Lagrange equationï¼‰æå®šï¼Œä½†æ˜¯è¿™æ¯•ç«Ÿæ˜¯ä¸ªæ•°å­¦è¯¾ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç”¨æœ€æš´åŠ›çš„åŸå§‹æ–¹æ³•è§£å†³ï¼š\nå¤§è‡´æ€è·¯ä¸ºï¼š\næŠŠ $u$ åŠ å…¥æ‰°åŠ¨ï¼ˆperturbationï¼‰å˜æˆ $u+Îµv$ï¼š $$u_{\\epsilon}(x) = u(x) + \\epsilon v(x)$$\n$J(u+Îµv)$ è¿›è¡Œæ˜¾å¼å±•å¼€ï¼š $$J(u + \\epsilon v) = J(u) + \\epsilon \\underbrace{\\delta J(u; v)}{\\text{ä¸€é˜¶å˜åˆ†}} + \\frac{1}{2} \\epsilon^2 \\underbrace{\\delta^2 J(u; v)}{\\text{äºŒé˜¶å˜åˆ†}} + \\cdots$$\nåœ¨å˜åˆ†æ³•æˆ–åŠ›å­¦çš„è¯­è¨€é‡Œï¼šé€šå¸¸æ˜¯æŒ‡åœ¨èƒ½é‡æˆ–ä½œç”¨é‡ï¼ˆactionï¼‰ç­‰æ³›å‡½æ„ä¹‰ä¸‹çš„é©»ç‚¹ï¼ˆstationary pointï¼‰ï¼šä¹Ÿå°±æ˜¯å¯¹ä»»æ„â€œå°æ‰°åŠ¨â€ $Îµv$ï¼Œè¯¥å‡½æ•° $u$ éƒ½ä½¿å¾—æ³›å‡½çš„ä¸€é˜¶å˜åŒ–é‡ä¸º 0ã€‚\næ±‚å¯¼ï¼Œæ¥æ‰¾$J$çš„æœ€å°å€¼ $$\\lim_{\\varepsilon \\to 0} \\frac{J(u + \\varepsilon v) - J(u)}{\\varepsilon} = 0, \\quad \\varepsilon \\in \\mathbb{R}$$\næ˜¾ç„¶æ˜“è§ï¼š\n$$ \\begin{align} \u0026amp;\\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx} + \\varepsilon\\frac{dv}{dx}\\right)^2 dx - \\int_0^1 f(u+\\varepsilon v) , dx - \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx - \\int_0^1 f \\cdot u , dx \\ \u0026amp;= \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx + \\frac{1}{2} \\cdot 2\\varepsilon \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx + \\frac{1}{2}\\varepsilon^2 \\int_0^1 \\mu\\left(\\frac{dv}{dx}\\right)^2 dx \\ \u0026amp;\\quad - \\int_0^1 f \\cdot u , dx - \\varepsilon\\int_0^1 f \\cdot v , dx - \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx - \\int_0^1 f \\cdot u , dx \\ \u0026amp;= \\varepsilon \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx + \\frac{1}{2}\\varepsilon^2 \\int_0^1 \\mu\\left(\\frac{dv}{dx}\\right)^2 dx - \\varepsilon\\int_0^1 f \\cdot v , dx\\end{align} $$\nè¿™å®é™…ä¸Šæ­£æ˜¯æ¬§æ‹‰â€“æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹æœ€æ—©çš„â€œåŸå§‹å˜åˆ†æ³•â€æ¨å¯¼ï¼Œä¹Ÿæ­£æ˜¯ E-L æ–¹ç¨‹çš„æ¥é¾™å»è„‰ã€‚åªä¸è¿‡ E-L æ–¹ç¨‹æŠŠè¿™ä¸ªè¿‡ç¨‹â€œå…¬å¼åŒ–â€äº†ï¼Œè®©æˆ‘ä»¬ä¸å¿…æ¯æ¬¡éƒ½å±•å¼€ä¸€å¤§å †é¡¹ã€å†åˆ†éƒ¨ç§¯åˆ†å»å‡‘å‡ºé‚£ä¸ªé€šç”¨å½¢å¼ã€‚\nç„¶å\n$$\\frac{J(u+\\varepsilon v) - J(u)}{\\varepsilon} = \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx + \\frac{1}{2} \\varepsilon \\int_0^1 \\mu\\left(\\frac{dv}{dx}\\right)^2 dx - \\int_0^1 f \\cdot v , dx$$\næé™ä¸ºï¼š\n$$\\lim_{\\varepsilon \\to 0} \\frac{J(u+\\varepsilon v) - J(u)}{\\varepsilon} = \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx - \\int_0^1 f \\cdot v , dx = 0$$\nä¸€ä¸ªâ€œå¹³è¡¡è§£â€ï¼ˆequilibrium solutionï¼‰ã€‚\nå˜åˆ†å½¢å¼æˆ–å¼±å½¢å¼ï¼ˆVariational/Weakï¼‰: # ç”±æ­¤æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªæ³›å‡½çš„â€œå˜åˆ†æ¡ä»¶â€ï¼š\n[!claim|*] $$\\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx - \\int_0^1 f \\cdot v , dx = 0$$\nåˆ†éƒ¨ç§¯åˆ†:\n$$ \\begin{align} \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx \u0026amp;= \\mu\\left[\\frac{du}{dx}v\\right]_0^1 - \\mu\\int_0^1 \\frac{d^2u}{dx^2} v , dx \\ \u0026amp;= -\\mu\\int_0^1 \\frac{d^2u}{dx^2} v , dx \\end{align} $$\nSince $v(0) = v(1) = 0$\n$$\\frac{d}{dx}\\left(\\frac{du}{dx}\\right) = \\frac{d^2u}{dx^2}$$\n$$\\int \\frac{dv}{dx} , dx = v$$\nè¾¹ç•Œé¡¹å› ä¸ºBCè€Œæ¶ˆå¤±ï¼Œæ‰€ä»¥å¼±å½¢å¼$\\rightarrow$å¼ºå½¢å¼:\n$$-\\mu\\int_0^1 \\frac{d^2u}{dx^2} v , dx - \\int_0^1 f \\cdot v , dx = 0$$\n$$-\\int_0^1 \\left(\\mu\\frac{d^2u}{dx^2} + f\\right) \\cdot v , dx = 0$$\nWe want it to be true $\\forall v$. So, it must be: $$\\mu\\frac{d^2u}{dx^2} + f = 0$$\næˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå¸¸è§çš„é™„å¸¦è¾¹ç•Œæ¡ä»¶çš„å¼ºå½¢å¼å¸¸å¾®åˆ†æ–¹ç¨‹(ODE)ã€‚\n[!claim|*] We obtain a Boundary Value Problem (BVP): $$ \\begin{align} \\mu u\u0026rsquo;\u0026rsquo;(x) +f\u0026amp;= 0 \\ u(0) \u0026amp;= u(1) = 0 \\end{align} $$\nAssumption: $u$ is at least $C^2$.\n1.4 ä¸¤ç§è¡¨è¿°è¾¹å€¼é—®é¢˜ï¼ˆBVPï¼‰çš„æ–¹å¼ # å¯»æ‰¾å‡½æ•° $u$ï¼Œä½¿å¾—å¯¹æ‰€æœ‰æ»¡è¶³ $v(0) = v(1) = 0$ çš„ $v$ï¼Œå‡æœ‰ $$ \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx ;=; \\int_0^1 f \\cdot v , dx $$\n$\\Rightarrow$ $u$ åªéœ€ä¿è¯â€œä¸€é˜¶å¯å¾®â€ $\\Rightarrow$ é€šå¸¸é‡‡ç”¨ æœ‰é™å…ƒæ³• (Finite Element) å¯»æ‰¾å‡½æ•° $u$ï¼Œä½¿å¾— $$ \\begin{aligned} -\\mu \\frac{\\partial^2 u}{\\partial x^2} \u0026amp;= f, \\quad x \\in (0,1),\\ u(0) \u0026amp;= 0,\\quad u(1) = 0 \\end{aligned} $$\n$\\Rightarrow$ $u$ éœ€è¦è‡³å°‘â€œäºŒé˜¶å¯å¾®â€ $\\Rightarrow$ é€šå¸¸é‡‡ç”¨ æœ‰é™å·®åˆ†æ³• (Finite Difference) "},{"id":58,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.2-Finite-Difference/","title":"9.2 æœ‰é™å·®åˆ†æ³•","section":"ç¬¬ä¹ç« ","content":" 2.1 è¾¹å€¼é—®é¢˜ï¼ˆBVPï¼‰çš„æœ‰é™å·®åˆ†: # [!claim|*] Consider the Boundary-Value Problem: $$ \\begin{align} -\\mu\\frac{d^2u}{dx^2} \u0026amp;= f, \\quad x \\in (0,1) \\ u(0) \u0026amp;= u(1) = 0 \\end{align} $$ with discrete points: $$0 = x_0 \u0026lt; x_1 \u0026lt; \\cdots \u0026lt; x_{N} = 1\\quad\\Longrightarrow \\quad-\\mu\\frac{d^2u}{dx^2}(x_i) = f(x_i)$$\n2.2 æ¨å¯¼äºŒé˜¶ä¸­å¿ƒå·®åˆ†è¿‘ä¼¼æ³• # 2.2.1 Poisson å¾®åˆ†æ–¹ç¨‹ # å¯¹ä¸ä»»æ„ä¸€ä¸ªç¦»æ•£çš„ç‚¹$x_{i}$ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨ç½‘æ ¼ç‚¹ $x_{i+1} = x_i + \\Delta x$ å’Œ $x_{i-1} = x_i - \\Delta x$ å¤„å¯¹å‡½æ•° $u(x)$ è¿›è¡Œæ³°å‹’çº§æ•°å±•å¼€ï¼š\n$$ \\begin{align} u(x_{i+1}) \u0026amp;= u(x_i) + \\frac{du}{dx}(x_i)\\Delta x + \\frac{1}{2}\\frac{d^2u}{dx^2}(x_i)\\Delta x^2 + \\cdots \\ u(x_{i-1}) \u0026amp;= u(x_i) - \\frac{du}{dx}(x_i)\\Delta x + \\frac{1}{2}\\frac{d^2u}{dx^2}(x_i)\\Delta x^2 + \\cdots \\end{align} $$\nå½“æˆ‘ä»¬å°†è¿™ä¸¤ä¸ªæ–¹ç¨‹ç›¸åŠ æ—¶ï¼Œç”±äºå¥‡æ•°é˜¶å¯¼æ•°é¡¹çš„ç¬¦å·ç›¸åï¼Œå®ƒä»¬ä¼šç›¸äº’æŠµæ¶ˆï¼š\n$$ \\begin{align} u(x_{i+1}) + u(x_{i-1}) \u0026amp;= 2u(x_i) + 2\\left(\\frac{1}{2}\\frac{d^2u}{dx^2}(x_i)\\Delta x^2\\right) + 2\\left(\\frac{1}{24}\\frac{d^4u}{dx^4}(x_i)\\Delta x^4\\right) + \\mathcal{O}(\\Delta x^6) \\ \\ \u0026amp;= 2u(x_i) + \\frac{d^2u}{dx^2}(x_i)\\Delta x^2 + \\frac{1}{12}\\frac{d^4u}{dx^4}(x_i)\\Delta x^4 + \\mathcal{O}(\\Delta x^6) \\end{align} $$\næ¥ç€, æˆ‘ä»¬é‡æ–°æ•´ç†æ–¹ç¨‹ä»¥åˆ†ç¦»å‡ºäºŒé˜¶å¯¼æ•°é¡¹ï¼ˆèˆå»é«˜é˜¶é¡¹ï¼‰\n$$\\begin{align} \\frac{d^2u}{dx^2}(x_i)\\Delta x^2 \u0026amp;= u(x_{i+1}) + u(x_{i-1}) - 2u(x_i) - \\frac{1}{12}\\frac{d^4u}{dx^4}(x_i)\\Delta x^4 + \\mathcal{O}(||\\Delta x||^6)\\ \\frac{d^2u}{dx^2}(x_i) \u0026amp;= \\frac{u(x_{i+1}) + u(x_{i-1}) - 2u(x_i)}{\\Delta x^2} - \\frac{1}{12}\\frac{d^4u}{dx^4}(x_i)\\Delta x^2 + \\mathcal{O}(||\\Delta x||^2) \\ \u0026amp;\\approx \\boxed{ \\frac{u(x_{i+1}) + u(x_{i-1}) - 2u(x_i)}{\\Delta x^2}} \\end{align}$$\nè®° $u_i = u(x_i)$ ä¸ $f_i = f(x_i)$. å› æ­¤æˆ‘ä»¬å¾—åˆ°å¾®åˆ†æ–¹ç¨‹\n[!claim|*] $$-\\mu\\frac{d^2u}{dx^2}(x_i) = -\\mu\\frac{u_{i+1} + u_{i-1} - 2u_i}{\\Delta x^2} = f_i$$\næˆªæ–­è¯¯å·®ï¼ˆTruncation Errorï¼‰ä¸º $\\mathcal{O}(\\Delta x^2)$ è¿™è¯å®äº†è¯¥è¿‘ä¼¼æ˜¯äºŒé˜¶ç²¾åº¦ ï¼ˆsecond-order accuracyï¼‰ 2.2.2 æ„å»ºçº¿æ€§ç³»ç»Ÿ # ç”¨è¿™ç§ç¦»æ•£åŒ–æ–¹æ³•æ¨å¯¼å‡ºä¸€ä¸ªçº¿æ€§æ–¹ç¨‹ç»„ï¼ˆlinear systemï¼‰:\n$$Au = f$$\nwhere $A$ is given by:\n$$A = \\frac{\\mu}{\\Delta x^2} \\begin{bmatrix} 2 \u0026amp; -1 \u0026amp; 0 \u0026amp; \u0026amp; \\ -1 \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \u0026amp; \\ 0 \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; -1 \\ \u0026amp; \u0026amp; \u0026amp; -1 \u0026amp; 2 \\end{bmatrix} $$\n2.3 What is the Accuracy of FD? # çŸ©é˜µAçš„å…³é”®æ€§è´¨ (Key Properties of Matrix A) # ä»å·®åˆ†ç¦»æ•£åŒ–å¾—åˆ°çš„matrix $A$æœ‰å‡ ä¸ªé‡è¦æ€§è´¨ï¼š\næ­£å®šæ€§ (Positive Definiteness)ï¼š$x^TAx \u0026gt; 0 \\quad \\forall x \\neq 0$ $\\Longrightarrow$ solvableã€‚ å¯¹ç§°æ€§ (Symmetry)ï¼šSymmetry $\\Longrightarrow \\forall ,\\lambda \\in \\mathbb{R}$ ç‰¹å¾å€¼æ€§è´¨ (Eigenvalue Properties)ï¼šNon-singular æ¡ä»¶æ•°å…³ç³» (Condition Number Relation)ï¼šAçš„æœ€å°ç‰¹å¾å€¼ä¸æœ€å¤§ç‰¹å¾å€¼ä¹‹æ¯”ä¸Î”xæˆæ­£æ¯”ï¼Œå³$$\\frac{\\lambda_{min}}{\\lambda_{max}} \\propto \\Delta x$$ è¯¯å·® # å½“æˆ‘ä»¬è§£ç¦»æ•£ç³»ç»Ÿï¼ˆ$Au = f$ï¼‰æ—¶ï¼Œç²¾ç¡®è§£$u_{ex}$ä¸è¿‘ä¼¼è§£$u$ä¹‹é—´å­˜åœ¨è¯¯å·®ï¼Œæ‰€ä»¥$Au_{ex} \\neq f$ã€‚\nç²¾ç¡®å…³ç³»å®é™…ä¸Šä¸ºï¼š\n[!claim|*] $$Au_{ex} = f + T$$ where $T_i = C(x_i)\\Delta x^2$ is truncation error\nå…¶ä¸­$C(x_i)$ä¸å››é˜¶å¯¼æ•°ç›¸å…³ï¼š$$C(x_i) = C\\frac{d^4u}{dx^4}(x_i)$$\nè¯¯å·®æ–¹ç¨‹ (Error Equation) # è‹¥å®šä¹‰è¯¯å·®$e = u_{ex} - u$ï¼Œåˆ™ $$Ae = T$$\n$$\\Longrightarrow e = A^{-1}T$$\nå› æ­¤\n$$||e|| = ||A^{-1}T|| \\leq ||A^{-1}|| \\cdot ||T||$$\næ”¶æ•›æ€§è¯æ˜ (Convergence Proof) # ä¸ºäº†è¯æ˜æ–¹æ³•æ”¶æ•›ï¼Œéœ€è¦æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š1. ç¨³å®šæ€§ï¼š$A^{-1}$æœ‰ç•Œ (Boundedness of $A^{-1}$) 2. ä¸€è‡´æ€§ï¼šæˆªæ–­è¯¯å·®è¶‹é›¶ (Truncation Error Tends to Zero)\n[!lemma|*] Conditions to show convergence: $$||A^{-1}|| \u0026lt; \\infty \\quad \\text{and} \\quad ||T|| \\to 0 \\quad \\text{as} \\quad \\Delta x \\to 0$$\nå…³äº $|A^{-1}|$ çš„æœ‰ç•Œæ€§ # çŸ©é˜µçš„æ¡ä»¶æ•°å®šä¹‰ä¸ºï¼š $$\\kappa(A) = |A| \\cdot |A^{-1}|=\\frac{\\lambda_{max}}{\\lambda_{min}}$$\n$\\frac{\\lambda_{min}}{\\lambda_{max}} \\propto \\Delta x \\longrightarrow\\kappa(A) \\propto \\frac{1}{\\Delta x}$ hence, $||A^{-1}||$ is bounded, regardless of $\\Delta x$. å…³äº$T$çš„ä¸€è‡´æ€§ # å› ä¸ºæˆªæ–­è¯¯å·®èŒƒæ•°$|T| \\sim \\Delta x^2$ï¼Œæ‰€ä»¥\n$$|T| \\to 0 ,,\\text{as},, \\Delta x \\to 0$$\nè¿™ä¹Ÿæ„å‘³ç€ï¼š\n$$|e| \\to 0 ,,\\text{as},, \\Delta x \\to 0$$\nå› æ­¤ï¼Œè¯¥æœ‰é™å·®åˆ†æ–¹æ³•æ˜¯æ”¶æ•›çš„ (convergent) è™½ç„¶$\\kappa$éš $\\Delta x$ å˜åŒ–ï¼Œä½† $|A^{-1}|$ çš„å¢é•¿è¢« $|T|$ çš„æ›´å¿«å‡å°æ‰€æŠµæ¶ˆã€‚\nå®é™…æ„ä¹‰ï¼Ÿ # è¯¥æœ‰é™å·®åˆ†æ³•éšç€ç½‘æ ¼é—´è· (grid spacing) å‡å°è€Œæ”¶æ•›åˆ°ç²¾ç¡®è§£ æ”¶æ•›é€Ÿç‡ (convergence rate) æ˜¯$O(\\Delta x^2)$ï¼Œå³äºŒé˜¶ç²¾åº¦ (second-order accuracy) è¯¯å·®ä¸»è¦å—æ§äºå››é˜¶å¯¼æ•°çš„å¤§å°å’Œç½‘æ ¼é—´è·çš„å¹³æ–¹ è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆåœ¨å®é™…è®¡ç®—ä¸­ï¼Œå½“æˆ‘ä»¬å°†ç½‘æ ¼é—´è·å‡åŠæ—¶ï¼Œè¯¯å·®å¤§çº¦ä¼šå‡å°åˆ°åŸæ¥çš„å››åˆ†ä¹‹ä¸€ è¿™ç§æ•°å­¦è¯æ˜ä¸ºæˆ‘ä»¬ä½¿ç”¨æœ‰é™å·®åˆ†æ³•æ±‚è§£å¾®åˆ†æ–¹ç¨‹æä¾›äº†ç†è®ºä¿éšœï¼Œç¡®ä¿äº†åœ¨è¶³å¤Ÿç»†çš„ç½‘æ ¼ä¸‹ï¼Œæ•°å€¼è§£ (numerical solution) ä¼šä»¥å¯é¢„æµ‹çš„é€Ÿç‡æ¥è¿‘çœŸå®è§£ (exact solution)ã€‚\n"},{"id":59,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.3-Advection-Diffusion-Equation/","title":"9.3 å¯¹æµ-æ‰©æ•£æ–¹ç¨‹","section":"ç¬¬ä¹ç« ","content":" 3.1 å¯¹æµ-æ‰©æ•£æ–¹ç¨‹ # å¯¹æµ-æ‰©æ•£æ–¹ç¨‹æ˜¯ä¸€ç§æè¿°ç‰©è´¨æˆ–çƒ­é‡åœ¨æµä½“ä¸­åŒæ—¶å—åˆ°å¯¹æµï¼ˆä¹Ÿç§°ä¸ºå¹³æµï¼‰å’Œæ‰©æ•£ä½œç”¨å½±å“çš„åå¾®åˆ†æ–¹ç¨‹ã€‚è¿™ä¸ªæ–¹ç¨‹åœ¨æµä½“åŠ›å­¦ã€ä¼ çƒ­å­¦å’Œç‰©è´¨ä¼ è¾“ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚\n$$ \\begin{align} -\\mu\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} \u0026amp;= f \\ \\text{diffusion} \\quad \u0026amp; \\text{advection} \\end{align} $$\n$$u(0) = u_L, \\quad u(1) = u_R$$\n3.1.1 ç¦»æ•£åŒ–è¿‡ç¨‹ï¼š # ä¸€é˜¶å¯¼æ•°ï¼šå¯¹æµé¡¹ ï¼ˆAdvectionï¼‰ # æ³°å‹’å±•å¼€ï¼ˆå‘å‰ï¼‰ # $$ \\begin{align}\nu(x_{j+1}) \u0026amp; = u(x_j) + \\frac{\\partial u}{\\partial x}(x_j)\\Delta x + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x^2 \\ \u0026amp;\\quad\\quad\\quad\\quad-\\frac{1}{6}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^3 + \\frac{1}{12}\\frac{\\partial^4 u}{\\partial x^4}(x_j)\\Delta x^4 + \\mathcal{O}(||x||^5)\\ \\frac{\\partial u}{\\partial x}(x_j)\\Delta x \u0026amp; = u(x_{j+1}) - u(x_j) + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x^2\\ \\frac{\\partial u}{\\partial x}(x_j) \u0026amp; = \\frac{u_{j+1} - u_j}{\\Delta x} + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x \\end{align}$$\næ³°å‹’å±•å¼€ï¼ˆå‘åï¼‰ # $$u(x_{j-1}) = u(x_j) - \\frac{\\partial u}{\\partial x}(x_j)\\Delta x + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x^2 - \\frac{1}{6}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^3 + \\frac{1}{12}\\frac{\\partial^4 u}{\\partial x^4}(x_j)\\Delta x^4 + \\mathcal{O}(||x||^5)$$\næˆ‘ä»¬å–(forward) - (backward)ä¸¤è€…å·®å€¼\n$$u(x_{j+1}) - u(x_{j-1}) = 2\\frac{\\partial u}{\\partial x}(x_j)\\Delta x + \\frac{1}{3}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^3 + \\mathcal{O}(||\\Delta x||^5)$$\næ¥ç€\n$$\\frac{\\partial u}{\\partial x}(x_j) = \\frac{u(x_{j+1}) - u(x_{j-1})}{2\\Delta x} - \\frac{1}{6}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^2 + \\mathcal{O}\\left(\\frac{||\\Delta x||^4}{2}\\right)$$\näºŒé˜¶å¯¼æ•°ï¼šæ‰©æ•£é¡¹ï¼ˆDiffusionï¼‰ # è¿™éƒ½è¿˜ä¸ä¼šå—ï¼Ÿé€€ç¾¤å§ã€‚\nå¯¹æµ-æ‰©æ•£ç¦»æ•£æ–¹ç¨‹ # [!claim|*] Final numerical solution: $$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = f_j$$ with second-order accuracy: $$\\sim \\mathcal{O}(\\Delta x^2)$$\n3.2 ä¾‹å­ï¼šç²¾ç¡®è§£ä¸æ•°å€¼è§£åœ¨å¯¹æµä¸»å¯¼é—®é¢˜ä¸­çš„å·®å¼‚ # æˆ‘ä»¬ç°åœ¨è€ƒè™‘ä¸€ä¸ªç‰¹æ®Šæƒ…å†µçš„å¯¹æµ-æ‰©æ•£æ–¹ç¨‹ï¼Œå…¶ä¸­æºé¡¹ï¼ˆSource Termï¼‰ä¸ºé›¶ï¼Œå…·æœ‰ä»¥ä¸‹è¾¹ç•Œæ¡ä»¶ï¼š\n$$ \\begin{align} -\\mu\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} \u0026amp;= 0 \\ u(0) \u0026amp;= 0 \\ u(a) \u0026amp;= 1 \\end{align} $$\n3.2.1 ç²¾ç¡®è§£ # è¿™ä¸ªæ–¹ç¨‹çš„ç²¾ç¡®è§£æ˜¯ï¼š\n$$u_{ex} = \\frac{e^{\\frac{\\beta}{\\mu}x} - 1}{e^{\\frac{\\beta}{\\mu}a} - 1}$$\nè¿™ä¸ªè§£çš„ç‰¹ç‚¹æ˜¯ï¼šå½“æ¯”å€¼ $\\frac{|\\beta|}{\\mu} \\gg 1$ æ—¶ï¼ˆå³advectionè¿œå¤§äºdiffusionï¼‰ï¼Œè§£åœ¨è¾¹ç•Œ $x=a$ é™„è¿‘ä¼šå½¢æˆä¸€ä¸ªé™¡å³­çš„è¾¹ç•Œå±‚ã€‚è¿™è¢«ç§°ä¸º**\u0026ldquo;å¯¹æµä¸»å¯¼é—®é¢˜\u0026rdquo;ï¼ˆadvection-dominated problemï¼‰**ã€‚\nåœ¨å¯¹æµä¸»å¯¼çš„æƒ…å†µä¸‹ï¼Œè§£åœ¨å¤§éƒ¨åˆ†åŒºåŸŸæ¥è¿‘äº0ï¼Œåªåœ¨æ¥è¿‘ $x=a$ çš„å°åŒºåŸŸå†…å¿«é€Ÿä¸Šå‡åˆ°1ã€‚è¿™å¯¹æ•°å€¼æ–¹æ³•çš„è§£æ³•æ˜¯å¾ˆå¤§çš„éº»çƒ¦ã€‚\n3.2.2 æ•°å€¼è§£ # å½“æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„ä¸­å¿ƒå·®åˆ†ï¼ˆcentral differenceï¼‰æ–¹æ³•ç¦»æ•£åŒ–è¿™ä¸ªæ–¹ç¨‹æ—¶ï¼š\n$$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = 0$$\né‡æ–°æ•´ç†è¿™ä¸ªæ–¹ç¨‹ï¼š\n$$\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)u_{i+1} + \\left(\\frac{2\\mu}{\\Delta x^2}\\right)u_i - \\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)u_{i-1} = 0$$\næ•°å€¼å®éªŒè¡¨æ˜ï¼Œå½“ $|\\beta|$ è¾ƒå¤§æ—¶ï¼Œæ•°å€¼è§£ä¼šå‡ºç°ä¸ä¸€è‡´ï¼Œéç‰©ç†çš„æŒ¯è¡ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ\næ•°å­¦è§£é‡Š # ä¸ºäº†ç†è§£è¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å·®åˆ†æ–¹ç¨‹è¿›è¡Œæ·±å…¥åˆ†æã€‚æˆ‘ä»¬å‡è®¾å·®åˆ†æ–¹ç¨‹çš„è§£å…·æœ‰å½¢å¼ $u_j = C\\rho^j$ï¼Œå…¶ä¸­ $C$ æ˜¯å¸¸æ•°ï¼Œ$\\rho$ æ˜¯å¾…å®šå‚æ•°ã€‚å°†è¿™ä¸ªå‡è®¾ä»£å…¥åˆ°å·®åˆ†æ–¹ç¨‹ä¸­ï¼š\n$$\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)C\\rho^{j+1} + \\left(\\frac{2\\mu}{\\Delta x^2}\\right)C\\rho^j - \\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)C\\rho^{j-1} = 0$$\næ¶ˆå» $C$ å¹¶æ•´ç†ï¼š\n$$\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)\\rho^2 + \\left(\\frac{2\\mu}{\\Delta x^2}\\right)\\rho - \\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right) = 0$$\nè¿™æ˜¯å…³äº $\\rho$ çš„äºŒæ¬¡æ–¹ç¨‹ï¼Œå®ƒæœ‰ä¸¤ä¸ªè§£ $\\rho_1$ å’Œ $\\rho_2$ã€‚å·®åˆ†æ–¹ç¨‹çš„ä¸€èˆ¬è§£æ˜¯è¿™ä¸¤ä¸ªç‰¹è§£çš„çº¿æ€§ç»„åˆï¼š\n$$u_j = C_1\\rho_1^j + C_2\\rho_2^j$$\nå…¶ä¸­ $C_1$ å’Œ $C_2$ æ˜¯ç”±è¾¹ç•Œæ¡ä»¶ç¡®å®šçš„å¸¸æ•°ã€‚\næŒ¯è¡è§£çš„æ¡ä»¶ # æ ¹æ®äºŒæ¬¡æ–¹ç¨‹çš„æ€§è´¨ï¼Œä¸¤ä¸ªæ ¹çš„ä¹˜ç§¯ç­‰äºå¸¸æ•°é¡¹ä¸äºŒæ¬¡é¡¹ç³»æ•°çš„æ¯”å€¼ï¼š\n$$\\rho_1\\rho_2 = \\frac{-\\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)}{\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)} = \\frac{1 + \\frac{\\beta\\Delta x}{2\\mu}}{1 - \\frac{\\beta\\Delta x}{2\\mu}}$$\nè¿™é‡Œå¼•å…¥äº†ä¸€ä¸ªé‡è¦çš„æ— é‡çº²å‚æ•°ï¼Œç§°ä¸ºç½‘æ ¼ä½©å…‹è±æ•°ï¼ˆGrid PÃ©clet numberï¼‰ï¼š\n$$\\text{Pe} = \\frac{|\\beta|\\Delta x}{2\\mu}$$\nä½©å…‹è±æ•°è¡¨ç¤ºå¯¹æµä¼ è¾“ä¸æ‰©æ•£ä¼ è¾“çš„ç›¸å¯¹å¼ºåº¦ã€‚\nå½“ $\\text{Pe} \u0026gt; 1$ï¼ˆå³ $\\frac{|\\beta|\\Delta x}{2\\mu} \u0026gt; 1$ï¼‰æ—¶ï¼Œæˆ‘ä»¬æœ‰ $\\rho_1\\rho_2 \u0026lt; 0$ï¼Œè¿™æ„å‘³ç€ä¸¤ä¸ªæ ¹ä¸€æ­£ä¸€è´Ÿã€‚å½“ä¸€ä¸ªè§£åŒ…å«è´Ÿçš„å¹‚æ—¶ï¼Œä¼šå¯¼è‡´è§£åœ¨ç©ºé—´ä¸Šå‘ˆç°æŒ¯è¡ç‰¹æ€§ï¼Œè¿™ä¸ç‰©ç†ç›´è§‰ç›¸è¿èƒŒï¼Œå› ä¸ºæ‰©æ•£è¿‡ç¨‹åº”è¯¥æ˜¯å¹³æ»‘çš„ã€‚\nç‰©ç†è§£é‡Šä¸æ”¹è¿›æ–¹æ³• # ä¸ºä»€ä¹ˆä¼šå‡ºç°æŒ¯è¡ï¼Ÿ # ç‰©ç†è§’åº¦çœ‹ï¼šä¿¡æ¯ä¸»è¦æ²¿ç€æµåŠ¨æ–¹å‘ä¼ æ’­ã€‚Central Difference æ–¹æ³•å¯¹ä¸Šæ¸¸å’Œä¸‹æ¸¸çš„ä¿¡æ¯ç»™äºˆç›¸åŒæƒé‡ï¼Œæ‰€ä»¥å¯¹æµä¸»å¯¼çš„æƒ…å†µä¸‹ä¸åˆé€‚ï¼Œé™¤éæç»†çš„ç½‘æ ¼æ‰èƒ½å‡†ç¡®è§£æã€‚å³ä½¿æ•°å€¼æ–¹æ³•åœ¨æ•°å­¦ä¸Šå…·æœ‰äºŒé˜¶ç²¾åº¦ï¼Œå…¶å‡†ç¡®åº¦ä¾æ—§æ˜¯è¦å–å†³äºç‰¹å®šçš„ç‰©ç†é—®é¢˜ä¸­ã€‚ç†è§£æ•°å€¼æ–¹æ³•çš„ç¨³å®šæ€§æ¡ä»¶æ‰å¯ä»¥é€‰æ‹©åˆé€‚çš„æ±‚è§£ç­–ç•¥ã€‚\nè§£å†³æ–¹æ¡ˆï¼Ÿ # ç½‘æ ¼ç»†åŒ–ï¼šæœ€ç›´æ¥çš„æ–¹æ³•æ˜¯å‡å° $\\Delta x$ï¼Œä½¿ $\\text{Pe} \u0026lt; 1$ã€‚ä½†è¿™ä¼šå¤§å¤§å¢åŠ è®¡ç®—æˆæœ¬ã€‚\nè¿é£æ–¹æ³•ï¼šï¼ˆè¯¦ç»†è§ä¸‹æ–‡ï¼‰ä½¿ç”¨åå‘ä¸Šæ¸¸çš„å·®åˆ†æ ¼å¼ï¼Œå¦‚å‰å‘æˆ–åå‘å·®åˆ†ï¼Œå–å†³äº $\\beta$ çš„ç¬¦å·ã€‚ä¾‹å¦‚ï¼Œå½“ $\\beta \u0026gt; 0$ æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ï¼š $$\\frac{\\partial u}{\\partial x}(x_j) \\approx \\frac{u_j - u_{j-1}}{\\Delta x}$$\näººå·¥æ‰©æ•£ï¼šå¢åŠ ä¸€ä¸ªæ•°å€¼æ‰©æ•£é¡¹ï¼Œä½¿æœ‰æ•ˆçš„ä½©å…‹è±æ•°å°äº1ã€‚\né«˜é˜¶æ ¼å¼ï¼šä½¿ç”¨æ›´é«˜é˜¶çš„å·®åˆ†æ ¼å¼ï¼Œå¦‚QUICKã€TVDæˆ–ENO/WENOæ–¹æ¡ˆï¼Œè¿™äº›æ–¹æ³•å¯ä»¥æ›´å¥½åœ°æ•æ‰å¼ºæ¢¯åº¦åŒºåŸŸã€‚\n3.3 å¦ä¸€ç§æ–¹æ³•ï¼šè¿é£æ³•ï¼ˆUpwind Methodï¼‰ # 3.3.1 ä¿¡æ¯æµåŠ¨åˆ†æ (Information Flow) # å°±åƒåˆšåˆšçš„å¯¹æµä¸»å¯¼é—®é¢˜ï¼Œç°å®ä¸­ç»å¸¸å­˜åœ¨æ˜ç¡®çš„ç‰©ç†ä¿¡æ¯æµåŠ¨æ–¹å‘ï¼Œä½¿å¾—è¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šæ˜¯éå¯¹ç§°çš„ã€‚\nå¯¹äºå¯¹æµé¡¹ï¼ˆadvectionï¼‰ï¼Œå½“æµåŠ¨æ–¹å‘å·²çŸ¥æ—¶\n[!assumption|*] $\\beta \u0026gt; 0$ï¼Œmeaning that the information flows from left to right.\næˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¼å­ï¼š\n[!claim|*] $$\\beta\\frac{\\partial u}{\\partial x} \\approx \\beta\\frac{u_i - u_{i-1}}{\\Delta x} \\quad (\\beta \u0026gt; 0) \\quad \\text{(upwind)}$$\nè¿™ç§å·®åˆ†æ ¼å¼è€ƒè™‘äº†ä¿¡æ¯æµåŠ¨çš„æ–¹å‘ï¼Œä½¿ç”¨\u0026quot;ä¸Šæ¸¸\u0026quot;çš„èŠ‚ç‚¹æ¥è®¡ç®—å¯¼æ•°ï¼Œè€Œä¸æ˜¯åƒä¸­å¿ƒå·®åˆ†é‚£æ ·å¹³ç­‰å¯¹å¾…ä¸Šä¸‹æ¸¸èŠ‚ç‚¹ï¼ˆæ³¨æ„è¿™ä¸ªåªæœ‰ä¸€é˜¶ç²¾åº¦ï¼‰ã€‚\n3.3.2 è¿é£æ³•çš„ç¨³å®šæ€§åˆ†æ (Stability Analysis) # æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥è¯æ˜è¿é£æ–¹æ³•æ˜¯ç¨³å®šçš„ã€‚å°†è¿é£å·®åˆ†é‡å†™ä¸ºï¼š\n$$\\beta\\frac{u_i - u_{i-1}}{\\Delta x} = \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\beta\\frac{u_{i+1}}{2\\Delta x} - \\beta\\frac{2u_i}{2\\Delta x} + \\beta\\frac{u_{i-1}}{2\\Delta x}$$\næ•´ç†ä¸€ä¸‹ä¸Šå¼ï¼š\n$$= \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\frac{\\beta\\Delta x}{2} \\cdot \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}$$\nè¿™ä¸ªè¡¨è¾¾å¼å¯ä»¥åˆ†è§£ä¸ºä¸¤é¡¹ï¼š\nä¸­å¿ƒå·®åˆ†é¡¹ (Central mean): $\\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x}$ äºŒé˜¶å¯¼æ•°è¿‘ä¼¼é¡¹ (Approximation of 2nd derivative): $-\\frac{\\beta\\Delta x}{2} \\cdot \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}$ è¿™è¯´æ˜è¿é£æ–¹æ³•ç­‰ä»·äºå¯¹åŸå§‹æ–¹ç¨‹è¿›è¡Œä¸­å¿ƒå·®åˆ†ï¼Œä½†å¢åŠ äº†ä¸€ä¸ªé¢å¤–çš„æ‰©æ•£é¡¹ï¼ˆäººå·¥æ‰©æ•£ï¼Œartificial diffusionï¼‰ã€‚\nç­‰ä»·è¡¨è¿°ï¼šæ‰°åŠ¨æ–¹ç¨‹ (Perturbed Equation) # å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†åŸå§‹é—®é¢˜çš„è¿é£æ–¹æ³•çœ‹ä½œæ˜¯ä¸‹é¢è¿™ä¸ªæ‰°åŠ¨æ–¹ç¨‹çš„ä¸­å¿ƒå·®åˆ†è§£æ³•ï¼š\n$$-\\left(\\mu + \\frac{|\\beta|\\Delta x}{2}\\right)\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} = 0$$\nå¯¹è¿™ä¸ªæ‰°åŠ¨æ–¹ç¨‹åº”ç”¨ä¸­å¿ƒå·®åˆ†è¿‘ä¼¼ï¼š\n$$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = 0$$\nè¿™æ­£æ˜¯åŸå§‹é—®é¢˜çš„è¿é£è§£æ³•ã€‚æ¢å¥è¯è¯´ï¼š\nå¯¹æ‰°åŠ¨é—®é¢˜ä½¿ç”¨ä¸­å¿ƒå·®åˆ† (Central for Perturbed) = å¯¹åŸå§‹é—®é¢˜ä½¿ç”¨è¿é£å·®åˆ† (Upwind for Original) [!claim|*] $$\\text{Central (Perturbed) }= \\text{Upwind (Original)}$$\nè¿™ä¸ªç­‰ä»·å…³ç³»æ­ç¤ºäº†è¿é£æ³•çš„æœ¬è´¨ï¼šå®ƒéšå«åœ°å‘åŸå§‹æ–¹ç¨‹ä¸­æ·»åŠ äº†ä¸€ä¸ªæ•°å€¼æ‰©æ•£é¡¹ã€‚è¿™ä¸ªé¢å¤–çš„æ‰©æ•£é¡¹æ˜¯è¿é£æ³•èƒ½å¤ŸæŠ‘åˆ¶æ•°å€¼æŒ¯è¡çš„å…³é”®åŸå› ã€‚\nä½©å…‹è±æ•°åˆ†æ (PÃ©clet Number Analysis) # å›é¡¾ä¸€ä¸‹ä½©å…‹è±æ•°çš„å®šä¹‰ï¼š$\\text{Pe} = \\frac{|\\beta|\\Delta x}{2\\mu}$ã€‚\nå¯¹äºæ‰°åŠ¨ç³»ç»Ÿï¼Œæ‰©æ•£ç³»æ•°å˜ä¸º $\\mu^* = \\mu + \\frac{|\\beta|\\Delta x}{2} = \\mu(1 + \\text{Pe})$ã€‚\næ‰°åŠ¨ç³»ç»Ÿçš„ä½©å…‹è±æ•°ä¸ºï¼š\n$$\\text{Pe}^* = \\frac{|\\beta|\\Delta x}{2\\mu^*} = \\frac{|\\beta|\\Delta x}{2\\mu(1+\\text{Pe})} = \\frac{\\text{Pe}}{1+\\text{Pe}} \u0026lt; 1 \\quad \\forall |\\beta| \\text{ å’Œ } \\Delta x$$\nè¿™è¡¨æ˜æ— è®º $|\\beta|$ å’Œ $\\Delta x$ çš„å€¼å¦‚ä½•ï¼Œæ‰°åŠ¨ç³»ç»Ÿçš„ä½©å…‹è±æ•°æ°¸è¿œå°äº1ï¼Œå› æ­¤è¿é£æ–¹æ³•å§‹ç»ˆæ˜¯ç¨³å®šçš„ã€‚\nä¸€è‡´æ€§åˆ†æ (Consistency Analysis) # å½“ $\\Delta x \\to 0$ æ—¶ï¼Œ$\\mu^* \\to \\mu$ï¼Œæ‰°åŠ¨æ–¹ç¨‹è¶‹è¿‘äºåŸå§‹æ–¹ç¨‹ï¼Œè¿™ä¿è¯äº†æ–¹æ³•çš„ä¸€è‡´æ€§ã€‚\nå¯¹äºæ‰°åŠ¨ç³»ç»Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨äº†äºŒé˜¶ç²¾åº¦çš„ä¸­å¿ƒå·®åˆ†æ–¹æ³•ï¼Œä½†å¯¹äºåŸå§‹é—®é¢˜ï¼Œç”±äºå¼•å…¥äº†äººå·¥æ‰©æ•£é¡¹ï¼Œå®ƒåªæ˜¯ä¸€é˜¶ç²¾åº¦çš„æ–¹æ³•ã€‚\nè¯¦ç»†è§£é‡Šä¸ç‰©ç†æ„ä¹‰ # è¿é£æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯è€ƒè™‘ç‰©ç†ä¿¡æ¯çš„ä¼ æ’­æ–¹å‘ã€‚åœ¨æµä½“æµåŠ¨ä¸­ï¼Œå½“æŸç‚¹çš„ç‰¹æ€§ï¼ˆå¦‚æ¸©åº¦ã€æµ“åº¦ï¼‰å—åˆ°ä¸Šæ¸¸ç‚¹çš„å½±å“æ›´å¤§æ—¶ï¼Œè¿é£æ–¹æ³•ä½¿ç”¨ä¸Šæ¸¸ç‚¹æ¥è®¡ç®—å¯¼æ•°ï¼Œä»è€Œæ›´å¥½åœ°åæ˜ ç‰©ç†ç°å®ã€‚\nä»æ•°å€¼åˆ†æçš„è§’åº¦çœ‹ï¼Œè¿é£æ–¹æ³•å¼•å…¥äº†\u0026quot;äººå·¥æ‰©æ•£\u0026quot;(artificial diffusion)ï¼Œå¢å¼ºäº†æ•°å€¼æ–¹æ³•çš„ç¨³å®šæ€§ã€‚è¿™ç§äººå·¥æ‰©æ•£æ°å¥½èƒ½å¤ŸæŠµæ¶ˆä¸­å¿ƒå·®åˆ†åœ¨é«˜ä½©å…‹è±æ•°æƒ…å†µä¸‹äº§ç”Ÿçš„æ•°å€¼æŒ¯è¡ã€‚\nç„¶è€Œï¼Œè¿™ç§ç¨³å®šæ€§æ˜¯ä»¥ç²¾åº¦ä¸ºä»£ä»·çš„â€”â€”è¿é£æ–¹æ³•çš„ç²¾åº¦é™ä½åˆ°ä¸€é˜¶ï¼ˆè¯¯å·®ä¸ $\\Delta x$ æˆæ­£æ¯”ï¼‰ï¼Œè€Œä¸­å¿ƒå·®åˆ†æ˜¯äºŒé˜¶ç²¾åº¦ï¼ˆè¯¯å·®ä¸ $\\Delta x^2$ æˆæ­£æ¯”ï¼‰ã€‚è¿™åœ¨æ•°å€¼æ–¹æ³•ä¸­æ˜¯ä¸€ä¸ªå¸¸è§çš„æƒè¡¡ï¼šæ›´é«˜çš„ç¨³å®šæ€§å¾€å¾€ä¼´éšç€æ›´ä½çš„ç²¾åº¦ã€‚\nåœ¨å¯¹æµä¸»å¯¼çš„é—®é¢˜ä¸­ï¼Œç¨³å®šæ€§é€šå¸¸æ¯”é«˜ç²¾åº¦æ›´é‡è¦ï¼Œå› ä¸ºä¸ç¨³å®šçš„è§£ä¼šäº§ç”Ÿä¸¥é‡çš„éç‰©ç†æŒ¯è¡ï¼Œä½¿ç»“æœå®Œå…¨æ— ç”¨ã€‚å› æ­¤ï¼Œå¯¹äºé«˜ä½©å…‹è±æ•°æµåŠ¨ï¼Œè¿é£æ–¹æ³•å°½ç®¡ç²¾åº¦è¾ƒä½ï¼Œä½†å¾€å¾€æ˜¯æ›´å®ç”¨çš„é€‰æ‹©ã€‚\næ›´é«˜é˜¶çš„æ–¹æ³•ï¼Œå¦‚TVD (Total Variation Diminishing)ã€ENO (Essentially Non-Oscillatory) å’Œ WENO (Weighted Essentially Non-Oscillatory) æ–¹æ¡ˆï¼Œè¯•å›¾åœ¨ä¿æŒç¨³å®šæ€§çš„åŒæ—¶æé«˜ç²¾åº¦ï¼Œä½†å®ƒä»¬çš„å®ç°æ›´ä¸ºå¤æ‚ã€‚\nOur previous computation relies on symmetry. However, there is a clear physical information flow. So, this problem is asymmetric in reality. We don\u0026rsquo;t want as fancy as $\\sim \\mathcal{O}(\\Delta x^2)$ solutions, but we can use a $\\sim \\mathcal{O}(\\Delta x)$ method.\n$$\\beta\\frac{\\partial u}{\\partial x} \\approx \\beta\\frac{u_i - u_{i-1}}{\\Delta x} \\quad (\\beta \u0026gt; 0) \\quad \\text{(upwind)}$$\nNow, let\u0026rsquo;s show (upwind) is stable.\n$$\\beta\\frac{u_i - u_{i-1}}{\\Delta x} = \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\beta\\frac{u_{i+1}}{2\\Delta x} - \\beta\\frac{2u_i}{2\\Delta x} + \\beta\\frac{u_{i-1}}{2\\Delta x}$$\n$$= \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\frac{\\beta\\Delta x}{2} \\cdot \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}$$\nCentral mean $\\quad$ Approx. of 2nd derivative\nSo, we can consider the equation: $$-\\left(\\mu + \\frac{|\\beta|\\Delta x}{2}\\right)\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} = 0$$\nApply a central approximation: $$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = 0$$\nUpwind solution of the original problem.\nRecall PÃ©clet: $Pe = \\frac{|\\beta|\\Delta x}{2\\mu}$. Then, $\\mu^* = \\mu(1 + Pe)$.\nPÃ©clet of this perturbed system: $$Pe^* = \\frac{|\\beta|\\Delta x}{2\\mu^*} = \\frac{|\\beta|\\Delta x}{2\\mu(1+Pe)} = \\frac{Pe}{1+Pe} \u0026lt; 1 \\quad \\forall |\\beta| \\text{ and } \\Delta x$$\nSo, this upwind method is always stable.\nConsistency: when $\\Delta x \\to 0$, $\\mu^* \\to \\mu$.\nFor the perturbed system, we have a 2nd order approach, but with the original problem, it is only a 1st order method.\n3.4 Design a Better Method # $$\\mu^{smart} = \\mu(1 + \\Phi(Pe))$$\ns.t.\n$\\Phi(Pe) \\to 0 \\quad \\text{as} \\quad \\Delta x \\to 0$ $Pe^{smart} = \\frac{|\\beta|\\Delta x}{2\\mu^{smart}} \u0026lt; 1$ Our upwind method takes $\\Phi(Pe) = Pe \\sim \\mathcal{O}(\\Delta x)$. But can we take some $\\Phi(Pe) \\sim \\mathcal{O}(\\Delta x^2)$?\n$\\Rightarrow$ Scharfetter-Gummel Method: $\\Phi(Pe) = Pe - 1 + \\frac{2Pe}{e^{2Pe} - 1}$\n$\\Phi(Pe) \\uparrow$\n$\\Phi(Pe) = Pe$\n$\\Phi(Pe) \\sim \\mathcal{O}(\\Delta x)$\nThe worst case order of Scharfetter-Gummel is $\\sim \\mathcal{O}(\\Delta x^2)$.\nScharfetter-Gummel is also a special $\\Phi(Pe)$ that produces exact solutions.\n"},{"id":60,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.4-2D-Problem/","title":"9.4 4.1 äºŒç»´ï¼ˆ2Dï¼‰åå¾®åˆ†æ–¹ç¨‹é—®é¢˜","section":"ç¬¬ä¹ç« ","content":" æ¤­åœ†å‹ # 4.1.1 é—®é¢˜è®¾å®š # æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªäºŒç»´ç©ºé—´çš„é—®é¢˜ï¼Œè®°ä¸ºåŒºåŸŸ $\\Omega$ã€‚è¾¹ç•Œè®°ä¸º $\\partial\\Omega$ã€‚\næ–¹ç¨‹ä¸€èˆ¬å½¢å¼å¦‚ä¸‹ï¼š $$-\\mu \\Delta u + \\beta \\cdot \\nabla u = f,\\quad \\text{åœ¨}\\ \\Omega å†…$$\nè¾¹ç•Œæ¡ä»¶ä¸ºï¼š $$u(\\partial \\Omega) = d \\quad (\\text{ç»™å®šçš„æ•°æ®})$$\nè¿™é‡Œçš„ç¬¦å·è§£é‡Šï¼ˆè§ä¸Šä¸€ç« ï¼‰ï¼š $\\beta$ï¼šè¡¨ç¤º\u0026quot;é£\u0026quot;æˆ–è€…å¯¹æµçš„æ–¹å‘å’Œå¼ºåº¦ $\\mu$ï¼šæ‰©æ•£ç³»æ•° $f$ï¼šå¤–åŠ›æˆ–æºé¡¹ å› æ­¤ï¼Œä¸Šé¢çš„æ–¹ç¨‹å¯ä»¥å±•å¼€å†™æˆï¼š\n$$-\\mu \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right)\n\\beta_x \\frac{\\partial u}{\\partial x} + \\beta_y \\frac{\\partial u}{\\partial y} = f(x,y)$$ 4.1.2 ç®€åŒ–æƒ…å½¢ï¼ˆåªæœ‰æ‰©æ•£ï¼Œæ²¡æœ‰é£ï¼‰ # é¦–å…ˆè€ƒè™‘æ›´ç®€å•çš„æƒ…å†µï¼Œå¿½ç•¥å¯¹æµï¼ˆå³â€œå…³æ‰é£â€ï¼‰ï¼Œå˜æˆçº¯æ‰©æ•£é—®é¢˜ï¼š\n$$-\\mu\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = f(x,y)$$\n4.1.3 ç¦»æ•£åŒ–æ–¹æ³•ï¼ˆæœ‰é™å·®åˆ†æ³•ï¼‰ # ç”¨æœ‰é™å·®åˆ†æ³•æ¥æ•°å€¼æ±‚è§£ï¼š\nå‡è®¾ç©ºé—´è¢«åˆ’åˆ†ä¸ºä¸€ä¸ªç½‘æ ¼ï¼Œæ¯ä¸ªç½‘æ ¼ç‚¹ç”¨åæ ‡ $(i,j)$ æ¥è¡¨ç¤ºä½ç½®ï¼š\nåœ¨$x$æ–¹å‘çš„é—´è·ä¸º $\\Delta x$ åœ¨$y$æ–¹å‘çš„é—´è·ä¸º $\\Delta y$ åˆ™å¯¹äºäºŒç»´çš„æ‹‰æ™®æ‹‰æ–¯ç®—å­ï¼Œå¸¸ç”¨çš„ä¸­å¿ƒå·®åˆ†æ ¼å¼ä¸ºï¼š\n$$\\frac{\\partial^2 u}{\\partial x^2}\\approx \\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{\\Delta x^2}$$\n$$\\frac{\\partial^2 u}{\\partial y^2}\\approx \\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{\\Delta y^2}$$\nä»£å…¥æ‰©æ•£æ–¹ç¨‹å¾—åˆ°ï¼š\n$$-\\mu\\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{\\Delta x^2} -\\mu\\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{\\Delta y^2} = f(x_i,y_j)$$\n4.1.4 å½¢æˆçº¿æ€§ä»£æ•°æ–¹ç¨‹ç»„ # æ‰€æœ‰ç½‘æ ¼ç‚¹çš„æœªçŸ¥æ•°$u_{i,j}$æ”¾åˆ°ä¸€ä¸ªå‘é‡ä¸­ï¼ˆå‘é‡è®°ä¸º$u$ï¼‰ï¼Œè¿™æ ·å°±å¯ä»¥æŠŠç¦»æ•£åçš„æ–¹ç¨‹å†™æˆä¸€ä¸ªçŸ©é˜µæ–¹ç¨‹ï¼š\n$$A u = b$$\nè¿™é‡Œï¼š\n$A$ æ˜¯ç³»æ•°çŸ©é˜µï¼ˆç¨€ç–ã€å¯¹ç§°ã€æ­£å®šï¼Œç®€ç§°SPDï¼‰ $u$ æ˜¯æœªçŸ¥é‡å‘é‡ï¼ˆæ‰€æœ‰ç½‘æ ¼ç‚¹çš„è§£ï¼‰ $b$ æ˜¯å·²çŸ¥é¡¹çš„å‘é‡ï¼ˆæºé¡¹$f$å’Œè¾¹ç•Œæ¡ä»¶çš„ç»„åˆï¼‰ 4.2 æ—¶é—´ç›¸å…³é—®é¢˜ï¼šæŠ›ç‰©å‹ï¼ˆParabolicï¼‰ # è€ƒè™‘çš„é—®é¢˜å½¢å¼ï¼š\n$$\\frac{\\partial u}{\\partial t}-\\mu\\frac{\\partial^2 u}{\\partial x^2}=f,\\quad x\\in(0,1),\\quad 0\u0026lt;t\u0026lt;T$$\nåˆå€¼ä¸è¾¹ç•Œæ¡ä»¶ä¸ºï¼š\nåˆå€¼ï¼š$u(x,t=0)=u_0(x)$ è¾¹ç•Œæ¡ä»¶ï¼š$u(0,t)=u_L(t),\\quad u(1,t)=u_R(t)$ åŠç¦»æ•£åŒ–æ–¹æ³•ï¼ˆç©ºé—´ç¦»æ•£ï¼Œæ—¶é—´è¿ç»­ï¼‰ # æˆ‘ä»¬é¦–å…ˆåªå¯¹ç©ºé—´ï¼ˆ$x$ï¼‰åšç¦»æ•£åŒ–ï¼Œå¾—åˆ°ï¼š\n$$\\frac{d u_j(t)}{d t}-\\mu\\frac{u_{j+1}(t)-2u_j(t)+u_{j-1}(t)}{\\Delta x^2}=f_j(t)$$\nè®°ï¼š\nå‘é‡å½¢å¼ï¼š$u(t)=[u_1(t),u_2(t),\u0026hellip;,u_n(t)]^T$ æºé¡¹å‘é‡ï¼š$f(t)=[f_1(t),f_2(t),\u0026hellip;,f_n(t)]^T$ ç³»æ•°çŸ©é˜µï¼š$A=\\frac{\\mu}{\\Delta x^2}\\text{tridiag}(-1,2,-1)$ äºæ˜¯é—®é¢˜å˜æˆå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰çš„ç³»ç»Ÿå½¢å¼ï¼š\n$$\\frac{d u}{d t}-A u=f$$\næ—¶é—´ç¦»æ•£åŒ–ï¼ˆODEæ–¹æ³•ï¼‰ # æ¥ä¸‹æ¥æˆ‘ä»¬å¯¹æ—¶é—´è¿›è¡Œç¦»æ•£åŒ–ï¼Œé‡‡ç”¨ä¸¤ç§æ–¹æ³•ï¼š\næ–¹æ³•1ï¼šæ˜¾å¼æ¬§æ‹‰ï¼ˆExplicit Euler, FEï¼‰ # å°†æ—¶é—´å¯¼æ•°åœ¨æ—¶é—´ç‚¹$t_n$è¿‘ä¼¼ä¸ºï¼š\n$$\\frac{u^{n+1}-u^n}{\\Delta t}=f^n+A u^n$$\nè§£å¾—ï¼š\n$$u^{n+1} = (I+\\Delta t A)u^n+\\Delta t f^n$$\næ˜¾å¼æ–¹æ³•å®¹æ˜“è®¡ç®—ï¼Œä½†ç¨³å®šæ€§æœ‰é™ï¼Œæ—¶é—´æ­¥é•¿ä¸èƒ½å¤ªå¤§ã€‚\næ–¹æ³•2ï¼šéšå¼æ¬§æ‹‰ï¼ˆImplicit Euler, IE/BEï¼‰ # å¦ä¸€ç§æ–¹æ³•æ˜¯åœ¨æ—¶é—´ç‚¹$t_{n+1}$å¤„æ±‚å¯¼ï¼š\n$$\\frac{u^{n+1}-u^n}{\\Delta t}=f^{n+1}+A u^{n+1}$$\nè§£å‡ºéšå¼æ–¹ç¨‹ï¼š\n$$(I-\\Delta t A)u^{n+1}=u^n+\\Delta t f^{n+1}$$\nè¿™ä¸ªæ–¹æ³•æ›´ç¨³å®šï¼Œä½†éœ€è¦åœ¨æ¯ä¸ªæ—¶é—´æ­¥è§£ä¸€ä¸ªçº¿æ€§ç³»ç»Ÿã€‚\nå…³äºçŸ©é˜µæ€§è´¨çš„æ€»ç»“ï¼š # $A$ ä¸ºSPDçŸ©é˜µï¼ˆå¯¹ç§°æ­£å®šï¼‰ å½“ $A$ å’Œ $b$ æ˜¯ä¸æ—¶é—´æ— å…³æ—¶ï¼Œé€šå¸¸æˆ‘ä»¬æ›´å–œæ¬¢éšå¼æ–¹æ³•ï¼Œå› ä¸ºå¯ä»¥åˆ†è§£çŸ©é˜µä¸€æ¬¡ï¼ˆå¦‚LUåˆ†è§£ï¼‰å¹¶åå¤ä½¿ç”¨ï¼ŒåŠ å¿«è®¡ç®—é€Ÿåº¦ã€‚ æ€»ç»“ä¸€ä¸‹ï¼š # ä»¥ä¸Šæ¶‰åŠäº†ä¸¤ç±»åå¾®åˆ†æ–¹ç¨‹é—®é¢˜ï¼š\næ¤­åœ†å‹ï¼ˆç©ºé—´äºŒç»´ï¼‰é—®é¢˜ï¼Œé€šè¿‡ç©ºé—´ç¦»æ•£åŒ–ç›´æ¥å¾—åˆ°çº¿æ€§æ–¹ç¨‹ç»„ï¼› æŠ›ç‰©å‹ï¼ˆç©ºé—´ä¸€ç»´+æ—¶é—´ï¼‰é—®é¢˜ï¼Œå…ˆå¯¹ç©ºé—´ç¦»æ•£å˜ä¸ºODEï¼Œå†å¯¹æ—¶é—´ç¦»æ•£ä½¿ç”¨ODEæ•°å€¼æ–¹æ³•ï¼ˆæ˜¾å¼/éšå¼æ¬§æ‹‰æ–¹æ³•ï¼‰è¿›è¡Œæ±‚è§£ã€‚ ä»¥ä¸Šæ­¥éª¤é€æ­¥è§£é‡Šäº†é—®é¢˜å¦‚ä½•ä»è¿ç»­å½¢å¼å˜æˆæ•°å€¼å¯æ±‚è§£çš„ç¦»æ•£å½¢å¼ã€‚\n$$ \\begin{align} -\\mu\\Delta u + \\vec{\\beta} \\cdot \\nabla u \u0026amp;= f \\ u(\\partial\\Omega) \u0026amp;= \\text{data} \\end{align} $$\nWrite it out: $$ \\begin{align} -\\mu\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) + \\beta_x\\frac{\\partial u}{\\partial x} + \\beta_y\\frac{\\partial u}{\\partial y} \u0026amp;= f(x,y) \\ u(\\partial\\Omega) \u0026amp;= d \\end{align} $$\n4.1 Only Consider Diffusion. Turn off Wind: # To form a system: $(i,j) \\to k$.\n$Au = b$.\n$A$ is symmetric, SPD.\n4.2 Turn on the Wind. Upwind # With upwind, the pts are not good.\n5. æŠ›ç‰©å‹ï¼ˆParabolicï¼‰é—®é¢˜ï¼ˆæ—¶é—´ç›¸å…³é—®é¢˜ï¼‰ # $$ \\begin{align} \\frac{\\partial u}{\\partial t} - \\mu\\frac{\\partial^2 u}{\\partial x^2} \u0026amp;= f, \\quad x \\in (0,1) \\quad 0 \u0026lt; t \u0026lt; T\\ u(0,t) \u0026amp;= u_L(t), \\quad u(1,t) = u_R(t) \\ u(x, t=0) \u0026amp;= u_0(x) \\end{align} $$\nDiscretization along $x$: semi-discretization: $u_j(t) \\approx u(x_j, t)$. $$\\frac{du_j}{dt} - \\mu\\frac{u_{j+1}(t) - 2u_j(t) + u_{j-1}(t)}{\\Delta x^2} = f_j(t) = f(x_j, t)$$\nSo, we form system $Au = f$.\n$$A = \\frac{\\mu}{\\Delta x^2}\\text{Triad}(-1, 2, 1), \\quad u(t) = \\begin{bmatrix} u_1(t) \\ \\vdots \\ u_n(t) \\end{bmatrix}, \\quad f(t) = \\begin{bmatrix} f_1(t) \\ \\vdots \\ f_n(t) \\end{bmatrix}$$\nThen, we have a system of ODE: $$\\frac{du}{dt} - Au = f$$\nWe can now do time discretization and use ODE methods.\nEE/FE: $$u^n = u(t^n), \\quad \\left.\\frac{du}{dt}\\right|_{t^n} \\approx \\frac{u^{n+1} - u^n}{\\Delta t} = f^n + Au^n$$\n$$u^{n+1} = u^n + \\Delta t Au^n + \\Delta t f^n = (I + \\Delta t A)u^n + \\Delta t f^n = (I + \\Delta t A)^n u_0 + \\Delta t f^n$$\nIE/BE: $$\\left.\\frac{du}{dt}\\right|_{t^n} = \\frac{u^n - u^{n-1}}{\\Delta t} = f^n + Au^n$$\n$$u^n - u^{n-1} = \\Delta t f^n + \\Delta t Au^n$$\n$$u^n - \\Delta t Au^n = \\Delta t f^n + u^{n-1}$$\n$$(I - \\Delta t A)u^n = u^{n-1} + \\Delta t f^n \\quad \\leftarrow \\text{A linear system to solve}$$\n$I - \\Delta t A$ is SPD and $A$ is time-independent. So, we may favor direct method (as we can store $A = LU$ and reuse it) over iterative methods.\nTo discuss stability, set $f = 0$:\nEE is conditionally stable: Let $\\lambda_i$ be eigenvalues of $A$.\n$$\\Delta t \u0026lt; \\frac{2}{|\\lambda_i|} \\quad \\text{for stability}$$\nFurther, $A = \\frac{\\mu}{\\Delta x^2}\\text{triad}(1, -2, 1)$, $\\rho(A) \\sim \\frac{c}{\\Delta x^2}$. So,\n$$\\Delta t \u0026lt; \\frac{2}{|\\lambda_i|} \\leq \\frac{2}{\\rho(A)} = \\frac{2}{c}\\Delta x^2$$\nSo, if we decrease $\\Delta x$ by 2, to have stability\n$$\\Delta t_{new} \u0026lt; \\frac{2}{c}\\left(\\frac{\\Delta x}{2}\\right)^2 = \\frac{\\Delta t_{old}}{4} \\quad \\Rightarrow \\text{we need finer intervals for time}$$\nIE is unconditionally stable.\nDef. ($\\theta$ Methods). $$\\frac{u^{n+1} - u^n}{\\Delta t} = \\theta Au^{n+1} + (1-\\theta)Au^n + \\theta f^{n+1} + (1-\\theta)f^n, \\quad \\theta \\in [0,1]$$\nEE: $\\theta = 0 \\quad \\mathcal{O}(\\Delta t) \\quad$ explicit $\\quad$ conditional stability\nIE: $\\theta = 1 \\quad \\mathcal{O}(\\Delta t) \\quad$ implicit $\\quad$ unconditional stability\nCN: $\\theta = \\frac{1}{2} \\quad \\mathcal{O}(\\Delta t^2) \\quad$ implicit $\\quad$ unconditional stability\nSuppose $f = 0$. Then, $$\\frac{u^{n+1} - u^n}{\\Delta t} = \\theta Au^{n+1} + (1-\\theta)Au^n$$\n$$u^{n+1} - u^n = \\Delta t \\theta Au^{n+1} + \\Delta t(1-\\theta)Au^n$$\n$$(I - \\Delta t \\theta A)u^{n+1} = (I + \\Delta t(1-\\theta)A)u^n$$\nWe essentially solve a linear system in each iteration.\n5.1 Thm (Stability and Order of $\\theta$ Methods) # $\\theta$ methods are unconditionally stable for $\\theta \\geq \\frac{1}{2}$. Otherwise, it is conditionally stable for $\\theta \u0026lt; \\frac{1}{2}$ and the stability condition for parabolic problem is $\\Delta t \u0026lt; c\\Delta x^2$. Meanwhile, the method is order 1 for $\\theta \\neq \\frac{1}{2}$ and order 2 for $\\theta = \\frac{1}{2}$. Although the $\\theta$ method is 2nd order in space, the order of error is dominant and determined by the order in time. CN is the most vulnerable to lack of regularity and sensitive to non-smoothness. 6. Hyperbolic Problems # 6.1 # $ \\begin{align} \\frac{\\partial u}{\\partial t} + \\alpha\\frac{\\partial u}{\\partial x} \u0026amp;= 0, \\quad \\alpha \u0026gt; 0, \\text{ constant} \\ u(x,0) \u0026amp;= u_0(x) \\end{align} $\nExact solution: $u(x,t) = u_0(x - \\alpha t)$\n6.2 Example: Modeling Density of Pollutant # $u$: pollutant, $x$: displacement of boat, $t$: time.\n$$ \\begin{align} \\frac{du}{dx} \u0026amp;= 0 \\quad \\text{(i.e, pollutant and boat moves at the same velocity)} \\ \\frac{dx}{dt} \u0026amp; = a \\quad \\text{(i.e., boat moves at velocity of $a$)} \\end{align}\n$$\n$x(t) = x_0 + at \\Rightarrow$ characteristic curves\n$u(x,t) = u_0(x-at)$. Solution to $\\begin{cases} \\frac{dx}{dt} = a \\ x(0) = x_0 \\end{cases}$\nConsider $u(x(t),t)$: $\\frac{du}{dt} = \\frac{\\partial u}{\\partial t} + \\frac{\\partial u}{\\partial x} \\cdot \\frac{dx}{dt} = \\frac{\\partial u}{\\partial t} + a \\cdot \\frac{\\partial u}{\\partial x} = 0$.\n6.3 Similar Problems: # Conservation law $\\frac{\\partial u}{\\partial t} + \\frac{\\partial q(u)}{\\partial x} = 0$\n$q(u) = v(u) \\cdot u$ with $v = v_{max}\\left(1 - \\frac{u}{u_{max}}\\right)$\n$\\Rightarrow \\frac{\\partial u}{\\partial t} + v_{max}\\left(1 - \\frac{u}{u_{max}}\\right)\\frac{\\partial u}{\\partial x} = 0$. Models the density of traffic.\n$= \\alpha$, but $\\alpha$ is not constant here.\nHeat equation $\\frac{\\partial^2 u}{\\partial t^2} - v^2\\frac{\\partial^2 u}{\\partial x^2} = f$\nDefine $w_1 = \\frac{\\partial u}{\\partial x}$ and $w_2 = \\frac{\\partial u}{\\partial t}$.\n$ \\begin{align} \\frac{\\partial w_1}{\\partial t} - v^2\\frac{\\partial w_2}{\\partial x} \u0026amp;= f \\ \\frac{\\partial w_2}{\\partial t} - \\frac{\\partial w_1}{\\partial x} \u0026amp;= 0 \\quad \\frac{\\partial^2 u}{\\partial x \\partial t} = \\frac{\\partial^2 u}{\\partial t \\partial x} \\end{align} $\nDefine $w = \\begin{bmatrix} w_1 \\ w_2 \\end{bmatrix}$, $A = \\begin{bmatrix} 0 \u0026amp; -v^2 \\ -1 \u0026amp; 0 \\end{bmatrix}$\nThen, the original equation becomes a system: $\\frac{\\partial w}{\\partial t} + A\\frac{\\partial w}{\\partial x} = 0$\nThe eigenvalues of $A$: $\\lambda_{1,2} = \\pm iv$. $\\Rightarrow$ Diagonalizable.\nFind the numerical solution.\n$\\frac{\\partial u}{\\partial t}\\bigg|_{t^{n+1}, x_j} = \\frac{u_j^{n+1} - u_j^n}{\\Delta t}$\n$\\alpha\\frac{\\partial u}{\\partial x}\\bigg|{t^{n+1}, x_j} = \\frac{\\alpha}{2} \\cdot \\frac{u{j+1}^{n+1} - u_{j-1}^{n+1}}{\\Delta t}$\nWith BE-C: $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2} \\cdot \\frac{u_{j+1}^{n+1} - u_{j-1}^{n+1}}{\\Delta t} = 0$\n$ \\Rightarrow \\begin{bmatrix} \\frac{1}{\\Delta t} \u0026amp; \\frac{\\alpha}{2\\Delta t} \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \\ -\\frac{\\alpha}{2\\Delta t} \u0026amp; \\frac{1}{\\Delta t} \u0026amp; \\frac{\\alpha}{2\\Delta t} \u0026amp; 0 \u0026amp; \\cdots \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \u0026amp; \\end{bmatrix} $\nWith FE-C: unconditionally unstable. NEVER USE IT! $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2} \\cdot \\frac{u_{j+1}^n - u_{j-1}^n}{\\Delta t} = 0$\n$\\Rightarrow u_j^{n+1} = u_j^n + \\frac{\\alpha\\Delta t}{2\\Delta t}(u_{j+1}^n - u_{j-1}^n)$\nWith FE-Upwind: $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\alpha\\frac{u_j^n - u_{j-1}^n}{\\Delta x} = 0 \\quad \\alpha \u0026gt; 0$\n$\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\alpha\\frac{u_{j+1}^n - u_j^n}{\\Delta x} = 0 \\quad \\alpha \u0026lt; 0$\n$\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2}\\frac{u_{j+1}^n - u_{j-1}^n}{\\Delta x} - \\frac{|\\alpha|\\Delta t}{2}\\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\\Delta x^2} = 0$\nDiffusion\nLax Wendroff: FE-upwind with modified coefficient $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2}\\frac{u_{j+1}^n - u_{j-1}^n}{\\Delta x} - \\frac{\\alpha^2\\Delta t}{2}\\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\\Delta x^2} = 0$\n$u(x_j, t^{n+1}) = u(x_j, t^n) + \\frac{\\partial u}{\\partial t}\\bigg|{t^n, x_j}(t^{n+1} - t^n) + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial t^2}\\bigg|{t^n, x_j}(t^{n+1} - t^n)^2 + \\mathcal{O}(||t^{n+1} - t^n||^3)$\n$\\frac{\\partial u}{\\partial t} = -\\alpha\\frac{\\partial u}{\\partial x}, \\quad \\frac{\\partial^2 u}{\\partial x \\partial t} = -\\alpha\\frac{\\partial^2 u}{\\partial x^2}, \\quad \\frac{\\partial^2 u}{\\partial t^2} = \\alpha^2\\frac{\\partial^2 u}{\\partial x^2}$\nSubstitute: $u_j^{n+1} = u_j^n - \\alpha\\left(\\frac{u_{j+1}^n - u_{j-1}^n}{2\\Delta x}\\right)\\Delta t + \\frac{\\alpha^2}{2}\\left(\\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\\Delta x^2}\\right)\\Delta t^2$\nStability: $\\left|\\frac{\\alpha\\Delta t}{\\Delta x}\\right| \\leq 1$\n"},{"id":61,"href":"/docs/Mathematics/MATH-351-352/Heat-Equation/The-Fourier-Series/","title":"å‚…ç«‹å¶çº§æ•°","section":"çƒ­æ–¹ç¨‹","content":" 1. å‚…ç«‹å¶çº§æ•°çš„åŸºæœ¬å½¢æ€ # 1.1 å‘¨æœŸ # åœ¨ç»å†äº†å‘¨æœŸ$T$åï¼Œé‡æ–°è·å¾—åŸå€¼çš„å‡½æ•°ä¸ºå‘¨æœŸå‡½æ•°ï¼š\n$$ \\varphi(t+T)=\\varphi $$\n1.2 æ­£å¼¦å‹é‡ # æ­£å¼¦å‹é‡å½¢å¦‚ï¼š\n$y(t)=Asin(\\omega t+\\alpha)$ where $\\omega = \\cfrac{2\\pi}{T}$ is the frequency.\nNotice that for values $s.t.$:\n$$ y_0 = A_0, \\ y_1=A_1\\sin(\\omega t + \\alpha_1), \\ y_2=A_2\\sin(2\\omega t + \\alpha_2), \\ y_3=A_3\\sin(3\\omega t + \\alpha_3) \u0026hellip; $$\nwe have frequency as the multiple of the smallest frequency with their period:\n$\\omega$, $2\\omega$, $3\\omega$â€¦\n$T$, $\\frac{1}{2}T$, $\\frac{1}{3}T$â€¦\n![[Fourier Series.png]]\nIf we add them together, we getâ€¦\n1.3.1 ä¸‰è§’çº§æ•° - $\\varphi(t)$å‡½æ•° # ğŸ’¡ **It is possible to represent any periodic function with finite or infinite summations of sin functions:** $$ \\varphi(t)=A_0 + \\sum_{n=1}^{\\infty}A_n\\sin(n\\omega t+ \\alpha_n) $$\nwhere $A_n, \\alpha_n$ are constants\n$\\varphi(t)$å¯ä»¥è¢«åˆ†è§£æˆæ— æ•°ä¸ªè°ƒå’Œéœ‡åŠ¨ æ¯ä¸€é¡¹ç§°ä¹‹ä¸ºè°ƒå’Œç´  å¯¹$\\varphi(t)$è¿›è¡Œåˆ†è§£çš„æ‰‹æ³•è¢«ç§°ä¹‹ä¸ºè°ƒå’Œåˆ†æ 1.3.2 ä¸‰è§’çº§æ•° - æœ€ç»ˆå±•å¼€å¼ # æ³¨æ„ppté‡Œçš„å…¬å¼æ˜¯ä¸ä¸€æ ·çš„æ¢å…ƒï¼Œ$x=\\frac{2Lt}{T}$ï¼Œæ‰€ä»¥$\\frac{\\pi}{L}$è¢«æŠ½å‡ºæ¥äº†ã€‚è¿™é‡Œæˆ‘ä»¬ä¼šç”¨æ­£å¸¸çš„é€»è¾‘æ¢å…ƒæ¨å¯¼ï¼Œç­‰åˆ°åé¢éœ€è¦è®¡ç®—generic intervalï¼Œæ‰é‡æ–°æ¢ä¸€ä¸ª$L$è¿›å»***\nå½“æˆ‘ä»¬ç”¨$x=\\omega t = \\frac{2\\pi t}{T}$ æ¥æ¢å…ƒ $s.t.$ $f(x)=\\varphi(t) = \\varphi(\\frac{x}{\\omega})$ ï¼Œæˆ‘ä»¬ç”¨ä¸‰è§’æ­£å¼¦å…¬å¼å±•å¼€ï¼š\n$$ \\begin{align}f(x)\u0026amp;= A_0 + \\sum_{n=1}^{\\infty}A_n\\sin(nx+ \\alpha_n)\\ \u0026amp;=A_0 + \\sum_{n=1}^{\\infty}A_n(\\cos nx\\sin \\alpha_n+ \\sin nx \\cos \\alpha_n) \\\\end{align} $$\næ­£å¼¦å…¬å¼ï¼š$\\sin(a+b)=\\cos a\\sin b + \\sin a\\cos b$ å±•å¼€\n$$ f(x)=A_0+\\sum_{n=1}^{\\infty}[(A_n\\sin \\alpha_n)\\cos nx+ (A_n \\cos \\alpha_n)\\sin nx )] $$\nå¹¶ä»¤ $A_0=a_0, \\ A_n\\sin\\alpha_n = b_n, \\ A_n\\cos\\alpha_n = b_n$\nğŸ’¡ äºæ˜¯æˆ‘ä»¬å°±æœ‰äº†ä¼šæœ‰ä¸‰è§’çº§æ•°çš„**æœ€ç»ˆå±•å¼€å½¢æ€**ï¼š $$ f(x)=a_0 + \\sum_{n=1}^{\\infty}(a_n\\cos nx+ b_n\\sin nx) $$\nthe period for $f$ is $2\\pi$ due to our definition of new independent variable $x$ 2. å‚…ç«‹å¶çº§æ•°çš„ç³»æ•° # 2.1 æ¬§æ‹‰-å‚…ç«‹å¶å…¬å¼ï¼ˆEuler-Fourier formulaï¼‰ # è¿™ä¸ªæ˜¯ä¸€ä¸ª18ä¸–çºªåˆæ¬§æ‹‰ä½¿ç”¨çš„ç³»æ•°ç¡®å®šæ³•ã€‚åé¢æˆ‘ä»¬è¿˜å­¦äº†æ³›å‡½åˆ†æçš„inner productç®—ç³»æ•°çš„æ–¹æ³•ã€‚\nAssuming $f(x)$ under $[-\\pi,\\pi]$ is an integrable function, if we assume Fourier Expansion for $f(x)$ is true, then directly we have:\n$$ \\begin{align}\\int_{-\\pi}^{\\pi} f(x) , dx = 2\\pi a_0 + \\sum_{n=1}^{\\infty} \\left[ a_n \\int_{-\\pi}^{\\pi} \\cos nx , dx + b_n \\int_{-\\pi}^{\\pi} \\sin nx , dx \\right]\\end{align} $$\nobviously, the integration for $\\cos$ and $\\sin$ for $[-\\pi,\\pi]$ is 0 regardless of the values, so we only left with:\n$$ a_0= \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} f(x) , dx $$\nnext, if to find the specific value for arbitrary $a_m$ ($m=1,2,3\u0026hellip;$), we multiply $(3)$ by $\\cos (ma)$ so that the terms we needed wouldnâ€™t cancel out, eventually we reached at:\n$$ a_m= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x) \\cos(ma) , dx $$\nSimilarly, multiply by $\\sin(ma)$, we derive:\n$$ b_m= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x) \\sin(ma) , dx $$\nè¿™äº›å…¬å¼è¢«ç§°ä¹‹ä¸ºæ¬§æ‹‰-å‚…ç«‹å¶å…¬å¼ï¼ˆEuler-Fourier formulaï¼‰ï¼Œè€Œä»–ä»¬ç®—å‡ºæ¥çš„æ•°å€¼è¢«ç§°ä¸ºå·²ç»™å‡½æ•°çš„å‚…ç«‹å¶ç³»æ•°ï¼ˆFourier Coefficientï¼‰ã€‚\n2.2* ç‹„åˆ©å…‹é›·ç§¯åˆ†ï¼ˆDirichlet integralï¼‰ # é€šè¿‡å‚…ç«‹å¶å±•å¼€çš„ä¸€ä¸ªå®šç‚¹$x=x_0$çš„æ€§è´¨ï¼Œè·å¾—çš„é‡è¦ç§¯åˆ†ï¼š\n$$ s_n(x_0) = \\frac{1}{\\pi} \\int_{0}^{\\pi} \\left[ f(x_0 + t) + f(x_0 - t) \\right] \\frac{\\sin (n + \\frac{1}{2})t }{2 \\sin\\frac{t}{2}} , dt\n$$\ne.x. https://www.bilibili.com/video/BV1XE411p7ZN/\n3*. å¹¿ä¹‰å½¢æ€çš„å‚…ç«‹å¶çº§æ•°å’Œç³»æ•°å…¬å¼ # 3.1 ä»»æ„åŒºé—´çš„æƒ…å†µ # ğŸ’¡ We can separate out a $\\frac{1}{2}$ from the original $a_0$ coefficient, so the first term becomes a special case for $n=0$. åœ¨ä»»æ„$2L$å¤§å°çš„åŒºé—´ $(-L,L]$ ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å˜æ¢ï¼š$x=\\frac{Ly}{\\pi} (-\\pi\u0026lt;y\\leq\\pi)$ ä½¿å¾—$f(x)\\rightarrow f(\\frac{Ly}{\\pi})$ ã€‚äºæ˜¯ï¼Œæˆ‘ä»¬æ ¹æ®å…¬å¼è·å¾—ï¼š\n$$ f\\left( \\frac{Ly}{\\pi} \\right) = \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} \\left( a_n \\cos ny + b_n \\sin ny \\right) $$\nä»¥åŠå…¶ç³»æ•°ï¼š\n$$ a_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f\\left( \\frac{y}{\\pi} \\right) \\cos ny , dy \\quad (n = 0, 1, 2, \\ldots) $$\n$$\nb_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f\\left( \\frac{y}{\\pi} \\right) \\sin ny , dy \\quad (n = 1, 2, \\ldots) $$\nå½“æˆ‘ä»¬é‡æ–°å˜æ¢å›å»ï¼Œå³ä½¿å¾—$y = \\cfrac{\\pi x}{L}$ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ä¼šè·å¾—ä¸€ä¸ªå¹¿ä¹‰ä¸Šçš„å‚…ç«‹å¶å±•å¼€ï¼\nğŸ’¡ The general form of **Fourier Expansion**: $$ \\begin{align}f(x)=\\frac{A_0}{2} + \\sum_{n=1}^{\\infty} \\left( A_n \\cos \\frac{n\\pi x}{L} + B_n \\sin \\frac{n\\pi x}{L} \\right) \\end{align} $$\nHere, $x$ is no longer the angle, but the integer multiples of $\\frac{\\pi x}{L}$, such that the Fourier Coefficient for generic interval $[-L,L]$ are:\n$$ \\begin{align} A_n \\equiv \\frac{1}{L} \\int_{-L}^{L} f(x) \\cos\\left(\\frac{n\\pi x}{L}\\right) dx, \\quad n = 0, 1, 2, \\ldots \\ B_n \\equiv \\frac{1}{L} \\int_{-L}^{L} f(x) \\sin\\left(\\frac{n\\pi x}{L}\\right) dx, \\quad n = 1, 2, 3, \\ldots \\end{align}\n$$\n3.2 èœå°±å¤šç»ƒï¼Œç»™çˆ·å±• # $f(x)=e^{ax}$, $a\\neq0$ on the interval of $(-\\pi,\\pi)$\nç­”æ¡ˆ\n$f(x)=\\frac{\\pi-x}{2}$ on the interval of $(0,2\\pi)$\nç­”æ¡ˆ\n$f(x)=x^2$ on the interval of $[-\\pi,\\pi]$, expand as cosine series.\nç­”æ¡ˆ\nTwo functions ($a$ is assumed to be non-integers):\n$f_1(x)=\\cos(ax)$ on the interval of $[-\\pi,\\pi]$, expand as cosine series.\nç­”æ¡ˆ\n$f_2(x)=\\sin(ax)$ on the interval of $(-\\pi,\\pi)$, expand as sine series.\nç­”æ¡ˆ\n$f(x)=e^{ax}$, $a\\neq0$ on the interval of $(0,\\pi)$:\nexpand as cosine series expand as sine series ç­”æ¡ˆ\n4. å‚…é‡Œå¶çº§æ•°çš„æ‹“æ‰‘ç©ºé—´ # ğŸ’¡ ç†è§£å‚…é‡Œå¶çº§æ•°å˜æ¢çš„æ‹“æ‰‘ç©ºé—´çš„é€»è¾‘æ˜¯ï¼š ä¸‰ç»´ç©ºé—´â€”â€”Â $n$ç»´ç©ºé—´â€”â€”$\\infty$ç»´ç©ºé—´â€”â€”å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆ$L^2$ç©ºé—´ï¼‰â€”â€”å‚…ç«‹å¶çº§æ•°â€”â€”å‚…ç«‹å¶å˜æ¢\n4.1 $\\infty$ç»´æ¬§å¼ç©ºé—´ - $\\R^{\\infty}$ # 4.1.1 $n$ç»´ç©ºé—´çš„å®šä¹‰- $\\R^{n}$ # èŒƒæ•°ï¼ˆNormï¼‰:\n$$ N_p(x) = | x |_p = \\left( |x_1|^p + |x_2|^p + \\cdots + |x_n|^p \\right)^{\\frac{1}{p}} $$\nå•ä½æ­£äº¤å‘é‡åŸºï¼ˆstandard orthogonal basisï¼‰:\n$$ \\left{Â \\begin{aligned}\\vec{e}_1 \u0026amp;= (1, 0, \\ldots, 0), \\\\vec{e}_2 \u0026amp;= (0, 1, \\ldots, 0), \\\u0026amp; \\vdots \\\\vec{e}_n \u0026amp;= (0, 0, \\ldots, 1)\\end{aligned}\\right. $$\nå› æ­¤$n$ç»´æ¬§å¼ç©ºé—´çš„ä»»ä½•å‘é‡å¯ä»¥è¢«ç›´æ¥è¡¨è¾¾æˆï¼š\n$$ \\vec{a} = \\sum_{i=1}^{n} a_i \\vec{e}_i $$\n4.1.2 ç”±æ­¤æ¨å‡º$\\infty$ç»´ç©ºé—´ - $\\R^{\\infty}$ # åŒç†ï¼Œå½“æˆ‘ä»¬æœ‰æ— æ•°ä¸ªæ ‡å‡†åŸºï¼Œ${ \\vec{e}_1 , \\vec{e}_2 , \\vec{e}_3 \\ldots}$ï¼Œçš„æ—¶å€™ï¼Œå‘é‡å˜æˆäº†\n$$ \\vec{a} = \\sum_{i=1}^{\\infty} a_i \\vec{e}_i $$\nä½†æ˜¯ä»–ä»¬ä¾ç„¶æ˜¯ç¦»æ•£çš„å…ƒç´  4.2 Hilbertç©ºé—´ # æˆ‘ä»¬éœ€è¦æŠŠç¦»æ•£çš„å…ƒç´ è¿‡åº¦åˆ°è¿ç»­çš„å‡½æ•°ï¼ŒHilbertç©ºé—´ï¼š\nè®¾å®šä¸€ä¸ªå‘é‡å‡½æ•°$\\vec{f}(x)$\nå¹¶ä¸”å­˜åœ¨ä¸€ç»„åŸºå‡½æ•°ï¼ˆstandard orthogonal functionsï¼‰\n${ \\vec{\\varphi}_1 , \\vec{\\varphi}_2 , \\vec{\\varphi}_3 \\ldots}$ $s.t.$ we have\n$$ \\vec{f}(x) = \\sum_{i=1}^{\\infty} a_i \\vec{\\varphi_i}(x) $$\nå› æ­¤ï¼Œæˆ‘ä»¬åœ¨Hilbertç©ºé—´å°±æœ‰äº†è¿™æ ·çš„æ€§è´¨ï¼š\nğŸ’¡ 1. å‡½æ•°åœ¨åŒºé—´$[a,b]$ä¸Šçš„æ¨¡ï¼ˆnormï¼‰ï¼š $$ | f(x) | = \\sqrt{\\int_a^b f^2(x) , dx} ; $$\nå¦‚æœä¸¤ä¸ªå‡½æ•°çš„æ­£äº¤æ¡ä»¶ï¼ˆorthogonalityï¼‰ä¸ºå…¶å†…ç§¯ï¼ˆinner productï¼‰ä¸ºé›¶ï¼š $$ \\int_{a}^{b} f(x) g(x) , dx = 0 $$\nä¸¤è€…çš„è§’çš„ä½™å¼¦ä¸º $$ \\cos(\\theta) = \\frac{\\langle f(x), g(x) \\rangle}{| f(x) | \\cdot | g(x) |} = \\frac{\\int f(x)g(x) , dx}{\\sqrt{\\int f^2(x) , dx} \\sqrt{\\int g^2(x) , dx}} ; $$\n4.3 $L^p$ ç©ºé—´ï¼ˆLebesgue Spaceï¼‰ # å®šä¹‰ï¼šSuppose $f(x)$ is measurable functions on $E\\subset R^n$.\nFor $0\u0026lt;p\u0026lt;\\infty$, we denote:\n$$ ||f||_p=(\\int_E|f(x)|^pdx)^{1/p} $$\næˆ‘ä»¬ç”¨$L^p(E)$æ¥è¡¨ç¤º$||f||_p\u0026lt;\\infty$çš„å…¨ä½“å‡½æ•°ï¼Œç§°å…¶ä¸º$L^p$ç©ºé—´ã€‚\nå…¶èŒƒæ•°ï¼ˆnormï¼‰ä¸ºï¼š\n$$ L_p = | \\varphi |p = \\left( \\sum{i=1}^{n} |\\varphi_i|^p \\right)^{\\frac{1}{p}}, \\quad x = (x_1, x_2, \\ldots, x_n) $$\n$L^p$ç©ºé—´çš„ä¸€äº›åŸºç¡€å±æ€§ï¼š\n$L$ç©ºé—´é‡Œçš„æ¯ä¸€ä¸ªå‡½æ•°éƒ½æ˜¯Lebesgueå¯ç§¯çš„ ç©ºé—´ç»´åº¦æ˜¯æ— ç©·è€Œä¸”ä¸å¯æ•°çš„åº¦é‡ç©ºé—´ï¼ˆMetric Spaceï¼‰ Banachç©ºé—´ï¼Œæˆ–è€…å®Œå¤‡èµ‹èŒƒå‘é‡ç©ºé—´ (complete normed vector space) å½“$p=2$ï¼Œ$L^2$å˜æˆäº†ä¸€ä¸ªHilbertç©ºé—´ï¼Œä¸€ä¸ªå¸¦æœ‰å†…ç§¯çš„Banachç©ºé—´ AÂ Hilbert spaceÂ is aÂ realÂ orÂ complexÂ inner product spaceÂ that is also aÂ complete metric spaceÂ with respect to the distance functionÂ inducedÂ by the inner product. In every Hilbert Space, we have functions that\n$$ \\langle x, y \\rangle = \\sum_{k=1}^{n} \\overline{x_k} y_k $$\n$L^2$ç©ºé—´çš„èŒƒæ•°**ï¼ˆnormï¼‰**ï¼Œæ ¹æ®ä¹‹å‰çš„å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—çŸ¥ï¼š\n$$ | \\varphi |2 = \\sqrt{ \\sum{i=1}^{n} |\\varphi_i|^2 }, \\quad x = (x_1, x_2, \\ldots, x_n) $$\np.s. ä»¥ï¼ˆp=2ï¼‰ä¸ºä¾‹ï¼Œç©ºé—´ä¸­åˆ°åŸç‚¹çš„æ¬§æ°è·ç¦»ä¸º1çš„ç‚¹æ„æˆäº†ä¸€ä¸ªçƒé¢ã€‚\n![[L-space-1.png]]\n4.4 å‚…é‡Œå¶çº§æ•°çš„æ­£äº¤å‡½æ•°ç³»ä¸è§„èŒƒç³» # å®šä¹‰ï¼šif $\\int_{a}^{b} \\varphi(x) \\psi(x) , dx = 0$ for interval $[a,b]$, then $\\varphi(x), \\psi(x)$ are orthogonal.\n4.4.1 æ­£äº¤å‡½æ•°ç³» (orthogonal functions) # å½“æ¯å¯¹åŒå‡½æ•°$\\varphi_n(x), \\ \\varphi_m(x)$ éƒ½ç¬¦åˆå®šä¹‰ä»¥ä¸‹ï¼Œæˆ‘ä»¬ç§°è¿™æ ·çš„å‡½æ•°ç¾¤ä½“ä¸ºæ­£äº¤å‡½æ•°ç³» ï¼ˆorthogonal groupï¼‰ï¼š\n$$ \\int_{a}^{b} \\varphi_n(x) \\varphi_m(x) , dx = 0 \\space \\space\\space\\space\\space\\space\\space\\space {n,m\\in \\N\\ | \\ n\\neq m } $$\n4.4.2 è§„èŒƒæ­£äº¤å‡½æ•°ç³» (orthonormal functions) # è‹¥å‡½æ•°çš„ $\\lambda_n=1$ $(n=1,2,3\u0026hellip;)$, é‚£ä¹ˆè¿™ä¾¿æ˜¯è§„èŒƒç³»ï¼ˆorthonormal groupï¼‰ï¼š\n$$ \\int_{a}^{b} \\varphi_n^2(x) , dx = \\lambda_n $$\nè‹¥ä¸æ˜¯orthogonalï¼Œåˆ™æˆ‘ä»¬å¯ä»¥é€šè¿‡$\\left{ \\cfrac{\\varphi_n(x)}{\\sqrt{\\lambda_n}} \\right}$ æ¥è¿›è¡Œæ¢å–æ–°çš„orthogonal functionsã€‚\n4.5 å‚…é‡Œå¶çº§æ•°åœ¨$L^2$ ç©ºé—´çš„åŸºå‡½æ•° # Notice back to this set of functions inside the interval $[-\\pi,\\pi]$:\n$$1, \\cos(x), \\sin(x), \\cos(2x), \\sin(2x),\u0026hellip;\\cos(nx), \\sin(nx),\u0026hellip;$$\nThese are orthogonal basis functions for their vector space because we see for any two functions in this set $\\int_{-\\pi}^{\\pi} \\cos(mx) \\cdot \\sin(nx) , dx = 0.$\nHowever, they are not standard, or normed, because $\\lambda_n \\neq1$.\næˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨¡ï¼ˆnormï¼‰çš„å®šä¹‰è·å¾—ä»–ä»¬çš„normalizing coefficientï¼š\n$$|1| = \\sqrt{\\int_{-\\pi}^{\\pi} 1^2 , dx} = \\sqrt{2\\pi}$$\n$$|\\cos(nx)| = \\sqrt{\\int_{-\\pi}^{\\pi} \\cos^2(nx) , dx} = \\sqrt{\\pi}$$\n$$|\\sin(nx)| = \\sqrt{\\int_{-\\pi}^{\\pi} \\sin^2(nx) , dx} = \\sqrt{\\pi}$$ Then, we would have a standard orthonormal functions: $$\\frac{1}{\\sqrt{2\\pi}}, \\ldots, , \\frac{\\cos nx}{\\sqrt{\\pi}}, , \\frac{\\sin nx}{\\sqrt{\\pi}}, , \\ldots$$\nLetâ€™s define them:\n$$\\psi_0=\\frac{1}{\\sqrt{2\\pi}}, \\psi_j=\\frac{1}{\\sqrt{\\pi}}\\cos(jx), \\varphi_j=\\frac{1}{\\sqrt{\\pi}}\\sin(jx)$$\n$$\\psi_0\\equiv\\sqrt L\\quad \\psi_j\\equiv\\sqrt L\\cos\\left ( \\frac{j\\pi}{L}x \\right )\\quad \\varphi_j\\equiv\\sqrt L\\sin\\left ( \\frac{j\\pi}{L}x \\right) $$\nsuch that, we obtain the coefficient for each standard basis functions needed:\n$$\\begin{align} a_0 \u0026amp;= \\langle f,\\psi_0\\rangle=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\pi}^{\\pi} f(x) , dx\\a_j \u0026amp;=\\langle f,\\psi_k\\rangle=\\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\cos(jx) , dx\\ b_j \u0026amp;=\\langle f,\\varphi_k\\rangle =\\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\sin(kx) , dx \\end{align} $$\nThen, for the function $\\mathcal X$ and its coffecient $C_k$ defined as\n$$ \\mathcal X_k=( \\psi_k ,\\varphi_k ), \\quad C_k=(a_j,b_j) $$\nwe have a function space:\n$$ \\mathcal F(f)=\\sum_{k=0}^\\infty C_k \\cdot \\mathcal X_k $$\nå¦‚åŒ$\\vec{a} = \\sum_{i=1}^{n} a_i \\vec{e}_i \\quad$ä¸€æ ·ï¼Œæˆ‘ä»¬æœ‰$f(x) = a_0 \\frac{1}{\\sqrt{2\\pi}} + a_1 \\frac{\\cos x}{\\sqrt{\\pi}} + b_1 \\frac{\\sin x}{\\sqrt{\\pi}} + a_2 \\frac{\\cos 2x}{\\sqrt{\\pi}} + b_2 \\frac{\\sin 2x}{\\sqrt{\\pi}} + \\cdots$ï¼Œæˆ–è€…\n$$\nf(x) = a_0 \\frac{1}{\\sqrt{2\\pi}} + \\sum_{k=1}^{\\infty} a_k \\frac{\\cos kx}{\\sqrt{\\pi}} + \\sum_{k=1}^{\\infty} b_k \\frac{\\sin kx}{\\sqrt{\\pi}} $$\nä»£å…¥$a_0,a_k,b_k$åˆ°ä¸Šå¼ä¸­ï¼Œæˆ‘ä»¬è·å¾—ï¼š\n$$ f(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\pi}^{\\pi} f(x) , dx \\frac{1}{\\sqrt{2\\pi}} + \\sum_{k=1}^{\\infty} \\frac{\\cos kx}{\\sqrt{\\pi}}\\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\cos kx , dx + \\sum_{k=1}^{\\infty} \\frac{\\sin kx}{\\sqrt{\\pi}} \\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\sin kx , dx $$\nå½“æˆ‘ä»¬ç®€åŒ–ä¹‹åï¼Œå¹¶ä¸”å°†ç³»æ•°é¡¹æ›´æ–°ï¼Œæˆ‘ä»¬æœ€ç»ˆæœ‰äº†ç°åœ¨çš„å‚…é‡Œå¶å±•å¼€å¼ï¼š\n$$ f(x)=A_0 + \\sum_{n=1}^{\\infty}(A_n\\cos nx+ B_n\\sin nx) $$\n$$ \\begin{align} A_0 \u0026amp;= \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} f(x) , dx\\A_n \u0026amp;= \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\cos(nx) , dx\\ B_n \u0026amp;= \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\sin(nx) , dx \\end{align} $$\n"},{"id":62,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E5%9B%9B%E7%AB%A0/4.2-Hydrogen-Atom/","title":"æ°¢åŸå­","section":"ç¬¬å››ç« ","content":" 4.2.1 ç»å…¸å›¾åƒ (Classical Picture) # The hydrogen atom in classical physics consists of an electron orbiting around a proton. Let\u0026rsquo;s examine the key parameters:\nParticle Property Symbol Charge Value Unit Electron Mass $m_e$ - $9.1 \\times 10^{-31}$ kg Electron Charge $q_{electron}$ $-e$ $-1.6 \\times 10^{-19}$ C Proton Mass $M$ - $1.6 \\times 10^{-27}$ kg Proton Charge $q_{proton}$ $+e$ $+1.6 \\times 10^{-19}$ C The mass ratio $\\frac{M}{m_e} \\simeq 1800$, which means we can effectively treat the proton as motionless.\nç›¸äº’ä½œç”¨ (Interactions) # The hydrogen atom involves two fundamental interactions:\nElectromagnetic Gravitational The ratio of electric force to gravitational force is:\n$$\\frac{\\text{Electric force}}{\\text{Gravitational force}} = \\frac{F_e}{F_g} = \\frac{\\frac{e^2}{4\\pi\\varepsilon_0 r^2}}{\\frac{GM m_e}{r^2}} = \\frac{e^2}{4\\pi\\varepsilon_0} \\frac{1}{GM m_e} \\approx 10^{39}$$\nWhere: $\\frac{1}{4\\pi\\varepsilon_0} = 9 \\times 10^9 \\frac{\\text{N m}^2}{\\text{C}^2}$\n$G = 6.67 \\times 10^{-11} \\text{ m}^3 \\text{ kg}^{-1} \\text{ s}^{-2}$\nGiven this enormous ratio, gravitational effects can be safely ignored in atomic physics.\n4.2.2 åŸå­å¤§å°ä¼°è®¡ (Estimate of Typical Atom Size) # Using the uncertainty principle, we can estimate the size of the hydrogen atom.\nConsider an electron in a spherical cage of radius $a$:\nThe electrostatic interaction energy is:\n$$\\text{Electrostatic interaction} = -\\frac{e^2}{4\\pi\\varepsilon_0 a}$$\nThis energy varies as $\\frac{1}{a}$ and decreases with distance.\nThe kinetic energy due to quantum uncertainty is approximately:\n$$\\text{Kinetic energy} \\sim \\frac{(\\hbar/a)^2}{2m_e}$$\nThis energy varies as $\\frac{1}{a^2}$ and increases as the confinement gets tighter.\nThe total energy is the sum of these contributions:\n$$E(a) = \\frac{\\hbar^2}{2m_e a^2} - \\frac{e^2}{4\\pi\\varepsilon_0 a}$$\nAt equilibrium, the energy is minimized, so:\n$$E\u0026rsquo;(a) = -\\frac{\\hbar^2}{m_e a^3} + \\frac{e^2}{4\\pi\\varepsilon_0 a^2} = 0$$\nSolving for $a$, we get the Bohr radius:\n$$a_B = \\frac{4\\pi\\varepsilon_0 \\hbar^2}{m_e e^2} \\simeq 0.5 \\times 10^{-10} \\text{ m} = 0.5 \\text{ Ã…}$$\nThis $a_B$ is the natural length scale of the atom.\nInterestingly, the Bohr radius is about 100,000 times larger than the size of the proton ($\\sim 10^{-15} \\text{ m}$). To put this in perspective, if the proton were scaled to about 1 inch, then the typical size of the atom would be scaled to about 1.5 miles. This illustrates the vast \u0026ldquo;emptiness\u0026rdquo; within atoms.\n4.2.3 æ°¢åŸå­çš„ç»“åˆèƒ½ (Binding Energy of the H Atom) # Substituting $a_B$ into the energy expression $E(a)$, we can estimate the binding energy of the hydrogen atom:\n$$E_1 = \\frac{\\hbar^2}{2m_e a_B^2} - \\frac{e^2}{4\\pi\\varepsilon_0 a_B}$$\n$$\\frac{E_1}{a_B} = \\frac{\\hbar^2}{2m_e a_B^3} - \\frac{e^2}{4\\pi\\varepsilon_0 a_B^2} = \\frac{\\hbar^2}{2m_e a_B^3} - \\frac{\\hbar^2}{m_e a_B^3} = -\\frac{\\hbar^2}{2m_e a_B^3}$$\n$$E_1 = -\\frac{\\hbar^2}{2m_e a_B^2} = -\\frac{1}{2} \\frac{\\hbar^2}{m_e} \\left(\\frac{m_e e^2}{4\\pi\\varepsilon_0 \\hbar^2}\\right)^2 = -\\frac{m_e}{2\\hbar^2} \\left(\\frac{e^2}{4\\pi\\varepsilon_0}\\right)^2$$\nSubstituting the values of these parameters:\n$$E_1 = -13.6 \\text{ eV}$$\nThis means that to make the electron \u0026ldquo;break free\u0026rdquo; from the proton, an energy of approximately 13.6 eV must be supplied.\nIs this energy \u0026ldquo;big\u0026rdquo; or \u0026ldquo;small\u0026rdquo;? To gauge this, let\u0026rsquo;s compare it with the thermal energy $k_B T$ at room temperature.\nThe Boltzmann constant: $k_B = 8.617 \\times 10^{-5} \\text{ eV/K}$ (Boltzmann constant)\nAt room temperature ($T_{room} \\simeq 300 \\text{ K}$):\n$$k_B T_{room} \\simeq 25 \\times 10^{-3} \\text{ eV} = 25 \\text{ meV}$$\nSince $k_B T_{room} \\ll |E_1|$, the probability of unbinding the electron due to thermal excitation at room temperature is negligible.\n4.2.4 èƒ½çº§å’Œå…‰å­å‘å°„ (Energy Levels and Photon Emission) # Quantum mechanics shows that the hydrogen atom has discrete energy levels. These can be categorized as:\nContinuum states ($E \u0026gt; 0$): These represent unbound electrons. Bound states ($E \u0026lt; 0$): These are the discrete energy levels of the hydrogen atom. The bound states include:\nGround state ($E_1$): The lowest energy level Excited states ($E_2$, $E_3$, etc.): Higher energy levels When an electron transitions from a higher energy level to a lower one, a photon is emitted: $E_m - E_n = E_\\gamma$\nThis is known as photon absorption (when the atom absorbs energy) or photon emission (when the atom releases energy).\nAccording to the Einstein-Planck relation:\n$$E_\\gamma = h\\nu = \\frac{hc}{\\lambda}$$\nwhere: $h\\nu = \\Delta E, \\quad \\lambda = \\frac{hc}{\\Delta E}$\nGiven: $h = 4.13 \\times 10^{-15} \\text{ eV} \\cdot \\text{sec}$ $c = 3 \\times 10^8 \\text{ m/sec}$ $hc \\simeq 1.24 \\times 10^{-6} \\text{ eV} \\cdot \\text{m} = 1.24 \\times 10^3 \\text{ eV} \\cdot \\text{nm} \\simeq 1239 \\text{ eV} \\cdot \\text{nm}$\nFor the hydrogen atom, with $\\Delta E = 13.6 \\text{ eV}$, the photon wavelength is: $\\lambda \\simeq 91 \\text{ nm}$ (UV light)\n4.2.5 ç›¸å¯¹è®ºæ•ˆåº”æ˜¯å¦é‡è¦? (Are Relativistic Effects Important?) # The energy of a free relativistic particle of mass $m$ and momentum $\\vec{p}$ is:\n$$E = \\sqrt{(\\vec{p}c)^2 + (mc^2)^2} = mc^2 \\left(1 + \\left(\\frac{pc}{mc^2}\\right)^2\\right)^{1/2}$$\nUsing the binomial expansion:\n$$E \\simeq mc^2 \\left[1 + \\frac{1}{2}\\left(\\frac{pc}{mc^2}\\right)^2 + \\ldots\\right] = mc^2 + \\frac{p^2}{2m} + \\text{relativistic corrections}$$\nWhen $\\frac{p^2}{2m_e} \\ll mc_e^2$, the non-relativistic treatment is valid.\nLet\u0026rsquo;s check this for the electron in hydrogen: $m_e c^2 = 9.1 \\times 10^{-31} \\text{ kg} \\times (3 \\times 10^8 \\text{ m/s})^2 \\simeq 0.5 \\times 10^6 \\text{ eV} = 0.5 \\text{ MeV}$\nSince this is much larger than the few eV scale of atomic energies, the non-relativistic approach is justified.\n4.2.6 è–›å®šè°”æ–¹ç¨‹ (SchrÃ¶dinger Equation) # All in all, the effective theory is described by the non-relativistic SchrÃ¶dinger equation:\n$$\\left(-\\frac{\\hbar^2}{2m_e} \\nabla^2 - \\frac{e^2}{4\\pi\\varepsilon_0 r}\\right) \\Psi(\\vec{r}) = E \\Psi(\\vec{r})$$\nWith rotational symmetry, we can write: $\\Psi(r,\\theta,\\phi) = R(r)Y_\\ell^m(\\theta,\\phi)$\nThe radial equation, with $u(r) = r R(r)$, becomes:\n$$-\\frac{\\hbar^2}{2m_e} \\frac{d^2u}{dr^2} + \\left[-\\frac{e^2}{4\\pi\\varepsilon_0 r} + \\frac{\\hbar^2 \\ell(\\ell+1)}{2m_e r^2}\\right]u = Eu$$\nWe will solve this equation for $E \u0026lt; 0$ to determine the bound states of the hydrogen atom.\n"},{"id":63,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1-Computing-Integrals/","title":"ç§¯åˆ†è®¡ç®—","section":"ç¬¬ä¹ç« ","content":" Chapter 9: Computing Integrals # 9.1.1 Introduction # In practice, how do we compute integral $\\int_A f(x)dx$?\nIn $\\mathbb{R}^1$ (one-dimensional space): $$\\int_a^b f(x)dx = F(x)|_a^b = F(b) - F(a)$$\nFTC (Fundamental Theorem of Calculus) In $\\mathbb{R}^n$ (n-dimensional space):\nReduce to $\\mathbb{R}^1$ case by Fubini\u0026rsquo;s Theorem Change of Variables (Substitution) first 9.1.2 Fubini\u0026rsquo;s Theorem # 1. Statement of Main Result # Theorem 1: Let $A = {(x,y): a \\leq x \\leq b, c \\leq y \\leq d}$ be a rectangle in $\\mathbb{R}^2$ and $f: A \\to \\mathbb{R}$ be integrable. Suppose, for each $x \\in [a,b]$, the following integral exists: $$g(x) = \\int_c^d f(x,y)dy$$\nThen $g(x)$ is integrable on $[a,b]$ and $\\int_A f = \\int_a^b \\left(\\int_c^d f(x,y)dy\\right)dx = \\int_a^b g(x)dx$\n9.1.3 Corollaries # Corollary 1: If $f: A \\to \\mathbb{R}$ is continuous, then $$\\int_A f = \\int_a^b \\left(\\int_c^d f(x,y)dy\\right)dx = \\int_c^d \\left(\\int_a^b f(x,y)dx\\right)dy$$\nThis shows the symmetry of the double integral - we can integrate first with respect to $y$ and then with respect to $x$, or vice versa.\nCorollary 2: Let $A$ be a region given by $A = {(x,y): a \\leq x \\leq b, \\varphi(x) \\leq y \\leq \\psi(x)}$ where $\\varphi(x)$ and $\\psi(x)$ are continuous functions. If $f: A \\to \\mathbb{R}$ is continuous, then $\\int_A f = \\int_a^b \\left(\\int_{\\varphi(x)}^{\\psi(x)} f(x,y)dy\\right)dx$\n[Note: The image shows a graphical representation of region $A$ bounded by $y = \\varphi(x)$ below and $y = \\psi(x)$ above, with $x$ ranging from $a$ to $b$.]\nRemarks # Roles of $x$ and $y$ can be interchanged Results true in higher dimensions\nLet $C = A \\times B \\subset \\mathbb{R}^{n+m}$ where $A \\subset \\mathbb{R}^n$, $B \\subset \\mathbb{R}^m$ [The image shows a diagram of the Cartesian product $A \\times B$ as a rectangle in a coordinate system with axes labeled $\\mathbb{R}^n$ and $\\mathbb{R}^m$] Then: $\\int_{A \\times B} f = \\int_A \\left(\\int_B f(x,y)dy\\right)dx$\n9.1.4 Example: Computing a Double Integral # Problem # Compute $\\int_A (x+y) , dxdy$\nWhere $A$ is a triangle in the first quadrant bounded by the lines:\n$x = 0$ $y = 0$ $x + y = 1$ Solution # Using Fubini\u0026rsquo;s Theorem, we can compute this double integral as an iterated integral:\n$$\\int_A (x+y) , dxdy = \\int_0^1 \\left(\\int_0^{1-x} (x+y) , dy\\right) , dx$$\nFirst, we evaluate the inner integral with respect to $y$:\n$$\\int_0^{1-x} (x+y) , dy = \\left[xy + \\frac{y^2}{2}\\right]_{y=0}^{y=1-x}$$\n$$= x(1-x) + \\frac{(1-x)^2}{2} - \\left(0 + 0\\right)$$\n$$= x - x^2 + \\frac{1 - 2x + x^2}{2}$$\n$$= x - x^2 + \\frac{1}{2} - x + \\frac{x^2}{2}$$\n$$= \\frac{1}{2} - \\frac{x^2}{2}$$\nNow we evaluate the outer integral with respect to $x$:\n$$\\int_0^1 \\left(\\frac{1}{2} - \\frac{x^2}{2}\\right) , dx = \\frac{1}{2}\\int_0^1 (1 - x^2) , dx$$\n$$= \\frac{1}{2}\\left[x - \\frac{x^3}{3}\\right]_0^1$$\n$$= \\frac{1}{2}\\left(1 - \\frac{1}{3} - 0\\right)$$\n$$= \\frac{1}{2} \\cdot \\frac{2}{3} = \\frac{1}{3}$$\nTherefore, $\\int_A (x+y) , dxdy = \\frac{1}{3}$\nNote: The calculation in the original blackboard image showed a final result of $\\frac{1}{2}$, but the correct answer is $\\frac{1}{3}$ as demonstrated in the steps above.\n9.1.5 Proof of Theorem 1 # I\u0026rsquo;ll write out a concise proof for just the part shown in the image:\n3. Proof of Theorem 1 # Let $g(x) = \\int_c^d f(x,y)dy$\nWe need to show:\n$g$ is integrable on $[a,b]$ $\\int_a^b g(x)dx = \\int_A f$ We will compare upper and lower sums of $f$ and $g$.\nFix any partition $P_A$ of $A$. We can write $P_A = {S_{ij}}$ where $S_{ij} = V_i \\times W_j$ represents rectangular cells in the partition.\nThen $P_A$ induces:\nA partition of $[a,b]$: $P_{[a,b]} = {V_i}$ A partition of $[c,d]$: $P_{[c,d]} = {W_j}$ For each cell $S_{ij}$, we define:\n$M_{ij} = \\sup{f(x,y): (x,y) \\in S_{ij}}$ $m_{ij} = \\inf{f(x,y): (x,y) \\in S_{ij}}$ The upper and lower sums for $f$ over partition $P_A$ are:\n$U(f, P_A) = \\sum_{i,j} M_{ij}|V_i||W_j|$ $L(f, P_A) = \\sum_{i,j} m_{ij}|V_i||W_j|$ When we consider the integrable function $g(x)$, we can establish that: $L(f, P_A) \\leq \\int_a^b g(x)dx \\leq U(f, P_A)$\nAs we refine the partition, the upper and lower sums converge, proving that $g$ is integrable on $[a,b]$ and that $\\int_a^b g(x)dx = \\int_A f$.\nI\u0026rsquo;ll continue the proof based on the additional image:\nI\u0026rsquo;ll rewrite the proof using proper display math formatting with $$ delimiters:\nNext, examine the lower sum $L(f, P_A)$:\n$$L(f, P_A) = \\sum_{i,j} m_{ij}(f) \\cdot V(S_{ij})$$\n$$= \\sum_{i,j} m_{ij}(f) \\cdot V(V_i) \\cdot V(W_j)$$\nWhere $m_{ij}(f) = \\inf{f(x,y): (x,y) \\in S_{ij}}$\nKey Observation: $$\\inf{f(x,y): (x,y) \\in V_i \\times W_j} \\leq \\inf{f(x,y): y \\in W_j} \\text{ for all } x \\in V_i$$ $$= m_j(f, x)$$\nThen for any $x \\in [a,b]$, we have: $$\\sum_j m_j(f) \\cdot V(W_j) \\leq \\sum_j m_j(f,x) \\cdot V(W_j)$$\nThis is the lower sum of $f(x,y)$ in the variable $y$ with partition $P_{[c,d]}$: $$= L(f(x,Â·), P_{[c,d]})$$ $$\\leq \\int_c^d f(x,y)dy = g(x) \\text{ for all } x$$\nThus: $$\\sum_i \\left(\\sum_j m_j(f) \\cdot V(W_j)\\right) \\cdot V(V_i) \\leq \\sum_i \\inf(g(x)) \\cdot V(V_i)$$\ni.e., $$\\sum_{i,j} m_{ij}(f) \\cdot V(W_j) \\cdot V(V_i) \\leq \\sum_i (\\inf g(x)) \\cdot V(V_i)$$\nTherefore: $$L(f, P_A) \\leq L(g, P_{[a,b]})$$\nSimilarly, we have:\n$$U(f, P_A) \\geq U(g, P_{[a,b]})$$\nThus we have:\n$$L(f, P_A) \\leq L(g, P_{[a,b]}) \\leq U(g, P_{[a,b]}) \\leq U(f, P_A)$$\nBy Riemann\u0026rsquo;s criterion, if $f$ is integrable on $A$, then:\n$g$ is integrable on $[a,b]$, and $$\\int_A f = \\int_a^b g(x)dx$$ This completes the proof of Fubini\u0026rsquo;s Theorem, showing that we can compute a double integral by first integrating with respect to one variable and then with respect to the other.\n"}]