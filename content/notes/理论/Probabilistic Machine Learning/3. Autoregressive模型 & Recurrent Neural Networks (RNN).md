---
date: 2025-09-08
lecture: "3"
---
# Autoregressive Model (Continue)
An AR (autoregressive) generative model represents the **joint probability of a sequence** by chaining conditionals left-to-right (the chain rule
$$P\left(w_{1: T}\right)=\prod_{t=1}^T P\left(w_t \mid w_{<t}\right)$$
## 1. Decoding / Inference in Language Models

- local higher probability do not always produce the global solution.
- we care about the joint probability
### 1. 1. Argmax (Greedy Decoding)
Choose the most likely token:
$$
w_t = \arg\max_{w} p_\theta(w \mid w_{<t})
$$
- Deterministic, always picks the top token.  
- Can lead to repetitive or dull text.
### Beam search

# 2 Unrolled recurrent neural network

## 2.1 Setup & notation (shapes first)

- Inputs: $\mathbf{x}_t\in\mathbb{R}^{p}$  
- Hidden states: $\mathbf{h}_t\in\mathbb{R}^{H}$  
- Logits / probabilities / targets: $\mathbf{z}_t,\mathbf{p}_t,\mathbf{y}_{t+1}\in\mathbb{R}^{R}$  
- Parameters: 
  - $W_{xh}\in\mathbb{R}^{H\times p}$, $W_{hh}\in\mathbb{R}^{H\times H}$, $\mathbf{b}_h\in\mathbb{R}^{H}$  
  - $W_{ho}\in\mathbb{R}^{R\times H}$, $\mathbf{b}_o\in\mathbb{R}^{R}$  
- Nonlinearity $f:\mathbb{R}^{H}\to\mathbb{R}^{H}$ (elementwise), with derivative $f'$.

> **Shape cue** used below: after each line, brackets like `[H×p]` or `[H]` indicate resulting shapes where helpful.

## 2.2 Forward definitions

**Hidden pre-activation, hidden state, logits, softmax:**
$$
\begin{aligned}
\mathbf{a}_t &= W_{xh}\,\mathbf{x}_t \;+\; W_{hh}\,\mathbf{h}_{t-1} \;+\; \mathbf{b}_h 
&&[\;H\times p\;\cdot\;p \to H\;+\;H\times H\cdot H \to H\;+\;H\;=\;H\;] \\
\mathbf{h}_t &= f(\mathbf{a}_t) &&[\;H\;] \\
\mathbf{z}_t &= W_{ho}\,\mathbf{h}_t \;+\; \mathbf{b}_o &&[\;R\times H\cdot H \to R\;+\;R\;=\;R\;] \\
\mathbf{p}_t &= \operatorname{softmax}(\mathbf{z}_t),\quad
(\mathbf{p}_t)_i = \dfrac{e^{z_{t,i}}}{\sum_{j=1}^{R} e^{z_{t,j}}}
&&[\;\mathbf{p}_t\in\Delta^{R-1}\;]
\end{aligned}
$$

> **Observation model:** $P(X_{t+1}\mid X_{\le t})=\mathbf{p}_t=\operatorname{softmax}(W_{ho}\mathbf{h}_t+\mathbf{b}_o)$.

## 2.3 Sequence loss = sum of per-time cross-entropies

$$
\begin{aligned}
\mathcal{L} 
&= \sum_{t=1}^{T} \ell_t,
\qquad
\ell_t \;=\; -\,\mathbf{y}_{t+1}^{\!\top}\log \mathbf{p}_t
\;=\; -\sum_{i=1}^{R} y_{t+1,i}\,\log p_{t,i}.
\end{aligned}
$$

If $\mathbf{y}_{t+1}$ is one-hot ($\mathbf{y}_{t+1}=e_{k_t}$), then
$$
\ell_t = -\log p_{t,k_t}\quad\text{(only the true class contributes)}.
$$

---

## Step 3. Softmax + cross-entropy gradient w.r.t. logits (local output node)

**Derivation (math only):**
$$
\begin{aligned}
&\text{softmax: } p_{t,i}=\frac{e^{z_{t,i}}}{\sum_j e^{z_{t,j}}}, 
\quad 
\frac{\partial p_{t,i}}{\partial z_{t,j}} = p_{t,i}\left(\delta_{ij}-p_{t,j}\right) \\
&\text{CE: } \ell_t = -\sum_i y_{t+1,i}\log p_{t,i}
\;\Rightarrow\;
\frac{\partial \ell_t}{\partial p_{t,i}} = -\frac{y_{t+1,i}}{p_{t,i}} \\
&\text{chain: }
\frac{\partial \ell_t}{\partial z_{t,j}}
= \sum_i \frac{\partial \ell_t}{\partial p_{t,i}} \frac{\partial p_{t,i}}{\partial z_{t,j}}
= \sum_i \left(-\frac{y_{t+1,i}}{p_{t,i}}\right) p_{t,i}(\delta_{ij}-p_{t,j}) \\
&\qquad\quad= -y_{t+1,j} + \Big(\sum_i y_{t+1,i}\Big)p_{t,j}
= -y_{t+1,j} + p_{t,j}
\;\Rightarrow\;
\boxed{\nabla_{\mathbf{z}_t}\ell_t = \mathbf{p}_t - \mathbf{y}_{t+1}} \\
&\text{and since }\mathbf{z}_t=W_{ho}\mathbf{h}_t+\mathbf{b}_o,\;
\boxed{\;\nabla_{\mathbf{h}_t}\ell_t = W_{ho}^{\!\top}\,(\mathbf{p}_t-\mathbf{y}_{t+1})\;}
\end{aligned}
$$

> **Note:** The “$p - y$” result is independent of which class is correct; with one-hot labels it reduces to subtracting 1 at the true index.

---

## Step 4. Transition Jacobian of the hidden dynamics

Define $D_t:=\operatorname{diag}\!\big(f'(\mathbf{a}_t)\big)\in\mathbb{R}^{H\times H}$.

**Derivation (math only):**
$$
\begin{aligned}
\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}}
&= 
\frac{\partial \mathbf{h}_t}{\partial \mathbf{a}_t}
\cdot
\frac{\partial \mathbf{a}_t}{\partial \mathbf{h}_{t-1}}
=
D_t \cdot W_{hh}
\;\; \Rightarrow\;\;
\boxed{J_t := \frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}} = D_t\,W_{hh}} \\
&\text{so } J_t^{\!\top}=W_{hh}^{\!\top}D_t.
\end{aligned}
$$

> **Shape cue:** $D_t[H\times H] \cdot W_{hh}[H\times H] \Rightarrow J_t[H\times H]$.

---

## Step 5. Backpropagation through time (BPTT): local + future error

Let $\mathbf{s}_t := \nabla_{\mathbf{h}_t}\mathcal{L}$ be the **total** gradient on the hidden at time $t$, and define the **local** term   
$\boldsymbol{\delta}_t := \nabla_{\mathbf{h}_t}\ell_t = W_{ho}^{\!\top}(\mathbf{p}_t-\mathbf{y}_{t+1})$.

**Derivation (math only):**
$$
\begin{aligned}
\text{future pullback: } 
&\mathbf{s}_{t+1} \xrightarrow{\text{chain via } \mathbf{h}_{t+1}=f(\cdots \mathbf{h}_t)}
\frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t}^{\!\top}\,\mathbf{s}_{t+1}
= J_{t+1}^{\!\top}\,\mathbf{s}_{t+1} \\
\text{combine: } 
&\boxed{\;\mathbf{s}_t = \boldsymbol{\delta}_t \;+\; J_{t+1}^{\!\top}\,\mathbf{s}_{t+1}\;}
\end{aligned}
$$

> **Reading cue:** “total at $t$ = local at $t$ + future error back-propagated one step”.

---

## Step 6. Parameter gradients (outer-product form)

Let $\mathbf{r}_t := D_t\,\mathbf{s}_t = f'(\mathbf{a}_t)\odot \mathbf{s}_t$ (elementwise product).

**Derivation (math only):**
$$
\begin{aligned}
\text{Output layer: } \quad
&\nabla_{W_{ho}}\mathcal{L} = \sum_{t=1}^{T} (\mathbf{p}_t-\mathbf{y}_{t+1})\,\mathbf{h}_t^{\!\top},
\qquad
\nabla_{\mathbf{b}_o}\mathcal{L} = \sum_{t=1}^{T} (\mathbf{p}_t-\mathbf{y}_{t+1}) \\
\text{Hidden update: } \quad
&\nabla_{W_{xh}}\mathcal{L} = \sum_{t=1}^{T} \mathbf{r}_t\,\mathbf{x}_t^{\!\top},
\qquad
\nabla_{W_{hh}}\mathcal{L} = \sum_{t=1}^{T} \mathbf{r}_t\,\mathbf{h}_{t-1}^{\!\top},
\qquad
\nabla_{\mathbf{b}_h}\mathcal{L} = \sum_{t=1}^{T} \mathbf{r}_t
\end{aligned}
$$

> **Outer-product cue:** each linear map’s gradient is “upstream vector” $\times$ “its input”$^{\top}$.

---

## Step 7. Put the boxed pieces together (elementwise → general)

Collecting the key boxed results used repeatedly:

$$
\begin{aligned}
\text{(i)}\;&\nabla_{\mathbf{z}_t}\ell_t \;=\; \mathbf{p}_t - \mathbf{y}_{t+1} \\
\text{(ii)}\;&\boldsymbol{\delta}_t \;=\; W_{ho}^{\!\top}(\mathbf{p}_t-\mathbf{y}_{t+1}) \\
\text{(iii)}\;&J_t \;=\; D_t\,W_{hh},\quad J_t^{\!\top}=W_{hh}^{\!\top}D_t \\
\text{(iv)}\;&\mathbf{s}_t \;=\; \boldsymbol{\delta}_t \;+\; J_{t+1}^{\!\top}\,\mathbf{s}_{t+1} \\
\text{(v)}\;&\mathbf{r}_t \;=\; D_t\,\mathbf{s}_t \;=\; f'(\mathbf{a}_t)\odot \mathbf{s}_t \\
\text{(vi)}\;&\nabla_{W_{ho}}\mathcal{L} = \sum_t (\mathbf{p}_t-\mathbf{y}_{t+1})\,\mathbf{h}_t^{\!\top},\quad
\nabla_{\mathbf{b}_o}\mathcal{L} = \sum_t (\mathbf{p}_t-\mathbf{y}_{t+1}) \\
\text{(vii)}\;&\nabla_{W_{xh}}\mathcal{L} = \sum_t \mathbf{r}_t\,\mathbf{x}_t^{\!\top},\quad
\nabla_{W_{hh}}\mathcal{L} = \sum_t \mathbf{r}_t\,\mathbf{h}_{t-1}^{\!\top},\quad
\nabla_{\mathbf{b}_h}\mathcal{L} = \sum_t \mathbf{r}_t
\end{aligned}
$$

---

## (Optional) One-line forward summary (for the top of a slide)

$$
\begin{aligned}
\mathbf{a}_t &= W_{xh}\mathbf{x}_t + W_{hh}\mathbf{h}_{t-1} + \mathbf{b}_h,
\quad
\mathbf{h}_t = f(\mathbf{a}_t), \quad
\mathbf{z}_t = W_{ho}\mathbf{h}_t + \mathbf{b}_o, \quad
\mathbf{p}_t = \mathrm{softmax}(\mathbf{z}_t).
\end{aligned}
$$

