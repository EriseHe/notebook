---
lecture:
date:
title: Einstein求和约定
---
# Dot product
##  1.1 Set-up (space, basis, components)

Let $e=\{e_1,\dots,e_n\}$ be the **standard orthonormal basis**  in $\mathbb{R}^n$.  
### 1.1.1 Orthonormality 
$$
e_i\cdot e_j=\delta_{ij}\quad\text{where}\quad
\delta_{ij}=\begin{cases}1,& i=j\\[2pt]0,& i\neq j.\end{cases}
$$
### 1.1.2 Vectors
Every vector ${} a\in \mathbb{R}^n$ can be written as a linear combination of the basis
$$a=c_1 e_1+c_2 e_2+\cdots+c_n e_n=\sum_{i=1}^n c_i e_i$$
for some *unique* scalar $c_{i}$. 


In other words:

$$
a=\begin{bmatrix}a_1\\a_2\\\vdots\\a_n\end{bmatrix}=\sum_{i=1}^{n} a_{i}e_{i}
$$

and as basis expansions,

$$
a=\sum_{i=1}^n a_i\,e_i,\qquad b=\sum_{j=1}^n b_j\,e_j.
$$


$$
\begin{aligned}
a\cdot b &= a^\top b = \begin{bmatrix}a_1 & a_2 & \cdots & a_n\end{bmatrix}
\begin{bmatrix}b_1\\ b_2\\ \vdots\\ b_n\end{bmatrix} \\
&= a_1 b_1 + a_2 b_2 + \cdots + a_n b_n \\
&= \sum_{i=1}^n a_i b_i \\
&= \Big(\sum_{i=1}^n a_i e_i\Big)\!\cdot\!\Big(\sum_{j=1}^n b_j e_j\Big) \\
&= \sum_{i=1}^n\sum_{j=1}^n a_i b_j(e_i\cdot e_j) \\
&= \sum_{i=1}^n\sum_{j=1}^n a_i b_j\delta_{ij} \\
&= \sum_{i=1}^n a_i b_i \\
&= a_i b_i \qquad\text{(Einstein)}
\end{aligned}
$$

## Sanity check (n=3, fully expanded)

$$
[a_1,a_2,a_3]\cdot[b_1,b_2,b_3]
\;=\; a_1b_1+a_2b_2+a_3b_3
\;=\; a_i b_i\quad(\text{Einstein}).
$$

Also, the squared norm is just a special case:

$$
\|a\|^2=a\cdot a=\sum_{i=1}^n a_i a_i\;=\;a_i a_i.
$$

---

**Your turn (one quick check):**
Does every step in the derivation from

$$
a=\sum_i a_i e_i,\quad b=\sum_j b_j e_j
\quad\Longrightarrow\quad
a\cdot b=\sum_i a_i b_i
$$

make sense so far? If yes, I’ll do **matrix–vector** next with the same level of detail.
