{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 3: Recurrent and Masking-Based Autoregressive Models\n",
        "\n",
        "## Recap: Neural Autoregressive Models\n",
        "\n",
        "**Factorization of joint distribution:**\n",
        "\n",
        "$$\n",
        "P(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^T P(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "**How to compute conditionals with neural nets?**\n",
        "1. Process context  \n",
        "   - RNNs (sequential)  \n",
        "   - Masking-based models (parallel)  \n",
        "2. Generate probability distribution for next token  \n",
        "\n",
        "**Example:**\n",
        "\"What do pigs __\"\n",
        "\n",
        "Predict:\n",
        "$$\n",
        "P(\\text{eat} \\mid \\text{\"What do pigs\"})\n",
        "$$\n"
      ],
      "metadata": {
        "id": "DCNTslhn303I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks (RNNs)\n",
        "\n",
        "\n",
        "![RNN](https://aiml.com/wp-content/uploads/2023/10/RNN-Language-model1-1024x704.png)\n",
        "\n",
        "**Hidden state recurrence:**\n",
        "\n",
        "$$\n",
        "h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)\n",
        "$$\n",
        "$$\n",
        "P(x_{t+1} \\mid x_{\\le t}) = \\text{softmax}(W_{ho}h_t + b_o)\n",
        "$$\n",
        "\n",
        "- $h_t$: hidden state at time $t$  \n",
        "- $f$: nonlinearity (tanh, ReLU)  \n",
        "\n",
        "**Strengths**\n",
        "- Fits sequential data naturally  \n",
        "- Parameter sharing across time  \n",
        "- Compact memory footprint  \n",
        "\n",
        "**Weaknesses**\n",
        "- Sequential training (slow)  \n",
        "- Vanishing/exploding gradients  \n",
        "- Poor long-range memory  \n"
      ],
      "metadata": {
        "id": "Uggr4wcz4ERJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vanishing and exploding gradient problem is typically analyzed through the hidden-state gradients  \n",
        "\n",
        "$$\n",
        "\\delta_t = \\frac{\\partial L_T}{\\partial h_t},\n",
        "$$\n",
        "\n",
        "since they are the recursive quantities that accumulate products of Jacobians across time.  \n",
        "The parameter gradients for the recurrent and input weights are directly proportional to these signals.\n",
        "\n",
        "---\n",
        "\n",
        "#### Derivation\n",
        "\n",
        "Hidden pre-activation:\n",
        "$$\n",
        "a_t = W_{xh}x_t + W_{hh}h_{t-1} + b_h, \\quad h_t = f(a_t).\n",
        "$$\n",
        "\n",
        "Gradient w.r.t. $W_{hh}$:\n",
        "$$\n",
        "\\frac{\\partial L_T}{\\partial W_{hh}}\n",
        "= \\sum_{t=1}^T \\frac{\\partial L_T}{\\partial a_t}\\;\\frac{\\partial a_t}{\\partial W_{hh}}.\n",
        "$$\n",
        "\n",
        "- **First factor $\\frac{\\partial L_T}{\\partial a_t}$:**  \n",
        "  Recall that $h_t = f(a_t)$. By the chain rule,  \n",
        "  $$\n",
        "  \\frac{\\partial L_T}{\\partial a_t}\n",
        "  \\;=\\;  \\frac{\\partial L_T}{\\partial h_t}\\frac{\\partial h_t}{\\partial a_t}\n",
        "  \\;=\\; \\delta_t^{\\top}D_t,\n",
        "  $$\n",
        "  where\n",
        "  $$\n",
        "  D_t = \\mathrm{diag}\\!\\big(f'(a_t)\\big)\n",
        "  $$\n",
        "  is the diagonal Jacobian of the elementwise nonlinearity $f$.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "- **Second factor $\\frac{\\partial a_t}{\\partial W_{hh}}$:**  \n",
        "  The pre-activation is\n",
        "  $$\n",
        "  a_t = W_{xh}x_t + W_{hh}h_{t-1} + b_h.\n",
        "  $$\n",
        "$$\n",
        "\\frac{\\partial L_T}{\\partial W_{hh}}\n",
        "= \\sum_{t=1}^T (D_t \\delta_t)\\, h_{t-1}^\\top.\n",
        "$$\n",
        "\n",
        "Similarly, for the input weights $W_{xh}$,\n",
        "$$\n",
        "\\frac{\\partial L_T}{\\partial W_{xh}}\n",
        "= \\sum_{t=1}^T (D_t \\delta_t)\\, x_t^\\top.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Interpretation\n",
        "\n",
        "- If $\\delta_t$ **vanishes**, the parameter gradients also vanish → the RNN fails to learn long-term dependencies.  \n",
        "- If $\\delta_t$ **explodes**, the parameter gradients explode → training becomes unstable.  \n",
        "- The output-layer gradients (e.g. for $W_{ho}$) do not involve long Jacobian chains and remain well-behaved.  \n",
        "\n",
        "Hence, analyzing $\\delta_t$ suffices to understand how vanishing and exploding gradients propagate to recurrent parameter updates."
      ],
      "metadata": {
        "id": "05drioIeLeV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why the result is an outer product (tensor → matrix via contraction)\n",
        "\n",
        "Let  \n",
        "$$\n",
        "a_t = W_{xh}x_t + W_{hh}h_{t-1} + b_h \\in \\mathbb{R}^H,\\quad\n",
        "h_{t-1}\\in\\mathbb{R}^H.\n",
        "$$  \n",
        "We want $\\frac{\\partial L_T}{\\partial W_{hh}}\\in\\mathbb{R}^{H\\times H}$.\n",
        "\n",
        "---\n",
        "\n",
        "#### Elementwise derivation\n",
        "\n",
        "Write components:  \n",
        "$$\n",
        "(a_t)_i = \\sum_{j=1}^H (W_{hh})_{ij}\\,(h_{t-1})_j + \\cdots\n",
        "$$  \n",
        "Then  \n",
        "$$\n",
        "\\frac{\\partial (a_t)_i}{\\partial (W_{hh})_{pq}}\n",
        "= \\mathbf{1}_{\\{i=p\\}}\\,(h_{t-1})_q.\n",
        "$$  \n",
        "Chain rule:  \n",
        "$$\n",
        "\\frac{\\partial L_T}{\\partial (W_{hh})_{pq}}\n",
        "= \\sum_{i=1}^H \\frac{\\partial L_T}{\\partial (a_t)_i}\\,\n",
        "   \\frac{\\partial (a_t)_i}{\\partial (W_{hh})_{pq}}\n",
        "= \\left(\\frac{\\partial L_T}{\\partial a_t}\\right)_p\\,(h_{t-1})_q.\n",
        "$$  \n",
        "Using $\\frac{\\partial L_T}{\\partial a_t}=D_t\\delta_t$, we get  \n",
        "$$\n",
        "\\frac{\\partial L_T}{\\partial (W_{hh})_{pq}}\n",
        "= (D_t\\delta_t)_p\\,(h_{t-1})_q.\n",
        "$$  \n",
        "Stacking all $(p,q)$ yields the **outer product**:  \n",
        "$$\n",
        "\\boxed{\\;\n",
        "\\frac{\\partial L_T}{\\partial W_{hh}}\n",
        "= (D_t\\delta_t)\\,h_{t-1}^\\top\n",
        "\\;}\n",
        "$$  \n",
        "Summing over time steps gives  \n",
        "$$\n",
        "\\boxed{\\;\n",
        "\\frac{\\partial L_T}{\\partial W_{hh}}\n",
        "= \\sum_{t=1}^T (D_t\\delta_t)\\,h_{t-1}^\\top\n",
        "\\;}\n",
        "$$  \n",
        "\n",
        "---\n",
        "\n",
        "**Key idea:** $\\frac{\\partial a_t}{\\partial W_{hh}}$ is a tensor, but in reverse-mode AD it is **immediately contracted** with the upstream vector $ \\frac{\\partial L_T}{\\partial a_t} $, producing the matrix outer product $(D_t\\delta_t)\\,h_{t-1}^\\top$.\n"
      ],
      "metadata": {
        "id": "_Ya1nPVDY8LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation Through Time (BPTT)\n",
        "\n",
        "**Training objective:**\n",
        "\n",
        "$$\n",
        "{L} = - \\frac{1}{T}\\sum_{t=1}^T \\log P(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "**Unroll RNN across time:**\n",
        "- Looks like a deep feedforward net of depth \\(T\\).  \n",
        "- Apply backprop through the unrolled graph.  \n",
        "\n",
        "**Challenges**\n",
        "- Gradients flow across many steps.  \n",
        "- Leads to vanishing/exploding gradients.  \n",
        "\n",
        "**Tricks**\n",
        "- Gradient clipping (for exploding)  \n",
        "- Truncated BPTT (limit to last \\(k\\) steps)  \n",
        "- Gated RNNs (LSTM/GRU)  \n"
      ],
      "metadata": {
        "id": "BZhDyQg95VYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanishing & Exploding Gradients\n",
        "\n",
        "### Conventions\n",
        "- **Gradient (column vector):** for a scalar $f:\\mathbb{R}^n\\!\\to\\!\\mathbb{R}$,\n",
        "  $$\n",
        "  \\nabla_x f \\;\\equiv\\; \\frac{\\partial f}{\\partial x}\\in\\mathbb{R}^{n}.\n",
        "  $$\n",
        "- **Jacobian (matrix):** for a vector $y:\\mathbb{R}^n\\!\\to\\!\\mathbb{R}^m$,\n",
        "  $$\n",
        "  J_y(x)\\;\\equiv\\;\\frac{\\partial y}{\\partial x}\\in\\mathbb{R}^{m\\times n}.\n",
        "  $$\n",
        "We will **only** call something a Jacobian if its output is a **matrix**. All derivatives of a scalar loss are **gradients (vectors)**.\n",
        "\n",
        "---\n",
        "\n",
        "## Setup\n",
        "\n",
        "**Hidden state recurrence**\n",
        "$$\n",
        "h_t \\;=\\; f\\!\\big(W_{xh}x_t + W_{hh}h_{t-1} + b_h\\big)\n",
        "$$\n",
        "\n",
        "**Softmax head**\n",
        "$$\n",
        "P(x_{t+1}\\mid x_{\\le t}) \\;=\\; \\mathrm{softmax}(W_{ho}h_t+b_o)\n",
        "$$\n",
        "\n",
        "Total loss up to time $T$:\n",
        "$$\n",
        "L_T \\;=\\; \\frac{1}{T}\\sum_{k=1}^T \\ell_k,\\qquad\n",
        "\\ell_k=\\mathrm{CE}\\!\\big(\\mathrm{softmax}(W_{ho}h_k+b_o),\\;x_{k+1}\\big).\n",
        "$$\n",
        "\n",
        "**Backprop signal (gradient)**\n",
        "$$\n",
        "\\delta_t \\;\\equiv\\; \\frac{\\partial L_T}{\\partial h_t}\\in\\mathbb{R}^{H}\\quad\\text{(a column gradient vector)}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Local quantities\n",
        "\n",
        "Let\n",
        "$$\n",
        "a_t \\;\\equiv\\; W_{xh}x_t + W_{hh}h_{t-1} + b_h, \\quad h_t=f(a_t).\n",
        "$$\n",
        "\n",
        "- **Jacobian of the transition** (matrix):\n",
        "  $$\n",
        "  J_t \\;\\equiv\\; \\frac{\\partial h_t}{\\partial h_{t-1}}\n",
        "  \\;=\\; \\frac{\\partial f(a_t)}{\\partial a_t}\\,\\frac{\\partial a_t}{\\partial h_{t-1}}\n",
        "  \\;=\\; D_t\\,W_{hh}\\;\\in\\;\\mathbb{R}^{H\\times H},\n",
        "  $$\n",
        "  where $D_t=\\mathrm{diag}\\!\\big(f'(a_t)\\big)$.\n",
        "\n",
        "- **Local loss gradient w.r.t. $h_t$** (vector, **not** a Jacobian):\n",
        "  $$\n",
        "  g_t \\;\\equiv\\; \\frac{\\partial \\ell_t}{\\partial h_t}\n",
        "  \\;=\\; W_{ho}^\\top\\!\\big(p_t - y_{t+1}\\big)\\;\\in\\;\\mathbb{R}^{H},\n",
        "  \\quad p_t=\\mathrm{softmax}(W_{ho}h_t+b_o).\n",
        "  $$\n",
        "\n"
      ],
      "metadata": {
        "id": "orqcIeI_Rhj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expanding the Local Loss Gradient w.r.t. $h_t$\n",
        "\n",
        "## 1. Softmax output\n",
        "\n",
        "Define the logits at time $t$:\n",
        "$$\n",
        "z_t = W_{ho} h_t + b_o \\in \\mathbb{R}^V,\n",
        "$$\n",
        "where $V$ is the vocabulary size.\n",
        "\n",
        "The softmax distribution is\n",
        "$$\n",
        "p_t = \\text{softmax}(z_t),\n",
        "\\qquad\n",
        "p_{t,i} = \\frac{e^{z_{t,i}}}{\\sum_{j=1}^V e^{z_{t,j}}}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Cross-entropy loss\n",
        "\n",
        "Given the one-hot target vector $y_{t+1} \\in \\{0,1\\}^V$,\n",
        "$$\n",
        "\\ell_t = -\\sum_{i=1}^V y_{t+1,i}\\, \\log p_{t,i}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Gradient wrt logits $z_t$\n",
        "\n",
        "Differentiate $\\ell_t$ wrt $z_t$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell_t}{\\partial z_t} \\in \\mathbb{R}^V.\n",
        "$$\n",
        "\n",
        "Using the well-known **softmax + cross-entropy gradient identity**:\n",
        "$$\n",
        "\\frac{\\partial \\ell_t}{\\partial z_t} = p_t - y_{t+1}.\n",
        "$$\n",
        "\n",
        "Derivation of the above equality\n",
        "---\n",
        "\n",
        "### Step 1: Cross-entropy loss\n",
        "For one time step,\n",
        "$$\n",
        "\\ell_t = -\\sum_{i=1}^V y_{t+1,i} \\log p_{t,i}.\n",
        "$$\n",
        "\n",
        "Since $y_{t+1}$ is one-hot, $\\ell_t = -\\log p_{t,c}$ where $c$ is the correct class index.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Derivative wrt probabilities $p_{t,i}$\n",
        "\n",
        "The gradient of $\\ell_t$ wrt $p_{t,i}$ is\n",
        "$$\n",
        "\\frac{\\partial \\ell_t}{\\partial p_{t,i}} = -\\frac{y_{t+1,i}}{p_{t,i}}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Softmax derivative wrt logits $z_t$\n",
        "\n",
        "The softmax function is\n",
        "$$\n",
        "p_{t,i} = \\frac{e^{z_{t,i}}}{\\sum_{j=1}^V e^{z_{t,j}}}.\n",
        "$$\n",
        "\n",
        "Its derivative wrt $z_{t,j}$ is\n",
        "$$\n",
        "\\frac{\\partial p_{t,i}}{\\partial z_{t,j}}\n",
        "= p_{t,i}(\\delta_{ij} - p_{t,j}),\n",
        "$$\n",
        "where $\\delta_{ij}$ is the Kronecker delta.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Chain rule\n",
        "\n",
        "By chain rule,\n",
        "$$\n",
        "\\frac{\\partial \\ell_t}{\\partial z_{t,j}}\n",
        "= \\sum_{i=1}^V \\frac{\\partial \\ell_t}{\\partial p_{t,i}} \\cdot \\frac{\\partial p_{t,i}}{\\partial z_{t,j}}.\n",
        "$$\n",
        "\n",
        "Substitute the formulas:\n",
        "$$\n",
        "\\frac{\\partial \\ell_t}{\\partial z_{t,j}}\n",
        "= \\sum_{i=1}^V \\left(-\\frac{y_{t+1,i}}{p_{t,i}}\\right) \\cdot p_{t,i}(\\delta_{ij} - p_{t,j}).\n",
        "$$\n",
        "\n",
        "Simplify:\n",
        "$$\n",
        "\\frac{\\partial \\ell_t}{\\partial z_{t,j}}\n",
        "= \\sum_{i=1}^V \\big(-y_{t+1,i}\\delta_{ij} + y_{t+1,i}p_{t,j}\\big).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: Collapse the sums\n",
        "\n",
        "- The first term gives $-y_{t+1,j}$ (since only $i=j$ contributes).\n",
        "- The second term gives $p_{t,j}\\sum_{i=1}^V y_{t+1,i}$.\n",
        "\n",
        "Since $y_{t+1}$ is one-hot, $\\sum_{i=1}^V y_{t+1,i} = 1$.\n",
        "\n",
        "So,\n",
        "$$\n",
        "\\frac{\\partial \\ell_t}{\\partial z_{t,j}} = p_{t,j} - y_{t+1,j}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Final vector form\n",
        "\n",
        "Stacking over all $j=1,\\dots,V$,\n",
        "$$\n",
        "\\boxed{\\;\\frac{\\partial \\ell_t}{\\partial z_t} = p_t - y_{t+1}\\;}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- This is a **gradient vector** in $\\mathbb{R}^V$, *not* a Jacobian.  \n",
        "- Each entry is simply the difference between predicted probability and the target indicator.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Gradient wrt hidden state $h_t$\n",
        "\n",
        "Since $z_t = W_{ho} h_t + b_o$, applying the chain rule:\n",
        "$$\n",
        "\\frac{\\partial \\ell_t}{\\partial h_t}\n",
        "= \\left(\\frac{\\partial z_t}{\\partial h_t}\\right)^\\top\n",
        "\\frac{\\partial \\ell_t}{\\partial z_t}.\n",
        "$$\n",
        "\n",
        "Here,\n",
        "$$\n",
        "\\frac{\\partial z_t}{\\partial h_t} = W_{ho},\n",
        "$$\n",
        "so\n",
        "$$\n",
        "\\boxed{\\;\\frac{\\partial \\ell_t}{\\partial h_t}\n",
        "= W_{ho}^\\top \\big(p_t - y_{t+1}\\big).\\;}\n",
        "$$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "aBIBYItkNB2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Chain rule at time $t$\n",
        "\n",
        "Differentiate $L_T$ w.r.t. $h_t$:\n",
        "$$\n",
        "\\frac{\\partial L_T}{\\partial h_t}\n",
        "= \\underbrace{\\frac{\\partial \\ell_t}{\\partial h_t}}_{g_t}\n",
        "+ \\sum_{k=t+1}^T \\frac{\\partial \\ell_k}{\\partial h_k}\\;\n",
        "      \\frac{\\partial h_k}{\\partial h_{k-1}}\\cdots\n",
        "      \\frac{\\partial h_{t+2}}{\\partial h_{t+1}}\\;\n",
        "      \\frac{\\partial h_{t+1}}{\\partial h_t}.\n",
        "$$\n",
        "\n",
        "Group the future part by factoring the **next** step:\n",
        "$$\n",
        "\\sum_{k=t+1}^T \\frac{\\partial \\ell_k}{\\partial h_k}\n",
        "      \\frac{\\partial h_k}{\\partial h_{k-1}}\\cdots\n",
        "      \\frac{\\partial h_{t+2}}{\\partial h_{t+1}}\n",
        "= \\underbrace{\\frac{\\partial L_T}{\\partial h_{t+1}}}_{\\delta_{t+1}}.\n",
        "$$\n",
        "\n",
        "Thus\n",
        "$$\n",
        "\\frac{\\partial L_T}{\\partial h_t}\n",
        "= g_t + \\left(\\frac{\\partial h_{t+1}}{\\partial h_t}\\right)^\\top \\frac{\\partial L_T}{\\partial h_{t+1}}.\n",
        "$$\n",
        "\n",
        "Using the shorthand $ \\delta_t = \\frac{\\partial L_T}{\\partial h_t} $ and $ J_{t+1} = \\frac{\\partial h_{t+1}}{\\partial h_t} $,\n",
        "$$\n",
        "\\boxed{\\;\\delta_t = g_t + J_{t+1}^\\top \\delta_{t+1}\\;}\n",
        "$$\n",
        "with terminal condition $\\delta_{T+1}=0$.\n"
      ],
      "metadata": {
        "id": "TeKxQ6aUXVDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation Through Time (BPTT): Vanishing & Exploding Gradients\n",
        "\n",
        "---\n",
        "\n",
        "## General recursion (loss at every time step)\n",
        "\n",
        "By the chain rule under the **column-gradient convention**,\n",
        "$$\n",
        "\\boxed{\\;\\delta_t = g_t + J_{t+1}^\\top\\,\\delta_{t+1}\\;}, \\qquad t=T,T-1,\\dots,1,\n",
        "$$\n",
        "with terminal condition\n",
        "$$\n",
        "\\delta_{T+1} = 0.\n",
        "$$\n",
        "\n",
        "Here:\n",
        "- $\\delta_t = \\dfrac{\\partial L_T}{\\partial h_t} \\in \\mathbb{R}^H$ is the backprop signal,\n",
        "- $g_t = \\dfrac{\\partial \\ell_t}{\\partial h_t} \\in \\mathbb{R}^H$ is the local gradient at time $t$,\n",
        "- $J_{t+1} = \\dfrac{\\partial h_{t+1}}{\\partial h_t} \\in \\mathbb{R}^{H\\times H}$ is the Jacobian of the recurrence.\n",
        "\n",
        "---\n",
        "\n",
        "## Unrolled form\n",
        "\n",
        "Expanding the recursion step by step yields:\n",
        "$$\n",
        "\\delta_t = g_t\n",
        "+ J_{t+1}^\\top g_{t+1}\n",
        "+ J_{t+1}^\\top J_{t+2}^\\top g_{t+2}\n",
        "+ \\cdots\n",
        "+ J_{t+1}^\\top J_{t+2}^\\top \\cdots J_T^\\top g_T.\n",
        "$$\n",
        "\n",
        "Compactly:\n",
        "$$\n",
        "\\boxed{\\;\\delta_t = \\sum_{k=t}^T \\left( \\Big(\\prod_{j=t+1}^k J_j^\\top\\Big) g_k \\right)\\;}\n",
        "$$\n",
        "\n",
        "- If $k=t$, the product is empty $\\Rightarrow I$, giving $g_t$.  \n",
        "- Each future $g_k$ is transported back through all Jacobians from step $k$ down to step $t$.\n",
        "\n",
        "---\n",
        "\n",
        "# Why the recursion $\\delta_t = g_t + J_{t+1}^\\top \\delta_{t+1}$ unrolls into a sum\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1. Recursion formula\n",
        "\n",
        "We start from the backpropagation-through-time recurrence:\n",
        "$$\n",
        "\\delta_t = g_t + J_{t+1}^\\top \\,\\delta_{t+1}.\n",
        "$$\n",
        "\n",
        "Here:\n",
        "- $\\delta_t = \\dfrac{\\partial L_T}{\\partial h_t} \\in \\mathbb{R}^H$ (gradient vector),\n",
        "- $g_t = \\dfrac{\\partial \\ell_t}{\\partial h_t} \\in \\mathbb{R}^H$ (local gradient at time $t$),\n",
        "- $J_{t+1} = \\dfrac{\\partial h_{t+1}}{\\partial h_t} \\in \\mathbb{R}^{H\\times H}$ (Jacobian of the hidden recurrence).\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2. Substitute one step ahead\n",
        "\n",
        "Insert the definition of $\\delta_{t+1}$:\n",
        "$$\n",
        "\\delta_{t+1} = g_{t+1} + J_{t+2}^\\top \\delta_{t+2}.\n",
        "$$\n",
        "\n",
        "So,\n",
        "$$\n",
        "\\delta_t = g_t + J_{t+1}^\\top g_{t+1} + J_{t+1}^\\top J_{t+2}^\\top \\delta_{t+2}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3. Substitute further\n",
        "\n",
        "Now expand $\\delta_{t+2}$:\n",
        "$$\n",
        "\\delta_{t+2} = g_{t+2} + J_{t+3}^\\top \\delta_{t+3}.\n",
        "$$\n",
        "\n",
        "Plugging this in:\n",
        "$$\n",
        "\\delta_t = g_t\n",
        "+ J_{t+1}^\\top g_{t+1}\n",
        "+ J_{t+1}^\\top J_{t+2}^\\top g_{t+2}\n",
        "+ J_{t+1}^\\top J_{t+2}^\\top J_{t+3}^\\top \\delta_{t+3}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Step 4. Continue until the end\n",
        "\n",
        "Repeating this expansion up to time $T$ (where $\\delta_{T+1}=0$), we collect all contributions:\n",
        "$$\n",
        "\\delta_t = g_t\n",
        "+ J_{t+1}^\\top g_{t+1}\n",
        "+ J_{t+1}^\\top J_{t+2}^\\top g_{t+2}\n",
        "+ \\cdots\n",
        "+ J_{t+1}^\\top J_{t+2}^\\top \\cdots J_T^\\top g_T.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5. Compact summation notation\n",
        "\n",
        "We can write the whole expansion in a single summation:\n",
        "$$\n",
        "\\delta_t = \\sum_{k=t}^{T}\n",
        "\\left( \\prod_{j=t+1}^k J_j^\\top \\right) g_k.\n",
        "$$\n",
        "\n",
        "- For $k=t$: the product $\\prod_{j=t+1}^t$ is **empty**, so by convention it equals the identity $I$, giving just $g_t$.\n",
        "- For $k=t+1$: the term is $J_{t+1}^\\top g_{t+1}$.\n",
        "- For $k=t+2$: the term is $J_{t+1}^\\top J_{t+2}^\\top g_{t+2}$.\n",
        "- …\n",
        "- For $k=T$: the term is $J_{t+1}^\\top J_{t+2}^\\top \\cdots J_T^\\top g_T$.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Special case: loss only at the final step\n",
        "\n",
        "If $\\ell_t = 0$ for all $t < T$, then $g_k=0$ for $k<T$, and\n",
        "$$\n",
        "\\delta_t = \\delta_T \\prod_{j=t+1}^T J_j^\\top\n",
        "= \\big(J_T J_{T-1}\\cdots J_{t+1}\\big)^\\top \\delta_T.\n",
        "$$\n",
        "\n",
        "This is the **gradient recurrence** when the loss depends only on $h_T$.\n",
        "\n",
        "---\n",
        "\n",
        "## Vanishing and exploding gradients\n",
        "\n",
        "Using the spectral norm $\\|\\cdot\\|_2$ (note $\\|A^\\top\\|_2=\\|A\\|_2$),\n",
        "$$\n",
        "\\|\\delta_t\\|\n",
        "\\;\\le\\; \\sum_{k=t}^T \\|g_k\\| \\prod_{j=t+1}^k \\|J_j^\\top\\|\n",
        "\\;=\\; \\sum_{k=t}^T \\|g_k\\| \\prod_{j=t+1}^k \\|J_j\\|\n",
        "\\;\\le\\; \\sum_{k=t}^T \\|g_k\\| \\prod_{j=t+1}^k \\|D_j\\|\\,\\|W_{hh}\\|.\n",
        "$$\n",
        "\n",
        "- For $f$ is ReLU in normal regimes, $\\|D_j\\|\\le 1$ (often $<1$).\n",
        "- If $\\rho(W_{hh}) < 1$ (spectral radius), products decay exponentially ⇒ **vanishing gradients**.  \n",
        "- If $\\rho(W_{hh}) > 1$, products grow exponentially ⇒ **exploding gradients**.\n",
        "\n",
        "---\n",
        "\n",
        "## Simple linear example\n",
        "\n",
        "Let $f$ be the identity ($D_t=I$, $J_t=W_{hh}$).  \n",
        "With loss only at the final step:\n",
        "$$\n",
        "\\delta_t = \\big(W_{hh}^{\\,T-t}\\big)^\\top \\delta_T.\n",
        "$$\n",
        "\n",
        "If $W_{hh}=\\alpha I$,\n",
        "$$\n",
        "\\|\\delta_t\\| = |\\alpha|^{\\,T-t}\\,\\|\\delta_T\\|.\n",
        "$$\n",
        "\n",
        "- **Vanishing:** $\\alpha=0.5 \\;\\Rightarrow\\; (0.5)^{20}\\approx 10^{-6}$.  \n",
        "- **Exploding:** $\\alpha=1.5 \\;\\Rightarrow\\; (1.5)^{20}\\approx 3.3\\times 10^3$.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_V8lM_PJ8w_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep RNNs\n",
        "\n",
        "**Motivation**\n",
        "- Single-layer RNN has limited capacity.  \n",
        "- Stacking multiple layers improves expressiveness.  \n",
        "\n",
        "**Equations (2-layer RNN):**\n",
        "\n",
        "$$\n",
        "h_t^{(1)} = f(W_{xh}x_t + W_{hh}^{(1)}h_{t-1}^{(1)})\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_t^{(2)} = f(W_{h^{(1)}h^{(2)}}h_t^{(1)} + W_{hh}^{(2)}h_{t-1}^{(2)})\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_t = \\text{softmax}(W_{hy}h_t^{(2)})\n",
        "$$\n",
        "\n",
        "**Pros**\n",
        "- Capture hierarchical representations.  \n",
        "\n",
        "**Cons**\n",
        "- Even worse vanishing/exploding issues."
      ],
      "metadata": {
        "id": "xPf0mLBH68pe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Deep RNN](https://i.imgur.com/J3DwxSF.png)\n"
      ],
      "metadata": {
        "id": "ErHgiYQ_7jUc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU7rPdvV3sJA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ------------------------\n",
        "# Single-layer RNN\n",
        "# ------------------------\n",
        "rnn_single = nn.RNN(input_size=5, hidden_size=8, num_layers=1, batch_first=True)\n",
        "\n",
        "# Example input: batch=2, seq_len=4, input_dim=5\n",
        "x = torch.randn(2, 4, 5)\n",
        "\n",
        "# Initial hidden state: [num_layers, batch, hidden_size]\n",
        "h0_single = torch.zeros(1, 2, 8)\n",
        "\n",
        "# Forward pass\n",
        "output_single, hn_single = rnn_single(x, h0_single)\n",
        "\n",
        "print(\"=== Single-layer RNN ===\")\n",
        "print(\"Input shape:\", x.shape)             # [2, 4, 5]\n",
        "print(\"Output shape:\", output_single.shape) # [2, 4, 8]\n",
        "print(\"Last hidden state shape:\", hn_single.shape) # [1, 2, 8]\n",
        "\n",
        "# ------------------------\n",
        "# Deep RNN (3 layers stacked)\n",
        "# ------------------------\n",
        "rnn_deep = nn.RNN(input_size=5, hidden_size=8, num_layers=3, batch_first=True)\n",
        "\n",
        "# Initial hidden state: [num_layers, batch, hidden_size]\n",
        "h0_deep = torch.zeros(3, 2, 8)\n",
        "\n",
        "# Forward pass\n",
        "output_deep, hn_deep = rnn_deep(x, h0_deep)\n",
        "\n",
        "print(\"\\n=== Deep RNN (3 layers) ===\")\n",
        "print(\"Input shape:\", x.shape)              # [2, 4, 5]\n",
        "print(\"Output shape:\", output_deep.shape)   # [2, 4, 8]\n",
        "print(\"Last hidden state shape:\", hn_deep.shape) # [3, 2, 8]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long Short-Term Memory (LSTM)\n",
        "\n",
        "**Key idea:** Cell state with gates → stable long-range memory.  \n",
        "\n",
        "![LSTM](https://external-preview.redd.it/jL97dVEQcqoX79lFAI5J4fomXXU_hBHtlzyAs-7xQ-Q.png?format=pjpg&auto=webp&s=6400685c6a57df685241f8d6d456d4aae3c74105)\n",
        "\n",
        "**Equivalent Equations:**\n",
        "$$\n",
        "\\begin{aligned}\n",
        "i_t &= \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i) \\quad &\\text{(input gate)} \\\\\n",
        "f_t &= \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f) \\quad &\\text{(forget gate)} \\\\\n",
        "o_t &= \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_o) \\quad &\\text{(output gate)} \\\\\n",
        "\\tilde{c}_t &= \\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c) \\quad &\\text{(candidate)} \\\\\n",
        "c_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\quad &\\text{(cell state)} \\\\\n",
        "h_t &= o_t \\odot \\tanh(c_t) \\quad &\\text{(hidden state)} \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "S2YJpaxX7dYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ------------------------\n",
        "# Single-layer LSTM\n",
        "# ------------------------\n",
        "lstm_single = nn.LSTM(input_size=5, hidden_size=8, num_layers=1, batch_first=True)\n",
        "\n",
        "# Example input: batch=2, seq_len=4, feature_dim=5\n",
        "x = torch.randn(2, 4, 5)\n",
        "\n",
        "# Initial hidden state (h0) and cell state (c0): [num_layers, batch, hidden_size]\n",
        "h0_single = torch.zeros(1, 2, 8)\n",
        "c0_single = torch.zeros(1, 2, 8)\n",
        "\n",
        "# Forward pass\n",
        "output_single, (hn_single, cn_single) = lstm_single(x, (h0_single, c0_single))\n",
        "\n",
        "print(\"=== Single-layer LSTM ===\")\n",
        "print(\"Input shape:\", x.shape)              # [2, 4, 5]\n",
        "print(\"Output shape:\", output_single.shape) # [2, 4, 8]\n",
        "print(\"hn shape:\", hn_single.shape)         # [1, 2, 8]\n",
        "print(\"cn shape:\", cn_single.shape)         # [1, 2, 8]\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Deep LSTM (3 layers)\n",
        "# ------------------------\n",
        "lstm_deep = nn.LSTM(input_size=5, hidden_size=8, num_layers=3, batch_first=True)\n",
        "\n",
        "# Initial hidden & cell states: [num_layers, batch, hidden_size]\n",
        "h0_deep = torch.zeros(3, 2, 8)\n",
        "c0_deep = torch.zeros(3, 2, 8)\n",
        "\n",
        "# Forward pass\n",
        "output_deep, (hn_deep, cn_deep) = lstm_deep(x, (h0_deep, c0_deep))\n",
        "\n",
        "print(\"\\n=== Deep LSTM (3 layers) ===\")\n",
        "print(\"Input shape:\", x.shape)               # [2, 4, 5]\n",
        "print(\"Output shape:\", output_deep.shape)    # [2, 4, 8]\n",
        "print(\"hn shape:\", hn_deep.shape)            # [3, 2, 8] (one hidden state per layer)\n",
        "print(\"cn shape:\", cn_deep.shape)            # [3, 2, 8] (one cell state per layer)\n"
      ],
      "metadata": {
        "id": "Th7eii_P6erI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![GRU](https://miro.medium.com/1*DwL2ygleKXtRbYeVi8Qb_g.png)"
      ],
      "metadata": {
        "id": "vbOUVF2oZTWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gated Recurrent Unit (GRU)\n",
        "\n",
        "For input $x_t$ and previous hidden state $h_{t-1}$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "z_t &= \\sigma(W_z x_t + U_z h_{t-1} + b_z) &\\quad& \\text{(update gate)} \\\\\n",
        "r_t &= \\sigma(W_r x_t + U_r h_{t-1} + b_r) &\\quad& \\text{(reset gate)} \\\\\n",
        "\\tilde{h}_t &= \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h) &\\quad& \\text{(candidate state)} \\\\\n",
        "h_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t &\\quad& \\text{(new hidden state)} \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition\n",
        "- **Update gate $z_t$:** how much of the old state to keep.  \n",
        "- **Reset gate $r_t$:** how much past information to forget.  \n",
        "- **Candidate $\\tilde{h}_t$:** proposed new state (mix of input + reset history).  \n",
        "- **Final hidden state $h_t$:** interpolation between old state and candidate.  \n",
        "\n"
      ],
      "metadata": {
        "id": "AddJz1voZlpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ------------------------\n",
        "# Single-layer GRU\n",
        "# ------------------------\n",
        "gru = nn.GRU(input_size=5, hidden_size=8, num_layers=1, batch_first=True)\n",
        "\n",
        "# Example input: batch=2, seq_len=4, feature_dim=5\n",
        "x = torch.randn(2, 4, 5)\n",
        "\n",
        "# Initial hidden state: [num_layers, batch, hidden_size]\n",
        "h0 = torch.zeros(1, 2, 8)\n",
        "\n",
        "# Forward pass\n",
        "output, hn = gru(x, h0)\n",
        "\n",
        "print(\"Input shape:\", x.shape)        # [2, 4, 5]\n",
        "print(\"Output shape:\", output.shape)  # [2, 4, 8] (hidden states for all timesteps)\n",
        "print(\"hn shape:\", hn.shape)          # [1, 2, 8] (last hidden state for each sequence)\n",
        "\n",
        "# Optional: map hidden states to logits (e.g., for vocab size = 10)\n",
        "head = nn.Linear(8, 10)\n",
        "logits = head(output)  # [2, 4, 10]\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "\n",
        "# ------------------------\n",
        "# (Optional) Deep GRU: just change num_layers\n",
        "# ------------------------\n",
        "# gru_deep = nn.GRU(input_size=5, hidden_size=8, num_layers=3, batch_first=True)\n",
        "# h0_deep = torch.zeros(3, 2, 8)\n",
        "# output_deep, hn_deep = gru_deep(x, h0_deep)\n",
        "# print(\"Deep GRU hn shape:\", hn_deep.shape)  # [3, 2, 8]\n"
      ],
      "metadata": {
        "id": "Gcy07YcRZ8DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masking-Based Autoregressive Models\n",
        "\n",
        "**Key property:**  \n",
        "- All conditionals computed **in parallel**.  \n",
        "- Enforced by **causal masking**.  \n",
        "\n",
        "$$\n",
        "P(x_1, \\ldots, x_T) = \\prod_{t=1}^T P(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "**Examples**\n",
        "- Masked MLP (MADE)  \n",
        "- Masked convolutions (PixelCNN)  \n",
        "- Masked self-attention (Transformers, next lecture)  \n",
        "\n",
        "**Benefits**\n",
        "- Parallelizable training  \n",
        "- Still autoregressive  \n",
        "- Parameter sharing across time  \n"
      ],
      "metadata": {
        "id": "I7Utjam1aIBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masked Autoregressive Models\n",
        "\n",
        "We want to model the joint distribution of a sequence or vector:\n",
        "\n",
        "$$\n",
        "P(x_1, x_2, \\dots, x_D) = \\prod_{i=1}^D P(x_i \\mid x_{<i})\n",
        "$$\n",
        "\n",
        "- Each conditional $P(x_i \\mid x_{<i})$ should depend **only** on earlier variables.\n",
        "- This is the **autoregressive property**.\n",
        "\n",
        "---\n",
        "\n",
        "## Problem\n",
        "- A standard feedforward neural network (MLP, CNN, Transformer) connects **all inputs to all outputs**.\n",
        "- Without constraints, the output for $x_i$ might depend on \"future\" inputs (like $x_j$ with $j > i$).\n",
        "- That breaks the autoregressive property.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KD4NvpWKbX8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Solution: Masking\n",
        "- Introduce a **binary mask** (matrix of 0s and 1s) applied to the network weights.\n",
        "- The mask zeroes out connections that would allow information flow from \"future\" variables.\n",
        "\n",
        "**Rule:**\n",
        "$$\n",
        "W_{jk} = 0 \\quad \\text{if input index } k \\geq \\text{ output index } j\n",
        "$$\n",
        "\n",
        "- Each output neuron is only connected to valid past inputs.\n",
        "\n",
        "---\n",
        "## Example 1: MADE (Masked Autoencoder for Distribution Estimation)\n",
        "\n",
        "Suppose we want to model a simple 3D vector:\n",
        "\n",
        "$$\n",
        "x = (x_1, x_2, x_3)\n",
        "$$\n",
        "\n",
        "with autoregressive factorization:\n",
        "\n",
        "$$\n",
        "P(x) = P(x_1) \\cdot P(x_2 \\mid x_1) \\cdot P(x_3 \\mid x_1, x_2).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Without a mask\n",
        "- A normal MLP would connect **every input to every output**.\n",
        "- That means the output for $x_2$ could depend on $x_3$, which **breaks the autoregressive rule**.\n",
        "\n",
        "---\n",
        "\n",
        "### With a mask\n",
        "- We add a binary mask to the MLP weights so that:\n",
        "  - **Output for $x_1$**: no inputs allowed → only a bias term.\n",
        "  - **Output for $x_2$**: can only depend on $x_1$.\n",
        "  - **Output for $x_3$**: can depend on both $x_1$ and $x_2$.\n",
        "\n",
        "**Masked connections table:**\n",
        "\n",
        "| Output | Visible inputs | Mask vector |\n",
        "|--------|----------------|-------------|\n",
        "| o1     | —              | (0, 0, 0)   |\n",
        "| o2     | x1             | (1, 0, 0)   |\n",
        "| o3     | x1, x2         | (1, 1, 0)   |\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition\n",
        "- Think of the mask as a **set of rules** saying:\n",
        "  - \"Who can look at whom?\"\n",
        "- MADE enforces that each output **only looks left** (earlier variables).\n",
        "\n",
        "\n",
        "✅ In practice: one forward pass of MADE gives you **all conditionals in parallel**, respecting autoregressive order.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_AKkqvzCbvcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why the First Row of the Mask is All Zeros in MADE\n",
        "\n",
        "### 1. What the rows mean\n",
        "In a **masked linear layer**:\n",
        "\n",
        "- **Columns** = inputs (e.g., $x_1, x_2, x_3$)  \n",
        "- **Rows** = outputs (e.g., logits for $o_1, o_2, o_3$)  \n",
        "\n",
        "So, each row of the mask tells us:  \n",
        "> Which inputs this output is allowed to see.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Why row 1 (for $o_1$) is all zeros\n",
        "- Output $o_1$ corresponds to $P(x_1)$.  \n",
        "- By definition of autoregressive factorization:\n",
        "\n",
        "$$\n",
        "P(x_1) \\quad \\text{has no conditioning variables.}\n",
        "$$\n",
        "\n",
        "- This means it **cannot depend** on:\n",
        "  - $x_1$ itself (future),  \n",
        "  - or $x_2, x_3$ (future).  \n",
        "\n",
        "✅ Therefore, row 1 of the mask must be **all zeros**.  \n",
        "The only thing that influences $o_1$ is the **bias term**, which acts like a learnable unconditional prior.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Contrast with other rows\n",
        "- **Row 2 (output $o_2$):**  \n",
        "  Models $P(x_2 \\mid x_1)$.  \n",
        "  - Allowed to see $x_1$.  \n",
        "  - Not allowed to see $x_2, x_3$.  \n",
        "\n",
        "- **Row 3 (output $o_3$):**  \n",
        "  Models $P(x_3 \\mid x_1, x_2)$.  \n",
        "  - Allowed to see $x_1, x_2$.  \n",
        "  - Not allowed to see $x_3$.  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. Big picture\n",
        "- Row 1 (all zeros): \"first variable has no context.\"  \n",
        "- Row 2 (partially filled): \"second variable conditioned on earlier ones.\"  \n",
        "- Row 3 (more filled): \"third variable conditioned on first two.\"  \n",
        "\n",
        "This is how MADE enforces **causality** with a feedforward mask.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary:**  \n",
        "The first row of the mask is all zeros because $P(x_1)$ is unconditional.  \n",
        "The model should only learn its marginal distribution, not cheat by looking at the inputs themselves or any future variables.\n"
      ],
      "metadata": {
        "id": "f3dV9XbD0gck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Custom masked linear layer\n",
        "class MaskedLinear(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, mask):\n",
        "        super().__init__(in_features, out_features, bias=True)\n",
        "        self.register_buffer(\"mask\", mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.weight * self.mask, self.bias)\n",
        "\n",
        "# Suppose input vector is (x1, x2, x3)\n",
        "D = 3\n",
        "# Create a mask so that o1 depends on nothing, o2 depends only on x1, o3 depends on x1,x2\n",
        "mask = torch.tensor([[0,0,0],   # o1 ← no inputs\n",
        "                     [1,0,0],   # o2 ← x1\n",
        "                     [1,1,0]], dtype=torch.float)\n",
        "\n",
        "layer = MaskedLinear(D, D, mask)\n",
        "\n",
        "# Example batch of inputs (2 samples, each of dimension 3)\n",
        "x = torch.tensor([[1., 2., 3.],\n",
        "                  [0.5, -1., 2.]])\n",
        "\n",
        "output = layer(x)\n",
        "\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"Masked Weight Matrix:\\n\", layer.weight * layer.mask)\n",
        "print(\"Output:\\n\", output)"
      ],
      "metadata": {
        "id": "CbedDZZjygm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refresh: Convolution\n",
        "\n",
        "**Operation:**  \n",
        "- Slide kernel across input.  \n",
        "- Compute weighted sum at each location.  \n",
        "\n",
        "![CNN](https://miro.medium.com/v2/resize:fit:1400/0*LeK_gmCf3DfO3gj_.jpeg)\n",
        "\n",
        "\n",
        "![Convolution](https://viso.ai/wp-content/uploads/2024/04/Illustrating-the-first-5-steps-of-convolution-operation-1.jpg)\n",
        "\n",
        "![2D Convolution](https://miro.medium.com/v2/resize:fit:1400/0*H_6KDnWyFj_JDstS)\n",
        "$$\n",
        "y[i] = \\sum_k w[k] \\cdot x[i+k]\n",
        "$$\n",
        "\n",
        "**Key property:** Translation equivariance  \n",
        "- Input shift → Output shift  \n",
        "- Same filter applies everywhere  \n",
        "\n",
        "**Why useful?**  \n",
        "- Strong inductive bias for natural signals  \n",
        "- Local patterns + weight sharing + efficiency  "
      ],
      "metadata": {
        "id": "88P7awHA08L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PixelCNN: Convolution with Causal Masks\n",
        "\n",
        "![Masked CNN](https://velog.velcdn.com/images/2712qwer/post/b776ad05-fdd8-43b1-b6d5-a7fd60dc09ee/image.png)\n",
        "\n",
        "**Goal.** Model the joint distribution of an image $x \\in \\mathbb{R}^{H \\times W \\times C}$ as an autoregressive factorization in **raster order** (left→right, top→bottom):\n",
        "\n",
        "$$\n",
        "P(x) \\;=\\; \\prod_{i=1}^{H}\\prod_{j=1}^{W}\\prod_{c=1}^{C}\n",
        "P\\!\\big(x_{i,j,c}\\,\\big|\\,x_{<i,j,:},\\,x_{i,j,<c}\\big).\n",
        "$$\n",
        "\n",
        "- At pixel $(i,j)$ and channel $c$, the model may only “see” previously generated pixels ($x_{<i,j,:}$) and **earlier channels of the same pixel** ($x_{i,j,<c}$).\n",
        "\n",
        "---\n",
        "\n",
        "### Causal masking for convolutions\n",
        "\n",
        "A standard $k\\times k$ convolution at $(i,j)$ would read from a $(k\\times k)$ patch centered at $(i,j)$, which includes **future** pixels.  \n",
        "**PixelCNN** enforces causality by **masking** the kernel weights (elementwise multiply with a 0/1 matrix) so the conv never reads from future positions.\n",
        "\n",
        "For a $3\\times 3$ kernel (raster order):\n",
        "\n",
        "- **Mask A (first conv layer):** forbid the center as well as the future.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mWsq469zFP9a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68cb9ef8"
      },
      "source": [
        "![Masked CNN](https://velog.velcdn.com/images/2712qwer/post/b776ad05-fdd8-43b1-b6d5-a7fd60dc09ee/image.png)\n",
        "\n",
        "## PixelCNN: Convolution with Causal Masks\n",
        "\n",
        "**Goal.** Model the joint distribution of an image $x \\in \\mathbb{R}^{H \\times W \\times C}$ as an autoregressive factorization in **raster order** (left→right, top→bottom):\n",
        "\n",
        "$$\n",
        "P(x) \\;=\\; \\prod_{i=1}^{H}\\prod_{j=1}^{W}\\prod_{c=1}^{C}\n",
        "P\\!\\big(x_{i,j,c}\\,\\big|\\,x_{<i,j,:}\\,x_{i,j,<c}\\big).\n",
        "$$\n",
        "\n",
        "- At pixel $(i,j)$ and channel $c$, the model may only “see” previously generated pixels ($x_{<i,j,:}$) and **earlier channels of the same pixel** ($x_{i,j,<c}$).\n",
        "\n",
        "* * *\n",
        "\n",
        "### Causal masking for convolutions\n",
        "\n",
        "A standard $k\\times k$ convolution at $(i,j)$ would read from a $(k\\times k)$ patch centered at $(i,j)$, which includes **future** pixels.\\\n",
        "**PixelCNN** enforces causality by **masking** the kernel weights (elementwise multiply with a 0/1 matrix) so the conv never reads from future positions.\n",
        "\n",
        "For a $3\\times 3$ kernel (raster order):\n",
        "\n",
        "- **Mask A (first conv layer):** forbid the center as well as the future.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & 1 \\\\\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- **Mask B (subsequent conv layers):** allow the center (information from the current location that is already causal through previous layers), still forbid the future.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & 1 \\\\\n",
        "1 & 1 & 0 \\\\\n",
        "0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "- “1” = allowed connection, “0” = masked (blocked).\n",
        "\n",
        "**Why two masks?**  \n",
        "The first layer must not look at the current pixel’s raw value; deeper layers can read the current-location **features** as long as those features are themselves built from causal context.\n",
        "\n",
        "---\n",
        "\n",
        "### Multi-channel (RGB) masking\n",
        "\n",
        "Within a pixel, channels are ordered (e.g., R → G → B). The conditional becomes:\n",
        "\n",
        "$$\n",
        "P(x) \\;=\\; \\prod_{i,j} P(x_{i,j,R}\\mid x_{<i,j,:})\\;\n",
        "P(x_{i,j,G}\\mid x_{<i,j,:},x_{i,j,R})\\;\n",
        "P(x_{i,j,B}\\mid x_{<i,j,:},x_{i,j,R},x_{i,j,G}).\n",
        "$$\n",
        "\n",
        "Practically, the mask is extended so that for channel $c$ at $(i,j)$ the conv can only see:\n",
        "- all channels at **past** pixels, and\n",
        "- **earlier** channels at the **same** pixel.\n",
        "\n",
        "---\n",
        "\n",
        "### Training objective (discretized pixels)\n",
        "\n",
        "For 8-bit images, each channel has 256 classes. The model outputs a categorical distribution (or a mixture of logistics as in PixelCNN++).  \n",
        "Training is **fully parallel** (teacher forcing): feed the whole image, predict all conditionals at once, minimize cross-entropy:\n",
        "\n",
        "$$\n",
        "{L} \\;=\\; - \\sum_{i,j,c} \\log P_\\theta\\!\\big(x_{i,j,c}\\,\\big|\\,x_{<i,j,:},x_{i,j,<c}\\big).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Generation (sampling) procedure\n",
        "\n",
        "1. For $i=1..H$:  \n",
        "2. &nbsp;&nbsp;for $j=1..W$:  \n",
        "3. &nbsp;&nbsp;&nbsp;&nbsp;for $c=1..C$:  \n",
        "4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sample $x_{i,j,c} \\sim P_\\theta(\\cdot \\mid x_{<i,j,:},x_{i,j,<c})$  \n",
        "\n",
        "This is **sequential** over pixels/channels (slow at sample time), but **parallel** during training.\n",
        "\n",
        "---\n",
        "\n",
        "### Architectural notes\n",
        "\n",
        "- Deep stacks of **masked** $3\\times 3$ convolutions (often with residual connections) grow the **receptive field**, letting each conditional see a larger causal neighborhood.\n",
        "- Variants:\n",
        "  - **Gated PixelCNN**: replaces ReLU with gated activations for better modeling.\n",
        "  - **PixelCNN++**: discretized logistic mixture likelihood, downsampling, and other refinements.\n",
        "- Common speedups:\n",
        "  - Caching intermediate features during generation.\n",
        "  - Coarse-to-fine pyramids or parallelizable approximations.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}