{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Recap – Autoregressive Models\n",
        "\n",
        "**Chain Rule Decomposition**  \n",
        "$$\n",
        "p(x_1, \\ldots, x_T) = \\prod_{t=1}^{T} p(x_t \\mid x_1, \\ldots, x_{t-1})\n",
        "$$\n",
        "\n",
        "**Maximum Likelihood Estimation (MLE)**  \n",
        "$$\n",
        "\\widehat{\\theta} = \\arg \\max_{\\theta} \\sum_{t=1}^T \\log p_\\theta(x_t \\mid x_{1}, x_{2}, \\ldots, x_{t-1})\n",
        "$$\n",
        "\n",
        "- Factorizes the joint distribution into conditional probabilities.  \n",
        "- Training objective: maximize log-likelihood of observed sequence.  \n",
        "- Examples: Language modeling with RNNs, Transformers, masked CNNs."
      ],
      "metadata": {
        "id": "MvpMTHBXtuIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch demo: compute autoregressive log-likelihood for a toy sequence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Toy setup\n",
        "vocab_size = 10\n",
        "seq_len = 6\n",
        "batch_size = 2\n",
        "\n",
        "# Fake \"model\" = simple embedding + linear classifier\n",
        "class ToyARModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.embed(x)\n",
        "        out, _ = self.rnn(h)\n",
        "        logits = self.fc(out)  # shape: (batch, seq_len, vocab_size)\n",
        "        return logits\n",
        "\n",
        "# Sample data: two sequences\n",
        "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "print(\"Input sequences:\\n\", x)\n",
        "\n",
        "model = ToyARModel(vocab_size)\n",
        "logits = model(x[:, :-1])  # predict next token given prefix\n",
        "\n",
        "# Compute autoregressive log-likelihood\n",
        "log_probs = F.log_softmax(logits, dim=-1)\n",
        "target = x[:, 1:]  # predict x_t given x_{<t}\n",
        "nll = F.nll_loss(\n",
        "    log_probs.reshape(-1, vocab_size),\n",
        "    target.reshape(-1),\n",
        "    reduction=\"mean\"\n",
        ")\n",
        "\n",
        "print(\"Negative Log-Likelihood (to minimize):\", nll.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ0fCE7Pt82-",
        "outputId": "5ac432ee-dc18-4401-9559-eabec9d6aef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequences:\n",
            " tensor([[4, 7, 0, 4, 3, 6],\n",
            "        [9, 7, 5, 9, 6, 0]])\n",
            "Negative Log-Likelihood (to minimize): 2.16798996925354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(log_probs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9o7BOrhucU_",
        "outputId": "a8be17ce-8a5f-41a6-dd60-3b6841a0febf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoregressive Models\n",
        "\n",
        "Autoregressive models factorize the joint distribution into conditionals  \n",
        "and can be implemented in several ways:\n",
        "\n",
        "| Model Type          | Example Architectures               | Parallel Training | Parameter Sharing |\n",
        "|---------------------|-------------------------------------|------------------|------------------|\n",
        "| **Recurrent**       | RNN, LSTM                           | ❌ No             | ✅ Yes           |\n",
        "| **Masked MLP**      | MADE (Masked Autoencoder for Dist.) | ✅ Yes            | ❌ No            |\n",
        "| **Masked CNN / Transformer** | PixelCNN, Transformer, Linear Attention | ✅ Yes | ✅ Yes |\n",
        "\n",
        "- **Parallel training:** Can multiple time steps be computed at once?  \n",
        "- **Parameter sharing:** Do different positions reuse the same parameters?\n"
      ],
      "metadata": {
        "id": "jWUJiyHHumrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitation of CNNs\n",
        "\n",
        "- Convolutional Neural Networks (CNNs) capture **local dependencies** via small receptive fields.  \n",
        "- Stacking layers gradually increases receptive field size, but this grows **slowly**.  \n",
        "- To model **long-range dependencies**, CNNs require:\n",
        "  - Very deep networks, or  \n",
        "  - Dilated convolutions (to expand receptive field faster).  \n",
        "- Transformers address this limitation with **self-attention**, which directly connects all positions.\n"
      ],
      "metadata": {
        "id": "04okNZJnu_45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Self-Attention\n",
        "\n",
        "Convolutional models face a recurring problem:\n",
        "- **Limited receptive field** → hard to capture long-range dependencies.\n",
        "\n",
        "Self-Attention provides an alternative with key advantages:\n",
        "- **Unlimited receptive field** (unlike PixelCNN).  \n",
        "- **Parameter efficiency**: $O(1)$ scaling w.r.t input dimension (unlike MADE).  \n",
        "- **Parallel computation** across positions (unlike RNNs).  \n",
        "\n",
        "**Masked Attention** ensures autoregressive behavior:  \n",
        "Each position $t$ can only attend to $\\{1, \\ldots, t\\}$.\n"
      ],
      "metadata": {
        "id": "w4ql23bjvNQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Architecture\n",
        "\n",
        "![More details](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ_zVcjcs2fTW_1qGoPk9yO5_oRfacPwnCbmg&s)\n",
        "\n",
        "![More details](https://i.sstatic.net/eAKQu.png)\n",
        "\n",
        "\n",
        "\n",
        "## Attention\n",
        "![Transformer Architecture](https://lilianweng.github.io/posts/2018-06-24-attention/transformer.png)\n",
        "\n",
        "- **Inputs:**  \n",
        "  - A sequence of **(key, value)** pairs.  \n",
        "    - *Values* are hidden states from a previous layer.  \n",
        "    - In encoder–decoder attention,  \n",
        "      - **Values = final encoder hidden states**  \n",
        "      - **Keys = encoder hidden states aligned with target sequence**.  \n",
        "  - A sequence of **queries**, representing the *current focus*.  \n",
        "\n",
        "- **Mechanism:**  \n",
        "  - Compute **scores** between each query and all keys.  \n",
        "  - Normalize scores with softmax.  \n",
        "  - Produce **weighted sum of values**.  \n",
        "\n",
        "**Scaled Dot-Product Attention**  \n",
        "$$\n",
        "\\text{Attention}(Q, K, V)\n",
        "= \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
        "$$\n",
        "\n",
        "where  \n",
        "- $Q \\in \\mathbb{R}^{n_q \\times d_k}$ (queries)  \n",
        "- $K \\in \\mathbb{R}^{n_k \\times d_k}$ (keys)  \n",
        "- $V \\in \\mathbb{R}^{n_k \\times d_v}$ (values)  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Input](https://miro.medium.com/v2/resize:fit:1400/0*yGkD4RobNX5VUsxP.png)\n",
        "\n",
        "\n",
        "![Input](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder_step_by_step.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wqRb4dFUvGhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch demo: scaled dot-product attention\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Example dimensions\n",
        "batch_size = 1\n",
        "n_q, n_k = 3, 5\n",
        "d_k, d_v = 4, 6\n",
        "\n",
        "# Random queries, keys, values\n",
        "Q = torch.randn(batch_size, n_q, d_k)\n",
        "K = torch.randn(batch_size, n_k, d_k)\n",
        "V = torch.randn(batch_size, n_k, d_v)\n",
        "\n",
        "# Compute scaled dot-product attention\n",
        "scores = Q @ K.transpose(-2, -1) / (d_k ** 0.5)   # (batch, n_q, n_k)\n",
        "weights = F.softmax(scores, dim=-1)               # attention distribution\n",
        "output = weights @ V                              # (batch, n_q, d_v)\n",
        "\n",
        "print(\"Attention scores (before softmax):\\n\", scores[0])\n",
        "print(\"\\nAttention weights (after softmax):\\n\", weights[0])\n",
        "print(\"\\nAttention output shape:\", output.shape)\n"
      ],
      "metadata": {
        "id": "JDZUGy4gvgH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention: Parallelization & Multi-Head\n",
        "\n",
        "![Multihead](https://substackcdn.com/image/fetch/$s_!Q6zJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1497d87-2b8c-45eb-b7b9-3a0a8ebe0d3d_1783x747.png)\n",
        "\n",
        "- At each position $j$, attention computations are **independent**.  \n",
        "- This allows us to parallelize using **matrix multiplications**.  \n",
        "\n",
        "**Scaled Dot-Product Attention**  \n",
        "$$\n",
        "\\text{Attention}(Q,K,V) \\;=\\; \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "- Queries $Q \\in \\mathbb{R}^{n_q \\times d_k}$  \n",
        "- Keys $K \\in \\mathbb{R}^{n_k \\times d_k}$  \n",
        "- Values $V \\in \\mathbb{R}^{n_k \\times d_v}$  \n",
        "\n",
        "---\n",
        "\n",
        "### Multi-Head Attention\n",
        "- Run **multiple attention layers in parallel** (\"heads\").  \n",
        "- Each head has its own projection of $Q, K, V$.  \n",
        "- Outputs from all heads are concatenated and projected.  \n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n",
        "$$\n"
      ],
      "metadata": {
        "id": "USYak5xAvl0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Multihead](https://data-science-blog.com/wp-content/uploads/2022/01/mha_visualization-930x1030.png)"
      ],
      "metadata": {
        "id": "QOoyXdVKzdES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bjQeody00YC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch demo: Scaled Dot-Product Attention (matrix form) and Multi-Head\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, n_q, n_k = 2, 4, 5\n",
        "d_model, d_k, d_v, n_heads = 16, 16, 16, 4\n",
        "\n",
        "# Random Q, K, V\n",
        "Q = torch.randn(batch_size, n_q, d_model)\n",
        "K = torch.randn(batch_size, n_k, d_model)\n",
        "V = torch.randn(batch_size, n_k, d_model)\n",
        "\n",
        "# ---- 1. Scaled Dot-Product Attention ----\n",
        "scores = Q @ K.transpose(-2, -1) / (d_k ** 0.5)  # (batch, n_q, n_k)\n",
        "weights = F.softmax(scores, dim=-1)\n",
        "out = weights @ V  # (batch, n_q, d_model)\n",
        "\n",
        "print(\"Scaled Dot-Product Attention output shape:\", out.shape)\n",
        "\n",
        "# ---- 2. Multi-Head Attention (PyTorch built-in) ----\n",
        "mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True)\n",
        "mha_out, attn_weights = mha(Q, K, V)\n",
        "\n",
        "print(\"Multi-Head Attention output shape:\", mha_out.shape)\n",
        "print(\"Attention weights shape (batch, n_q, n_k):\", attn_weights.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlkQioaovhPS",
        "outputId": "60375042-e14b-40f9-fe9d-855480984e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Dot-Product Attention output shape: torch.Size([2, 4, 16])\n",
            "Multi-Head Attention output shape: torch.Size([2, 4, 16])\n",
            "Attention weights shape (batch, n_q, n_k): torch.Size([2, 4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder–Decoder Transformer: Output Distribution\n",
        "\n",
        "At decoding step $t$, the model predicts the probability of the next token:\n",
        "\n",
        "$$\n",
        "P(y_t = i \\mid x_{1:T}, y_{1:t-1}) =\n",
        "\\frac{\\exp \\big( (E \\widehat{y}_t)[i] \\big)}\n",
        "     {\\sum_j \\exp \\big( (E \\widehat{y}_t)[j] \\big)}\n",
        "$$\n",
        "\n",
        "- $\\widehat{y}_t$: decoder hidden state at position $t$.  \n",
        "- $E$: embedding (projection) matrix.  \n",
        "- Output is a **softmax distribution** over the vocabulary.  \n",
        "- This connects decoder hidden states to predicted tokens.\n"
      ],
      "metadata": {
        "id": "uY3goL5FwBeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch demo: Transformer output layer (projection to vocab)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Suppose vocab of size 8 and decoder hidden dim = 16\n",
        "vocab_size = 8\n",
        "d_model = 16\n",
        "\n",
        "# Decoder hidden state at time t\n",
        "y_t_hat = torch.randn(1, d_model)  # (batch=1, d_model)\n",
        "\n",
        "# Embedding / projection matrix\n",
        "E = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "# Logits and softmax distribution\n",
        "logits = E(y_t_hat)                  # (1, vocab_size)\n",
        "probs = F.softmax(logits, dim=-1)    # (1, vocab_size)\n",
        "\n",
        "print(\"Logits:\\n\", logits)\n",
        "print(\"\\nSoftmax distribution over vocabulary:\\n\", probs)\n",
        "print(\"\\nPredicted token index:\", torch.argmax(probs, dim=-1).item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvHUNwSAv8qY",
        "outputId": "932da549-dc90-4e02-bcf2-24049da80208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits:\n",
            " tensor([[-0.6512,  0.2615,  0.4667,  0.1221,  0.5225,  0.0916, -0.4690,  0.7660]],\n",
            "       grad_fn=<MmBackward0>)\n",
            "\n",
            "Softmax distribution over vocabulary:\n",
            " tensor([[0.0516, 0.1286, 0.1578, 0.1118, 0.1669, 0.1085, 0.0619, 0.2129]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "Predicted token index: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding\n",
        "\n",
        "- Transformers are **permutation invariant** by default.  \n",
        "- Word order matters in language:  \n",
        "  - *\"The mouse ate the cat\"* vs *\"The cat ate the mouse\"*.  \n",
        "- To inject sequence order, we add a **positional encoding** to each token embedding.\n",
        "\n",
        "\n",
        "![Positional Encoding](https://substackcdn.com/image/fetch/$s_!IcmJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b94aaa0-59fb-45b8-86e5-9c2ce9c1cb12_1937x1388.png)\n",
        "\n",
        "![Attention](https://deepgram.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F96965%2F1684228093-7-transformers-explained.png&w=3840&q=90)\n",
        "\n",
        "**Key idea:**  \n",
        "$$\n",
        "\\text{Input}_t = \\text{TokenEmbedding}(x_t) + \\text{PositionalEncoding}(t)\n",
        "$$\n",
        "\n",
        "Common choices:\n",
        "- **Sinusoidal encoding** (fixed, deterministic).  \n",
        "- **Learned positional embeddings** (trainable).\n",
        "\n",
        "![Add](https://substackcdn.com/image/fetch/$s_!Vo8t!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a77f49-ed6d-4614-9c64-505455bd0c83_2043x1300.png)\n",
        "\n",
        "# 🌊 Sinusoidal Positional Encoding\n",
        "\n",
        "Transformers need a way to represent **token positions**, since self-attention itself is order-agnostic.  \n",
        "The original *Attention is All You Need* paper uses **sinusoidal encodings**:\n",
        "\n",
        "$$\n",
        "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right), \\quad\n",
        "PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
        "$$\n",
        "\n",
        "- $pos$: position in the sequence $(0, 1, 2, …)$  \n",
        "- $i$: dimension index  \n",
        "- $d$: embedding dimension  \n",
        "\n",
        "Even indices use sine, odd indices use cosine.\n",
        "\n",
        "---\n",
        "\n",
        "### ✨ Key Properties\n",
        "- **Nearby positions** have similar encodings.  \n",
        "- **Multiple frequencies** capture both coarse and fine positional info.  \n",
        "- **Extrapolation**: encodings generalize beyond training length.  \n",
        "\n"
      ],
      "metadata": {
        "id": "r_vec5piwU9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch demo: sinusoidal positional encoding\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "def positional_encoding(seq_len, d_model):\n",
        "    \"\"\"Return sinusoidal positional encodings.\"\"\"\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)   # even dimensions\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)   # odd dimensions\n",
        "    return pe\n",
        "\n",
        "seq_len, d_model = 10, 16\n",
        "pe = positional_encoding(seq_len, d_model)\n",
        "\n",
        "print(\"Shape of positional encoding:\", pe.shape)\n",
        "print(\"\\nFirst position encoding:\\n\", pe[0])\n",
        "print(\"\\nSecond position encoding:\\n\", pe[1])\n"
      ],
      "metadata": {
        "id": "7M4acp9dwYQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Cos](https://erdem.pl/static/3dbeb6eb036c37502841a6a0238b0ebd/21b4d/position-values-45k.png)"
      ],
      "metadata": {
        "id": "uMROFk032yJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization & Residual Connections in Transformers\n",
        "\n",
        "- **Normalization** dramatically improves trainability.  \n",
        "  - **Post-norm (original)**: apply LayerNorm *after* residual addition.  \n",
        "  - **Pre-norm (modern)**: apply LayerNorm *before* sublayer (stabilizes training for deep Transformers).  \n",
        "\n",
        "- **Residual connections** ensure:  \n",
        "  - Input and output of each sublayer have the **same shape**.  \n",
        "  - The sublayer computes a *residual update*, following the ResNet idea:  \n",
        "\n",
        "$$\n",
        "\\text{Output} = \\text{Input} + \\text{Sublayer}(\\text{Input})\n",
        "$$\n"
      ],
      "metadata": {
        "id": "Drcg1ddNvdFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ResNet](https://i.ytimg.com/vi/r0HvOIjziw4/maxresdefault.jpg)"
      ],
      "metadata": {
        "id": "m3JE7Fvz2-63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loss surface comparison\n",
        "![resnet](https://miro.medium.com/v2/resize:fit:723/1*_Qd_txKxRlsMdfuH2J-k4g.png)"
      ],
      "metadata": {
        "id": "gNenClinwl4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![normalization](https://miro.medium.com/1*GwxdzLlnWf1NQ5yHnTSAHA.png)"
      ],
      "metadata": {
        "id": "vRieEEZJw6Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch demo: residual + layer norm (pre-norm vs post-norm)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "x = torch.randn(2, 5, 16)  # (batch, seq_len, d_model)\n",
        "layer_norm = nn.LayerNorm(16)\n",
        "linear = nn.Linear(16, 16)\n",
        "\n",
        "# --- Post-norm (original Transformer) ---\n",
        "out_post = layer_norm(x + linear(x))\n",
        "\n",
        "# --- Pre-norm (modern Transformers) ---\n",
        "out_pre = x + linear(layer_norm(x))\n",
        "\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Post-norm output shape:\", out_post.shape)\n",
        "print(\"Pre-norm output shape:\", out_pre.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGG_fmG3wk9X",
        "outputId": "bbbd9170-b1e2-4c12-9b03-cbe822acd5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 5, 16])\n",
            "Post-norm output shape: torch.Size([2, 5, 16])\n",
            "Pre-norm output shape: torch.Size([2, 5, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Block: Step by Step\n",
        "\n",
        "Each encoder layer applies the following sequence:\n",
        "\n",
        "1. **Multi-Head Attention (MHA)**  \n",
        "   - Self-attention over the input sequence.  \n",
        "\n",
        "2. **Add & Norm**  \n",
        "   - Residual connection + LayerNorm.  \n",
        "\n",
        "3. **Feed Forward (FFN)**  \n",
        "   - Position-wise MLP applied independently to each token.  \n",
        "\n",
        "4. **Add & Norm**  \n",
        "   - Residual connection + LayerNorm.  \n",
        "\n",
        "---\n",
        "![Repeat](https://miro.medium.com/v2/resize:fit:2000/1*XV0h6aTDdEPmL8Gn4gYFoA.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2KKiFNIBwhio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Decoder Block: Step by Step\n",
        "\n",
        "Each decoder layer applies the following sequence:\n",
        "\n",
        "1. **Masked Multi-Head Attention (MHA)**  \n",
        "   - Self-attention over the *target sequence so far*.  \n",
        "   - Causal mask prevents attending to future tokens.  \n",
        "\n",
        "2. **Add & Norm**  \n",
        "   - Residual connection + LayerNorm.  \n",
        "\n",
        "3. **Encoder–Decoder Multi-Head Attention**  \n",
        "   - Queries come from the decoder.  \n",
        "   - Keys/Values come from the encoder output.  \n",
        "\n",
        "4. **Add & Norm**  \n",
        "\n",
        "5. **Feed Forward (FFN)**  \n",
        "   - Position-wise MLP.  \n",
        "\n",
        "6. **Add & Norm**  \n",
        "\n",
        "---\n",
        "\n",
        "![More details](https://i.sstatic.net/eAKQu.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "dGia7T8Wwrj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Decoder](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/encoder_decoder_detail.png)"
      ],
      "metadata": {
        "id": "QMUeqp5RxNSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNNs vs Transformers\n",
        "\n",
        "### RNNs\n",
        "- (+) LSTMs work reasonably well for **long sequences**.  \n",
        "- (–) Require **ordered inputs** (cannot handle sets).  \n",
        "- (–) **Sequential computation**: each hidden state depends on the previous one.  \n",
        "\n",
        "### Transformers\n",
        "- (+) Handle **long sequences** effectively: attention looks at all inputs at once.  \n",
        "- (+) Can operate on **unordered sets** or **ordered sequences** (with positional encodings).  \n",
        "- (+) **Parallel computation**: all attention scores are computed simultaneously.  \n",
        "- (–) **Memory expensive**: need to compute & store \\(N \\times M\\) attention scores per head.  \n"
      ],
      "metadata": {
        "id": "ww-RVKBswuBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch demo: Sequential RNN vs Parallel Transformer Attention\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "batch_size, seq_len, d_model = 1, 6, 8\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# --- RNN: sequential processing ---\n",
        "rnn = nn.RNN(d_model, d_model, batch_first=True)\n",
        "h, _ = rnn(x)  # output (batch, seq_len, d_model)\n",
        "\n",
        "print(\"RNN processes sequentially:\")\n",
        "for t in range(seq_len):\n",
        "    print(f\" Step {t}: hidden state depends on all x[:{t+1}]\")\n",
        "\n",
        "# --- Transformer: parallel attention ---\n",
        "mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=2, batch_first=True)\n",
        "attn_out, attn_weights = mha(x, x, x)\n",
        "\n",
        "print(\"\\nTransformer processes in parallel:\")\n",
        "print(\" Attention weights shape:\", attn_weights.shape)  # (batch, num_heads, seq_len, seq_len)\n",
        "print(\" Each position attends to ALL positions simultaneously.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teWowQXxxIam",
        "outputId": "5ed4a0e4-666d-409e-c278-46b478c7c6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN processes sequentially:\n",
            " Step 0: hidden state depends on all x[:1]\n",
            " Step 1: hidden state depends on all x[:2]\n",
            " Step 2: hidden state depends on all x[:3]\n",
            " Step 3: hidden state depends on all x[:4]\n",
            " Step 4: hidden state depends on all x[:5]\n",
            " Step 5: hidden state depends on all x[:6]\n",
            "\n",
            "Transformer processes in parallel:\n",
            " Attention weights shape: torch.Size([1, 6, 6])\n",
            " Each position attends to ALL positions simultaneously.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems with Vanilla Transformers\n",
        "\n",
        "1. **Quadratic Complexity**  \n",
        "   - Attention is $O(n^2)$ in sequence length $n$.  \n",
        "   - Training with long sequences becomes expensive.  \n",
        "\n",
        "2. **Memory Bottleneck**  \n",
        "   - Attention requires storing all $n \\times n$ alignment scores.  \n",
        "   - Inference on long inputs hits GPU memory limits.  \n",
        "\n",
        "3. **Positional Encoding Limitations**  \n",
        "   - Standard sinusoidal encodings may not extrapolate well to sequences longer than those seen in training.  \n"
      ],
      "metadata": {
        "id": "Sci_6lhyxNFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch demo: Attention cost grows quadratically with sequence length\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "def attention_cost(seq_len, d_model=64, n_heads=4):\n",
        "    mha = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "    x = torch.randn(1, seq_len, d_model)\n",
        "    start = time.time()\n",
        "    _ = mha(x, x, x)  # forward pass\n",
        "    elapsed = time.time() - start\n",
        "    return elapsed\n",
        "\n",
        "for n in [64, 128, 256, 512, 1024]:\n",
        "    t = attention_cost(n)\n",
        "    print(f\"Seq Len = {n:4d}, Time = {t*1000:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGoZlXOLxMkp",
        "outputId": "103dfaf5-a8aa-40d7-8235-693fe66e750f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq Len =   64, Time = 18.29 ms\n",
            "Seq Len =  128, Time = 1.07 ms\n",
            "Seq Len =  256, Time = 3.46 ms\n",
            "Seq Len =  512, Time = 11.30 ms\n",
            "Seq Len = 1024, Time = 39.70 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers, BERT, and GPT-2\n",
        "\n",
        "- **Full Transformer (Seq2Seq)**  \n",
        "  - Encoder stack processes input.  \n",
        "  - Decoder stack generates output, attending to encoder output.  \n",
        "  - Example: **Machine Translation**.  \n",
        "\n",
        "- **BERT (Encoder-only)**  \n",
        "  - Use only the **encoder stack**.  \n",
        "  - Pretrain with **masked language modeling** (bidirectional).  \n",
        "  - Good for representation learning (classification, QA, etc.).  \n",
        "\n",
        "- **GPT (Decoder-only)**  \n",
        "  - Use only the **decoder stack**.  \n",
        "  - Train with **causal/next-word prediction** (autoregressive).  \n",
        "  - Good for generative tasks (text completion, chat, etc.).  \n"
      ],
      "metadata": {
        "id": "z8r_rb77xcyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder-Only Models: GPT-2 Scaling\n",
        "\n",
        "GPT-2 is a **decoder-only Transformer** trained autoregressively.  \n",
        "Its performance improves significantly with scale:\n",
        "\n",
        "| Model     | Parameters |\n",
        "|-----------|------------|\n",
        "| GPT-2 Small  | 117M |\n",
        "| GPT-2 Medium | 345M |\n",
        "| GPT-2 Large  | 762M |\n",
        "| GPT-2 XL     | 1542M |\n",
        "\n",
        "**Scaling laws:**  \n",
        "- Larger models capture longer dependencies & richer structure.  \n",
        "- Requires proportional scaling in compute & data.  \n"
      ],
      "metadata": {
        "id": "KUXsgE7DxfcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretraining Task: Next-Token Prediction\n",
        "\n",
        "- **Training objective:** predict the next token given all previous tokens.  \n",
        "  $$\n",
        "  P(y_t \\mid y_{<t})\n",
        "  $$\n",
        "\n",
        "- **Naive decoding:**  \n",
        "  - At step $t$, recompute queries, keys, values for the **entire prefix**.  \n",
        "  - Total cost grows **quadratically** with sequence length.  \n",
        "\n",
        "- **Caching trick (used in GPT-style models):**  \n",
        "  - Store keys/values from previous steps.  \n",
        "  - At step $t$, compute only $(q_t, k_t, v_t)$ for the new token.  \n",
        "  - Additional cost per step becomes **linear** instead of quadratic.  \n"
      ],
      "metadata": {
        "id": "nCSw53CexnXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch demo: caching key/value states for autoregressive decoding\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "d_model, nhead = 16, 2\n",
        "mha = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
        "\n",
        "# Suppose we decode tokens step by step\n",
        "seq_len = 5\n",
        "cache_k, cache_v = None, None\n",
        "\n",
        "for t in range(seq_len):\n",
        "    x_t = torch.randn(1, 1, d_model)  # new token embedding at step t\n",
        "\n",
        "    # Compute q_t\n",
        "    q_t = x_t\n",
        "\n",
        "    # Update cache (append new k,v)\n",
        "    k_t, v_t = x_t, x_t\n",
        "    cache_k = k_t if cache_k is None else torch.cat([cache_k, k_t], dim=1)\n",
        "    cache_v = v_t if cache_v is None else torch.cat([cache_v, v_t], dim=1)\n",
        "\n",
        "    # Attention: query = q_t, keys/values = cache\n",
        "    out, _ = mha(q_t, cache_k, cache_v)\n",
        "\n",
        "    print(f\"Step {t}: cache size = {cache_k.shape[1]}, output shape = {out.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quFuclWYxyKa",
        "outputId": "64bc613c-5fb9-498b-a75b-b8d38b61ea9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: cache size = 1, output shape = torch.Size([1, 1, 16])\n",
            "Step 1: cache size = 2, output shape = torch.Size([1, 1, 16])\n",
            "Step 2: cache size = 3, output shape = torch.Size([1, 1, 16])\n",
            "Step 3: cache size = 4, output shape = torch.Size([1, 1, 16])\n",
            "Step 4: cache size = 5, output shape = torch.Size([1, 1, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downstream Tasks with GPT-2\n",
        "\n",
        "- **No task-specific fine-tuning** (unlike BERT).  \n",
        "- GPT-2 is pretrained on large, diverse text corpora with **next-token prediction**.  \n",
        "- At inference, the same model can be applied directly to many tasks:  \n",
        "\n",
        "**Examples:**\n",
        "- 🌐 Translation  \n",
        "- ❓ Question Answering  \n",
        "- 📝 Summarization  \n",
        "- 📖 Reading Comprehension  \n",
        "- 🔮 Language Modeling  \n",
        "\n",
        "**Key Idea:**  \n",
        "Pretraining on broad data gives GPT-2 *zero-shot* and *few-shot* abilities — no architectural changes, just different prompts.\n"
      ],
      "metadata": {
        "id": "VBuYcGlmxwjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch demo: One GPT-2 style decoder reused for multiple tasks\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Toy GPT-like decoder-only model\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size=100, d_model=32, nhead=4, nlayers=2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=nlayers)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, memory=None, tgt_mask=None):\n",
        "        h = self.embed(x)\n",
        "        dec_out = self.decoder(h, memory if memory is not None else h, tgt_mask=tgt_mask)\n",
        "        return self.out(dec_out)\n",
        "\n",
        "# Same model reused for different tasks\n",
        "model = MiniGPT()\n",
        "\n",
        "# Example: predict next word in a sentence (language modeling)\n",
        "x = torch.randint(0, 100, (1, 6))  # toy input sequence\n",
        "logits = model(x)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "print(\"Output vocab distribution shape:\", probs.shape)  # (batch, seq_len, vocab_size)\n",
        "print(\"Predicted next token at last step:\", torch.argmax(probs[:, -1, :], dim=-1).item())\n",
        "\n",
        "# The same model could be prompted differently for QA, summarization, translation, etc.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9GE7ALWxeEh",
        "outputId": "57d542dd-b397-4e61-f047-4ed50ed0f6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output vocab distribution shape: torch.Size([1, 6, 100])\n",
            "Predicted next token at last step: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![GPT4](https://www.stylefactoryproductions.com/wp-content/uploads/2023/04/chatgpt-4-training-data-size.png)"
      ],
      "metadata": {
        "id": "M6InfbdVyblD"
      }
    }
  ]
}