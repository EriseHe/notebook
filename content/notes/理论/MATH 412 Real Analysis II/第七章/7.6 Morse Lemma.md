---
title: 7.6 莫尔斯引理
weight: "7.6"
date: 2025-03-22
---
## Morse Theory: Local Behavior Near a Critical Point

Let $f: A \subset \mathbb{R}^n \to \mathbb{R}$ be of class $C^2$ and $x_0$ is a critical point $\rightarrow$ one can use $H(x_0)$ to classify critical points. 

Morse theory: makes this classification **more precise.**
### Morse Lemma

> [!lemma|*]
> Let $f: A \subset \mathbb{R}^n \to \mathbb{R}$ be of class $C^2$ with critical point $x_0 \in A$. If $H(x_0)$ is **non-degenerate**, then there exists a neighborhood of $x_0$ and a diffeomorphism $g$ such that the function $f \circ g$ has the form:
$$f \circ g(y) = f(x_0) + \sum_{i=1}^{\lambda} -y_i^2 + \sum_{i=\lambda+1}^n y_i^2$$
where $\lambda$ is an integer called the **index** of $f$ at $x_0$.
### Applications
- $\lambda = 0$: $x_0$ is local minimum (paraboloid opens upward)
- $\lambda = n$: $x_0$ is local maximum (paraboloid opens downward)
- $0 < \lambda < n$: $x_0$ is saddle point (hyperboloid)

### Idea: Diagonalization of Hessian Matrix
$H(f) = \begin{pmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \\
\vdots & \vdots & \ddots
\end{pmatrix}$

$\lambda$ = # of negative eigenvalues of $H(f)(x_0)$

#### Example: Determine the shape of the surface $z = f(x,y)$ near $(0,0)$

**Solution**: 
At $(0,0)$, $f_x = 0$, $f_y = 0$

Compute eigenvalues of the Hessian matrix. If there is one negative eigenvalue, $\lambda = 1$, it's a saddle point (hyperboloid).

## Constrained Extremal Problem

**Goal**: To maximize or minimize a function $f(x): \mathbb{R}^n \to \mathbb{R}$ under the condition $g(x) = c$.

### Lagrange Multiplier Method

#### A Necessary Condition
**Theorem**: Let $f, g: U \subset \mathbb{R}^n \to \mathbb{R}$ be of class $C^1$. Assume $g(x_0) = c_0$ with $\nabla g(x_0) \neq 0$. If $f$ restricted to the surface $S = \{g(x) = c_0\}$ has a max or min at $x_0$, then there exists a real number $\lambda$ such that:

$$\nabla f(x_0) = \lambda \nabla g(x_0)$$

#### Geometric Meaning
1. $\nabla f(x_0)$ points in the direction of steepest ascent
2. At the extremum, this direction is perpendicular to the surface $S$

#### Geometric Proof
Let $c(t)$ be a fixed curve on $S = \{g(x) = c_0\}$ passing through $x_0$ at $t = t_0$. If $f$ restricted to $S$ has a max at $x_0$, then $f(c(t))$ has max at $t_0$.

Therefore: $\frac{d}{dt}f(c(t))|_{t=t_0} = 0$

By chain rule: $\nabla f(c(t_0)) \cdot c'(t_0) = 0$

Since $c'(t_0)$ is tangent to $S$ at $x_0$, $\nabla f(x_0)$ must be perpendicular to the tangent space of $S$ at $x_0$. Therefore, it must be parallel to $\nabla g(x_0)$, which is normal to the surface.

### Procedure to Solve Extremal Problem
**Step 1**: Solve the system of equations 
$$\nabla f(x) = \lambda \nabla g(x)$$
$$g(x) = c$$

This gives $n+1$ equations with $n+1$ variables $(x_1, x_2, ..., x_n, \lambda)$

**Step 2**: Compute values of $f$ at these critical points and determine which are maxima, minima, or saddle points

#### Example: Find the extrema of
$$f(x,y) = x^2 y^2$$ subject to the condition $x^2 + y^2 = 1$

**Solution**:
1. Applying Lagrange multiplier method:
   $\nabla f = (2xy^2, 2x^2y) = \lambda \nabla g = \lambda(2x, 2y)$
   
2. This gives us:
   - $xy^2 = \lambda x$
   - $x^2y = \lambda y$
   
3. Analyzing by cases:
   - Case 1: If $x = 0$, then $y = \pm 1$ (from constraint)
   - Case 2: If $y = 0$, then $x = \pm 1$ (from constraint)
   - Case 3: If $x \neq 0$ and $y \neq 0$:
     - From the first equation: $y^2 = \lambda$ (dividing by $x$)
     - From the second equation: $x^2 = \lambda$ (dividing by $y$)
     - This gives $x^2 = y^2$
     - With the constraint $x^2 + y^2 = 1$, we get $2x^2 = 1$, so $x = \pm\frac{1}{\sqrt{2}}$ and $y = \pm\frac{1}{\sqrt{2}}$
   
4. Critical points: $(0, \pm 1), (\pm 1, 0), (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}), (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}), (-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}), (-\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}})$

5. Evaluating $f(x,y) = x^2y^2$ at these points:
   - $f(0, \pm 1) = 0$
   - $f(\pm 1, 0) = 0$
   - $f(\pm\frac{1}{\sqrt{2}}, \pm\frac{1}{\sqrt{2}}) = (\frac{1}{2})^2 = \frac{1}{4}$

6. Therefore, the maximum value is $\frac{1}{4}$ at $(\pm\frac{1}{\sqrt{2}}, \pm\frac{1}{\sqrt{2}})$, and the minimum value is $0$ at $(0, \pm 1)$ and $(\pm 1, 0)$.

## Extremal Problem with Multiple Constraints

Maximize/minimize $f(x)$ with constraints:
- $g_1(x) = c_1$
- $g_2(x) = c_2$
- ...
- $g_m(x) = c_m$

### Procedure: Solve the system of equations
$$\nabla f(x) = \lambda_1 \nabla g_1(x) + \lambda_2 \nabla g_2(x) + ... + \lambda_m \nabla g_m(x)$$
$$g_1(x) = c_1, g_2(x) = c_2, ..., g_m(x) = c_m$$

With $m+n$ equations and $m+n$ variables $(x_1,...,x_n, \lambda_1,...,\lambda_m)$

### Analytical Proof of the Theorem (Lagrange Multiplier)

We want to substitute the condition $g(x) = c_0$ into the function $f(x)$ to eliminate the constraint.

Since $\nabla g(x_0) \neq 0$, we may assume without loss of generality that $\frac{\partial g}{\partial x_n} \neq 0$ at $x_0$. 

By the implicit function theorem, the equation $g(x_1, x_2, ..., x_n) = c_0$ can be solved for $x_n$ in a neighborhood of $x_0$:
$$x_n = h(x_1, x_2, ..., x_{n-1})$$

Let $k(x_1, x_2, ..., x_{n-1}) = f(x_1, x_2, ..., x_{n-1}, h(x_1, x_2, ..., x_{n-1}))$

Thus, an extremum of $f$ subject to the constraint corresponds to an extremum of $k$ without constraints.

At an extremum of $k$, we have:
$$0 = \frac{\partial k}{\partial x_i} = \frac{\partial f}{\partial x_i} + \frac{\partial f}{\partial x_n}\frac{\partial h}{\partial x_i}, \quad i = 1, 2, ..., n-1$$

Since $g(x_1, x_2, ..., x_{n-1}, h(x_1, x_2, ..., x_{n-1})) = c_0$ identically, we can differentiate with respect to $x_i$:

$$\frac{\partial g}{\partial x_i} + \frac{\partial g}{\partial x_n}\frac{\partial h}{\partial x_i} = 0, \quad i = 1, 2, ..., n-1$$

Solving for $\frac{\partial h}{\partial x_i}$:

$$\frac{\partial h}{\partial x_i} = -\frac{\frac{\partial g}{\partial x_i}}{\frac{\partial g}{\partial x_n}}, \quad i = 1, 2, ..., n-1$$

Substituting this into our extremum condition:

$$\frac{\partial f}{\partial x_i} - \frac{\partial f}{\partial x_n}\frac{\frac{\partial g}{\partial x_i}}{\frac{\partial g}{\partial x_n}} = 0, \quad i = 1, 2, ..., n-1$$

Rearranging:

$$\frac{\partial f}{\partial x_i}\frac{\partial g}{\partial x_n} - \frac{\partial f}{\partial x_n}\frac{\partial g}{\partial x_i} = 0, \quad i = 1, 2, ..., n-1$$

Let $\lambda = \frac{\partial f}{\partial x_n} / \frac{\partial g}{\partial x_n}$, then:

$$\frac{\partial f}{\partial x_i} = \lambda \frac{\partial g}{\partial x_i}, \quad i = 1, 2, ..., n-1$$

And by definition of $\lambda$, we also have $\frac{\partial f}{\partial x_n} = \lambda \frac{\partial g}{\partial x_n}$

Therefore, in vector form: $\nabla f(x_0) = \lambda \nabla g(x_0)$