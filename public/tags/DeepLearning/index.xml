<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DeepLearning on 学习笔记</title><link>http://localhost:1319/tags/DeepLearning/</link><description>Recent content in DeepLearning on 学习笔记</description><generator>Hugo</generator><language>en</language><atom:link href="http://localhost:1319/tags/DeepLearning/index.xml" rel="self" type="application/rss+xml"/><item><title>Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</title><link>http://localhost:1319/notes/Clippings/Deep-Learning/Geometric-Deep-Learning-Grids-Groups-Graphs-Geodesics-and-Gauges/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1319/notes/Clippings/Deep-Learning/Geometric-Deep-Learning-Grids-Groups-Graphs-Geodesics-and-Gauges/</guid><description>&lt;p>[Submitted on 27 Apr 2021 (
 &lt;a href="https://arxiv.org/abs/2104.13478v1">v1&lt;/a>), last revised 2 May 2021 (this version, v2)]&lt;/p>
&lt;h2 id="titlegeometric-deep-learning-grids-groups-graphs-geodesics-and-gauges">
 Title:Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges
 &lt;a class="anchor" href="#titlegeometric-deep-learning-grids-groups-graphs-geodesics-and-gauges">#&lt;/a>
&lt;/h2>
&lt;p>Authors:, , ,&lt;/p>
&lt;p>
 &lt;a href="https://arxiv.org/pdf/2104.13478">View PDF&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>Abstract:The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach &amp;ndash; such as computer vision, playing Go, or protein folding &amp;ndash; are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation.&lt;br>
While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications.&lt;br>
Such a &amp;lsquo;geometric unification&amp;rsquo; endeavour, in the spirit of Felix Klein&amp;rsquo;s Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.&lt;/p></description></item></channel></rss>