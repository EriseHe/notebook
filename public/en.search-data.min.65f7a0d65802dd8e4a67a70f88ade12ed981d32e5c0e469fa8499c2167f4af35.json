[{"id":0,"href":"/posts/Short-Cut-for-GIST/","title":"Short Cut for Gist","section":"Blog","content":"Enter\nssh astrogroup@170.140.162.12\rPassword:\nNGC6814\rRunning\r#\rgistPipeline --config configFiles/MasterConfig --default-dir configFiles/defaultDir\rUpload directly from PowerShell\r#\rscp \"C:\\\\Users\\\\19175\\\\Desktop\\\\TNG Research\\\\GIST\\\\gistTutorial.tar.gz\" astrogroup@170.140.162.12:~/Erise/\rscp \u0026ldquo;D:\\illustrateTNG Files\\SLOAN_MUSE.py” astrogroup@170.140.162.12:~/miniconda3/envs/gist/lib/python3.6/site-packages/gistPipeline/readData\nscp \u0026ldquo;D:\\illustrateTNG Files\\SLOAN_LR.py” astrogroup@170.140.162.12:~/miniconda3/envs/gist/lib/python3.6/site-packages/gistPipeline/readData\nThe Directory for Read-File\n~/miniconda3/envs/gist/lib/python3.6/site-packages/gistPipeline/readData\rCD command\nGo to gistTutorial cd ~/Erise/gistTutorial\rOpen LogFile\nnano ~/Erise/gistTutorial/results/Test/LOGFILE\rOpen masterConfig\nnano ~/Erise/gistTutorial/configFiles/MasterConfig\rExtract at your folder in Linux server:\ntar -xzvf gistTutorial.tar.gz\r~/miniconda3/envs/gist/lib/python3.6/site-packages/vorbin/voronoi_2d_binning.py\nunzip the gz file:\ngzip -d -k TNG50-reds-0.035-angle-010-FOV-61-re_kpc-10-snap-98-460756.cube.fits.gz\rFinding\nfind ~/Erise/gistTutorial -name _______\rRemove File\nrm\rRemove Dir\nrmdir\rC:\\Users\\19175\\Desktop\\TNG Research\\YinziyuanResults\\TNG50-reds-0.06-angle-010-FOV-61-re_kpc-15-snap-98-460756.cube.fits.gz\nscp \u0026ldquo;C:\\Users\\19175\\Desktop\\TNG Research\\YinziyuanResults\\TNG50-reds-0.06-angle-010-FOV-61-re_kpc-15-snap-98-460756.cube.fits.gz\u0026rdquo; astrogroup@170.140.162.12:~/Erise/gistTutorial/inputData\nC:\\Users\\19175\\Desktop\\TNG Research\\YinziyuanResults\\TNG50-reds-0.06-angle-010-FOV-61-re_kpc-15-snap-98-460756.cube.fits.gz\nUpload SAURON_LR\ngistpipline\nNGC0000Example\nQuestion\n"},{"id":1,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/01-Mid-Point-Method/","title":"01 Mid Point Method","section":"数值方法","content":"\rFormula Derivation\r#\rThe derivative approximation:\r$$ \\begin{aligned} f(t_i, u_i) \u0026amp;= \\frac{dy}{dt} \\Big|{t_i} \\approx \\frac{y(t{i+1}) - y(t_{i-1})}{2\\Delta t} + \\mathcal{O}(\\Delta t^2) \\end{aligned} $$\nUpdate formula:\n$$ \\begin{aligned} u_{i+1} \u0026amp;= u_{i-1} + 2\\Delta t f(t_i, u_i) \\end{aligned} $$\nInitial Conditions\r#\rInitial Condition Calculation: Given $y_0$ $$ \\begin{aligned} u_2 \u0026amp;= y_0 + 2\\Delta t f(t_1, u_1) \\end{aligned} $$\nPredictor-Corrector Steps:\n$$ \\begin{aligned} u_1^* \u0026amp;= y_0 + \\Delta t f(t_0, y_0), \\ u_1 \u0026amp;= y_0 + \\frac{\\Delta t}{2} (f(t_0, y_0) + f(t_1, u_1^*)) \\end{aligned} $$\nTwo-Step Method\r#\rHigher-order method that improves accuracy using information from two previous time steps. The method achieves second-order accuracy due to the central difference approximation. Absolute Stability\r#\rWe consider the test equation:\n$$ \\begin{aligned} \\frac{dy}{dt} \u0026amp;= -\\lambda y, \\quad y(0) = y_0 \\end{aligned} $$\nwhere $\\lambda \u0026gt; 0$.\nDiscretization\r#\rThe numerical update formula is:\n$$ \\begin{aligned} u_{n+1} \u0026amp;= u_{n-1} - 2\\Delta t \\lambda u_n \\end{aligned} $$\nRearranging:\n$$ \\begin{aligned} u_{n+1} + 2\\Delta t \\lambda u_n - u_{n-1} \u0026amp;= 0 \\end{aligned} $$\nIterative Computation\r#\rStarting with initial conditions ($y_0$, $y_1$):\n$$ \\begin{aligned} u_2 \u0026amp;= y_0 - 2\\Delta t \\lambda y_1, \\ u_3 \u0026amp;= y_1 - 2\\Delta t \\lambda u_2, \\ u_4 \u0026amp;= u_2 - 2\\Delta t \\lambda u_3, \\ u_5 \u0026amp;= u_3 - 2\\Delta t \\lambda u_4 \\end{aligned} $$\nThis formulation helps analyze the stability of the numerical scheme by checking whether the sequence $u_n$ grows or decays as $n \\to \\infty$.\nStability Analysis of the Numerical Scheme\r#\rWe assume a solution of the form:\n$$ \\begin{aligned} u_i \u0026amp;= C \\beta^i \\end{aligned} $$\nSubstituting into the Recurrence Relation\r#\r$$ \\begin{aligned}\n\\end{aligned} $$ $$ \\begin{aligned} C \\beta^{i+1} + C 2\\Delta t \\lambda \\beta^i - C \\beta^{i-1} \u0026amp;= 0\\ \\beta^2 + 2\\Delta t \\lambda \\beta - 1 \u0026amp;= 0 \\quad \\text{devided by $C \\beta^{i-1}$.} \\end{aligned} $$ This is a characteristic equation for the recurrence relation. And solving for $\\beta$,\n$$ \\begin{aligned} \\beta \u0026amp;= \\frac{-2\\Delta t \\lambda \\pm \\sqrt{(2\\Delta t \\lambda)^2 + 4}}{2} \\end{aligned} $$ To ensure stability, we require:\n$$ \\begin{aligned} |\\beta_0|, |\\beta_1| \u0026amp;\\leq 1. \\end{aligned} $$\nGeneral Solution\r#\rSince the recurrence relation is second-order, the general solution is: $$ \\begin{aligned} u_i \u0026amp;= C_0 \\beta_0^i + C_1 \\beta_1^i. \\end{aligned} $$ From the initial conditions:\n$$ \\begin{aligned} u_0 \u0026amp;= C_0 + C_1 = y_0, \\\n\\nu_1 \u0026amp;= C_0 \\beta_0 + C_1 \\beta_1 = y_1 \\end{aligned} $$ which can be solved for $C_0$ and $C_1$.\nStability Condition\r#\rFor stability, the roots $\\beta_0, \\beta_1$ must satisfy:\n$$ \\begin{aligned}|\\beta_0 \\beta_1| \u0026amp;= 1.\\end{aligned} $$\nFrom the characteristic equation:\n$$ \\begin{aligned}\\beta_0 \\beta_1 \u0026amp;= -\\frac{1}{\\beta_1}. \\end{aligned} $$\nEnsuring $|\\beta| \\leq 1$ determines the absolute stability region.\nFinite Difference Approximation and Stability Analysis\r#\rFinite Difference Approximation\r#\r$$ \\begin{aligned} \\alpha u_{i+1} + \\beta u_i + \\sigma u_{i-1} + \\delta u_{i-2} \u0026amp;= (\\alpha + \\beta + \\sigma + \\delta) u_i + \\mathcal{O}(\\Delta t^5) \\end{aligned} $$\nTime Discretization\r#\r$$ \\begin{aligned} \\frac{du}{dt} \\Big|{t_i} \u0026amp;= \\frac{u_i - u{i-1}}{\\Delta t} + \\mathcal{O}(\\Delta t^2) \\end{aligned} $$\nStability Analysis\r#\r$$ \\begin{aligned} \\alpha + 4\\beta \u0026amp;= 0, \\ \\alpha \u0026amp;= -4\\beta, \\ 4\\beta - 2\\beta \u0026amp;= 1 \\Rightarrow \\beta = \\frac{1}{2}, \\quad \\alpha = -2. \\end{aligned} $$\nThese constraints ensure numerical stability and proper convergence of the finite difference scheme.\nTaylor Expansions\r#\rApplying Taylor expansions to express the function values at different time steps: $$ \\begin{aligned} \u0026amp; \\alpha\\left[u_{i+1}=u_i-\\left.\\frac{d u}{d t}\\right|{t_i} \\Delta t+\\ldots\\right] \\ \u0026amp; \\beta\\left[u{i+2}=u_i-\\left.\\frac{d u}{d t}\\right|{t_i} 2 \\Delta t+\\ldots\\right] \\ \u0026amp; \\gamma\\left[u{i+3}=u_i-\\left.\\frac{d u}{d t}\\right|{t_i} 3 \\Delta t+\\ldots\\right] \\ \u0026amp; \\delta\\left[u{i+4}=u_i-\\left.\\frac{d u}{d t}\\right|_{t_i} 4 \\Delta t+\\ldots\\right] \\end{aligned} $$ Summing these expansions, we obtain the system of equations,\n$$ \\left{\\begin{aligned} -\\alpha-2 \\beta-3 \\gamma-4 \\delta \u0026amp; =1 \\ \\alpha+4 \\beta+8 \\gamma+16 \\delta \u0026amp; =0 \\end{aligned}\\right. $$\n"},{"id":2,"href":"/docs/Mathematics/MATH-351-352/Heat-Equation/","title":"热方程","section":"偏微分方程","content":"The general form of heat equation (diffusion equation) is represented as\n$$ u_t-k\\Delta u=0 $$\nwhere $x\\in U$ for $U \\in \\R^n$ (n-dimentional) s.t we have $u:\\bar{U} \\times [0,\\infty) \\to \\R$.\n$k$ is the thermal diffusivity of the material. prototype of parabolic PDEs normalized heat equation where $k=1$ is specified for theoretical studies (focusing on mathematical analysis) For 1-dimentional special case:\n$$ u_t-ku_{xx}=0 $$\nwhere ${x,t\\in(-\\infty,\\infty), [0, \\infty)}$.\nLecture 10\r#\r下面的排序是按照推导热方程解析解的顺序来的。\n基本思路：\n我们先通过\r标度不变性（scale invariance）找到一个所有解通用的形式，并且嵌入原方程进行运算。\n最终，我们得到一维的：\n10.1 热传导方程的基本解\r#\rThe Fundamental Solution to Heat Equation\n$$ \\Phi(x,t)=\\frac{1}{\\sqrt{4\\pi k t}} e^{-\\frac{x^2}{4kt}} $$\n10.1.1 正态分布函数的属性\r#\r以及与它作为一个正态分布函数的属性：\nProperties of The Fundamental Solution\n10.2 对于柯西问题的解\r#\r接着，我们再用过平移不变性 **（**translation invariance）和卷积来进一步确定柯西问题（$t=0$）的解。\nThe Solution to Cauchy Problem\n$$ u(x,t) = \\frac{1}{(4\\pi k t)^{1/2}} \\int^{+\\infty}_{-\\infty} e^{-\\frac{(x-y)^2}{4kt}} g(y) , dy $$\n并且可以用误差函数来表示这个解\n$$ u(x, t) = \\lim_{x \\to \\infty} \\text{erf}(x\\sqrt{4\\pi kt}) $$\nLecture 11\r#\r11.1 三种不同的边界条件\r#\rDifferent Types of Boundary Conditions\n11.2 分离变量法\r#\rSeparation of Variables Lecture 12\r#\r12.1 傅里叶展开和变换\r#\rThe Fourier Series\nLecture 13\r#\r13.1 热传导公式解\r#\rThe Fourier Expansion for Heat Equation Solution\n13.2 Dirichlet问题的解\r#\rNon-Homogenous Dirichlet problem\nLecture 14\r#\r14.1 解的唯一性\r#\rUniqueness of Solution\n14.2 电梯方程\r#\rLifting Function\n💡 the transformed coefficient is defined as\r$$ \\hat \\mu =\\dfrac{\\mu}{(b-a)^2} $$\nLecture 15\r#\r15.1 施图姆-刘维尔理论\r#\rSturm Liouville Theory (SLE)\nLecture 16\r#\r16.1 极大值原理\r#\rThe Principle of Maximum\n"},{"id":3,"href":"/posts/Integrate-DeepL-Translation-Instruction/","title":"Integrate Deep L Translation Instruction","section":"Blog","content":"\r1. Install R and Babeldown\r#\r1.1 Install R\r#\rhttps://cran.r-project.org/.\n(The following task is using R console)\n1.2 Install Babeldown\r#\rMore specific instruction, check here: https://docs.ropensci.org/babeldown/\n1.2.1 This command install \u0026lsquo;remotes\u0026rsquo; from CRAN if not already installed:\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\rinstall.packages(\"remotes\")\r}\r1.2.2 Uses the ‘remotes’ package to install the ‘babeldown’ package from its GitHub repo:\ninstall.packages('babeldown', repos = c('https://ropensci.r-universe.dev', 'https://cloud.r-project.org'))\r2. Set up DeepL API (Inside of R console)\r#\rGo to DeepL\u0026rsquo;s website and get an API key: https://www.deepl.com/en/your-account/keys 3. Connect Babeldown to DeepL API\r#\rBabeldown uses the DeepL free API URL by default (no need to set up unless pro API).\n3.1 Download a keyring package (for secure API key retrieval)\r#\rinstall.packages(\"keyring\")\r3.2 Keyring requests your API key\r#\rlibrary(keyring)\rkeyring::key_set(\"deepl\", prompt = \"API key:\")\rEnter your API key and then in any script you use babeldown, you’d retrieve the key like so:\nSys.setenv(DEEPL_API_KEY = keyring::key_get(\"deepl\"))\r4. (optional) Set up working directory\r#\rIn R, use getwd() to check current working directory, and you may use setwd(\u0026quot;your absolute path\u0026quot;) to move your working directory for convenience.\nMy setup is:\nsetwd(\"/Users/erisehe/Documents/GitHub/erisehe.github.io\")\r4. Translates\r#\rSince my working directory is at (\u0026quot;\u0026hellip;/erisehe.github.io\u0026quot;), so I runs relative path. The commands, for example, is:\nbabeldown::deepl_translate_hugo(\rpost_path = \"content.en/docs/Mathematics/MATH 412 Real Analysis II/Lecture/name.md\",\rtarget_lang = \"ZH\",\rsource_lang = \"EN\",\rforce = TRUE\r)\rIt translate only one file at a time.\nHow to Translate an Entire Folder\r#\rYou can use base R functions (or packages like purrr) to list the files and apply the translation function.\n# Define the source folder containing your markdown files\rsource_dir \u003c- \"path/to/your/source_folder\"\r# Define the target folder where you want to save the translated files\rtarget_dir \u003c- \"path/to/your/target_folder\"\rif (!dir.exists(target_dir)) {\rdir.create(target_dir)\r}\r# List all markdown files in the source directory\rfiles \u003c- list.files(source_dir, pattern = \"\\\\.md$\", full.names = TRUE)\r# Loop through each file and translate it\rfor (f in files) {\r# Translate the file using babeldown's deepl_translate_hugo function\rbabeldown::deepl_translate_hugo(\rpost_path = f,\rtarget_lang = \"ZH\",\rsource_lang = \"EN\",\rforce = TRUE\r)\r# If the function writes the output file in a default location or with a predictable name,\r# you can move or copy it to your target directory. For example:\routput_file \u003c- file.path(dirname(f), paste0(\"translated_\", basename(f)))\rif (file.exists(output_file)) {\rfile.copy(output_file, file.path(target_dir, basename(output_file)))\r}\r}\r"},{"id":4,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/02-Three-Point-Backward-Differentiation-Formula/","title":"02 Three Point Backward Differentiation Formula","section":"数值方法","content":"\r1. Ok Honestly I Have No Idea Where He Started\r#\rMATH 212 is useless - Alessandro Veneziani\nGiven the population problem #Implicit:\n$$ \\dfrac{d y}{d x} = A\\left( 1 - \\dfrac{y}{B}\\right)y $$\r#\rNumerically, the problem is: $$ \\frac{u_{i+1} - u_i}{\\Delta t} = A \\left(1 - \\frac{u_{i+1}}{B} \\right) u_{i+1} $$ To solve this numerically, we rewrite the equation: $$ \\begin{align} x - u_i \u0026amp;= A \\left(1 - \\frac{x}{B} \\right) x \\ \u0026amp;= A x - \\frac{A}{B} x^2 \\end{align} $$\nusing $f(x)=0$, and we iterate using Newton\u0026rsquo;s method:\nyes he changed notation again\n[!remark|*] Newton\u0026rsquo;s Method $$\\underbrace{x^{(u+1)}}{y{\\text{new}}}=\\underbrace{x^{(u)}}{y{\\text{old}}}-\\frac{f(x^{(u)})}{f\u0026rsquo;(x^{(u)})}$$ We have $|y_{\\text{new}}-y_{\\text{old}}|\\leq \\text{tol}.$\n$$ \\begin{align} x - u_i \u0026amp; = \\Delta t A \\left(1 - \\frac{x}{B} \\right) x \\ \u0026amp; = \\Delta t A x - \\Delta t \\frac{A}{B} x^2 \\\n\\Longrightarrow \\quad x - u_{i}-\\Delta t A x + \\Delta t \\frac{A}{B} x^{2}\u0026amp;=0 \\end{align} $$ Define a function $g(x)$ from above: $$ \\begin{align} g(x) \u0026amp; = x - u_i - \\Delta t A x + \\Delta t \\frac{A}{B} x^{2}\\ \\end{align} $$ Taking the derivative: $$ \\begin{align} g\u0026rsquo;(x) \u0026amp;= \\frac{d}{dx} \\left( x - u_i - \\Delta t A x + \\Delta t \\frac{A}{B} x^2 \\right) \\ \u0026amp;= 1 - \\Delta t \\left(A - \\frac{2A}{B} x \\right) \\ \u0026amp;= 1 - \\Delta t f\u0026rsquo;(x). \\end{align} $$ For $f^{\\prime}(x)=A-\\frac{2 A}{B} x$.\n2. Approximate $\\frac{d y}{d x}$ Using a Three-Point Backward Differentiation Formula (BDF)\r#\rtracing back to last lecture on determination of coefficients $A, B, C$\n$$ \\begin{aligned} f(t_{i},y_{i})=\\left. \\frac{dy}{dx} \\right|{x_i} \u0026amp;\\approx \\frac{3}{2 \\Delta x} y_i - \\frac{4}{2 \\Delta x} y{i-1} + \\frac{1}{2 \\Delta x} y_{i-2} \\ \\ \\text{Taylor Expansion}\\Longrightarrow\\quad \u0026amp;\\left{ \\begin{aligned} y_{i-1} \u0026amp;= y_i - \\left. \\frac{dy}{dx} \\right|{x_i} \\Delta x \\dots \\ y{i-2} \u0026amp;= y_i - \\left. \\frac{dy}{dx} \\right|{x_i} 2 \\Delta x \\dots \\end{aligned} \\right. \\end{aligned}$$ we yield: $$ \\begin{align} \\frac{3}{2 \\Delta x} u_i\u0026amp;-\\frac{4}{2 \\Delta x} u{i-1}+\\frac{1}{2 \\Delta x} u_{i-2}=f\\left(t_i , u_i\\right)\\ \\end{align} $$\nImplicit formula: $$ u_i=\\frac{4}{3} u_{i-1}-\\frac{1}{3} u_{i-2}+\\frac{2}{3} \\Delta x\\left(t_i, u_i\\right) $$ we substitute $f(t_{i},u_{i})=\\lambda u_{i}$ to derive explicitly:\n$$\\begin{align} u_i - \\frac{4}{3} u_{i-1} + \\frac{1}{3} u_{i-2} - \u0026amp;\\frac{2}{3} \\Delta x f(t_i, u_i) = 0 \\ u_i - \\frac{4}{3} u_{i-1} + \\frac{1}{3} u_{i-2} - \u0026amp;\\frac{2}{3} \\Delta x \\lambda u_{i} = 0 \\ \\end{align} $$ Explicit formula: $$\n\\boxed{u_i = \\frac{1}{1 - \\frac{2}{3} \\Delta x\\lambda} \\left( \\frac{4}{3} u_{i-1} - \\frac{1}{3} u_{i-2} \\right) }\n$$\n[!definition|*] Generalized p-step BDF Form $$\\boxed{u_i-\\sum_{j=1}^{p} \\alpha_j u_{i-j}=\\Delta x \\beta_{-1} f\\left(t_i, u_i\\right)} $$ Generalized Implicit Multistep Method: $$\\boxed{u_{i+1}-\\sum_{j=1}^p \\alpha_j u_{i-j}=\\Delta x \\sum_{j={-1}}^p \\beta_j f\\left(t_{i-j}, u_{i+1-j}\\right)}$$\n"},{"id":5,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/03-Numerical-Solution-of-Ordinary-Differential-Equations/","title":"常微分方程的数值解","section":"数值方法","content":"\rIntroduction\r#\rThe subject of this Chapter is the numerical approximation of the Cauchy problem:\n$$(1) \\quad \\frac{dy}{dt} = f(t,y) \\quad \\text{in } t \u0026gt; 0 \\quad (I.C.)$$\nwith $$y(0) = y_0 \\text{ given}.$$\nor, more in general, a system:\n$$(2) \\quad \\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(t,\\mathbf{y}) \\quad \\text{in } t \u0026gt; 0$$\nwith $$\\mathbf{y}(0) = \\mathbf{y}_0.$$\nLet\u0026rsquo;s recall some basic results on the function $f$. It is said to be Lipschitz continuous in an interval $I$ if there exists a positive $L \u0026gt; 0$ s.t.\n$$|f(t,y_1) - f(t,y_2)| \u0026lt; L |y_1 - y_2|.$$\nClearly, if $f$ is differentiable with continuity in $I$, it is also Lipschitz continuous with $L \\leq \\max_I \\left|\\frac{\\partial f}{\\partial y}\\right|$.\nLocal Theorem\r#\rIf $f$ is Lipschitz continuous in a range $t \\in I_1$ and $y \\in I \\subseteq \\mathbb{R}$, then $\\exists$ an interval $\\hat{I} \\subseteq I$ where the solution to $(1)$ exists and is unique.\nGlobal Theorem\r#\rIf the $f$ is Lipschitz continuous $\\forall t \\in I$ and $y \\in \\mathbb{R}$, then the solution $\\exists$ uniquely in $I$.\nStability Definitions\r#\rFrom the practical point of view, it is important to consider also the perturbed case:\n$$(1_\\epsilon): \\quad \\frac{dy_\\epsilon}{dt} = f(t, y_\\epsilon) + \\delta(t) \\quad t \\geq 0$$\n$$y^\\epsilon(0) = y_0 + \\epsilon$$\nwith $|\\delta(t)| \\leq \\epsilon \\quad \\forall t \\geq 0$\nIf there exists a finite constant $C$ such that\n$$|y - y_\\epsilon| \u0026lt; C\\epsilon \\quad (*)$$\nthen we say that the solution is Lyapunov stable.\nIn general, $C$ may depend on $t$, so that $(*)$ may hold on finite intervals (and not over the entire $[0,\\infty)$ axis).\nTo have a stronger concept, we advocate the ASYMPTOTIC STABILITY:\n$$\\lim_{t \\to \\infty} |y(t) - y_\\epsilon(t)| = 0.$$\nFrom the engineering point of view, this is a central concept, as the theory of control (Automatical Control, Feedback control) relies on this definition.\nRemark\r#\rThe Cauchy problem has a formal (quite useless) solution:\n$$y(t) = y_0 + \\int_0^t f(\\tau, y(\\tau))d\\tau$$\nconnecting the solution in $t$ with its past.\nThis is basically an alternative formulation of the problem and, in fact, will be used to formulate possible numerical approximation alternative to the ones based on $(1)$.\nSome Simple Examples\r#\rTo begin with, we can split the time interval in subintervals, for instance with a uniform reticulation with step $\\Delta t$.\n![Time discretization with points at 0, Δt, 2Δt, 3Δt\u0026hellip;]\nThen, we can use the formula:\n$$\\frac{dy}{dt}(t_i) \\simeq \\frac{y(t_{i+1}) - y(t_i)}{\\Delta t}$$\nthat we know is accurate with an error scaling with $\\Delta t$. In this way, we have:\n[From now on, the approximation of the solution $y(t)$ in $t_i$ will be denoted with $u_i$]\n$$\\frac{u_{i+1} - u_i}{\\Delta t} = f(t_i, u_i)$$\nIn practice, starting at $t = 0$: $$\\begin{align} u_1 \u0026amp;= u_0 + \\Delta t , f(t_0, u_0) \\quad \\rightsquigarrow \\quad (u_0 = y_0) \\ u_2 \u0026amp;= u_1 + \\Delta t , f(t_1, u_1) \\end{align}$$\nWe can easily compute the approximation $u_i$ of $y(t_i)$.\nOn the other hand, we could do:\n$$\\frac{u_i - u_{i-1}}{\\Delta t} = f(t_i, u_i)$$\nleading to:\n$$u_1 = u_0 + \\Delta t , f(t_1, u_1) \\quad (u_0 = y_0)$$\nThis is not as easy as before: it\u0026rsquo;s a NONLINEAR (ALGEBRAIC) EQUATION; we know how to solve it (first Chapter), but it is a computational additional cost. Then, similarly, we have:\n$$u_2 = u_1 + \\Delta t , f(t_2, u_2)$$\nWe have also another option:\n$$\\frac{dy}{dt}(t_i) \\simeq \\frac{u_{i+1} - u_{i-1}}{2\\Delta t}$$\nIn this case, the error scales with $O(\\Delta t^2)$. So, in practice we have:\n$$u_{i+1} = 2\\Delta t , f(t_i, u_i) + u_{i-1}$$\nor specifically: $u_2 = 2\\Delta t , f(t_1, u_1) + u_0 \\quad (u_0 = y_0)$\nI need to know $u_1$, not just $u_0$, then we can use the method.\nWith these three examples, we have already found many possible types of methods:\nImplicit vs Explicit\nImplicit: Solve a non-linear equation Explicit: No need of solving equations One Step vs Multistep\nOne Step: $u_{i+1} = g(\\Delta t, u_i)$ Multistep: $u_{i+1} = g(\\Delta t, u_i, u_{i-1}, u_{i-2}\u0026hellip;)$ At the top of this, we have the problem of the accuracy and - even most importantly- of the CONVERGENCE.\nIn fact, the basic requirement we need is that the method is convergent:\n$$\\lim_{\\Delta t \\to 0} |y(t_i) - u_i| = 0$$\nThen, if we find that $|y(t_i) - u_i| \\sim O(\\Delta t^p)$ then the accuracy or the order of the method is $p$.\nNature (implicit/explicit), Number of steps and most important CONVERGENCE and accuracy (order) are the categories for analyzing and ranking different methods.\nBefore we embark ourselves in a general analysis, however, let\u0026rsquo;s focus on a specific case, where important concepts will be highlighted.\nAnalysis of Forward Euler\r#\rThe method:\n$$\\frac{u_{i+1} - u_i}{\\Delta t} = f(t_i, u_i)$$\nis called Forward Euler.\nLet\u0026rsquo;s consider it in detail.\nTo start with, let\u0026rsquo;s introduce the distinction of \u0026ldquo;consistency\u0026rdquo; and truncation error.\nIf we have the exact solution $y_{ex}(t)$ it is easily realized that\n$$\\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\\Delta t} \\neq f(t_i, y_{ex}(t_i))$$\nFor instance,\n$$\\frac{dy}{dt} = \\lambda y \\quad y(0) = 1 \\implies y_{ex} = e^{\\lambda t}$$\nthen\n$$\\frac{e^{\\lambda(t_i+\\Delta t)} - e^{\\lambda t_i}}{\\Delta t} = e^{\\lambda t_i} \\frac{e^{\\lambda \\Delta t} - 1}{\\Delta t} \\neq \\lambda e^{\\lambda t_i} \\quad (\\lambda y(t_i))$$\nWe can be more specific:\n$$y_{ex}(t_{i+1}) = y_{ex}(t_i) + \\frac{dy_{ex}}{dt}(t_i)\\Delta t + \\frac{1}{2}\\frac{d^2y_{ex}}{dt^2}(t_i)\\Delta t^2 + \u0026hellip;$$\nNow:\n$$\\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\\Delta t} = \\frac{dy_{ex}(t_i)}{dt} + \\frac{1}{2}\\frac{d^2y_{ex}(t_i)}{dt^2}\\Delta t + \u0026hellip;$$\nThis gives us:\n$$\\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\\Delta t} = f(t_i, y_{ex}(t_i)) + \\left[\\frac{\\Delta t}{2}\\frac{d^2y_{ex}}{dt^2}\\right]$$\nForward Euler $\\quad \\quad \\quad$ Local Truncation Error (LTE)\nIn some sense, the truncation error is the residual we get when we pretend the exact solution to solve the numerical scheme.\nNow, to investigate how the error of Forward Euler works, let\u0026rsquo;s consider the following picture:\n![Error propagation diagram showing exact solution trajectory and numerical approximation]\nFrom the picture it is evident that the error:\n$$e_{i+1} = y_{ex}(t_{i+1}) - u_{i+1}$$\nis the result of two contributions:\n$$e_{i+1} = \\underbrace{y_{ex}(t_{i+1}) - u^{i+1}}{\\text{generated at the local step}} + \\underbrace{u^{i+1} - u{i+1}}_{\\text{propagated from previous steps}}$$\nFrom the previous definition:\n$$y_{ex}(t_{i+1}) = y_{ex}(t_i) + \\Delta t , f(t_i, y_{ex}(t_i)) + \\Delta t , \\tau_{i+1}$$\nwhere $\\tau_{i+1} = \\frac{\\Delta t}{2}\\frac{d^2y_{ex}(t_i)}{dt^2}$ is the local TE.\n$$u^*{i+1} = y{ex}(t_i) + \\Delta t , f(t_i, y_{ex}(t_i))$$\n$$y_{ex}(t_{i+1}) - u^*_{i+1} = \\Delta t , \\tau_i$$\nNow, let\u0026rsquo;s focus on the other part.\nThis second component $u^*{i+1} - u{i+1}$ is inherited from the previous errors.\n$$u^*{i+1} = y{ex}(t_i) + \\Delta t , f(t_i, y_{ex}(t_i))$$ $$u_{i+1} = u_i + \\Delta t , f(t_i, u_i)$$\nUsing the Lipschitz assumption $|f(t,y) - f(t,u)| \\leq L |y-u|$ we have:\n$$|u^*{i+1} - u{i+1}| \\leq |e_i| + \\Delta t , L |e_i| = (1 + \\Delta t , L)|e_i|$$\nwhere $e_i = y_{ex}(t_i) - u_i$\nAll together, we have:\n$$|e_{i+1}| \\leq \\Delta t |\\tau_i| + (1 + \\Delta t , L)|e_i|$$\nNow, take $|\\tau^*| = \\max_i |\\tau_i| = \\text{GLOBAL TRUNCATION ERROR}$.\nThen:\n$$|e_{i+1}| \\leq \\underbrace{\\Delta t |\\tau^*|}{\\text{local}} + \\underbrace{(1 + \\Delta t , L)}{\\text{propagated}}|e_i|$$\nAssume that $e_0 = 0$ (no errors on the initial conditions). Then we have:\n$$|e_1| \\leq \\Delta t |\\tau^|$$ $$|e_2| \\leq \\Delta t |\\tau^| + (1 + \\Delta t , L)|e_1| \\leq \\Delta t |\\tau^|(1 + (1+\\Delta t , L))$$ $$|e_3| \\leq \\Delta t |\\tau^| + (1+\\Delta t , L)|e_2| \\leq \\Delta t |\\tau^*|(1 + (1+\\Delta t , L) + (1+\\Delta t , L)^2)$$\nWe infer:\n$$|e_K| \\leq \\Delta t |\\tau^| \\sum_{j=0}^{K-1}(1+\\Delta t , L)^j = \\Delta t |\\tau^|\\frac{(1+\\Delta t , L)^K - 1}{1 - 1 - \\Delta t , L} = \\frac{|\\tau^*|}{L}((1+\\Delta t , L)^K - 1)$$\nNotice that:\n$$(1 + x)^K \\leq \\exp(xK)$$\nSo:\n$$|e_K| \\leq \\frac{|\\tau^|}{L}(\\exp(LKh) - 1) = \\frac{|\\tau^|}{L}(\\exp(Lt_K) - 1)$$\nwhere $K\\Delta t = t_K$\nNow, if we want to have a bound on the error in the interval $[0,T]$, we have:\n$$|e| \\leq \\frac{|\\tau^*|}{L}(\\exp(LT) - 1)$$\nWe have proved the following Theorem:\nConvergence of Forward Euler\r#\rIf the initial data are exact, and $f$ is Lipschitz continuous with respect to the $y$-argument, with a solution $\\in C^2(0,T)$, then FE converges.\nIn fact:\n$$|\\tau^*| = \\frac{1}{2}\\Delta t \\max_i |y^{\u0026rsquo;\u0026rsquo;}| \\xrightarrow{\\Delta t \\to 0} 0$$\nand $|e| \\leq |\\tau^*|\\frac{\\exp(LT) - 1}{L} \\xrightarrow{\\Delta t \\to 0} 0$ is bounded.\nFE is convergent with order 1.\nBeyond the Theorem per se (we will generalize it to other methods), it is important to notice that the convergence is the consequences of two peculiarities:\n(1) $\\tau^* \\xrightarrow{\\Delta t \\to 0} 0$ the LTE/GTE vanishes as $\\Delta t \\to 0$, so locally the error is under control.\nThis property is called CONSISTENCY.\n(2) The factor $\\frac{\\exp(LT) - 1}{L}$ is independent of $\\Delta t$ (or, in general, doesn\u0026rsquo;t blow up for $\\Delta t \\to 0$). This is related to the way the error propagates so it is a \u0026ldquo;global\u0026rdquo; property through the constant $L$.\nThe control of the error in time is called STABILITY.\nIn some sense, we can say that:\nCONVERGENCE = CONSISTENCY + STABILITY\nIn spite of the intuitive arguments we used here, we will see that this is a good representation of general results (Lax-Richtmyer equivalence Theorem) holding for method using linear combinations of the function $f$ evaluated in the points $(t_i, u_i)$.\nOther One-Step Methods\r#\rSo far, we have two one-step methods (forward and backward Euler). Let\u0026rsquo;s see other two. It is instructive to see how they are devised.\nCrank-Nicolson\r#\rFrom $y(t) = y_0 + \\int_0^t f(\\tau, y(\\tau))d\\tau$ we can organize the following method:\n![Timeline showing points $t_0, t_1, t_2, t_3, t_4, t_5, t_6, t_7$]\nLocalize: $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(\\tau, y(\\tau)) d\\tau$\nDiscretize: approximate the integral with the trapezoidal rule:\n$$u_{n+1} = u_n + \\frac{f(t_{n+1}, u_{n+1}) + f(t_n, u_n)}{2} \\Delta t$$\nThis is a Second Order (EXERCISE) one-step implicit method.\nHeun\r#\rLet\u0026rsquo;s start from Crank-Nicolson and make it \u0026ldquo;explicit\u0026rdquo;.\nOn the right hand side of CN we set:\n$$u_{n+1} \\simeq u_n + \\Delta t , f(t_n, u_n) \\quad \\text{(Explicit Euler)}$$\nWe obtain the scheme:\n$$u_{n+1} = u_n + \\Delta t \\frac{f(t_{n+1}, u_n+\\Delta t f(t_n, u_n)) + f(t_n, u_n)}{2}$$\nThis is called Heun. It is One-step, still 2nd order (EXERCISE).\nThe Concept of Zero-Stability\r#\rLet\u0026rsquo;s start applying the concept to one-step methods in the form:\n$$u_{n+1} = u_n + \\Delta t , \\Phi(u_n, t_n, f_n; \\Delta t)$$\nLet\u0026rsquo;s consider the perturbed scheme:\n$$\\begin{cases} w_{n+1} = w_n + \\Delta t (\\Phi(z_n, t_n, f(t_n, z_n); \\Delta t) + \\delta_n) \\ w_0 = y_0 + \\delta_0 \u0026amp; \\text{with } |\\delta_i| \\leq \\varepsilon \\end{cases}$$\nWe say that the method is zero-stable if for $\\Delta t \u0026lt; \\Delta t_0$, there exists a constant $C \u0026gt; 0$ such that\n$$|u_n - z_n| \\leq C\\varepsilon$$\nfor $\\varepsilon \u0026gt; 0$ sufficiently small. ($C$ and $\\Delta t_0$ depend on problem data, $T_{fin}$, $f$).\nIt is possible to prove the following theorem:\nTheorem: If $\\Phi$ is Lipschitz continuous with respect to the dependence on $u_n$ ($\\exists \\Delta \u0026gt; 0$: $|\\Phi(u_n) - \\Phi(z_n)| \\leq \\Delta |u_n - z_n|$), then the One-step method is zero-stable.\nLipschitz continuity in this case is enough for zero-stability (and it is generally a consequence of Lipschitz continuity of $f$).\nThen, we have another theorem. It generalizes the theorem for the explicit Euler.\nTheorem: If $\\Phi$ is like in the previous theorem, then:\n$$|y(t_n) - u_n| \\leq \\left(|y_0 - u_0| + t_n , \\tau(\\Delta t)\\right) e^{L t_n}.$$\nTherefore, if:\n$\\tau(\\Delta t) \\to 0$ with $\\Delta t$ $y_0 - u_0 \\to 0$ with $\\Delta t$ the method is convergent\n(and the order is $\\Delta t^p$, with $p = \\min(p_1, p_2)$ where:\n$\\tau(\\Delta t) \\sim O(\\Delta t^{p_1})$ $y_0 - u_0 \\sim O(\\Delta t^{p_2})$ (generally $p = p_1$). In other terms:\nFor one-step method (not true for multi-step):\n$$\\Phi \\text{ Lipschitz continuous } \\Rightarrow \\text{ Method zero-stable} + \\text{Consistency} \\Rightarrow \\text{CONVERGENCE}$$\n$\\Rightarrow$ If $\\Phi$ is Lipschitz continuous, consistency $\\Rightarrow$ convergence.\nThe Concept of Absolute Stability\r#\rThe zero-stability is a property of the numerical scheme, required by the presence of roundoff errors and other possible perturbations.\nHowever, there is another (maybe more engineering) concept of stability related to the nature of the problem to solve.\nIn simple words, we want that, if a problem is asymptotically stable, the numerical approximation is asymptotically stable too.\nIs this happening? Let\u0026rsquo;s consider the prototype of asymptotically stable problem (Model Problem).\nLet\u0026rsquo;s consider the Cauchy problem:\n$$\\begin{cases} \\frac{dy}{dt} = \\lambda y \u0026amp; t \u0026gt; 0 \\ y(0) = y_0 \\end{cases}$$\nWe know that the solution is asymptotically stable for $\\lambda \u0026lt; 0$ $(y_{ex} = y_0 e^{\\lambda t} \\xrightarrow{t \\to \\infty} 0 \\text{ for } \\lambda \u0026lt; 0)$\nIn fact: $\\frac{dz}{dt} = \\lambda z$ with $z(0) = y_0 + \\varepsilon \\Rightarrow z - y = \\varepsilon e^{\\lambda t} \\xrightarrow{t\\to\\infty} 0$ for $\\lambda \u0026lt; 0$.\nRemark\r#\rFor a system: $\\begin{cases} \\frac{d\\mathbf{y}}{dt} = A\\mathbf{y} \u0026amp; \\mathbf{y} \\in \\mathbb{R}^n \\ \\mathbf{y}(0) = \\mathbf{y}_0 \u0026amp; A \\in \\mathbb{R}^{n \\times n} \\end{cases}$\nthe asymptotic stability is guaranteed if THE REAL PART OF ALL THE EIGENVALUES is negative.\nTo be general, from now on we will consider $\\lambda \\in \\mathbb{C}$ also for the scalar case. In particular, the left-plane $\\text{Real}(\\lambda) \u0026lt; 0$ is the region of the complex plane where the original problem is asymptotically stable.\nQuestion: is the solution of the model problem with Explicit Euler asymptotically vanishing as the exact solution?\r#\rNotice that the question is not related to the behavior of the solution for $\\Delta t \\to 0$, but for $\\Delta t$ given and $t \\to +\\infty$.\nLet\u0026rsquo;s see: $$\\frac{dy}{dt} = \\lambda y \\quad \\text{EE}: \\frac{u_{i+1} - u_i}{\\Delta t} = \\lambda u_i$$\n$$u_{i+1} = (1 + \\lambda \\Delta t) u_i \\quad (\\text{Re}(\\lambda) \u0026lt; 0)$$\n$$|u_{i+1}| = |1 + \\lambda \\Delta t| |u_i| \\Rightarrow |u_{i+1}| = |1 + \\lambda \\Delta t|^{i+1} |u_0|$$\nThe solution asymptotically vanishes if $|1 + \\lambda \\Delta t| \u0026lt; 1$\nIntuitively, if $\\lambda$ is Real and negative:\n$$|1 + \\lambda \\Delta t| \u0026lt; 1$$ $$\\Downarrow$$ $$-1 \u0026lt; 1 + \\lambda \\Delta t \u0026lt; 1$$ $$\\Downarrow$$ $$-2 \u0026lt; \\lambda \\Delta t \u0026lt; 0$$ $$\\Downarrow$$ $$\\Delta t \u0026lt; \\frac{2}{|\\lambda|}$$\nSo, if $\\Delta t \u0026gt; \\frac{2}{|\\lambda|}$ the numerical solution is not stable.\nFor $\\lambda$ complex, we can draw the region of the plane $\\lambda \\Delta t$ where the solution is stable.\n![Complex plane diagram showing unit circle with center at (-1,0)]\nUnit circle with center in $(-1, 0)$\n(In magenta the region of stability of the problem).\nSo, the method is reproducing the asymptotic behavior of the exact solution ONLY UNDER THE CONDITION THAT:\n$$\\lambda \\Delta t \\in \\text{Unit Circle centered in } (-1, 0).$$\nWhat happens with Implicit Euler?\r#\r$$\\frac{1}{|1 - \\lambda \\Delta t|} \u0026lt; 1 \\quad \\forall \\Delta t,$$\nso $u^{n+1} \\xrightarrow{n \\to \\infty} 0 \\quad \\forall \\Delta t$\nThere is a big difference between the two Eulers: one is stable under some conditions, the other is unconditionally stable.\nWhat about Crank-Nicolson?\r#\r$$u^{n+1} = u^n + \\Delta t \\frac{\\lambda u^{n+1} + \\lambda u^n}{2} \\Rightarrow u^{n+1} = \\frac{1 + \\frac{\\Delta t}{2}\\lambda}{1 - \\frac{\\Delta t}{2}\\lambda} u^n$$\nAgain, for $\\lambda \\in \\mathbb{C}$ we have:\n$$\\left|\\frac{1 + \\frac{\\Delta t}{2}\\lambda}{1 - \\frac{\\Delta t}{2}\\lambda}\\right| \u0026lt; 1 \\quad \\forall \\Delta t$$\nso also CN is stable with no condition.\nDEFINITION\r#\rA method is said to be ABSOLUTELY STABLE if the solution of the model problem $\\frac{dy}{dt} = \\lambda y$ vanishes asymptotically when $t \\to +\\infty$.\nWe say that the method is UNCONDITIONALLY ABSOLUTELY STABLE if this happens $\\forall \\Delta t \u0026gt; 0$. On the contrary, it is said to be CONDITIONALLY STABLE if we have limitations on $\\Delta t$.\nAnother Example: Heun\r#\r$$u^{n+1} = u^n + \\frac{\\Delta t}{2}(\\lambda(u^n + \\Delta t \\lambda u^n) + \\lambda u^n) = \\left(1 + \\Delta t \\lambda + \\frac{\\Delta t^2 \\lambda^2}{2}\\right) u^n$$\nNow, consider the curve $\\frac{\\Delta t^2 \\lambda^2}{2} + \\Delta t \\lambda + 1$ for $\\lambda \u0026lt; 0$\nWe see that we need:\n$$0 \u0026lt; \\Delta t \u0026lt; \\frac{2}{|\\lambda|}$$\n(as for Explicit Euler).\nIn the complex plane, the region is slightly larger than for Explicit Euler.\nREMARK: We found that the explicit methods are conditionally stable, while the implicit ones are unconditionally stable.\r#\rIn general, we do not have unconditionally stable explicit methods, but we do have implicit methods that are only conditionally stable.\nThe region of absolute stability is the portion of $\\mathbb{C}^-$ for $\\lambda \\Delta t$ to belong to such that the method is absolutely stable. Unconditional absolute stability (a.k.a A-stability) is when this region is the entire left plane $\\mathbb{C}^-$.\nThe concept of absolute stability somehow clarifies what are the criteria to select a method for a problem.\nIn fact, let\u0026rsquo;s first consider the general case(s):\n$\\frac{dy}{dt} = f(t,y) \\simeq f(t,y_0) + \\frac{\\partial f}{\\partial t}(t-t_0) + \\frac{\\partial f}{\\partial y}(t,y_0)(y-y_0)$\nso we can locally take $\\lambda \\simeq \\frac{\\partial f}{\\partial y}(t,y_0)$\nFor a system: $\\frac{d\\mathbf{y}}{dt} = A \\mathbf{y} \\Rightarrow \\lambda = eig(A)$\n$\\frac{d\\mathbf{y}}{dt} = \\mathbf{F}(t,\\mathbf{y})$ (nonlinear system)\n$\\Rightarrow \\lambda = eig\\left(\\frac{\\partial \\mathbf{F}}{\\partial \\mathbf{y}}(t_0, y_0) \\right)$ - Jacobian\nNow, for a general problem, we have:\nMethod Nature Accuracy Limitations on $\\Delta t$ FE Explicit 1 $\\Delta t \u0026lt; \\frac{2}{ BE Implicit 1 NO CN Implicit 2 NO H Explicit 2 $\\Delta t \u0026lt; \\frac{2}{ Implicit Methods are more computationally expensive.\nIn an extreme synthesis:\n![Comparison of FE and BE with timeline]\nWith FE each step is low-cost (Explicit) but we may need to do many for the conditional stability.\nWith BE each step is more expensive, but we need to do (generally) fewer steps.\n$\\Rightarrow$ The optimal choice is largely problem dependent.\nMultistep Methods\r#\rThe mid-point method is just an example of multi-step methods:\n$$u_{n+1} = u_{n-1} + 2\\Delta t , f(t_n, u_n)$$\nIn general, a multistep method with $p$ steps take the form:\n$$u_{n+1} - \\sum_0^p a_j u_{n-j} = \\Delta t \\sum_{j=-1}^p b_j f(t_{n-j}, u_{n-j})$$\nThe method is IMPLICIT when $b_{-1} \\neq 0$.\nExample:\r#\r$$y(t_{n+1}) = y(t_{n-1}) + \\int_{t_{n-1}}^{t_{n+1}} f(\\tau, y(\\tau)) d\\tau$$\n$$\\Downarrow \\text{ SIMPSON}$$\n$$u_{n+1} = u_{n-1} + 2\\Delta t \\frac{f(t_{n+1}, u_{n+1}) + 4f(t_n, u_n) + f(t_{n-1}, u_{n-1})}{6} =$$\n$$= u_{n-1} + \\frac{\\Delta t}{3}(f(t_{n+1}, u_{n+1}) + 4f(t_n, u_n) + f(t_{n-1}, u_{n-1}))$$\n$$\\mathbf{a} = \\begin{bmatrix} 0 \\ 1 \\end{bmatrix} a_0, a_1 \\quad \\mathbf{b} = \\begin{bmatrix} \\frac{1}{3} \\ \\frac{4}{3} \\ \\frac{1}{3} \\end{bmatrix} b_{-1}, b_0, b_1$$\nIn general, we have two approaches for deriving a Multi-step methods:\nBDF (Backward Difference Formulas)\r#\r$$\\frac{dy}{dt}(t_{n+1}) = f(t_{n+1}, u_{n+1})$$ $$\\downarrow$$ approximate this with a backward finite difference, e.g.,\n$$\\frac{dy}{dt}(t_{n+1}) \\simeq \\frac{\\frac{3}{2}u_{n+1} - 2u_n + \\frac{1}{2}u_{n-1}}{\\Delta t}$$\n$$\\Rightarrow u_{n+1} = \\frac{4}{3}u_n - \\frac{1}{3}u_{n-1} + \\Delta t f(t_{n+1}, u_{n+1})$$\nThey are all in the form:\n$$\\mathbf{b} = \\begin{bmatrix} 1 \\ 0 \\ \\vdots \\ 0 \\end{bmatrix}$$\nAdams\r#\rIn this case, we start from:\n$$y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(\\tau, y(\\tau)) d\\tau$$\nNow, we replace $f$ with an interpolation:\nWe interpolate $f$ over the nodes: $n-p, n-p+1, \u0026hellip; n$ so to have an explicit method (Adams-Bashforth) We interpolate $f$ over the nodes: $n-p, n-p+1, \u0026hellip; n, n+1$ Adams methods have always:\n$$\\mathbf{a} = \\begin{bmatrix} 1 \\ 0 \\ \\vdots \\ 0 \\end{bmatrix}$$\nA Rapid Recall of Difference Equations Theory\r#\rA linear difference equation is an equation in the form:\n$$u_{n+p} + a_{n+p-1}u_{n+p-1} + a_{n+p-2}u_{n+p-2} + \\ldots + a_0u_n = \\varphi_{n+p}$$\nwhere the solution is the set ${u_j}$ and initial conditions $u_0, u_1, \\ldots u_{p-1}$ are given.\nThe theory on difference equations has several similarities with the theory on differential equations. In particular, the general solution of a linear difference equation is the linear combination of a particular solution of the equation and the general solution of the homogeneous one.\nThe general solution of the homogeneous takes the form:\n$$u_n = \\sum_{j=0}^{N}\\left(\\sum_{s=0}^{m_j-1} V_{js}n^s \\right)r_j^n$$\nwhere $r_j$ are the roots of:\n$$r^p + a_{p-1}r^{p-1} + a_{p-2}r^{p-2} + \\ldots + a_0 = 0$$\nand\n$N$ is the number of distinct roots $m_j$ is the multiplicity of $r_j$ We will see a strong connection between this theory and the analysis of the linear multi-step methods.\nIn fact, a LMM reads like:\n$$u_{n+1} - \\sum_{j=0}^p a_j u_{n-j} = \\Delta t \\sum_{j=-1}^p b_j f_{n-j}$$\nIf we consider the model problem, we are lead to the linear difference equation:\n$$u_{n+1} - \\sum_{j=0}^p a_j u_{n-j} - \\Delta t \\lambda \\sum_{j=-1}^p b_j u_{n-j} = 0$$\nso we need to solve exactly a linear difference equation.\nMore precisely, we still define the LTE as the residual of the exact solution in the numerical scheme:\n$$\\Delta t , \\tau_{n+1} = y_{ex}(t_{n+1}) - \\sum_{j=0}^p a_j y_{ex}(t_{n-j}) - \\Delta t \\sum_{j=-1}^p b_j \\frac{dy_{ex}}{dt}(t_{n-j})$$\nThe method is CONSISTENT when $\\tau_{n+1} \\xrightarrow{\\Delta t \\to 0} 0 \\quad \\forall n$\nZero-Stability Definition\r#\rThe definition of zero-stability is similar to the one for One-step Methods.\nAlso, we define:\nFirst characteristic polynomial:\n$$\\rho(z) = z^{p+1} - \\sum_{j=0}^p a_j z^{p-j}$$\nSecond characteristic polynomial:\n$$\\sigma(z) = b_{-1}z^{p+1} - \\sum_{j=0}^p b_j z^{p-j}$$\nand the polynomial: $\\Pi(z) = \\rho(z) - \\Delta t \\lambda \\sigma(z)$ (this is the polynomial found for the model problem)\nBased on this we define:\nRoot Condition: Call $r_i$ the roots of $\\rho(z)$. The polynomial (or the associated LMM) fulfills the root condition when:\n(1) $|r_i| \\leq 1 \\quad \\forall i$ (2) The roots with $|r| = 1$ have multiplicity 1.\nStrong Root Condition: In addition to the R.C., only $r_0$ is s.t. $|r_0| = 1$, all the other roots $|r_j| \u0026lt; 1$ $(j \u0026gt; 1, \\ldots, p)$.\nAbsolute R.C.: $\\exists \\Delta t \\leq \\overline{\\Delta t}$ s.t. all the roots $r_j(\\Delta t)$ of $\\Pi_{\\Delta t}(z)$ are s.t. $|r_j(\\Delta t)| \u0026lt; 1$, $j = 0, \\ldots, p$, $\\Delta t \\leq \\overline{\\Delta t}$.\nAnalysis of Multistep Methods\r#\rWe have a sequence of theorems (no proofs):\nA LMM is consistent if and only if:\n$$\\sum_{j=0}^p a_j = 1 \\quad -\\sum_{j=0}^p j a_j + \\sum_{j=-1}^p b_j = 1$$\nAlso, the method is at least of order $p$ if the solution is $\\in C^{p+1}(I)$ and\n$$(*)\\ \\sum_{j=0}^p (j)^k a_j + k \\sum_{j=-1}^p (j)^{k-1} b_j = 1 \\quad k = 1, 2, \\ldots q$$\nRemark: The condition $\\sum_{j=0}^p a_j = 1$ means that the first characteristic polynomial $\\rho(z)$ has at least one root in 1.\nA consistent method is zero-stable if and only if it fulfills the root condition.\nWith Theorems (1) + (2) we have the convergence and order analysis.\n(1) + (2): If the LMM method falls into (1) and (2) with condition (*) (not true for q+1) and the initial conditions ($u_i \\to y_{ex}(t_i)$ $i = 0, \\ldots, p$) converge to the exact ones with order $q$, the resulting method is convergent with order $q$.\nRemark (First Dahlquist Barrier): There is no zero-stable $p$-LMM with order\n$\u0026gt; p+1$ for $p$ odd $\u0026gt; p+2$ for $p$ even Let\u0026rsquo;s turn now to the Absolute stability.\nThe absolute root condition is necessary and sufficient for the absolute stability. In fact, if $\\overline{\\Delta t} = +\\infty$, the absolute stability is unconditional. The analysis of a LMM is completed by analyzing the coefficients of the method and the roots of the two polynomials $\\rho$ and $\\Pi$.\nThe roots of $\\Pi$ can be analyzed numerically (and are tabulated for most of the methods) by Matlab/Python subroutines.\nSEE EXAMPLES (NODEPY library in Python, QSS in Matlab)\nRemark (Second Dahlquist Barrier): There are no explicit LMM absolutely unconditionally stable. There are no LMM absolutely unconditionally stable with order $q \u0026gt; 2$.\nA Clarification on Stability Concepts\r#\rTo clarify the different stability concepts:\nZero-stability:\n$$|u_j| \\leq C_{T_{fin}} (|u_0| + \\ldots |u_p|)$$\nwhere the $C$ may depend on $T_{fin}$\nAbsolute stability:\n$$C_{T_{fin}} \\xrightarrow{T_{fin} \\to \\infty} 0$$\n$C$ is bounded independently of $T_{fin}$\nWe call this \u0026ldquo;relative stability\u0026rdquo;\n$$\\text{for a consistent scheme} \\quad \\text{R.C.} \\Leftarrow \\text{Strong R.C.} \\Leftarrow\\text{A.R.C.}$$ $$\\text{CONVERGENCE} \\Leftarrow \\text{Zero-Stability} \\Leftarrow \\text{Relative Stability} \\Leftarrow \\text{Absolute Stability}$$\nExample (Extreme)\r#\rMid-point:\n$$u_{n+1} = u_{n-1} + 2\\Delta t , f(t_n, u_n)$$\n$$a_0 = 0 \\quad a_1 = 1 \\quad \\Rightarrow \\rho(z) = z^2 - 1 = 0 \\quad \\Rightarrow \\rho = \\pm 1$$\nR.C.: OK\n$$\\Pi_{\\Delta t}(z) = z^2 - 2\\lambda z - 1$$\nThe product of the two roots is always -1, if one root is \u0026lt; 1 in magnitude, the other is \u0026gt; 1.\nUnconditionally Absolutely UNSTABLE\nPredictor-Corrector Methods\r#\rA clear message from the previous examples is that, in general, explicit methods are less absolutely stable, they are never unconditionally stable and have smaller regions of stability.\nHowever, let\u0026rsquo;s reconsider implicit methods too.\nFor instance, let\u0026rsquo;s consider a generic LMM:\n$$u_{n+1} - \\sum_{j=0}^p a_j u_{n-j} = b_{-1} \\Delta t , f(t_{n+1}, u_{n+1}) + \\Delta t \\sum_{j=0}^p b_j f(t_{n-j}, u_{n-j})$$\nTo solve this nonlinear equation, we could use a Newton method but also a fixed-point method and a natural candidate is the following:\n$$u_{n+1}^{(m)} = \\sum_{j=0}^p a_j u_{n-j} + \\Delta t \\sum_{j=0}^p b_j f_{n-j} + \\Delta t b_{-1} f(t_{n+1}, u_{n+1}^{(m-1)})$$\nNotice that this method converges if:\n$$\\left|\\Delta t , b_{-1} , f\u0026rsquo;(t_{n+1}, u_{n+1}) \\right| \u0026lt; 1$$\nSo we have the condition:\n$$\\Delta t \u0026lt; \\frac{1}{|b_{-1}||f\u0026rsquo;|}$$\nEven if the method is unconditionally stable, we may have a condition on $\\Delta t$ for the convergence of the fixed-point.\nIn any case, a good initial condition is required, because with a good $u^{(0)}$ the number of iterations can be reduced: an explicit method can be used to this.\nThese considerations lead to the design of a new class of methods, Heun being one of the possible examples.\nPredictor [P] is an explicit method of order $q_P$\nCorrector [C] is an implicit method of order $q_C$\nPredictor Corrector method:\nP: do one step of P $\\Rightarrow u_{n+1}^{(0)}$\nE: evaluate $f(t_{n+1}, u_{n+1}^{(0)})$\nC: compute $m$ fixed-point iterations of C\nOptional: E: compute $f_{n+1} = f(t_{n+1}, u_{n+1}^{(m)})$\nThese methods go under the name of $\\text{PEC}^m$ or $\\text{PEC}^m\\text{E}$ if the last step is taken.\nExamples:\r#\rP = Adams-Bashforth of order 2 C = Adams-Moulton of order 3 $m = 1$\nPEC: $$\\begin{cases} u_{n+1}^{(0)} = u_n^{(1)} + \\frac{\\Delta t}{2}(3f_n^{(0)} - f_{n-1}^{(0)}) \\ f_{n+1}^{(0)} = f(t_{n+1}, u_{n+1}^{(0)}) \\ u_{n+1}^{(1)} = u_n^{(1)} + \\frac{\\Delta t}{12}(5f_{n+1}^{(0)} + 8f_n^{(0)} - f_{n-1}^{(0)}) \\end{cases}$$\nPECE: $$\\begin{cases} u_{n+1}^{(0)} = u_n^{(1)} + \\frac{\\Delta t}{2}(3f_n^{(1)} - f_{n-1}^{(1)}) \\ f_{n+1}^{(0)} = f(t_{n+1}, u_{n+1}^{(0)}) \\ u_{n+1}^{(1)} = u_n^{(1)} + \\frac{\\Delta t}{12}(5f_{n+1}^{(0)} + 8f_n^{(1)} - f_{n-1}^{(1)}) \\ f_{n+1}^{(1)} = f(t_{n+1}, u_{n+1}^{(1)}) \\end{cases}$$\nA natural question is: what is the accuracy of $\\text{PEC}^m$ or $\\text{PECE}^m$? What is the region of absolute stability?\nTheorem (Accuracy): $\\text{PECE}$ methods have (under regularity assumptions on the solution) order of accuracy:\n$$q_{PC} = min(q_P + m, q_C)$$\nFor the region of absolute stability, in general:\n$$\\text{Region}(P) \\subseteq \\text{Region}(\\text{PECE}) \\subseteq \\text{Region}(C)$$\nand $\\text{Region}(\\text{PECE}) \\xrightarrow{m \\to +\\infty} \\text{Region}(C)$\nRunge-Kutta Methods\r#\rHeun is also an RK method:\n$$u_{n+1} = u_n + \\Delta t\\left(f_n + f(t_{n+1}, u_n + \\Delta t f_n)\\right)$$\nThis is a 2nd order method but the accuracy is not obtained using more steps, but giving up the linearity of the method.\nIn general, RK are in the form:\n$$u_{n+1} = u_n + \\Delta t , \\mathbf{K} \\cdot \\mathbf{b}$$\nwhere $\\mathbf{K}, \\mathbf{b} \\in \\mathbb{R}^s$\nand $[\\mathbf{K}]_i = f(t_n + c_i \\Delta t, u_n + \\Delta t[A\\mathbf{K}]_i)$\nwhere $[\\mathbf{A}\\mathbf{K}]_i$ is the $i^{th}$ entry of the vector $\\mathbf{A}\\mathbf{K}$\nThe method is characterized by: $s$ (stages), $\\mathbf{b}$, $\\mathbf{c}$ and $\\mathbf{A} \\in \\mathbb{R}^{s \\times s}$\nIn particular, the three \u0026ldquo;ingredients\u0026rdquo; are generally written as:\n$$\\frac{\\mathbf{c} | \\mathbf{A}}{\\mathbf{b}^T} \\quad \\text{(Butcher array)}$$\nIt is assumed that $c_i = \\sum_{j=1}^s a_{ij} \\quad \\forall i = 1, \\ldots s$\nIf $\\mathbf{A}$ is such that $a_{ij} = 0 \\quad \\forall j \\geq i$ then:\n$$K_i = [\\mathbf{K}]i = f(t_n + \\Delta t c_i, u_n + \\Delta t \\sum{j \u0026lt; i} a_{ij} K_j)$$\nso the computation of $K_i$ is immediate. We call this case an explicit RK scheme.\nIf $a_{ij} = 0 \\quad \\forall j \u0026gt; i$, then:\n$$K_i = [\\mathbf{K}]i = f(t_n + \\Delta t c_i, u_n + \\Delta t \\sum{j \u0026lt; i} a_{ij} K_j + \\Delta t a_{ii} K_i)$$\nso we need to solve a non-linear equation to obtain $K_i$. We call this a semi-implicit method.\nIn general, computing $\\mathbf{K}$ requires the solution of a non-linear system (implicit method).\nClearly, the computational cost increases in the three cases.\nDerivation of an Explicit RK Method\r#\rA possible strategy to devise an explicit method is to maximize the order of the LTE with Taylor expansion analysis.\nFor instance, for $s = 2$:\n$$\\begin{pmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\ c_2 \u0026amp; a_{21} \u0026amp; 0 \\ \\hline \u0026amp; b_1 \u0026amp; b_2 \\end{pmatrix}$$\nWe have three parameters, but we set $a_{21} = c_2$.\n$$u_{n+1} = u_n + \\Delta t(b_1 f(t_n, u_n) + b_2 f(t_n + c_2 \\Delta t, u_n + \\Delta t c_2 f(t_n, u_n)))$$\n$$y_{ex}(t_n) = y_{ex}(t_n) + \\Delta t \\frac{dy_{ex}}{dt}(t_n) + \\frac{\\Delta t^2}{2}\\frac{d^2y_{ex}}{dt^2}(t_n) + \\frac{\\Delta t^3}{3!}\\frac{d^3y_{ex}}{dt^3}(t_n) + H.O.T.$$\n$$f(t_n + c_2 \\Delta t, u_n + \\Delta t c_2 f(t_n, u_n)) =$$\n$$= f(t_n, u_n) + \\frac{\\partial f}{\\partial t}(t_n, y_n)c_2 \\Delta t + \\frac{\\partial f}{\\partial y}(t_n, y_n)c_2 \\Delta t f(t_n, u_n) =$$\nNotice that: $$\\frac{d^2y}{dt^2} = \\frac{df}{dt} = \\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{dy}\\frac{dy}{dt} = \\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{\\partial y}f$$\n$$\\Rightarrow f(t_n, u_n) + c_2 \\Delta t \\frac{d^2y}{dt^2}(t_n)$$\nThe Taylor expansion applied to the scheme reads:\n$$u_{n+1} = u_n + \\Delta t(b_1 + b_2)f + c_2 b_2 \\Delta t^2 \\frac{d^2y}{dt^2}$$\nWe match therefore the first terms of the Taylor expansion for:\n$$\\begin{align} b_1 + b_2 \u0026amp;= 1 \\ b_2 c_2 \u0026amp;= \\frac{1}{2} \\end{align}$$\nFor $b_2 = \\frac{1}{2}$ we obtain the Heun method.\nThe L.T.E is $\\Delta t , \\tau \\sim O(\\Delta t^3)$\nso the method is 2nd order.\nImplicit methods can be devised from Gaussian quadratures.\nAnalysis of RK\r#\rCONSISTENCY: As the previous example shows, we need $\\sum b_i = 1$. This is necessary and sufficient for the consistency.\nZERO-STABILITY: RK are One-Step methods, if $f$ is Lipschitz-Continuous, they are zero-stable.\nORDER: In general, the order we can obtain depends on the number of stages in a (non-trivial) way:\nExplicit RK: ORDER 1 2 3 4 5 6 7 8 $s_{min}$ 1 2 3 4 6 7 9 11 $s_{min}$ = minimum number of stages to obtain the corresponding order.\nABSOLUTE STABILITY\r#\rIf we write the method for the model problem, we can write:\n$$u_{n+1} = \\mathcal{R}(\\Delta t \\lambda) u_n$$\nThe region of absolute stability is, in general, the non-trivial region where $|\\mathcal{R}(\\Delta t \\lambda)| \u0026lt; 1$ (in $\\mathbb{C}$).\nWhy are RK so popular?\r#\rRK are extremely popular, because they can be high order with \u0026ldquo;only\u0026rdquo; one-step.\nOne-step means that:\nwe do not need high-order approximation of the initial data needed by LMM (the initial condition is enough) we can easily perform the time-step ADAPTIVITY (much more difficult with LMM). For the adaptivity, there are smart combinations of RK that obtain the adaptivity (i.e. an error estimate to adapt the step) in an efficient way.\nOne of the most popular is: RK45 Fehlberg: smart combination of an explicit method of order 4 and an explicit of order 5 to adapt the step.\nA Final Note on Stiff problems\r#\rMany of the concepts used here can be extended to systems of ODEs:\n$$\\begin{cases} \\frac{d\\mathbf{y}}{dt} = \\mathbf{F}(t, \\mathbf{y}) \\ \\mathbf{y}(0) = \\mathbf{y}_0 \\end{cases}$$\nIn the case of a linear ODE system:\n$$\\mathbf{F} = A , \\mathbf{y} \\quad \\downarrow \\quad \\text{matrix}$$\nThere is, however, an important concept to clarify.\nConsider a simple 2×2 problem:\n$$\\frac{d\\mathbf{y}}{dt} = A\\mathbf{y}$$\nwhere $A$ has the two eigenvalues: $\\begin{cases} \\lambda_1 = -10^6 \\ \\lambda_2 = -1 \\end{cases}$\nIf we use Explicit Euler:\n$$\\mathbf{y}^{n+1} = \\mathbf{y}^n + \\Delta t , A \\mathbf{y}^n$$\nthe region of absolute stability is:\n$$\\Delta t \\leq \\min\\left(\\frac{2}{10^6}, \\frac{2}{1}\\right) = 2 \\cdot 10^{-6}$$\nThe solution, on the other hand, is the linear combination of the two functions:\n$$e^{-10^6 t}, e^{-t}$$\n[Graph showing rapid decay of $e^{-10^6 t}$ vs slower decay of $e^{-t}$]\nTo capture the fast dynamics ($\\lambda = 10^{-6}$), that fades away immediately, we need to take $\\Delta t \\sim 10^{-6}$!!!\nAn explicit method is certainly not a good choice here.\nIn general, we say that a problem is \u0026ldquo;stiff\u0026rdquo; when it may require very stringent time-step in a non-efficient way. The name \u0026ldquo;stiff\u0026rdquo; originates from the coupling of springs with different stiffness:\n[Simple diagram of a mass connected to two springs with different spring constants]\nto study the dynamics of the two real balls, one can write an ODE system.\nIf $K_1 \u0026laquo; K_2$ this is a stiff problem.\nEXERCISE on LMM\r#\rConsider the following family of methods (LMM):\n$$u_{n+1} = \\alpha u_n + (1-\\alpha)u_{n-1} + \\Delta t , \\gamma f_{n+1}$$\nInvestigate the convergence properties of the method as function of $\\alpha$ and $\\gamma$. Sol: For $\\alpha \\neq 1$, the method is 2-step. For $\\gamma \\neq 0$, the method is implicit.\nIt\u0026rsquo;s a LMM with: $\\mathbf{a} = \\begin{bmatrix} \\alpha \\ 1-\\alpha \\ 0 \\end{bmatrix} \\begin{bmatrix} 0 \\ 1 \\ 0 \\end{bmatrix}$, $\\mathbf{b} = \\begin{bmatrix} \\gamma \\ 0 \\ 0 \\end{bmatrix} \\begin{bmatrix} -1 \\ 0 \\ 1 \\end{bmatrix}$\nConsistency: $\\sum a_j = 1$: $\\alpha + (1-\\alpha) = 1$ ✓OK $-\\sum j a_j + \\sum b_j = 1$: $0 \\cdot \\alpha - 1 \\cdot (1-\\alpha) + \\gamma = 1$ $\\Rightarrow \\gamma = 2 - \\alpha$\nThe methods: $u_{n+1} = \\alpha u_n + (1-\\alpha)u_{n-1} + \\Delta t (2-\\alpha)f_{n+1}$ are consistent.\nOrder: $\\sum (j)^2 a_j + 2\\sum (-j)^1 b_j = 1$ ? $\\Rightarrow (1-\\alpha) + 2(2-\\alpha) = 1 \\Rightarrow \\alpha = \\frac{4}{3}, \\gamma = \\frac{2}{3}$\nInvestigate the absolute stability of the method with order 2. The method: $u_{n+1} = \\frac{4}{3}u_n - \\frac{1}{3}u_{n-1} + \\Delta t \\frac{2}{3}f_{n+1}$\nis of order 2 (it is, in fact, a BDF of order 2).\n$\\rho(z) = z^2 - \\frac{4}{3}z + \\frac{1}{3} = 0 \\quad r_{1,2} = \\frac{\\frac{4}{3} \\pm \\sqrt{\\frac{16}{9} - \\frac{4}{3}}}{2} = \\frac{4 \\pm 2}{6} = \\frac{2 \\pm 1}{3}$\nR.C. ✓\n$\\Pi_{\\Delta t}(z) = z^2 - \\frac{4}{3}z + \\frac{1}{3} - \\Delta t \\lambda \\frac{2}{3}z^2 = 0$\nLet\u0026rsquo;s consider $\\lambda \\in \\mathbb{R}^-$:\n$(3 - \\Delta t \\lambda 2)z^2 - 4z + 1 = 0$\n$z^2 - \\frac{4z}{3 + 2\\Delta t|\\lambda|} + \\frac{1}{3 + 2\\Delta t|\\lambda|} = 0$\n$r_1 \\cdot r_2 = \\frac{1}{3 + 2\\Delta t|\\lambda|}$\n$r_1 = \\frac{2 + \\sqrt{1 - 2\\Delta t|\\lambda|}}{3 + 2\\Delta t|\\lambda|} \\xrightarrow{\\Delta t \\to 0} 1$\n$|r_1(\\Delta t)| \u0026lt; 1 \\quad \\forall \\Delta t \u0026gt; 0$\n$r_2 = \\frac{2 - \\sqrt{1 - 2\\Delta t|\\lambda|}}{3 + 2\\Delta t|\\lambda|} \\xrightarrow{\\Delta t \\to 0} \\frac{1}{3}$\n$|r_2(\\Delta t)| \u0026lt; \\frac{1}{3}$\nMethod unconditionally stable (Verify with Python/Matlab).\nSolve $$\\begin{cases} \\frac{dy}{dt} = -(1 + t_g(t))y \u0026amp; t \\in [0, 1] \\ y(0) = 1 \\end{cases}$$ with the BDF2 method found and verify your results, knowing that the exact solution is $y_{ex} = e^{-x}\\cos(x)$.\nUsing MATLAB, the problem is easily solved with the QSS subroutines:\nqssstab.m (draws the region of absolute stability) qssmulti.m (solves with a generic LMM)\nWith Python there are many libraries: SciPy, odeint that uses RK, ode, and allows the selection of BDF methods, but generally with adaptive time step.\nNODEPY is potentially an excellent library but buggy.\nIt\u0026rsquo;s excellent for drawing the stability region (see hmm.py on Canvas)\nI have written a simple LMM solver with fixed-point iterations for implicit methods.\nUsing my-hmm.py you can verify that our method is 2nd order:\nmax error $3 \\cdot 10^{-4}$ $8 \\cdot 10^{-5}$ $2 \\cdot 10^{-5}$ $\\Delta t$ 0.05 0.025 0.0125 [Graph showing exact vs numerical solution]\n[Stability region diagram showing a circle in the complex plane] Red = Region of Stability\n"},{"id":6,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/Exercise/","title":"习题","section":"第九章","content":"\rProblem 1: Finite-Difference Solution\r#\rWe wish to discretize and solve the boundary-value problem\n$$-\\mu\\frac{d^2u}{dx^2} + \\beta\\frac{du}{dx} = f(x),\\quad x\\in(0,1),\\quad u(0)=u(1)=0$$\nOr equivalently:\n$$\\begin{cases} -\\mu u\u0026rsquo;\u0026rsquo;(x) + \\beta u\u0026rsquo;(x) = f(x), \u0026amp; 0\u0026lt;x\u0026lt;1,\\ u(0)=0,; u(1)=0. \\end{cases}$$\nWhere $\\mu\u0026gt;0$ and $\\beta$ is a constant (possibly negative), and $f\\in C^0(0,1)$.\n1. Finite-Difference Discretization\r#\rDivide $[0,1]$ into $N$ equal subintervals so that $\\Delta x = \\frac{1}{N}$. Let\n$$x_j = j\\Delta x,\\quad j=0,1,2,\\dots,N,$$\nso that $x_0=0$ and $x_N=1$. We approximate $u(x_j)\\approx u_j$. The boundary conditions become $u_0=0$ and $u_N=0$.\n1.1 Central Difference Scheme (for the interior points)\r#\rA standard centered second-difference for $u\u0026rsquo;\u0026rsquo;(x)$ at $x_j$ is:\n$$u\u0026rsquo;\u0026rsquo;(x_j) \\approx \\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2}.$$\nA standard centered first-difference for $u\u0026rsquo;(x)$ at $x_j$ is:\n$$u\u0026rsquo;(x_j) \\approx \\frac{u_{j+1} - u_{j-1}}{2\\Delta x}.$$\nHence, the PDE $-\\mu u\u0026rsquo;\u0026rsquo;(x) + \\beta u\u0026rsquo;(x)=f(x)$ becomes for $j=1,\\dots,N-1$:\n$$-\\mu\\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1} - u_{j-1}}{2\\Delta x} = f(x_j).$$\nWe impose $u_0 = 0$ and $u_N=0$.\n2. Local Truncation Error\r#\rThe second-order central difference for $u\u0026rsquo;\u0026rsquo;$ is $O((\\Delta x)^2)$ accurate. The central difference for $u\u0026rsquo;$ is also $O((\\Delta x)^2)$ accurate. Hence the local truncation error of the combined scheme is $O((\\Delta x)^2)$.\n3. Matrix Form\r#\rCollect unknowns $u_1,u_2,\\dots,u_{N-1}$ into a vector $\\mathbf{u}=(u_1,\\dots,u_{N-1})^T$. The boundary values $u_0=0$ and $u_N=0$ are known.\nRewrite the finite-difference equation for an interior index $j$:\n$$-\\mu\\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1} - u_{j-1}}{2\\Delta x} = f(x_j).$$\nWe can factor out coefficients:\nLet $\\alpha \\equiv \\frac{\\mu}{(\\Delta x)^2}$. Let $\\gamma \\equiv \\frac{\\beta}{2\\Delta x}$. Then the coefficient of $u_j$ is $2\\alpha$, the coefficient of $u_{j+1}$ is $-\\alpha + \\gamma$, and the coefficient of $u_{j-1}$ is $-\\alpha - \\gamma$. Thus, in matrix form:\n$$\\begin{pmatrix} 2\\alpha \u0026amp; -\\alpha+\\gamma \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\ -\\alpha-\\gamma \u0026amp; 2\\alpha \u0026amp; -\\alpha+\\gamma \u0026amp; \\cdots \u0026amp; 0\\ 0 \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\vdots\\ \\vdots \u0026amp; \u0026amp; -\\alpha-\\gamma \u0026amp; 2\\alpha \u0026amp; -\\alpha+\\gamma\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; -\\alpha-\\gamma \u0026amp; 2\\alpha \\end{pmatrix} \\begin{pmatrix} u_1\\ u_2\\ \\vdots \\ u_{N-2}\\ u_{N-1} \\end{pmatrix}\r#\r\\begin{pmatrix} f(x_1)\\ f(x_2)\\ \\vdots \\ f(x_{N-2})\\ f(x_{N-1}) \\end{pmatrix}.$$\nThis is a tridiagonal linear system, solvable by standard methods (e.g., Thomas algorithm).\n4. Quality of the Solution vs. $\\beta/\\mu$\r#\rThe ratio $\\frac{\\beta}{\\mu}$ often plays the role of a Péclet-type number in advection-diffusion problems.\nIf $\\frac{\\beta}{\\mu}$ is small (diffusion-dominated), the solution is usually smooth and well-behaved under central differencing. If $\\frac{\\beta}{\\mu}$ is large (advection-dominated), pure central differences may produce spurious oscillations unless $\\Delta x$ is refined or upwinding techniques are used to stabilize the discrete solution. 5. Upwind Method (First Order) for $\\beta\u0026lt;0$\r#\rWhen $\\beta\u0026lt;0$, the \u0026ldquo;flow\u0026rdquo; is from right to left, so an upwind difference for the first derivative $\\beta u\u0026rsquo;(x)$ uses values on the \u0026ldquo;right\u0026rdquo; side at each $j$. Concretely, for $\\beta\u0026lt;0$, we replace:\n$$u\u0026rsquo;(x_j) \\approx \\frac{u_{j+1} - u_j}{\\Delta x} \\quad \\text{(a \u0026ldquo;backward\u0026rdquo; upwind if flow is leftward)}.$$\nHence, the difference equation becomes:\n$$-\\mu\\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1} - u_j}{\\Delta x} = f(x_j), \\quad j=1,\\dots,N-1.$$\n(This replaces the central difference in the advective term by a one-sided upwind difference.)\n5.1 No Spurious Oscillations\r#\rThe first-order upwind scheme for linear advection-diffusion is known to be monotone for any $\\Delta x\u0026gt;0$ when $\\beta\u0026lt;0$ (or, more generally, for any sign of $\\beta$ if we choose the correct upwind direction). Monotonicity prevents nonphysical oscillations. In short:\nCentral difference can oscillate if $|\\beta|$ is large relative to $\\mu$. Upwind difference sacrifices some accuracy (only first order in $\\Delta x$ for the advective term) but remains stable and nonoscillatory for any step size $\\Delta x$. Thus, with $\\beta\u0026lt;0$, the upwind approach $u\u0026rsquo;(x_j)\\approx (u_{j+1}-u_j)/\\Delta x$ ensures a stable, physically plausible solution without oscillations.\n6. Summary\r#\rEquation \u0026amp; Discretization\n$$-\\mu u\u0026rsquo;\u0026rsquo; + \\beta u\u0026rsquo; = f(x), \\quad u(0)=u(1)=0 \\longrightarrow \\begin{cases} -\\mu\\frac{u_{j+1}-2u_j+u_{j-1}}{(\\Delta x)^2} +\\beta\\frac{u_{j+1}-u_{j-1}}{2\\Delta x} = f(x_j),\\ u_0=0,;u_N=0. \\end{cases}$$\nLocal Truncation Error is $O((\\Delta x)^2)$ for the centered scheme.\nMatrix Form: A standard tridiagonal system with bands $-\\alpha\\mp \\gamma$, $2\\alpha$, $-\\alpha\\pm \\gamma$.\nEffect of $\\beta/\\mu$: If $|\\beta|$ is large relative to $\\mu$, central differences can produce oscillatory solutions; upwind methods help.\nUpwind Method (for $\\beta\u0026lt;0$): $$-\\mu\\frac{u_{j+1}-2u_j+u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1}-u_j}{\\Delta x} = f(x_j)$$ prevents oscillations for any $\\Delta x$.\nProblem 2\r#\rSolution Outline\nWe have the linear advection equation\n$$\\frac{\\partial u}{\\partial t} ;+; a,\\frac{\\partial u}{\\partial x} ;=; 0,\\quad x\\in \\mathbb{R},;t\u0026gt;0,$$ with initial condition $u(x,0) = u_0(x)$. We wish to:\nDerive the Lax–Wendroff scheme for this PDE. Determine the CFL condition for stability, i.e.\\ find the condition on $\\displaystyle \\frac{|a|\\Delta t}{\\Delta x}$. 1) Derivation of the Lax–Wendroff Scheme\r#\rA succinct way to derive Lax–Wendroff is via a second-order Taylor expansion in time about $t^n$:\n$$u^{n+1}i ;\\approx; u(x_i,,t_n + \\Delta t) ;=; u(x_i,t_n) ;+;\\Delta t,\\frac{\\partial u}{\\partial t} ;+;\\tfrac{(\\Delta t)^2}{2},\\frac{\\partial^2 u}{\\partial t^2};\\bigg|{(x_i,t_n)}.$$\nFrom the PDE $\\partial_t u = -,a,\\partial_x u$, we can replace time-derivatives by spatial derivatives:\nFirst derivative in time: $$\\frac{\\partial u}{\\partial t} ;=; -,a,\\frac{\\partial u}{\\partial x}.$$\nSecond derivative in time: $$\\frac{\\partial^2 u}{\\partial t^2} ;=; \\frac{\\partial}{\\partial t}\\Bigl(-,a,\\frac{\\partial u}{\\partial x}\\Bigr) ;=; -,a,\\frac{\\partial}{\\partial x}\\Bigl(\\frac{\\partial u}{\\partial t}\\Bigr) ;=; -,a,\\frac{\\partial}{\\partial x}\\Bigl(-,a,\\frac{\\partial u}{\\partial x}\\Bigr) ;=; a^2,\\frac{\\partial^2 u}{\\partial x^2}.$$\nHence,\n$$u^{n+1}_i ;\\approx; u^{n}i ;-; a,\\Delta t ,\\frac{\\partial u}{\\partial x} ;+; \\frac{a^2(\\Delta t)^2}{2},\\frac{\\partial^2 u}{\\partial x^2} ;\\Bigg|{(x_i,t_n)}.$$\nDiscretizing the spatial derivatives\r#\rWe replace the first and second spatial derivatives by standard centered finite differences:\n$$\\frac{\\partial u}{\\partial x}\\bigg|{x_i} \\approx \\frac{u{i+1}^n - u_{i-1}^n}{2,\\Delta x}, \\qquad \\frac{\\partial^2 u}{\\partial x^2}\\bigg|{x_i} \\approx \\frac{u{i+1}^n - 2,u_i^n + u_{i-1}^n}{(\\Delta x)^2}.$$\nPutting this all together:\n$$u_{i}^{n+1} ;=; u_{i}^{n} ;-; a,\\Delta t ,\\frac{u_{i+1}^{n} - u_{i-1}^{n}}{2,\\Delta x} ;+; \\frac{a^2,(\\Delta t)^2}{2}, \\frac{u_{i+1}^{n} - 2,u_{i}^{n} + u_{i-1}^{n}}{(\\Delta x)^2}.$$\nIt is common to set $\\displaystyle \\nu ;=;\\frac{a,\\Delta t}{\\Delta x}$. Then the scheme reads\n$$\\boxed{ u_{i}^{n+1} ;=; u_{i}^{n} ;-;\\frac{\\nu}{2},\\bigl(u_{i+1}^{n} - u_{i-1}^{n}\\bigr) ;+;\\frac{\\nu^{2}}{2}, \\bigl(u_{i+1}^{n} ;-;2,u_{i}^{n} ;+;u_{i-1}^{n}\\bigr). }$$\nThis is the Lax–Wendroff scheme for the linear advection equation.\n2) The CFL Stability Condition\r#\rA standard von Neumann (Fourier) stability analysis, or the usual Lax–Richtmyer theory for hyperbolic PDEs, shows that Lax–Wendroff is stable if and only if the Courant number satisfies\n$$\\bigl|,\\nu,\\bigr| ;=; \\biggl|\\frac{a,\\Delta t}{\\Delta x}\\biggr| ;\\le; 1.$$\nTherefore among the multiple-choice options, the correct condition is\n$$\\boxed{;; \\bigl|\\tfrac{a,\\Delta t}{\\Delta x}\\bigr| ;\\le; 1.}$$\n3) Motivation\r#\rDomain of dependence argument. For the PDE $\\partial_t u + a,\\partial_x u = 0,$ characteristics travel with speed $a$. Numerically, we must ensure that information from these characteristics is captured on the grid from one time step to the next; that is the essence of the CFL condition. If $|a|\\Delta t \u0026gt; \\Delta x$, the method “jumps over” grid cells and fails to remain stable.\nVon Neumann analysis. Substituting $u_i^n = \\lambda^n e^{ikx_i}$ into the scheme shows that the amplification factor $|\\lambda|$ is $\\le 1$ if and only if $\\bigl|\\frac{a\\Delta t}{\\Delta x}\\bigr|\\le 1.$\nHence the short answer is:\nLax–Wendroff has the form $$u_{i}^{n+1}\r#\ru_{i}^{n}\r#\r\\frac{\\nu}{2},(u_{i+1}^{n} - u_{i-1}^{n}) + \\frac{\\nu^{2}}{2},(u_{i+1}^{n}-2u_{i}^{n}+u_{i-1}^{n}),$$ The method is stable if and only if $\\displaystyle \\bigl|\\frac{a\\Delta t}{\\Delta x}\\bigr|\\le 1.$ This condition is exactly the usual CFL requirement ensuring the numerical domain of dependence covers the PDE’s domain of dependence. Problem 3\r#\rBelow is a fairly detailed derivation and discussion of the Crank–Nicolson (CN) method (the $\\theta$-method with $\\theta = \\tfrac12$) for the heat equation\n$$\\frac{\\partial u}{\\partial t};-;\\frac{\\partial^2 u}{\\partial x^2};=;f, \\quad x\\in(0,1),;t\u0026gt;0, \\quad u(0,t) ;=;u(1,t);=;0, \\quad u(x,0);=;u_0(x).$$\nSince the PDE can be written as $$u_t ;=; u_{xx} + f,$$ we will discretize in both space and time.\n1. The Crank–Nicolson Discretization\r#\rLet $\\Delta x = \\frac{1}{M}$ partition $[0,1]$ into $M+1$ grid points $x_i = i,\\Delta x$, $i=0,\\dots,M$, and let $\\Delta t$ be a time step, so $t^n = n,\\Delta t$. We write $u_i^n\\approx u(x_i,t^n)$. The standard second‐order central difference for $u_{xx}$ is\n$$u_{xx}(x_i,t^n) ;\\approx; \\frac{u_{i+1}^n - 2,u_i^n + u_{i-1}^n}{(\\Delta x)^2}.$$\nA $\\theta$‐method (also called the $\\theta$-scheme) for $u_t = u_{xx} + f$ in time is:\n$$\\frac{u_i^{n+1} - u_i^n}{\\Delta t} ;=; \\theta\\Bigl[\\underbrace{\\tfrac{u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}}{(\\Delta x)^2} ;+; f_i^{n+1}}{\\text{“implicit” part}}\\Bigr] ;+; \\bigl(1-\\theta\\bigr)\\Bigl[\\underbrace{\\tfrac{u{i+1}^{n} - 2u_i^{n} + u_{i-1}^{n}}{(\\Delta x)^2};+; f_i^{n}}_{\\text{“explicit” part}}\\Bigr].$$\nCrank–Nicolson is the special case $\\theta = \\tfrac12$. Substituting $\\theta=\\tfrac12$, we get\n$$\\frac{u_i^{n+1} - u_i^n}{\\Delta t} ;=; \\tfrac12\\Bigl[\\tfrac{u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}}{(\\Delta x)^2} + f_i^{n+1}\\Bigr] ;+; \\tfrac12\\Bigl[\\tfrac{u_{i+1}^{n} - 2u_i^{n} + u_{i-1}^{n}}{(\\Delta x)^2} + f_i^n\\Bigr].$$\nRearranging terms gives $$u_i^{n+1} ;-; \\frac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}\\bigr) ;=; u_i^{n} ;+; \\frac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n} - 2u_i^n + u_{i-1}^{n}\\bigr) ;+; \\frac{\\Delta t}{2},\\bigl(f_i^{n+1} + f_i^n\\bigr).$$ One may think of this as the linear system $$\\bigl[I + \\tfrac{\\Delta t}{2,(\\Delta x)^2},A\\bigr],\\mathbf{u}^{n+1} ;=; \\bigl[I - \\tfrac{\\Delta t}{2,(\\Delta x)^2},A\\bigr],\\mathbf{u}^{n} ;+;\\frac{\\Delta t}{2},\\bigl(\\mathbf{f}^{n+1} + \\mathbf{f}^n\\bigr),$$ where $A$ is the usual tridiagonal matrix corresponding to the second‐difference operator (and where $\\mathbf{u}^n$ is the vector of $u_i^n$). After applying boundary conditions $u_0^n = u_M^n = 0$, one solves this tridiagonal system at each time step.\nBoundary Conditions\r#\rBecause $u(0,t)=u(1,t)=0$, we set $u_0^n=0$ and $u_M^n=0$ for all $n$. The updates are applied only for $i=1,\\dots,M-1$.\nSummary of the CN Update\r#\rIn “index form,” the Crank–Nicolson scheme is: $$\\boxed{ \\begin{aligned} \u0026amp;\\text{For }i=1,\\dots,M-1:\\quad u_i^{n+1} ;-; \\tfrac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}\\bigr) \\ \u0026amp;\\qquad\\quad;=; u_i^{n} ;+; \\tfrac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n} - 2u_i^n + u_{i-1}^{n}\\bigr) ;+;\\frac{\\Delta t}{2},\\bigl(f_i^{n+1} + f_i^n\\bigr). \\end{aligned} }$$ This is solved simultaneously for all $i$, respecting $u_0^{n+1}=u_M^{n+1}=0$.\n2. Properties: Stability \u0026amp; Accuracy\r#\rStability:\nUnconditional stability for the heat equation. In other words, there is no restriction on $\\Delta t$ relative to $\\Delta x$ needed solely for stability (unlike the explicit forward‐Euler method, which requires $\\Delta t \\le \\tfrac12 (\\Delta x)^2$). More precisely, CN is A‐stable as an ODE solver applied to the linear diffusion operator. One can show by a von Neumann analysis or standard Lax–Richtmyer theory that errors do not grow unboundedly for any $\\Delta t\u0026gt;0$. Accuracy:\nIn time, Crank–Nicolson is second‐order accurate, because it is essentially the trapezoidal rule in time (it uses $\\frac12$ of the “new” time‐level’s spatial derivative plus $\\frac12$ of the “old” time‐level’s spatial derivative). In space, if we use the standard second‐difference approximation, the scheme is also second‐order in $\\Delta x$. Overall, we often say “CN is second‐order in both space and time (for sufficiently smooth solutions).” 3. BONUS: Discontinuous Initial Condition\r#\rEven if $u_0(x)$ is not continuous, the heat equation itself is smoothing: for $t\u0026gt;0$, the exact solution becomes infinitely differentiable in $x$. Numerically:\nThe scheme remains stable and convergent. Because it is a diffusion‐type PDE, any jump discontinuity in the initial data gets smoothed out instantly as $t$ increases. Crank–Nicolson will faithfully capture that smoothing. You may see large gradients at early time steps near the discontinuity, but the method will not become unstable. Hence having a discontinuous initial condition does not cause instability for the heat equation with Crank–Nicolson. The scheme still converges (second‐order in both space and time) to the unique smooth solution that the parabolic PDE defines for $t\u0026gt;0$.\nProblem 4 求解二维拉普拉斯方程的五点差分格式\r#\r我们考虑如下的椭圆方程（Poisson 型）： $$-,\\Delta u ;=; f, \\quad (x,y)\\in [0,1] \\times [0,1].$$\n其中 $$\\Delta ;=;\\frac{\\partial^2}{\\partial x^2} ;+;\\frac{\\partial^2}{\\partial y^2}$$ 是二维拉普拉斯算子，$f$ 是已知的函数。\n1. 建立网格\r#\r令 $N_x$ 和 $N_y$ 分别表示在 $x$ 和 $y$ 方向上的网格划分数目（仅指内部节点数，不含边界），则步长为 $$\\delta x ;=;\\frac{1}{N_x+1}, \\quad \\delta y ;=;\\frac{1}{N_y+1}.$$ 我们在区间 $[0,1]\\times[0,1]$ 内取离散网格点 $$x_i ;=; i,\\delta x, \\quad y_j ;=; j,\\delta y,$$ 其中 $i=0,1,2,\\dots,N_x+1$, $j=0,1,2,\\dots,N_y+1$。\n在内部节点 $(x_i,y_j)$ 上，我们用 $u_{i,j}$ 表示对真解 $u(x_i,y_j)$ 的数值近似。\n2. 五点差分格式\r#\r方程 $-\\Delta u = f$ 可写成 $$-\\left( \\frac{\\partial^2 u}{\\partial x^2} ;+; \\frac{\\partial^2 u}{\\partial y^2} \\right) ;=; f,$$ 即 $$\\frac{\\partial^2 u}{\\partial x^2} ;+; \\frac{\\partial^2 u}{\\partial y^2} ;=; -,f.$$\n在网格上，二阶导数的中心差分近似分别为：\n在 $x$ 方向： $$\\frac{\\partial^2 u}{\\partial x^2}\\bigg|{(x_i,y_j)} ;\\approx; \\frac{u{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2},$$ 在 $y$ 方向： $$\\frac{\\partial^2 u}{\\partial y^2}\\bigg|{(x_i,y_j)} ;\\approx; \\frac{u{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2}.$$ 因此，$\\Delta u$ 在离散化后可写为 $$\\Delta u_{i,j} ;=; \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2}.$$\n由于方程是 $-\\Delta u = f$，则对应的五点差分格式为\n$$-\\left[, \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2} \\right] ;=; f_{i,j},$$ 或等价地写成 $$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2} ;=; -,f_{i,j}.$$\n这里 $f_{i,j} = f(x_i,y_j)$ 表示在网格点处的函数取值。\n3. 精度阶次\r#\r上述中心差分格式对二阶导数在空间步长上具有 二阶精度。也就是说，如果我们将 $\\delta x$ 和 $\\delta y$ 同步缩小（假设网格等距），则离散解相对于真解的误差在 $\\delta x, \\delta y \\to 0$ 时满足 $$\\mathcal{O}\\bigl((\\delta x)^2 + (\\delta y)^2\\bigr).$$\n简而言之，对拉普拉斯方程采用这种五点中心差分格式，在均匀网格下是二阶精度。\n4. 最终形成的线性方程组\r#\r对所有内部节点 $(i,j)$（即 $1\\le i\\le N_x$, $1\\le j\\le N_y$）应用上述离散方程，我们便得到一个关于所有未知量 ${u_{i,j}}$ 的线性方程组。若再结合边界条件（例如已知边界上的 $u_{0,j},u_{N_x+1,j},u_{i,0},u_{i,N_y+1}$），即可完全求解。\n在实际应用中，可以用各种迭代法（如 Jacobi、Gauss-Seidel、SOR 等）或直接法（如 LU 分解等）来求解这个离散方程组。\n5. 总结\r#\r五点差分格式的离散方程（在二维情况下）是： $$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2} ;=; -,f_{i,j}.$$ 在标准的（均匀）网格下，该方法的空间离散精度是二阶。 "},{"id":7,"href":"/posts/creating-a-new-theme/","title":"Creating a New Theme","section":"Blog","content":"\rIntroduction\r#\rThis tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\nWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\nIn this tutorial, commands that you enter will start with the \u0026ldquo;$\u0026rdquo; prompt. The output will follow. Lines that start with \u0026ldquo;#\u0026rdquo; are comments that I\u0026rsquo;ve added to explain a point. When I show updates to a file, the \u0026ldquo;:wq\u0026rdquo; on the last line means to save the file.\nHere\u0026rsquo;s an example:\n## this is a comment\r$ echo this is a command\rthis is a command\r## edit the file\r$ vi foo.md\r+++\rdate = \"2014-09-28\"\rtitle = \"creating a new theme\"\r+++\rbah and humbug\r:wq\r## show it\r$ cat foo.md\r+++\rdate = \"2014-09-28\"\rtitle = \"creating a new theme\"\r+++\rbah and humbug\r$\rSome Definitions\r#\rThere are a few concepts that you need to understand before creating a theme.\nSkins\r#\rSkins are the files responsible for the look and feel of your site. It’s the CSS that controls colors and fonts, it’s the Javascript that determines actions and reactions. It’s also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors.\nYou have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you don’t have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin.\nYour second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. It’s extra work, though, so why bother with it?\nThe difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ can’t be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it.\nThe rest of this tutorial will call a skin created in the themes/ directory a theme.\nNote that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you won’t need to update the site’s configuration file to use a theme.\nThe Home Page\r#\rThe home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html.\nSite Configuration File\r#\rWhen Hugo runs, it looks for a configuration file that contains settings that override default values for the entire site. The file can use TOML, YAML, or JSON. I prefer to use TOML for my configuration files. If you prefer to use JSON or YAML, you’ll need to translate my examples. You’ll also need to change the name of the file since Hugo uses the extension to determine how to process it.\nHugo translates Markdown files into HTML. By default, Hugo expects to find Markdown files in your content/ directory and template files in your themes/ directory. It will create HTML files in your public/ directory. You can change this by specifying alternate locations in the configuration file.\nContent\r#\rContent is stored in text files that contain two sections. The first section is the “front matter,” which is the meta-information on the content. The second section contains Markdown that will be converted to HTML.\nFront Matter\r#\rThe front matter is information about the content. Like the configuration file, it can be written in TOML, YAML, or JSON. Unlike the configuration file, Hugo doesn’t use the file’s extension to know the format. It looks for markers to signal the type. TOML is surrounded by “+++”, YAML by “---”, and JSON is enclosed in curly braces. I prefer to use TOML, so you’ll need to translate my examples if you prefer YAML or JSON.\nThe information in the front matter is passed into the template before the content is rendered into HTML.\nMarkdown\r#\rContent is written in Markdown which makes it easier to create the content. Hugo runs the content through a Markdown engine to create the HTML which will be written to the output file.\nTemplate Files\r#\rHugo uses template files to render content into HTML. Template files are a bridge between the content and presentation. Rules in the template define what content is published, where it\u0026rsquo;s published to, and how it will rendered to the HTML file. The template guides the presentation by specifying the style to use.\nThere are three types of templates: single, list, and partial. Each type takes a bit of content as input and transforms it based on the commands in the template.\nHugo uses its knowledge of the content to find the template file used to render the content. If it can’t find a template that is an exact match for the content, it will shift up a level and search from there. It will continue to do so until it finds a matching template or runs out of templates to try. If it can’t find a template, it will use the default template for the site.\nPlease note that you can use the front matter to influence Hugo’s choice of templates.\nSingle Template\r#\rA single template is used to render a single piece of content. For example, an article or post would be a single piece of content and use a single template.\nList Template\r#\rA list template renders a group of related content. That could be a summary of recent postings or all articles in a category. List templates can contain multiple groups.\nThe homepage template is a special type of list template. Hugo assumes that the home page of your site will act as the portal for the rest of the content in the site.\nPartial Template\r#\rA partial template is a template that can be included in other templates. Partial templates must be called using the “partial” template command. They are very handy for rolling up common behavior. For example, your site may have a banner that all pages use. Instead of copying the text of the banner into every single and list template, you could create a partial with the banner in it. That way if you decide to change the banner, you only have to change the partial template.\nCreate a New Site\r#\rLet\u0026rsquo;s use Hugo to create a new web site. I\u0026rsquo;m a Mac user, so I\u0026rsquo;ll create mine in my home directory, in the Sites folder. If you\u0026rsquo;re using Linux, you might have to create the folder first.\nThe \u0026ldquo;new site\u0026rdquo; command will create a skeleton of a site. It will give you the basic directory structure and a useable configuration file.\n$ hugo new site ~/Sites/zafta\r$ cd ~/Sites/zafta\r$ ls -l\rtotal 8\rdrwxr-xr-x 7 quoha staff 238 Sep 29 16:49 .\rdrwxr-xr-x 3 quoha staff 102 Sep 29 16:49 ..\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\r$\rTake a look in the content/ directory to confirm that it is empty.\nThe other directories (archetypes/, layouts/, and static/) are used when customizing a theme. That\u0026rsquo;s a topic for a different tutorial, so please ignore them for now.\nGenerate the HTML For the New Site\r#\rRunning the hugo command with no options will read all the available content and generate the HTML files. It will also copy all static files (that\u0026rsquo;s everything that\u0026rsquo;s not content). Since we have an empty site, it won\u0026rsquo;t do much, but it will do it very quickly.\n$ hugo --verbose\rINFO: 2014/09/29 Using config file: config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$ The \u0026ldquo;--verbose\u0026rdquo; flag gives extra information that will be helpful when we build the template. Every line of the output that starts with \u0026ldquo;INFO:\u0026rdquo; or \u0026ldquo;WARN:\u0026rdquo; is present because we used that flag. The lines that start with \u0026ldquo;WARN:\u0026rdquo; are warning messages. We\u0026rsquo;ll go over them later.\nWe can verify that the command worked by looking at the directory again.\n$ ls -l\rtotal 8\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\r$\rSee that new public/ directory? Hugo placed all generated content there. When you\u0026rsquo;re ready to publish your web site, that\u0026rsquo;s the place to start. For now, though, let\u0026rsquo;s just confirm that we have what we\u0026rsquo;d expect from a site with no content.\n$ ls -l public\rtotal 16\r-rw-r--r-- 1 quoha staff 416 Sep 29 17:02 index.xml\r-rw-r--r-- 1 quoha staff 262 Sep 29 17:02 sitemap.xml\r$ Hugo created two XML files, which is standard, but there are no HTML files.\nTest the New Site\r#\rVerify that you can run the built-in web server. It will dramatically shorten your development cycle if you do. Start it by running the \u0026ldquo;server\u0026rdquo; command. If it is successful, you will see output similar to the following:\n$ hugo server --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\rServing pages from /Users/quoha/Sites/zafta/public\rWeb Server is available at http://localhost:1313\rPress Ctrl+C to stop\rConnect to the listed URL (it\u0026rsquo;s on the line that starts with \u0026ldquo;Web Server\u0026rdquo;). If everything is working correctly, you should get a page that shows the following:\nindex.xml\rsitemap.xml\rThat\u0026rsquo;s a listing of your public/ directory. Hugo didn\u0026rsquo;t create a home page because our site has no content. When there\u0026rsquo;s no index.html file in a directory, the server lists the files in the directory, which is what you should see in your browser.\nLet’s go back and look at those warnings again.\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\rThat second warning is easier to explain. We haven’t created a template to be used to generate “page not found errors.” The 404 message is a topic for a separate tutorial.\nNow for the first warning. It is for the home page. You can tell because the first layout that it looked for was “index.html.” That’s only used by the home page.\nI like that the verbose flag causes Hugo to list the files that it\u0026rsquo;s searching for. For the home page, they are index.html, _default/list.html, and _default/single.html. There are some rules that we\u0026rsquo;ll cover later that explain the names and paths. For now, just remember that Hugo couldn\u0026rsquo;t find a template for the home page and it told you so.\nAt this point, you\u0026rsquo;ve got a working installation and site that we can build upon. All that’s left is to add some content and a theme to display it.\nCreate a New Theme\r#\rHugo doesn\u0026rsquo;t ship with a default theme. There are a few available (I counted a dozen when I first installed Hugo) and Hugo comes with a command to create new themes.\nWe\u0026rsquo;re going to create a new theme called \u0026ldquo;zafta.\u0026rdquo; Since the goal of this tutorial is to show you how to fill out the files to pull in your content, the theme will not contain any CSS. In other words, ugly but functional.\nAll themes have opinions on content and layout. For example, Zafta uses \u0026ldquo;post\u0026rdquo; over \u0026ldquo;blog\u0026rdquo;. Strong opinions make for simpler templates but differing opinions make it tougher to use themes. When you build a theme, consider using the terms that other themes do.\nCreate a Skeleton\r#\rUse the hugo \u0026ldquo;new\u0026rdquo; command to create the skeleton of a theme. This creates the directory structure and places empty files for you to fill out.\n$ hugo new theme zafta\r$ ls -l\rtotal 8\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\rdrwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes\r$ find themes -type f | xargs ls -l\r-rw-r--r-- 1 quoha staff 1081 Sep 29 17:31 themes/zafta/LICENSE.md\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/archetypes/default.md\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html\r-rw-r--r-- 1 quoha staff 93 Sep 29 17:31 themes/zafta/theme.toml\r$ The skeleton includes templates (the files ending in .html), license file, a description of your theme (the theme.toml file), and an empty archetype.\nPlease take a minute to fill out the theme.toml and LICENSE.md files. They\u0026rsquo;re optional, but if you\u0026rsquo;re going to be distributing your theme, it tells the world who to praise (or blame). It\u0026rsquo;s also nice to declare the license so that people will know how they can use the theme.\n$ vi themes/zafta/theme.toml\rauthor = \"michael d henderson\"\rdescription = \"a minimal working template\"\rlicense = \"MIT\"\rname = \"zafta\"\rsource_repo = \"\"\rtags = [\"tags\", \"categories\"]\r:wq\r## also edit themes/zafta/LICENSE.md and change\r## the bit that says \"YOUR_NAME_HERE\"\rNote that the the skeleton\u0026rsquo;s template files are empty. Don\u0026rsquo;t worry, we\u0026rsquo;ll be changing that shortly.\n$ find themes/zafta -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html\r$\rUpdate the Configuration File to Use the Theme\r#\rNow that we\u0026rsquo;ve got a theme to work with, it\u0026rsquo;s a good idea to add the theme name to the configuration file. This is optional, because you can always add \u0026ldquo;-t zafta\u0026rdquo; on all your commands. I like to put it the configuration file because I like shorter command lines. If you don\u0026rsquo;t put it in the configuration file or specify it on the command line, you won\u0026rsquo;t use the template that you\u0026rsquo;re expecting to.\nEdit the file to add the theme, add a title for the site, and specify that all of our content will use the TOML format.\n$ vi config.toml\rtheme = \"zafta\"\rbaseurl = \"\"\rlanguageCode = \"en-us\"\rtitle = \"zafta - totally refreshing\"\rMetaDataFormat = \"toml\"\r:wq\r$\rGenerate the Site\r#\rNow that we have an empty theme, let\u0026rsquo;s generate the site again.\n$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$\rDid you notice that the output is different? The warning message for the home page has disappeared and we have an additional information line saying that Hugo is syncing from the theme\u0026rsquo;s directory.\nLet\u0026rsquo;s check the public/ directory to see what Hugo\u0026rsquo;s created.\n$ ls -l public\rtotal 16\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:56 css\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:56 index.html\r-rw-r--r-- 1 quoha staff 407 Sep 29 17:56 index.xml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:56 js\r-rw-r--r-- 1 quoha staff 243 Sep 29 17:56 sitemap.xml\r$\rNotice four things:\nHugo created a home page. This is the file public/index.html. Hugo created a css/ directory. Hugo created a js/ directory. Hugo claimed that it created 0 pages. It created a file and copied over static files, but didn\u0026rsquo;t create any pages. That\u0026rsquo;s because it considers a \u0026ldquo;page\u0026rdquo; to be a file created directly from a content file. It doesn\u0026rsquo;t count things like the index.html files that it creates automatically. The Home Page\r#\rHugo supports many different types of templates. The home page is special because it gets its own type of template and its own template file. The file, layouts/index.html, is used to generate the HTML for the home page. The Hugo documentation says that this is the only required template, but that depends. Hugo\u0026rsquo;s warning message shows that it looks for three different templates:\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rIf it can\u0026rsquo;t find any of these, it completely skips creating the home page. We noticed that when we built the site without having a theme installed.\nWhen Hugo created our theme, it created an empty home page template. Now, when we build the site, Hugo finds the template and uses it to generate the HTML for the home page. Since the template file is empty, the HTML file is empty, too. If the template had any rules in it, then Hugo would have used them to generate the home page.\n$ find . -name index.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 20:21 ./public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 ./themes/zafta/layouts/index.html\r$ The Magic of Static\r#\rHugo does two things when generating the site. It uses templates to transform content into HTML and it copies static files into the site. Unlike content, static files are not transformed. They are copied exactly as they are.\nHugo assumes that your site will use both CSS and JavaScript, so it creates directories in your theme to hold them. Remember opinions? Well, Hugo\u0026rsquo;s opinion is that you\u0026rsquo;ll store your CSS in a directory named css/ and your JavaScript in a directory named js/. If you don\u0026rsquo;t like that, you can change the directory names in your theme directory or even delete them completely. Hugo\u0026rsquo;s nice enough to offer its opinion, then behave nicely if you disagree.\n$ find themes/zafta -type d | xargs ls -ld\rdrwxr-xr-x 7 quoha staff 238 Sep 29 17:38 themes/zafta\rdrwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes/zafta/archetypes\rdrwxr-xr-x 5 quoha staff 170 Sep 29 17:31 themes/zafta/layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/_default\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/partials\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/static\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/css\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/js\r$ The Theme Development Cycle\r#\rWhen you\u0026rsquo;re working on a theme, you will make changes in the theme\u0026rsquo;s directory, rebuild the site, and check your changes in the browser. Hugo makes this very easy:\nPurge the public/ directory. Run the built in web server in watch mode. Open your site in a browser. Update the theme. Glance at your browser window to see changes. Return to step 4. I’ll throw in one more opinion: never work on a theme on a live site. Always work on a copy of your site. Make changes to your theme, test them, then copy them up to your site. For added safety, use a tool like Git to keep a revision history of your content and your theme. Believe me when I say that it is too easy to lose both your mind and your changes.\nCheck the main Hugo site for information on using Git with Hugo.\nPurge the public/ Directory\r#\rWhen generating the site, Hugo will create new files and update existing ones in the public/ directory. It will not delete files that are no longer used. For example, files that were created in the wrong directory or with the wrong title will remain. If you leave them, you might get confused by them later. I recommend cleaning out your site prior to generating it.\nNote: If you\u0026rsquo;re building on an SSD, you should ignore this. Churning on a SSD can be costly.\nHugo\u0026rsquo;s Watch Option\r#\rHugo\u0026rsquo;s \u0026ldquo;--watch\u0026rdquo; option will monitor the content/ and your theme directories for changes and rebuild the site automatically.\nLive Reload\r#\rHugo\u0026rsquo;s built in web server supports live reload. As pages are saved on the server, the browser is told to refresh the page. Usually, this happens faster than you can say, \u0026ldquo;Wow, that\u0026rsquo;s totally amazing.\u0026rdquo;\nDevelopment Commands\r#\rUse the following commands as the basis for your workflow.\n## purge old files. hugo will recreate the public directory.\r##\r$ rm -rf public\r##\r## run hugo in watch mode\r##\r$ hugo server --watch --verbose\rHere\u0026rsquo;s sample output showing Hugo detecting a change to the template for the home page. Once generated, the web browser automatically reloaded the page. I\u0026rsquo;ve said this before, it\u0026rsquo;s amazing.\n$ rm -rf public\r$ hugo server --watch --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\rWatching for changes in /Users/quoha/Sites/zafta/content\rServing pages from /Users/quoha/Sites/zafta/public\rWeb Server is available at http://localhost:1313\rPress Ctrl+C to stop\rINFO: 2014/09/29 File System Event: [\"/Users/quoha/Sites/zafta/themes/zafta/layouts/index.html\": MODIFY|ATTRIB]\rChange detected, rebuilding site\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 1 ms\rUpdate the Home Page Template\r#\rThe home page is one of a few special pages that Hugo creates automatically. As mentioned earlier, it looks for one of three files in the theme\u0026rsquo;s layout/ directory:\nindex.html _default/list.html _default/single.html We could update one of the default templates, but a good design decision is to update the most specific template available. That\u0026rsquo;s not a hard and fast rule (in fact, we\u0026rsquo;ll break it a few times in this tutorial), but it is a good generalization.\nMake a Static Home Page\r#\rRight now, that page is empty because we don\u0026rsquo;t have any content and we don\u0026rsquo;t have any logic in the template. Let\u0026rsquo;s change that by adding some text to the template.\n$ vi themes/zafta/layouts/index.html\r\u003c!DOCTYPE html\u003e hugo says hello!\n:wq\r$\rBuild the web site and then verify the results.\n$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 78 Sep 29 21:26 public/index.html\r$ cat public/index.html \u003c!DOCTYPE html\u003e hugo says hello!\nLive Reload\r#\rNote: If you\u0026rsquo;re running the server with the --watch option, you\u0026rsquo;ll see different content in the file:\n$ cat public/index.html \u003c!DOCTYPE html\u003e hugo says hello!\nWhen you use --watch, the Live Reload script is added by Hugo. Look for live reload in the documentation to see what it does and how to disable it.\nBuild a \u0026ldquo;Dynamic\u0026rdquo; Home Page\r#\r\u0026ldquo;Dynamic home page?\u0026rdquo; Hugo\u0026rsquo;s a static web site generator, so this seems an odd thing to say. I mean let\u0026rsquo;s have the home page automatically reflect the content in the site every time Hugo builds it. We\u0026rsquo;ll use iteration in the template to do that.\nCreate New Posts\r#\rNow that we have the home page generating static content, let\u0026rsquo;s add some content to the site. We\u0026rsquo;ll display these posts as a list on the home page and on their own page, too.\nHugo has a command to generate a skeleton post, just like it does for sites and themes.\n$ hugo --verbose new post/first.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/first.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/default.md\rERROR: 2014/09/29 Unable to Cast to map[string]interface{}\r$ That wasn\u0026rsquo;t very nice, was it?\nThe \u0026ldquo;new\u0026rdquo; command uses an archetype to create the post file. Hugo created an empty default archetype file, but that causes an error when there\u0026rsquo;s a theme. For me, the workaround was to create an archetypes file specifically for the post type.\n$ vi themes/zafta/archetypes/post.md\r+++\rDescription = \"\"\rTags = []\rCategories = []\r+++\r:wq\r$ find themes/zafta/archetypes -type f | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 21:53 themes/zafta/archetypes/default.md\r-rw-r--r-- 1 quoha staff 51 Sep 29 21:54 themes/zafta/archetypes/post.md\r$ hugo --verbose new post/first.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/first.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md\rINFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/first.md\r/Users/quoha/Sites/zafta/content/post/first.md created\r$ hugo --verbose new post/second.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/second.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md\rINFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/second.md\r/Users/quoha/Sites/zafta/content/post/second.md created\r$ ls -l content/post\rtotal 16\r-rw-r--r-- 1 quoha staff 104 Sep 29 21:54 first.md\r-rw-r--r-- 1 quoha staff 105 Sep 29 21:57 second.md\r$ cat content/post/first.md +++\rCategories = []\rDescription = \"\"\rTags = []\rdate = \"2014-09-29T21:54:53-05:00\"\rtitle = \"first\"\r+++\rmy first post\r$ cat content/post/second.md +++\rCategories = []\rDescription = \"\"\rTags = []\rdate = \"2014-09-29T21:57:09-05:00\"\rtitle = \"second\"\r+++\rmy second post\r$ Build the web site and then verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"category\":\"categories\", \"tag\":\"tags\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$\rThe output says that it created 2 pages. Those are our new posts:\n$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 78 Sep 29 22:13 public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/second/index.html\r$\rThe new files are empty because because the templates used to generate the content are empty. The homepage doesn\u0026rsquo;t show the new content, either. We have to update the templates to add the posts.\nList and Single Templates\r#\rIn Hugo, we have three major kinds of templates. There\u0026rsquo;s the home page template that we updated previously. It is used only by the home page. We also have \u0026ldquo;single\u0026rdquo; templates which are used to generate output for a single content file. We also have \u0026ldquo;list\u0026rdquo; templates that are used to group multiple pieces of content before generating output.\nGenerally speaking, list templates are named \u0026ldquo;list.html\u0026rdquo; and single templates are named \u0026ldquo;single.html.\u0026rdquo;\nThere are three other types of templates: partials, content views, and terms. We will not go into much detail on these.\nAdd Content to the Homepage\r#\rThe home page will contain a list of posts. Let\u0026rsquo;s update its template to add the posts that we just created. The logic in the template will run every time we build the site.\n$ vi themes/zafta/layouts/index.html \u003c!DOCTYPE html\u003e\r{{ range first 10 .Data.Pages }}\r{{ .Title }}\r{{ end }}\r:wq\r$\rHugo uses the Go template engine. That engine scans the template files for commands which are enclosed between \u0026ldquo;{{\u0026rdquo; and \u0026ldquo;}}\u0026rdquo;. In our template, the commands are:\nrange .Title end The \u0026ldquo;range\u0026rdquo; command is an iterator. We\u0026rsquo;re going to use it to go through the first ten pages. Every HTML file that Hugo creates is treated as a page, so looping through the list of pages will look at every file that will be created.\nThe \u0026ldquo;.Title\u0026rdquo; command prints the value of the \u0026ldquo;title\u0026rdquo; variable. Hugo pulls it from the front matter in the Markdown file.\nThe \u0026ldquo;end\u0026rdquo; command signals the end of the range iterator. The engine loops back to the top of the iteration when it finds \u0026ldquo;end.\u0026rdquo; Everything between the \u0026ldquo;range\u0026rdquo; and \u0026ldquo;end\u0026rdquo; is evaluated every time the engine goes through the iteration. In this file, that would cause the title from the first ten pages to be output as heading level one.\nIt\u0026rsquo;s helpful to remember that some variables, like .Data, are created before any output files. Hugo loads every content file into the variable and then gives the template a chance to process before creating the HTML files.\nBuild the web site and then verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:23 public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/second/index.html\r$ cat public/index.html \u003c!DOCTYPE html\u003e\rsecond\rfirst\r$\rCongratulations, the home page shows the title of the two posts. The posts themselves are still empty, but let\u0026rsquo;s take a moment to appreciate what we\u0026rsquo;ve done. Your template now generates output dynamically. Believe it or not, by inserting the range command inside of those curly braces, you\u0026rsquo;ve learned everything you need to know to build a theme. All that\u0026rsquo;s really left is understanding which template will be used to generate each content file and becoming familiar with the commands for the template engine.\nAnd, if that were entirely true, this tutorial would be much shorter. There are a few things to know that will make creating a new template much easier. Don\u0026rsquo;t worry, though, that\u0026rsquo;s all to come.\nAdd Content to the Posts\r#\rWe\u0026rsquo;re working with posts, which are in the content/post/ directory. That means that their section is \u0026ldquo;post\u0026rdquo; (and if we don\u0026rsquo;t do something weird, their type is also \u0026ldquo;post\u0026rdquo;).\nHugo uses the section and type to find the template file for every piece of content. Hugo will first look for a template file that matches the section or type name. If it can\u0026rsquo;t find one, then it will look in the _default/ directory. There are some twists that we\u0026rsquo;ll cover when we get to categories and tags, but for now we can assume that Hugo will try post/single.html, then _default/single.html.\nNow that we know the search rule, let\u0026rsquo;s see what we actually have available:\n$ find themes/zafta -name single.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 132 Sep 29 17:31 themes/zafta/layouts/_default/single.html\rWe could create a new template, post/single.html, or change the default. Since we don\u0026rsquo;t know of any other content types, let\u0026rsquo;s start with updating the default.\nRemember, any content that we haven\u0026rsquo;t created a template for will end up using this template. That can be good or bad. Bad because I know that we\u0026rsquo;re going to be adding different types of content and we\u0026rsquo;re going to end up undoing some of the changes we\u0026rsquo;ve made. It\u0026rsquo;s good because we\u0026rsquo;ll be able to see immediate results. It\u0026rsquo;s also good to start here because we can start to build the basic layout for the site. As we add more content types, we\u0026rsquo;ll refactor this file and move logic around. Hugo makes that fairly painless, so we\u0026rsquo;ll accept the cost and proceed.\nPlease see the Hugo documentation on template rendering for all the details on determining which template to use. And, as the docs mention, if you\u0026rsquo;re building a single page application (SPA) web site, you can delete all of the other templates and work with just the default single page. That\u0026rsquo;s a refreshing amount of joy right there.\nUpdate the Template File\r#\r$ vi themes/zafta/layouts/_default/single.html \u003c!DOCTYPE html\u003e\r{{ .Title }}\r{{ .Title }}\r{{ .Content }}\r:wq\r$\rBuild the web site and verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 94 Sep 29 22:40 public/index.html\r-rw-r--r-- 1 quoha staff 125 Sep 29 22:40 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:40 public/post/index.html\r-rw-r--r-- 1 quoha staff 128 Sep 29 22:40 public/post/second/index.html\r$ cat public/post/first/index.html \u003c!DOCTYPE html\u003e\rfirst\rfirst\rmy first post\n$ cat public/post/second/index.html \u003c!DOCTYPE html\u003e\rsecond\rsecond\rmy second post\n$\rNotice that the posts now have content. You can go to localhost:1313/post/first to verify.\nLinking to Content\r#\rThe posts are on the home page. Let\u0026rsquo;s add a link from there to the post. Since this is the home page, we\u0026rsquo;ll update its template.\n$ vi themes/zafta/layouts/index.html\r\u003c!DOCTYPE html\u003e\r{{ range first 10 .Data.Pages }}\r{{ .Title }}\r{{ end }}\rBuild the web site and verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 149 Sep 29 22:44 public/index.html\r-rw-r--r-- 1 quoha staff 125 Sep 29 22:44 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:44 public/post/index.html\r-rw-r--r-- 1 quoha staff 128 Sep 29 22:44 public/post/second/index.html\r$ cat public/index.html \u003c!DOCTYPE html\u003e\rsecond\rfirst\r$\rCreate a Post Listing\r#\rWe have the posts displaying on the home page and on their own page. We also have a file public/post/index.html that is empty. Let\u0026rsquo;s make it show a list of all posts (not just the first ten).\nWe need to decide which template to update. This will be a listing, so it should be a list template. Let\u0026rsquo;s take a quick look and see which list templates are available.\n$ find themes/zafta -name list.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\rAs with the single post, we have to decide to update _default/list.html or create post/list.html. We still don\u0026rsquo;t have multiple content types, so let\u0026rsquo;s stay consistent and update the default list template.\nCreating Top Level Pages\r#\rLet\u0026rsquo;s add an \u0026ldquo;about\u0026rdquo; page and display it at the top level (as opposed to a sub-level like we did with posts).\nThe default in Hugo is to use the directory structure of the content/ directory to guide the location of the generated html in the public/ directory. Let\u0026rsquo;s verify that by creating an \u0026ldquo;about\u0026rdquo; page at the top level:\n$ vi content/about.md +++\rtitle = \"about\"\rdescription = \"about this site\"\rdate = \"2014-09-27\"\rslug = \"about time\"\r+++\r## about us\ri'm speechless\r:wq\rGenerate the web site and verify the results.\n$ find public -name '*.html' | xargs ls -l\r-rw-rw-r-- 1 mdhender staff 334 Sep 27 15:08 public/about-time/index.html\r-rw-rw-r-- 1 mdhender staff 527 Sep 27 15:08 public/index.html\r-rw-rw-r-- 1 mdhender staff 358 Sep 27 15:08 public/post/first-post/index.html\r-rw-rw-r-- 1 mdhender staff 0 Sep 27 15:08 public/post/index.html\r-rw-rw-r-- 1 mdhender staff 342 Sep 27 15:08 public/post/second-post/index.html\rNotice that the page wasn\u0026rsquo;t created at the top level. It was created in a sub-directory named \u0026lsquo;about-time/\u0026rsquo;. That name came from our slug. Hugo will use the slug to name the generated content. It\u0026rsquo;s a reasonable default, by the way, but we can learn a few things by fighting it for this file.\nOne other thing. Take a look at the home page.\n$ cat public/index.html\r\u003c!DOCTYPE html\u003e\rcreating a new theme\rabout\rsecond\rfirst\rNotice that the \u0026ldquo;about\u0026rdquo; link is listed with the posts? That\u0026rsquo;s not desirable, so let\u0026rsquo;s change that first.\n$ vi themes/zafta/layouts/index.html\r\u003c!DOCTYPE html\u003e\rposts\r{{ range first 10 .Data.Pages }}\r{{ if eq .Type \"post\"}}\r{{ .Title }}\r{{ end }}\r{{ end }}\rpages\r{{ range .Data.Pages }}\r{{ if eq .Type \"page\" }}\r{{ .Title }}\r{{ end }}\r{{ end }}\r:wq\rGenerate the web site and verify the results. The home page has two sections, posts and pages, and each section has the right set of headings and links in it.\nBut, that about page still renders to about-time/index.html.\n$ find public -name '*.html' | xargs ls -l\r-rw-rw-r-- 1 mdhender staff 334 Sep 27 15:33 public/about-time/index.html\r-rw-rw-r-- 1 mdhender staff 645 Sep 27 15:33 public/index.html\r-rw-rw-r-- 1 mdhender staff 358 Sep 27 15:33 public/post/first-post/index.html\r-rw-rw-r-- 1 mdhender staff 0 Sep 27 15:33 public/post/index.html\r-rw-rw-r-- 1 mdhender staff 342 Sep 27 15:33 public/post/second-post/index.html\rKnowing that hugo is using the slug to generate the file name, the simplest solution is to change the slug. Let\u0026rsquo;s do it the hard way and change the permalink in the configuration file.\n$ vi config.toml\r[permalinks]\rpage = \"/:title/\"\rabout = \"/:filename/\"\rGenerate the web site and verify that this didn\u0026rsquo;t work. Hugo lets \u0026ldquo;slug\u0026rdquo; or \u0026ldquo;URL\u0026rdquo; override the permalinks setting in the configuration file. Go ahead and comment out the slug in content/about.md, then generate the web site to get it to be created in the right place.\nSharing Templates\r#\rIf you\u0026rsquo;ve been following along, you probably noticed that posts have titles in the browser and the home page doesn\u0026rsquo;t. That\u0026rsquo;s because we didn\u0026rsquo;t put the title in the home page\u0026rsquo;s template (layouts/index.html). That\u0026rsquo;s an easy thing to do, but let\u0026rsquo;s look at a different option.\nWe can put the common bits into a shared template that\u0026rsquo;s stored in the themes/zafta/layouts/partials/ directory.\nCreate the Header and Footer Partials\r#\rIn Hugo, a partial is a sugar-coated template. Normally a template reference has a path specified. Partials are different. Hugo searches for them along a TODO defined search path. This makes it easier for end-users to override the theme\u0026rsquo;s presentation.\n$ vi themes/zafta/layouts/partials/header.html\r\u003c!DOCTYPE html\u003e\r{{ .Title }}\r:wq\r$ vi themes/zafta/layouts/partials/footer.html\r:wq\rUpdate the Home Page Template to Use the Partials\r#\rThe most noticeable difference between a template call and a partials call is the lack of path:\n{{ template \"theme/partials/header.html\" . }}\rversus\n{{ partial \"header.html\" . }}\rBoth pass in the context.\nLet\u0026rsquo;s change the home page template to use these new partials.\n$ vi themes/zafta/layouts/index.html\r{{ partial \"header.html\" . }}\rposts\r{{ range first 10 .Data.Pages }}\r{{ if eq .Type \"post\"}}\r{{ .Title }}\r{{ end }}\r{{ end }}\rpages\r{{ range .Data.Pages }}\r{{ if or (eq .Type \"page\") (eq .Type \"about\") }}\r{{ .Type }} - {{ .Title }} - {{ .RelPermalink }}\r{{ end }}\r{{ end }}\r{{ partial \"footer.html\" . }}\r:wq\rGenerate the web site and verify the results. The title on the home page is now \u0026ldquo;your title here\u0026rdquo;, which comes from the \u0026ldquo;title\u0026rdquo; variable in the config.toml file.\nUpdate the Default Single Template to Use the Partials\r#\r$ vi themes/zafta/layouts/_default/single.html\r{{ partial \"header.html\" . }}\r{{ .Title }}\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\rGenerate the web site and verify the results. The title on the posts and the about page should both reflect the value in the markdown file.\nAdd “Date Published” to Posts\r#\rIt\u0026rsquo;s common to have posts display the date that they were written or published, so let\u0026rsquo;s add that. The front matter of our posts has a variable named \u0026ldquo;date.\u0026rdquo; It\u0026rsquo;s usually the date the content was created, but let\u0026rsquo;s pretend that\u0026rsquo;s the value we want to display.\nAdd “Date Published” to the Template\r#\rWe\u0026rsquo;ll start by updating the template used to render the posts. The template code will look like:\n{{ .Date.Format \"Mon, Jan 2, 2006\" }}\rPosts use the default single template, so we\u0026rsquo;ll change that file.\n$ vi themes/zafta/layouts/_default/single.html\r{{ partial \"header.html\" . }}\r{{ .Title }}\r{{ .Date.Format \"Mon, Jan 2, 2006\" }}\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\rGenerate the web site and verify the results. The posts now have the date displayed in them. There\u0026rsquo;s a problem, though. The \u0026ldquo;about\u0026rdquo; page also has the date displayed.\nAs usual, there are a couple of ways to make the date display only on posts. We could do an \u0026ldquo;if\u0026rdquo; statement like we did on the home page. Another way would be to create a separate template for posts.\nThe \u0026ldquo;if\u0026rdquo; solution works for sites that have just a couple of content types. It aligns with the principle of \u0026ldquo;code for today,\u0026rdquo; too.\nLet\u0026rsquo;s assume, though, that we\u0026rsquo;ve made our site so complex that we feel we have to create a new template type. In Hugo-speak, we\u0026rsquo;re going to create a section template.\nLet\u0026rsquo;s restore the default single template before we forget.\n$ mkdir themes/zafta/layouts/post\r$ vi themes/zafta/layouts/_default/single.html\r{{ partial \"header.html\" . }}\r{{ .Title }}\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\rNow we\u0026rsquo;ll update the post\u0026rsquo;s version of the single template. If you remember Hugo\u0026rsquo;s rules, the template engine will use this version over the default.\n$ vi themes/zafta/layouts/post/single.html\r{{ partial \"header.html\" . }}\r{{ .Title }}\r{{ .Date.Format \"Mon, Jan 2, 2006\" }}\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\rNote that we removed the date logic from the default template and put it in the post template. Generate the web site and verify the results. Posts have dates and the about page doesn\u0026rsquo;t.\nDon\u0026rsquo;t Repeat Yourself\r#\rDRY is a good design goal and Hugo does a great job supporting it. Part of the art of a good template is knowing when to add a new template and when to update an existing one. While you\u0026rsquo;re figuring that out, accept that you\u0026rsquo;ll be doing some refactoring. Hugo makes that easy and fast, so it\u0026rsquo;s okay to delay splitting up a template.\n"},{"id":8,"href":"/posts/migrate-from-jekyll/","title":"Migrating from Jekyll","section":"Blog","content":"\rMove static content to static\r#\rJekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like\n▾ \u0026lt;root\u0026gt;/\r▾ images/\rlogo.png\rshould become\n▾ \u0026lt;root\u0026gt;/\r▾ static/\r▾ images/\rlogo.png\rAdditionally, you\u0026rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.\nCreate your Hugo configuration file\r#\rHugo can read your configuration as JSON, YAML or TOML. Hugo supports parameters custom configuration too. Refer to the Hugo configuration documentation for details.\nSet your configuration publish folder to _site\r#\rThe default is for Jekyll to publish to _site and for Hugo to publish to public. If, like me, you have _site mapped to a git submodule on the gh-pages branch, you\u0026rsquo;ll want to do one of two alternatives:\nChange your submodule to point to map gh-pages to public instead of _site (recommended).\ngit submodule deinit _site\rgit rm _site\rgit submodule add -b gh-pages git@github.com:your-username/your-repo.git public\rOr, change the Hugo configuration to use _site instead of public.\n{\r..\r\u0026quot;publishdir\u0026quot;: \u0026quot;_site\u0026quot;,\r..\r}\rConvert Jekyll templates to Hugo templates\r#\rThat\u0026rsquo;s the bulk of the work right here. The documentation is your friend. You should refer to Jekyll\u0026rsquo;s template documentation if you need to refresh your memory on how you built your blog and Hugo\u0026rsquo;s template to learn Hugo\u0026rsquo;s way.\nAs a single reference data point, converting my templates for heyitsalex.net took me no more than a few hours.\nConvert Jekyll plugins to Hugo shortcodes\r#\rJekyll has plugins; Hugo has shortcodes. It\u0026rsquo;s fairly trivial to do a port.\nImplementation\r#\rAs an example, I was using a custom image_tag plugin to generate figures with caption when running Jekyll. As I read about shortcodes, I found Hugo had a nice built-in shortcode that does exactly the same thing.\nJekyll\u0026rsquo;s plugin:\nmodule Jekyll\rclass ImageTag \u0026lt; Liquid::Tag\r@url = nil\r@caption = nil\r@class = nil\r@link = nil\r// Patterns\rIMAGE_URL_WITH_CLASS_AND_CAPTION =\rIMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;(\\s+)-\u0026gt;((https?:\\/\\/|\\/)(\\S+))(\\s*)/i\rIMAGE_URL_WITH_CAPTION = /((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;/i\rIMAGE_URL_WITH_CLASS = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))/i\rIMAGE_URL = /((https?:\\/\\/|\\/)(\\S+))/i\rdef initialize(tag_name, markup, tokens)\rsuper\rif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK\r@class = $1\r@url = $3\r@caption = $7\r@link = $9\relsif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION\r@class = $1\r@url = $3\r@caption = $7\relsif markup =~ IMAGE_URL_WITH_CAPTION\r@url = $1\r@caption = $5\relsif markup =~ IMAGE_URL_WITH_CLASS\r@class = $1\r@url = $3\relsif markup =~ IMAGE_URL\r@url = $1\rend\rend\rdef render(context)\rif @class\rsource = \u0026quot;\u0026lt;figure class='#{@class}'\u0026gt;\u0026quot;\relse\rsource = \u0026quot;\u0026lt;figure\u0026gt;\u0026quot;\rend\rif @link\rsource += \u0026quot;\u0026lt;a href=\\\u0026quot;#{@link}\\\u0026quot;\u0026gt;\u0026quot;\rend\rsource += \u0026quot;\u0026lt;img src=\\\u0026quot;#{@url}\\\u0026quot;\u0026gt;\u0026quot;\rif @link\rsource += \u0026quot;\u0026lt;/a\u0026gt;\u0026quot;\rend\rsource += \u0026quot;\u0026lt;figcaption\u0026gt;#{@caption}\u0026lt;/figcaption\u0026gt;\u0026quot; if @caption\rsource += \u0026quot;\u0026lt;/figure\u0026gt;\u0026quot;\rsource\rend\rend\rend\rLiquid::Template.register_tag('image', Jekyll::ImageTag)\ris written as this Hugo shortcode:\n\u0026lt;!-- image --\u0026gt;\r\u0026lt;figure {{ with .Get \u0026quot;class\u0026quot; }}class=\u0026quot;{{.}}\u0026quot;{{ end }}\u0026gt;\r{{ with .Get \u0026quot;link\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt;{{ end }}\r\u0026lt;img src=\u0026quot;{{ .Get \u0026quot;src\u0026quot; }}\u0026quot; {{ if or (.Get \u0026quot;alt\u0026quot;) (.Get \u0026quot;caption\u0026quot;) }}alt=\u0026quot;{{ with .Get \u0026quot;alt\u0026quot;}}{{.}}{{else}}{{ .Get \u0026quot;caption\u0026quot; }}{{ end }}\u0026quot;{{ end }} /\u0026gt;\r{{ if .Get \u0026quot;link\u0026quot;}}\u0026lt;/a\u0026gt;{{ end }}\r{{ if or (or (.Get \u0026quot;title\u0026quot;) (.Get \u0026quot;caption\u0026quot;)) (.Get \u0026quot;attr\u0026quot;)}}\r\u0026lt;figcaption\u0026gt;{{ if isset .Params \u0026quot;title\u0026quot; }}\r{{ .Get \u0026quot;title\u0026quot; }}{{ end }}\r{{ if or (.Get \u0026quot;caption\u0026quot;) (.Get \u0026quot;attr\u0026quot;)}}\u0026lt;p\u0026gt;\r{{ .Get \u0026quot;caption\u0026quot; }}\r{{ with .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt; {{ end }}\r{{ .Get \u0026quot;attr\u0026quot; }}\r{{ if .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;/a\u0026gt; {{ end }}\r\u0026lt;/p\u0026gt; {{ end }}\r\u0026lt;/figcaption\u0026gt;\r{{ end }}\r\u0026lt;/figure\u0026gt;\r\u0026lt;!-- image --\u0026gt;\rUsage\r#\rI simply changed:\n{% image full http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg \u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were \u0026quot;having fun\u0026quot; and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; -\u0026gt;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/ %}\rto this (this example uses a slightly extended version named fig, different than the built-in figure):\n{{% fig class=\u0026quot;full\u0026quot; src=\u0026quot;http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg\u0026quot; title=\u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were having fun and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; link=\u0026quot;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/\u0026quot; %}}\rAs a bonus, the shortcode named parameters are, arguably, more readable.\nFinishing touches\r#\rFix content\r#\rDepending on the amount of customization that was done with each post with Jekyll, this step will require more or less effort. There are no hard and fast rules here except that hugo server --watch is your friend. Test your changes and fix errors as needed.\nClean up\r#\rYou\u0026rsquo;ll want to remove the Jekyll configuration at this point. If you have anything else that isn\u0026rsquo;t used, delete it.\nA practical example in a diff\r#\rHey, it\u0026rsquo;s Alex was migrated in less than a father-with-kids day from Jekyll to Hugo. You can see all the changes (and screw-ups) by looking at this diff.\n"},{"id":9,"href":"/docs/Mathematics/AI/","title":"AI","section":"Mathematics","content":"\rEvolution of AI: Foundational Papers and Milestones (Chronological)\r#\rBelow is a chronological list of influential papers that have shaped artificial intelligence – from early symbolic reasoning and neural network concepts to the rise of deep learning and large language models. Each entry includes the work’s main contribution, an influence rating, and a beginner-friendly explanation of its significance.\n1943 – McCulloch \u0026amp; Pitts: “A Logical Calculus of the Ideas Immanent in Nervous Activity”\nContribution \u0026amp; Impact: Proposed the first mathematical model of how networks of artificial neurons could represent logical computations (\rA Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia). This seminal work showed that simple on/off neurons with weighted inputs can compute any logical function, laying the groundwork for neural networks (\rA Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia).\nInfluence (1–10): 10/10\nBeginner-Friendly Explanation: McCulloch and Pitts imagined the brain as a network of simple switches (neurons) that could be either on or off. They proved a bunch of these neuron-like switches could be connected to perform logical reasoning (like an electronic circuit). This was a foundational idea: it suggested machines could think by mimicking brain networks.\n1950 – Alan Turing: “Computing Machinery and Intelligence”\nContribution \u0026amp; Impact: Introduced the famous Turing Test as a criterion for machine intelligence (\rAlan Turing\u0026rsquo;s Contributions to Artificial Intelligence : History of Information). Turing argued that instead of asking “Can machines think?”, we should ask if a machine can imitate a human so well in conversation that an evaluator cannot tell the difference. This paper framed the philosophical and practical challenge of AI for decades.\nInfluence (1–10): 10/10\nBeginner-Friendly Explanation: Turing basically said: “If you can chat with a computer and can’t tell it’s not human, then for all practical purposes, that computer is ‘thinking’.” This idea – a computer fooling a person in a conversation – became a guiding goal for AI research and popular imagination.\n1956 – Newell \u0026amp; Simon: The Logic Theorist (RAND Corporation Report \u0026amp; Dartmoor Demo)\nContribution \u0026amp; Impact: Demonstrated the first AI program deliberately engineered to mimic human problem-solving (\rNewell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information). The Logic Theorist could prove mathematical theorems from Principia Mathematica, even finding an elegant proof for one theorem that was more efficient than the original (\rNewell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information). This “heuristic search” approach showed digital computers can perform symbolic reasoning, launching the field of symbolic AI.\nInfluence (1–10): 9/10\nBeginner-Friendly Explanation: Newell and Simon built a program that solved logic puzzles (proving math theorems) in a way a person might, by searching through possible steps. It was the first time a computer did something “brainy” beyond pure calculations. This success convinced people that computers could manipulate symbols and logic to solve problems, not just crunch numbers.\n1958 – Frank Rosenblatt: The Perceptron (Psychological Review \u0026amp; Mark I Perceptron)\nContribution \u0026amp; Impact: Introduced the perceptron, a simple neural network that learns from experience. Rosenblatt’s perceptron machine was the first computer that could learn new skills by trial and error using a neural network modeled on the brain (\rRosenblatt\u0026rsquo;s Perceptron Uses a Type of Neural Network : History of Information). It learned to classify patterns (like distinguishing shapes) by adjusting connection weights based on errors. This work pioneered the field of machine learning and inspired decades of neural network research.\nInfluence (1–10): 9/10\nBeginner-Friendly Explanation: The perceptron was essentially a mechanical “student.” It would make a guess about a pattern (for example, is this a picture of a dog or a cat?), then check if it was wrong. If it was wrong, it tweaked its internal settings to do better next time. Over many trials it gradually got more accurate (\rProfessor’s perceptron paved the way for AI – 60 years too soon | Cornell Chronicle). This was the first example of a machine learning from its mistakes – a key idea in AI.\n1959 – Arthur Samuel: “Some Studies in Machine Learning Using the Game of Checkers”\nContribution \u0026amp; Impact: Demonstrated one of the first successful self-learning programs. Samuel’s checkers (draughts) program learned to improve at the game by playing against itself thousands of times. Importantly, it introduced mechanisms for a computer to learn from past games – recording positions that led to wins or losses and updating its strategy accordingly (\rThe games that helped AI evolve | IBM). Samuel even coined the term “machine learning” for this approach. In 1962, his program was able to beat a respectable human player, proving that computers can learn complex tasks without being explicitly programmed for all situations.\nInfluence (1–10): 8/10\nBeginner-Friendly Explanation: Samuel’s checkers program was like a rookie player that got better by practicing. It kept track of board positions and whether it eventually won or lost from them. Over time it favored moves that led to wins and avoided those leading to losses (\rThe games that helped AI evolve | IBM). This was revolutionary: the computer wasn’t just following a fixed strategy given by a human – it was figuring out a winning strategy by itself through experience.\n1969 – Marvin Minsky \u0026amp; Seymour Papert: Perceptrons (MIT Press book)\nContribution \u0026amp; Impact: Delivered a thorough mathematical analysis of perceptrons and famously highlighted their limitations (\rMinsky \u0026amp; Papert’s “Perceptrons” – Building Babylon). They proved that a single-layer perceptron cannot learn certain simple functions (like the XOR problem – determining if an input has an odd number of 1’s), unless it uses an exponentially large number of features. This critique (published as a book) effectively punctured the hype around neural networks at the time. It led to a significant shift in AI research focus from connectionist (neural network) methods to symbolic AI approaches in the 1970s, contributing to an “AI winter” for neural nets (\rMinsky \u0026amp; Papert’s “Perceptrons” – Building Babylon).\nInfluence (1–10): 9/10\nBeginner-Friendly Explanation: Minsky and Papert showed that the perceptron – this early learning neural network – was very limited in what it could learn. For example, it couldn’t correctly learn the simple logic of an “either/or” (XOR) condition because of its single-layer design (\rMinsky \u0026amp; Papert’s “Perceptrons” – Building Babylon). Their analysis basically said, “Neural nets are neat, but they can’t handle some basic problems unless they get much more complex.” This turned many researchers away from neural networks for years, as they focused instead on logic and rule-based AI.\n1986 – Rumelhart, Hinton \u0026amp; Williams: “Learning Representations by Back-Propagating Errors”\nContribution \u0026amp; Impact: Introduced the backpropagation algorithm for training multi-layer neural networks efficiently (though the method had been conceptually described earlier, this paper popularized it). Backpropagation provided a practical way to adjust the weights in a network with many layers by propagating the error gradient backward from the output layer (\rBackpropagation - Wikipedia). This breakthrough overcame the training difficulty of multi-layer perceptrons and sparked a resurgence of interest in neural network research in the late 1980s (\rBackpropagation - Wikipedia). In short, it enabled “deep” neural networks to actually learn internal representations from data.\nInfluence (1–10): 10/10\nBeginner-Friendly Explanation: Backpropagation is an algorithm that finally let multi-layer neural networks learn. Think of it like teaching a multi-step math solution: you check the final answer, see how wrong it is, and then send feedback backward to correct each step. Similarly, backpropagation takes the error at the output and systematically adjusts each connection in all layers to reduce that error. Once this was introduced and shown to work (\rBackpropagation - Wikipedia), researchers could train networks with several layers – giving neural nets much more brain-like ability to form complex concepts (like recognizing shapes, then objects, then scenes). This revived neural networks as a viable AI approach.\n1988 – Judea Pearl: Probabilistic Reasoning in Intelligent Systems (book)\nContribution \u0026amp; Impact: Established the field of Bayesian networks for reasoning under uncertainty. Pearl introduced a formalism where cause-and-effect relationships and uncertain knowledge could be encoded in a graphical model (a Bayesian network) and updated with probability theory. This was a paradigm shift from rule-based AI to probabilistic AI: instead of logic with strict true/false values, AI systems could handle gray areas and uncertainty in a principled way. Pearl’s 1988 book became known as the “bible” of probabilistic AI (\rProbabilistic Reasoning (1993–2011) — Making Things Think: How AI and Deep Learning Power the Products We Use) and his techniques for probabilistic inference laid the groundwork for modern AI systems that need to deal with real-world ambiguity (including everything from medical diagnosis expert systems to speech recognition).\nInfluence (1–10): 10/10\nBeginner-Friendly Explanation: Pearl taught AI how to handle uncertainty. Earlier AI often used rigid rules (e.g., “IF X then Y”), but real life is full of maybes. Pearl’s Bayesian networks let a computer draw a graph of causes and effects (say, symptoms and diseases) and then reason with probabilities – for example, “given these symptoms, there’s an 80% chance of flu.” This made AI much better at dealing with uncertain, real-world information, and it was a huge turning point that influenced everything from machine vision to natural language understanding. (\rProbabilistic Reasoning (1993–2011) — Making Things Think: How AI and Deep Learning Power the Products We Use) (\rProbabilistic Reasoning (1993–2011) — Making Things Think: How AI and Deep Learning Power the Products We Use)\n1989 – Yann LeCun et al.: “Backpropagation Applied to Handwritten Zip Code Recognition”\nContribution \u0026amp; Impact: Demonstrated the first real-world success of a deep neural network (a convolutional neural network, or CNN) trained end-to-end with backpropagation. LeCun’s CNN, later known as LeNet-5, could read handwritten digits (like postal ZIP codes) from images with high accuracy ([\nDeep Neural Nets: 33 years ago and 33 years from now (Invited Post) · The ICLR Blog Track\n](\rhttps://iclr-blog-track.github.io/2022/03/26/lecun1989/#:~:text=The%20Yann%20LeCun%20et%20al,I%20set%20out%20to%20reproduce)). It introduced the convolutional layer architecture that mimics the visual cortex, extracting features through local receptive fields and shared weights. This work was historically significant as an early proof that multi-layer neural networks can solve practical pattern-recognition problems that other methods struggled with, foreshadowing the deep learning breakthroughs decades later ([\nDeep Neural Nets: 33 years ago and 33 years from now (Invited Post) · The ICLR Blog Track\n](\rhttps://iclr-blog-track.github.io/2022/03/26/lecun1989/#:~:text=The%20Yann%20LeCun%20et%20al,I%20set%20out%20to%20reproduce)).\nInfluence (1–10): 8/10\nBeginner-Friendly Explanation: LeCun and colleagues built a neural network that could read handwritten numbers – for example, automatically recognizing zip code digits on mail. They designed special layers (now called convolutional layers) that help the network focus on small patches of an image, just like how our eyes notice local patterns. By training this multi-layer network with backpropagation, it got really good at digit recognition. This was one of the first times deep learning beat other methods on a real task, proving that these layered neural networks weren’t just academic toys but could actually see things in images and make sense of them.\n1989 – Chris Watkins: “Learning from Delayed Rewards” (PhD thesis introducing Q-Learning) Contribution \u0026amp; Impact: Introduced Q-learning, a foundational algorithm in reinforcement learning. Q-learning provided a model-free way for an agent to learn an optimal action policy by trial-and-error, even when outcomes (rewards) are delayed (\rQ-learning - Wikipedia). Watkins proved that Q-learning converges to the optimal solution given sufficient exploration. This algorithm was crucial because it showed how an AI agent can learn to make sequences of decisions in an unknown environment to maximize reward, without needing a model of the environment’s dynamics. Q-learning (and the broader reinforcement learning framework) became a major branch of AI, underpinning later successes in game-playing AI, robotics, and beyond.\nInfluence (1–10): 9/10\nBeginner-Friendly Explanation: Q-learning is like learning to play a game by playing. Imagine you’re dropped into a video game without instructions. You try moves at random and eventually figure out what gives you points or causes losses. Over time, you assign a value (“Q-value”) to each move in each situation based on how good it turned out to be. Watkins’ Q-learning algorithm formalized this idea – the computer updates its estimations of future rewards for actions as it experiments (\rQ-learning - Wikipedia). The beauty is that the AI doesn’t need to know the rules in advance; it learns what actions are best just from feedback. This became a cornerstone of how we teach AI agents to learn from experience (like teaching a robot to navigate a maze or an AI to play Atari games).\n1995 – Cortes \u0026amp; Vapnik: “Support-Vector Networks” Contribution \u0026amp; Impact: Introduced Support Vector Machines (SVMs), a new supervised learning approach based on maximizing the margin between classes. SVMs framed learning as finding the optimal separating hyperplane in a high-dimensional space – and by using the kernel trick, they could efficiently handle complex, non-linear decision boundaries by implicitly mapping data into higher dimensions (\rSupport vector machine - Wikipedia). SVMs offered strong theoretical guarantees (rooted in Vapnik’s statistical learning theory) and delivered excellent performance on many tasks in the late 1990s and 2000s. They became one of the most dominant machine learning methods before the deep learning era, widely used in image recognition, text classification, and bioinformatics.\nInfluence (1–10): 8/10\nBeginner-Friendly Explanation: SVMs were a new way to do pattern recognition with math. Think of drawing a line to separate two groups of points on a paper: an SVM finds the line (or surface in higher dimensions) that not only separates the groups, but is as far away from all points as possible – this is the maximum-margin idea (\rSupport vector machine - Wikipedia) (\rSupport vector machine - Wikipedia). If the groups aren’t linearly separable, SVMs use a clever trick (the kernel) to imagine the data in a higher-dimensional space where a separation is possible, without having to enumerate all those dimensions explicitly. The result was a very powerful and robust classifier that for years was the go-to method when you wanted high accuracy in machine learning tasks.\n1997 – Hochreiter \u0026amp; Schmidhuber: “Long Short-Term Memory” (Neural Computation) Contribution \u0026amp; Impact: Developed the Long Short-Term Memory (LSTM) network, a type of recurrent neural network designed to overcome the vanishing gradient problem for long sequence learning. LSTM introduced gating mechanisms (input, output, and forget gates) that allow the network to maintain information over long time lags (\rLong short-term memory - Wikipedia) (\rLong short-term memory - Wikipedia). This architecture enabled RNNs to retain long-term dependencies in sequence data (e.g. remembering context from far earlier in a text or time series). LSTMs proved enormously successful in the 2000s and 2010s for tasks like speech recognition, language modeling, and translation – essentially any task involving sequential data – and were a key component in state-of-the-art models until the transformer era.\nInfluence (1–10): 9/10\nBeginner-Friendly Explanation: LSTMs made neural networks much better at remembering. Standard RNNs have short memories – they tend to “forget” things quickly as new inputs come in, partly because the error signal fades over time steps. LSTM units added gates that learn what to keep, what to throw away, and what to output (\rLong short-term memory - Wikipedia) (\rLong short-term memory - Wikipedia). For example, when processing a sentence, an LSTM can learn to carry forward the subject of the sentence so that many words later it still knows who or what the discussion is about. This was a big deal – it meant AI could understand sequences (like text, speech, or music) with far better context and consistency.\n2006 – Hinton et al.: “A Fast Learning Algorithm for Deep Belief Nets” (Science) Contribution \u0026amp; Impact: Revived deep neural networks by introducing an effective training strategy using Deep Belief Networks (DBNs). Hinton showed that a deep multi-layer network could be trained by greedily training one layer at a time in an unsupervised fashion (using Restricted Boltzmann Machines), then fine-tuning with supervised learning. This breakthrough in 2006 was the first to successfully train networks with many layers, overcoming previous optimization difficulties (\r[PDF] Why does Unsupervised Pre-training Help Deep Learning?). It proved that unsupervised pre-training could initialize deep networks in a good state, leading to much better results and reigniting research into “deep learning.” This work directly influenced subsequent deep architectures and is seen as a turning point that led to the deep learning boom.\nInfluence (1–10): 8/10\nBeginner-Friendly Explanation: By the 2000s, neural nets had mostly one or two hidden layers because training more was too hard – the signal just wouldn’t propagate well. Hinton’s team figured out a clever solution: train one layer at a time in a sort of self-supervised way (each layer learns to encode the data in a compressed form), then stack them up. With this layer-by-layer pre-training, they could finally train really deep networks (many layers) without things falling apart (\r[PDF] Why does Unsupervised Pre-training Help Deep Learning?). This showed the community that deep networks could work in practice, and it set the stage for the breakthroughs that followed (especially once big data and GPUs became available a few years later).\n2012 – Krizhevsky, Sutskever \u0026amp; Hinton: “ImageNet Classification with Deep Convolutional Neural Networks” Contribution \u0026amp; Impact: Better known as AlexNet, this paper rocked the computer vision world by winning the 2012 ImageNet challenge by a huge margin using a deep convolutional neural network (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone). AlexNet was an 8-layer CNN (with ReLU activations, dropout regularization, and GPU training) that achieved a top-5 error of 15.3% on ImageNet, while the next best approach was 26.2% – an unprecedented 10% jump in accuracy (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone). It was the first widely acknowledged, practical success of deep learning in a large-scale task (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone), marking the start of the deep learning revolution in computer vision (and soon other fields). After AlexNet, the research community rapidly pivoted to deep neural networks for vision tasks.\nInfluence (1–10): 10/10\nBeginner-Friendly Explanation: AlexNet’s victory was like an underdog team winning a championship by a landslide. In a contest of recognizing 1,000 different object types from images (ImageNet), this deep neural network crushed the traditional approaches – it was much more accurate (about 16% error vs. 26% for the best non-neural method) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone). People were stunned. This was proof that with enough data (millions of images), compute (GPUs), and a good architecture, neural networks dramatically outperformed other techniques. It instantly made deep learning the focus for anyone working on image recognition, and soon after, for speech and other areas as well.\n2014 – Goodfellow et al.: “Generative Adversarial Networks” Contribution \u0026amp; Impact: Introduced Generative Adversarial Networks (GANs), a novel framework for training generative models by pitting two neural networks against each other – a generator that tries to create fake data, and a discriminator that tries to detect fakes (\r10 AI milestones of the last 10 years | Royal Institution). This adversarial training scheme resulted in generative models that could produce remarkably realistic images (and other data) over time. GANs were a conceptual breakthrough in how to train networks to create rather than just recognize, and they spawned an entire subfield of research. They eventually led to high-fidelity image synthesis, deepfakes, and many creative AI applications.\nInfluence (1–10): 9/10\nBeginner-Friendly Explanation: A GAN is like having a counterfeiter and a detective who improve together. The counterfeiter (generator network) tries to make fake outputs – say, fake images of faces – that look real. The detective (discriminator network) looks at images and says “real” or “fake.” As they train, the counterfeiter gets better at fooling the detective, and the detective gets better at spotting fakes (\r10 AI milestones of the last 10 years | Royal Institution). Eventually, the fake outputs become so realistic that even humans can’t easily tell they’re generated by a computer. This idea of two AIs dueling with each other turned out to be a powerful way for machines to imagine and create realistic data.\n2015 – Mnih et al.: “Human-Level Control Through Deep Reinforcement Learning” Contribution \u0026amp; Impact: Demonstrated the power of deep reinforcement learning by introducing the Deep Q-Network (DQN) agent. This system combined Q-learning with a deep convolutional neural network, enabling an AI to learn to play Atari 2600 video games directly from raw pixel inputs (\rHuman-level control through deep reinforcement learning | Nature). Strikingly, the same DQN algorithm achieved human-level performance (or better) on dozens of Atari games – using only the game screen pixels and score as input – with no game-specific tweaks (\rHuman-level control through deep reinforcement learning | Nature). This was the first time a single AI agent learned a broad range of complex tasks end-to-end, bridging the gap between sensory perception and decision-making. It reinvigorated research in reinforcement learning and underscored the potential of combining deep learning with trial-and-error learning.\nInfluence (1–10): 10/10\nBeginner-Friendly Explanation: DeepMind’s DQN agent was a breakthrough in game-playing AI. They took a neural network and trained it via reinforcement learning to play classic video games like Breakout, Pac-Man, and Space Invaders. The input was just the pixels on the screen and the game score, and it had to figure out what to do from that alone. Amazingly, after training, the AI could play many of these games as well as or better than a human – using the same one algorithm for all (\rHuman-level control through deep reinforcement learning | Nature). For example, in Breakout it learned the clever strategy of tunneling around the bricks (a trick human experts use) without ever being told. This result showed that an AI can start from raw perception (seeing the screen) and learn intelligent control (playing the game) purely by trial and error, which was a big step toward more general learning systems.\n2016 – Silver et al.: “Mastering the Game of Go with Deep Neural Networks and Tree Search” (AlphaGo) Contribution \u0026amp; Impact: Achieved what was previously thought to be at least a decade away: a computer program defeating a top human professional in the game of Go (\rMastering the game of Go with deep neural networks and tree search | Nature). The AlphaGo system did this by combining deep policy networks (to choose moves) and value networks (to evaluate board positions) with Monte Carlo Tree Search. It learned first from expert human games and then via millions of games of self-play, refining its skills. In the published Nature paper, AlphaGo achieved a 99.8% win rate against other Go programs and beat the European Go champion 5–0 (\rMastering the game of Go with deep neural networks and tree search | Nature). This was a watershed moment for AI, showcasing the synthesis of deep learning and advanced search to conquer one of the most complex board games. It suggested that similar techniques could tackle other problems of high complexity.\nInfluence (1–10): 10/10\nBeginner-Friendly Explanation: For decades, Go was the ultimate challenge for AI – far more complex than chess. AlphaGo’s design had two brains: one that suggested likely good moves (a policy neural network) and one that judged board positions (a value neural network) (\rMastering the game of Go with deep neural networks and tree search | Nature). It also simulated future move sequences (tree search) but in a smarter way guided by those neural nets. The result was an AI that plays Go brilliantly. It first learned by studying human games, then got even better by playing against itself millions of times, each time learning from its mistakes. In 2016 it shocked the world by beating one of the best human Go players. This victory was about more than Go – it meant AI could tackle extremely complex, subtle problems that were once thought to require human intuition.\n2017 – Vaswani et al.: “Attention Is All You Need” Contribution \u0026amp; Impact: Introduced the Transformer architecture, built entirely on self-attention mechanisms and devoid of recurrence or convolution. This paper showed that attention mechanisms alone can capture relationships in sequential input (like words in a sentence) more efficiently and with better parallelization than previous RNN/CNN approaches. The transformer architecture led to dramatic improvements in machine translation and natural language processing. It is the direct precursor to today’s large language models. Indeed, this work provided the technological foundation for LLMs, as transformers can read entire sequences and learn contextual dependencies with ease (\r10 AI milestones of the last 10 years | Royal Institution). Subsequent models like BERT and GPT are built on the transformer, validating the paper’s title that “attention is all you need.”\nInfluence (1–10): 10/10\nBeginner-Friendly Explanation: The Transformer architecture threw out the old playbook for processing sequences. Instead of reading words one-by-one in order (like an RNN) or focusing only on a fixed-size window (like a CNN), a Transformer looks at all the words at once and learns which words to pay attention to in order to understand the meaning (\r10 AI milestones of the last 10 years | Royal Institution). For example, to translate a sentence or answer a question, it can see how each word relates to every other word (using a mechanism called self-attention). This was revolutionary because it made language models much better at capturing context (who did what to whom, etc.) and it could be massively parallelized (so it could train on huge datasets). Nearly all modern large language models (like GPT-3, BERT) are based on this Transformer design, which shows how impactful this paper was.\n2018 – Devlin et al.: “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” Contribution \u0026amp; Impact: Introduced BERT (Bidirectional Encoder Representations from Transformers), which demonstrated the power of pre-training a large transformer on unsupervised language tasks and then fine-tuning it for specific NLP tasks. BERT’s bidirectional training (masked language modeling and next-sentence prediction objectives) produced deep language representations that achieved state-of-the-art results on a wide array of NLP benchmarks (\r[PDF] BERT: Pre-training of Deep Bidirectional Transformers for Language \u0026hellip;). This paper popularized the paradigm of pre-train then fine-tune in NLP. After BERT, large pre-trained language models became the norm, fundamentally changing NLP research and leading to an explosion of models that improved on various tasks via fine-tuning rather than task-specific architectures from scratch.\nInfluence (1–10): 9/10\nBeginner-Friendly Explanation: BERT showed that an AI can learn a lot about language just by reading billions of words of text, without any specific task in mind. BERT is a huge Transformer network trained in a clever way: it hides some words in a sentence and forces itself to guess them (that’s masked language modeling). In doing so, it learns rich knowledge about syntax, semantics, and general language facts. Once trained on this “fill-in-the-blanks” task across Wikipedia and books, BERT can be quickly taught to solve all sorts of language problems (question answering, sentiment analysis, etc.) by fine-tuning on a small task-specific dataset (\r[PDF] BERT: Pre-training of Deep Bidirectional Transformers for Language \u0026hellip;). For NLP, this was a game-changer – instead of training separate models from scratch for each little task, we now pre-train one giant model that knows a lot, and then just tweak it for the task at hand.\n2018 – Radford et al.: “Improving Language Understanding by Generative Pre-Training” (GPT-1) Contribution \u0026amp; Impact: This OpenAI work introduced the first Generative Pre-Trained Transformer (GPT) model. GPT-1 demonstrated that a transformer trained as a language model (predicting the next word on a massive corpus of books) could be fine-tuned to perform downstream tasks like question answering with excellent results – even though it was not trained on those tasks directly. It established the effectiveness of unsupervised pre-training for transformers (using a left-to-right generative objective) and showed the versatility of the resulting representations. GPT-1 was able to generate coherent text and answer questions in a fluent way after pre-training on ~7000 unpublished books and then fine-tuning (\r10 AI milestones of the last 10 years | Royal Institution). While smaller in scale by today’s standards, GPT-1 laid the groundwork for the GPT series and the concept of large language models.\nInfluence (1–10): 7/10\nBeginner-Friendly Explanation: GPT-1 was the first step toward modern chatbots and LLMs. The idea was simple but powerful: train a Transformer to predict the next word in a sentence by feeding it a ton of books. By doing this, the model learns grammar, facts, and some reasoning just from the text. Then the researchers showed that you could take this generatively pre-trained model and fine-tune it on specific tasks (like having it answer questions or analyze sentiment) and it worked really well (\r10 AI milestones of the last 10 years | Royal Institution). In essence, GPT-1 was like teaching a student by letting it read an entire library (with the goal of guessing missing words), and then giving it a short internship for a specific job – and it turned out the student had learned enough from reading to do the job impressively well.\n2020 – Brown et al.: “Language Models are Few-Shot Learners” (GPT-3) Contribution \u0026amp; Impact: Introduced GPT-3, a 175-billion-parameter transformer that demonstrated astonishing capabilities in natural language generation and few-shot learning. GPT-3 showed that simply by scaling up model size and training on virtually all of the internet’s text, a language model could perform a wide range of tasks without explicit training for each one – given just a few examples in its prompt (a phenomenon called in-context learning). It achieved strong performance on translation, Q\u0026amp;A, and more by prompt alone (\r[2005.14165] Language Models are Few-Shot Learners - arXiv). GPT-3’s release dazzled the world with its ability to generate human-like essays, code, and dialogues, revealing emergent properties of language models at scale. This paper marked the point where “large language model” entered the mainstream vocabulary and led to widespread deployment of LLMs in applications.\nInfluence (1–10): 10/10\nBeginner-Friendly Explanation: GPT-3 was a quantum leap in scale and ability for AI language models. The OpenAI team scaled up the number of neurons (parameters) in the model to 175 billion (hundreds of times larger than prior models) and trained it on text from the internet and books. The surprising finding was that GPT-3 can learn to do a task from just a few examples given in the prompt, without any further training – for instance, give it two English-to-French translation examples in the prompt, and it can translate a new English sentence to French by analogy (\r[2005.14165] Language Models are Few-Shot Learners - arXiv). This “few-shot” ability felt almost like instant learning. Moreover, GPT-3 could generate paragraphs of coherent, often insightful text on almost any topic you prompted it with. It was the first AI that really felt like a general-purpose language generator, and it set the stage for the chatbots and creative AI that followed.\n2020 – Jumper et al.: “High Accuracy Protein Structure Prediction Using Deep Learning (AlphaFold)” Contribution \u0026amp; Impact: Solved a 50-year grand challenge in biology – the protein folding problem – using deep learning. AlphaFold2 (described in a 2020 Nature paper and CASP competition results) employed an attention-based neural network (in fact, a modified transformer) to predict 3D protein structures from amino acid sequences with atomic-level accuracy (\r10 AI milestones of the last 10 years | Royal Institution). It trained on the sequences and known structures in public databases and dramatically outperformed all prior methods, achieving accuracies comparable to experimental laboratory techniques. This was a milestone not just for AI but for science, showing that AI can make major contributions to scientific problems. AlphaFold’s success has accelerated research in drug discovery and biology, and it demonstrated the versatility of deep learning beyond traditional “AI tasks.”\nInfluence (1–10): 10/10\nBeginner-Friendly Explanation: AlphaFold was like an “AI biochemist” that figured out how proteins fold into their 3D shapes. Proteins are molecular machines in our bodies, and their function depends on their shape. Determining that shape used to take scientists years in the lab, but AlphaFold learned to predict shapes in hours with incredible accuracy (\r10 AI milestones of the last 10 years | Royal Institution). How? It looked at tens of thousands of known protein shapes and learned patterns. Using a transformer neural network, it could then take a new protein’s sequence (its amino acid recipe) and compute its likely folded structure. This achievement was considered a major scientific breakthrough – something many experts thought AI wouldn’t crack for a long time. It showed AI can not only play games or chat, but also solve hard scientific puzzles, potentially leading to new medicines and understandings of biology.\n2022 – Ouyang et al.: “Training Language Models to Follow Instructions with Human Feedback” (InstructGPT) Contribution \u0026amp; Impact: Demonstrated a successful approach to AI alignment by fine-tuning a large language model using human feedback. The paper introduced InstructGPT, a version of GPT-3 fine-tuned with Reinforcement Learning from Human Feedback (RLHF). Human evaluators ranked outputs, and those rankings were used to train a reward model which then guided the policy optimization. The result was an LLM that followed user instructions much more reliably and produced outputs that were more truthful and less toxic than the base GPT-3 (\rTraining language models to follow instructions with human feedback). InstructGPT showed that large models can be steered toward helpful behavior, and its techniques directly underlie OpenAI’s ChatGPT. This was a key step in making LLMs practically useful and safer for wide deployment.\nInfluence (1–10): 9/10\nBeginner-Friendly Explanation: InstructGPT took a big language model and basically gave it a manners class with human teachers. Instead of just predicting the next word in general, it was trained to follow instructions. Researchers accomplished this by showing the model many examples of prompts and ideal answers (crafted or rated by humans), and even having humans rank different AI responses to the same prompt. The model was then tweaked (using a reinforcement learning process) to prefer responses that humans liked (\rTraining language models to follow instructions with human feedback). The outcome: compared to the original GPT-3, InstructGPT is much better at doing what you ask – if you prompt it to be polite and to avoid certain topics, it will, and it makes fewer factual errors. This approach of using human feedback became crucial for turning big general models into helpful assistants (it’s essentially how ChatGPT was trained).\n2023 – OpenAI: “GPT-4 Technical Report” Contribution \u0026amp; Impact: Described GPT-4, a large-scale multimodal model that represents the state-of-the-art in 2023. GPT-4 is multimodal, accepting both image and text inputs and generating text outputs (\r[2303.08774] GPT-4 Technical Report - arXiv). It further improved the capability and alignment of LLMs – exhibiting more advanced reasoning, fewer mistakes, and the ability to handle much more complex instructions than its predecessors. While many details (like model size) remain unpublished, GPT-4’s impact has been to push the envelope of what AI systems can do (such as passing standardized tests, coding large programs, or analyzing images in detail) and to highlight new challenges (like hallucination reduction and transparency). GPT-4’s release solidified that LLMs are here to stay, being integrated into products and society at large, and it raised the urgency of addressing AI’s open problems.\nInfluence (1–10): 9/10\nBeginner-Friendly Explanation: GPT-4 is currently the most advanced iteration of the GPT series. One big new feature is that it can accept images as part of the prompt – so you can show it a picture and ask questions about it, and it will respond (for example, describing an image or interpreting a meme) (\r[2303.08774] GPT-4 Technical Report - arXiv). It’s also significantly smarter in many ways: it can handle much longer essays, it’s harder to trick into giving harmful answers, and it scores impressively on exams in math, law, medicine, etc. People have used GPT-4 to draft legal documents, create apps from scratch, and tutor themselves on complex topics. In short, GPT-4 pushed the boundary of AI capabilities forward, while also making it clear that we need to deal with issues like the model sometimes being confidently wrong (hallucinations) or the need for even better alignment with human values.\nOpen Problems and Future Directions\r#\rDespite these milestones and the immense progress in AI, several fundamental challenges remain unsolved:\nExplainability: Many advanced AI models (especially deep neural networks) operate as “black boxes,” making it hard to understand or trust their decisions (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Developing AI that can explain why it made a certain decision is an ongoing area of research.\nGeneralization and Robustness: AI systems can be brittle – a model trained on certain data may fail in unexpected ways when faced with novel situations or adversarial inputs (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Achieving human-like adaptability and reliability in the open world (not just controlled settings) is still an open problem.\nBias and Fairness: Models often inherit biases present in their training data, which can lead to unfair or incorrect outcomes, especially in sensitive applications (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Eliminating harmful biases and ensuring AI systems treat different groups fairly is crucial and challenging.\nCommon-Sense Reasoning: AI still notably lacks common sense. Tasks that are trivial for humans – understanding unstated assumptions, basic physical or social logic – can stump AI systems (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Building AI with a robust common-sense understanding of the world is an important unsolved challenge.\nSafety and Alignment: As AI systems become more powerful, ensuring they align with human values and intent is paramount (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). This involves preventing unintended harmful behavior, controlling misinterpretations of instructions, and, in the long term, making sure AI goals remain tethered to what humans actually want. Techniques like the RLHF used in InstructGPT are initial steps, but deeper solutions in AI safety, robustness, and governance are actively being sought.\nIn summary, the evolution of AI has been driven by a series of conceptual and technical breakthroughs – from the theoretical underpinnings of symbolic reasoning and neural networks to the empirical triumphs of deep learning and large language models. Each foundational paper above either opened a new avenue of research or dramatically accelerated progress in an existing one. A reader with a strong analytical background can appreciate how each innovation addressed a core limitation of the previous generation: giving machines the ability to reason with logic, to learn from data, to handle uncertainty, to perceive patterns, to remember sequences, to leverage big data and compute, and to align with human goals. While many “impossible” feats have now been achieved (playing Go, folding proteins, human-level dialogue), AI is not a solved problem – far from it. The community is now wrestling with making AI more understandable, general, fair, and safe. These open problems will define the next chapter of AI research, as we move from simply building powerful systems to ensuring those systems behave intelligently and beneficially in the messy, nuanced world we live in.\nSources: The descriptions above draw from the original papers and subsequent analyses, including historical accounts and summaries from Wikipedia and other secondary sources. Key references include McCulloch \u0026amp; Pitts (1943) (\rA Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia) (\rA Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia), Turing (1950) (\rAlan Turing\u0026rsquo;s Contributions to Artificial Intelligence : History of Information), Newell \u0026amp; Simon (1956) (\rNewell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information) (\rNewell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information), Rosenblatt’s perceptron (1958) (\rRosenblatt\u0026rsquo;s Perceptron Uses a Type of Neural Network : History of Information) (\rProfessor’s perceptron paved the way for AI – 60 years too soon | Cornell Chronicle), Samuel’s checkers (1959) (\rThe games that helped AI evolve | IBM), Minsky \u0026amp; Papert (1969) (\rMinsky \u0026amp; Papert’s “Perceptrons” – Building Babylon) (\rMinsky \u0026amp; Papert’s “Perceptrons” – Building Babylon), backpropagation (1986) (\rBackpropagation - Wikipedia), Pearl’s Bayesian networks (1988) (\rProbabilistic Reasoning (1993–2011) — Making Things Think: How AI and Deep Learning Power the Products We Use) (\rProbabilistic Reasoning (1993–2011) — Making Things Think: How AI and Deep Learning Power the Products We Use), LeCun’s CNN (1989) ([\nDeep Neural Nets: 33 years ago and 33 years from now (Invited Post) · The ICLR Blog Track\n](\rhttps://iclr-blog-track.github.io/2022/03/26/lecun1989/#:~:text=The%20Yann%20LeCun%20et%20al,I%20set%20out%20to%20reproduce)), Watkins’ Q-learning (1989) (\rQ-learning - Wikipedia), Cortes \u0026amp; Vapnik (1995) on SVMs (\rSupport vector machine - Wikipedia), Hochreiter \u0026amp; Schmidhuber (1997) on LSTM (\rLong short-term memory - Wikipedia), Hinton’s deep belief nets (2006) (\r[PDF] Why does Unsupervised Pre-training Help Deep Learning?), AlexNet (2012) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone), GANs (2014) (\r10 AI milestones of the last 10 years | Royal Institution), DeepMind’s DQN (2015) (\rHuman-level control through deep reinforcement learning | Nature), AlphaGo (2016) (\rMastering the game of Go with deep neural networks and tree search | Nature), the Transformer (2017) (\r10 AI milestones of the last 10 years | Royal Institution), BERT (2018) (\r[PDF] BERT: Pre-training of Deep Bidirectional Transformers for Language \u0026hellip;), GPT-1 (2018) (\r10 AI milestones of the last 10 years | Royal Institution), GPT-3 (2020) (\r[2005.14165] Language Models are Few-Shot Learners - arXiv), AlphaFold (2020) (\r10 AI milestones of the last 10 years | Royal Institution), InstructGPT (2022) (\rTraining language models to follow instructions with human feedback), and GPT-4 (2023) (\r[2303.08774] GPT-4 Technical Report - arXiv), as well as discussions on AI’s remaining challenges (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium) (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). These works collectively chart the conceptual history of AI’s evolution and the road ahead.\n"},{"id":10,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.5-Lebesgue-Convergence-Theorem/","title":"8.5 Lebesgue Convergence Theorem","section":"第八章 度量理论","content":"\rMain Question\r#\rWhen do we have: $$\\lim_{n\\to\\infty}\\int_A f_n(x)dx = \\int_A(\\lim_{n\\to\\infty}f_n(x))dx?$$\nLebesgue Monotone Convergence Theorem (Theorem 8.6.1)\r#\rLet $g_n: [0,1] \\to \\mathbb{R}$ be a sequence of non-negative measurable functions such that:\n$g_{n+1}(x) \\leq g_n(x)$ $\\forall x$ (decreasing sequence) $\\lim_{n\\to\\infty} g_n(x) = 0$ $\\forall x \\in [0,1]$ Then: $$\\lim_{n\\to\\infty}\\int_0^1 g_n(x)dx = \\int_0^1 0 dx = 0$$\nProof Details\r#\rFor LMCT to hold, we only need:\n$f_n(x) \\leq f_{n+1}(x) \\leq f(x)$ $\\forall x$ $f_n(x) \\to f(x)$, $\\forall x$ This implies: $f_n(x) \\uparrow f(x)$\nThe assumption $A \\subset [0,1] \\subset \\mathbb{R}$ is not essential. Result is true for any set $A \\subset \\mathbb{R}^n$.\nThe Monotonicity Assumption Cannot Be Removed\r#\rExample\r#\r$g_n(x) = \\begin{cases} n, \u0026amp; 0 \u0026lt; x \u0026lt; \\frac{1}{n} \\ 0, \u0026amp; \\text{else} \\end{cases}$\nNote that:\n$g_n(x) \\to 0$ $\\forall x \\in [0,1]$ $\\int_0^1 g_n(x)dx = 1$ $\\forall n$ Therefore $\\lim_{n\\to\\infty}\\int_0^1 g_n(x)dx = 1 \\neq 0 = \\int_0^1 0 dx$\n2nd Part of LMCT\r#\rLemma\r#\rSuppose $f: [0,1] \\to \\mathbb{R}$ is measurable with $|f| \\leq M$ and $\\int_0^1 f \\geq \\alpha \u0026gt; 0$. Then the set: $E = {x \\in [0,1]: f(x) \\geq \\frac{\\alpha}{2}}$ contains a finite union of disjoint open intervals of total length $\\geq \\frac{\\alpha}{4M}$\nResult\r#\r$0 \\leq \\int_0^1 f - L(f,P) \\leq \\frac{\\alpha}{4}$\nWhere $L$ denotes the total length of intervals $I \\in P$ with $I \\subset E$.\nThen: $$\\frac{3\\alpha}{4} \u0026lt; L(f, P) = \\sum_{I \\in P} \\left(\\inf_{I}(f(x))\\right)\\ell(I)$$\n$$= \\sum_{I \\subset E} + \\sum_{I \\not\\subset E} \\left(\\inf_{I}(f(x))\\right)\\ell(I)$$\n$$\\leq \\sum_{I \\subset E} M\\ell(I) + \\sum_{I} \\frac{\\alpha}{2}\\ell(I)$$\n$$\\leq \\ell M + \\frac{\\alpha}{2} \\cdot 1$$\nSince $\\ell M \u0026gt; \\frac{3\\alpha}{4} - \\frac{\\alpha}{2} = \\frac{\\alpha}{4}$, we can conclude:\n$$\\ell \u0026gt; \\frac{\\alpha}{4M}$$\nThis completes the proof of the lemma, demonstrating that the set $E$ contains intervals with sufficient total length, which is a key component in establishing the Lebesgue Monotone Convergence Theorem.\nThe boxed result at the end confirms that $\\ell \u0026gt; \\frac{\\alpha}{4M}$, which validates our earlier assertion about the minimum total length of the intervals in set $E$.\nProof of Theorem 8.6.1 (LMCT)\r#\rStep 1: Setup\r#\rGiven:\n$0 \\leq g_{n+1} \\leq g_n$ implies $\\int_0^1 g_{n+1}(x) \\leq \\int_0^1 g_n(x)$ This leads to the limit: $\\lim_{n\\to\\infty}\\int_0^1 g_n(x) = \\lambda \\geq 0$ We need to show that $\\lambda = 0$.\nAssuming $\\lambda \u0026gt; 0$ will lead to a contradiction (using our assumption that $g_n(x) \\to 0$ $\\forall x \\in [0,1]$).\nStep 2: Application of Lemma\r#\rWe apply the previously established lemma to the cut-off function:\n$$(g_n)_M = \\begin{cases} g_n(x), \u0026amp; g_n(x) \\leq M \\ M, \u0026amp; g_n(x) \u0026gt; M \\end{cases}$$\nThis implies:\n$$\\int_0^1 g_n(x) dx = \\lim_{M \\to \\infty} \\int_0^1 (g_n)M$$ 选择 $m\u0026gt; \\frac{2\\lambda}{5}$ s.t. $$0 \\leq \\int^{1}{0}(g_{n}-(g_{n}){M})$\\leq \\int^{1}{0}(g_{1}-(g_{1}){M})\\leq \\frac{\\lambda}{5}$$ 我们让$E{n}=\\left{ x\\in[1,0]:g_{n}(x)\\geq \\frac{2\\lambda}{5} \\right}$，然后\n$E_{n+1}\\subset E_{n}$ ${x \\in [0,1] : (g_n)_M(x) \\geq \\frac{\\alpha}{2}} \\subset E_n$ where we choose $\\alpha$ such that $\\frac{2\\lambda}{5} = \\frac{\\alpha}{2}$, giving us $\\alpha = \\frac{4\\lambda}{5}$ Key Step:\r#\rApplying the lemma to $(g_n)_M$ with $\\alpha = \\frac{4\\lambda}{5}$: this implies $E_n$ contains a finite union of disjoint open intervals of total length: $$\\ell \\geq \\frac{\\alpha}{4M} = \\frac{\\lambda}{5M}$$\nStep 3:\r#\rShow that $\\bigcap_{n=1}^{\\infty} E_n = \\emptyset$\nLet $D = \\bigcap_{n=1}^{\\infty} {x \\in [0,1] : g_n \\text{ not converging to } 0} = \\bigcap_{n=1}^{\\infty} D_n$\nThen $g_{n}$ intagrable $\\Rightarrow m(D_n) = 0 = m(D) = 0$.\nThus $D$ is covered by $U$, a countable union of open intervals of total length $\u0026lt; \\frac{\\lambda}{5M}$.\nBy Step 2,\n$E_n \\subset U$\n"},{"id":11,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.6-Morse-Lemma/","title":"7.6 莫尔斯引理","section":"第七章 逆函数和隐函数定理","content":"\rMorse Theory: Local Behavior Near a Critical Point\r#\rLet $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$ be of class $C^2$ and $x_0$ is a critical point $\\rightarrow$ one can use $H(x_0)$ to classify critical points.\nMorse theory: makes this classification more precise.\nMorse Lemma\r#\r[!lemma|*] Let $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$ be of class $C^2$ with critical point $x_0 \\in A$. If $H(x_0)$ is non-degenerate, then there exists a neighborhood of $x_0$ and a diffeomorphism $g$ such that the function $f \\circ g$ has the form: $$f \\circ g(y) = f(x_0) + \\sum_{i=1}^{\\lambda} -y_i^2 + \\sum_{i=\\lambda+1}^n y_i^2$$ where $\\lambda$ is an integer called the index of $f$ at $x_0$.\nApplications\r#\r$\\lambda = 0$: $x_0$ is local minimum (paraboloid opens upward) $\\lambda = n$: $x_0$ is local maximum (paraboloid opens downward) $0 \u0026lt; \\lambda \u0026lt; n$: $x_0$ is saddle point (hyperboloid) Idea: Diagonalization of Hessian Matrix\r#\r$H(f) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} \u0026amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \u0026amp; \\cdots \\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} \u0026amp; \\frac{\\partial^2 f}{\\partial x_2^2} \u0026amp; \\cdots \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \\end{pmatrix}$\n$\\lambda$ = # of negative eigenvalues of $H(f)(x_0)$\nExample: Determine the shape of the surface $z = f(x,y)$ near $(0,0)$\r#\rSolution: At $(0,0)$, $f_x = 0$, $f_y = 0$\nCompute eigenvalues of the Hessian matrix. If there is one negative eigenvalue, $\\lambda = 1$, it\u0026rsquo;s a saddle point (hyperboloid).\nConstrained Extremal Problem\r#\rGoal: To maximize or minimize a function $f(x): \\mathbb{R}^n \\to \\mathbb{R}$ under the condition $g(x) = c$.\nLagrange Multiplier Method\r#\rA Necessary Condition\r#\rTheorem: Let $f, g: U \\subset \\mathbb{R}^n \\to \\mathbb{R}$ be of class $C^1$. Assume $g(x_0) = c_0$ with $\\nabla g(x_0) \\neq 0$. If $f$ restricted to the surface $S = {g(x) = c_0}$ has a max or min at $x_0$, then there exists a real number $\\lambda$ such that:\n$$\\nabla f(x_0) = \\lambda \\nabla g(x_0)$$\nGeometric Meaning\r#\r$\\nabla f(x_0)$ points in the direction of steepest ascent At the extremum, this direction is perpendicular to the surface $S$ Geometric Proof\r#\rLet $c(t)$ be a fixed curve on $S = {g(x) = c_0}$ passing through $x_0$ at $t = t_0$. If $f$ restricted to $S$ has a max at $x_0$, then $f(c(t))$ has max at $t_0$.\nTherefore: $\\frac{d}{dt}f(c(t))|_{t=t_0} = 0$\nBy chain rule: $\\nabla f(c(t_0)) \\cdot c\u0026rsquo;(t_0) = 0$\nSince $c\u0026rsquo;(t_0)$ is tangent to $S$ at $x_0$, $\\nabla f(x_0)$ must be perpendicular to the tangent space of $S$ at $x_0$. Therefore, it must be parallel to $\\nabla g(x_0)$, which is normal to the surface.\nProcedure to Solve Extremal Problem\r#\rStep 1: Solve the system of equations $$\\nabla f(x) = \\lambda \\nabla g(x)$$ $$g(x) = c$$\nThis gives $n+1$ equations with $n+1$ variables $(x_1, x_2, \u0026hellip;, x_n, \\lambda)$\nStep 2: Compute values of $f$ at these critical points and determine which are maxima, minima, or saddle points\nExample: Find the extrema of\r#\r$$f(x,y) = x^2 y^2$$ subject to the condition $x^2 + y^2 = 1$\nSolution:\nApplying Lagrange multiplier method: $\\nabla f = (2xy^2, 2x^2y) = \\lambda \\nabla g = \\lambda(2x, 2y)$\nThis gives us:\n$xy^2 = \\lambda x$ $x^2y = \\lambda y$ Analyzing by cases:\nCase 1: If $x = 0$, then $y = \\pm 1$ (from constraint) Case 2: If $y = 0$, then $x = \\pm 1$ (from constraint) Case 3: If $x \\neq 0$ and $y \\neq 0$: From the first equation: $y^2 = \\lambda$ (dividing by $x$) From the second equation: $x^2 = \\lambda$ (dividing by $y$) This gives $x^2 = y^2$ With the constraint $x^2 + y^2 = 1$, we get $2x^2 = 1$, so $x = \\pm\\frac{1}{\\sqrt{2}}$ and $y = \\pm\\frac{1}{\\sqrt{2}}$ Critical points: $(0, \\pm 1), (\\pm 1, 0), (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}), (\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}), (-\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}), (-\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}})$\nEvaluating $f(x,y) = x^2y^2$ at these points:\n$f(0, \\pm 1) = 0$ $f(\\pm 1, 0) = 0$ $f(\\pm\\frac{1}{\\sqrt{2}}, \\pm\\frac{1}{\\sqrt{2}}) = (\\frac{1}{2})^2 = \\frac{1}{4}$ Therefore, the maximum value is $\\frac{1}{4}$ at $(\\pm\\frac{1}{\\sqrt{2}}, \\pm\\frac{1}{\\sqrt{2}})$, and the minimum value is $0$ at $(0, \\pm 1)$ and $(\\pm 1, 0)$.\nExtremal Problem with Multiple Constraints\r#\rMaximize/minimize $f(x)$ with constraints:\n$g_1(x) = c_1$ $g_2(x) = c_2$ \u0026hellip; $g_m(x) = c_m$ Procedure: Solve the system of equations\r#\r$$\\nabla f(x) = \\lambda_1 \\nabla g_1(x) + \\lambda_2 \\nabla g_2(x) + \u0026hellip; + \\lambda_m \\nabla g_m(x)$$ $$g_1(x) = c_1, g_2(x) = c_2, \u0026hellip;, g_m(x) = c_m$$\nWith $m+n$ equations and $m+n$ variables $(x_1,\u0026hellip;,x_n, \\lambda_1,\u0026hellip;,\\lambda_m)$\nAnalytical Proof of the Theorem (Lagrange Multiplier)\r#\rWe want to substitute the condition $g(x) = c_0$ into the function $f(x)$ to eliminate the constraint.\nSince $\\nabla g(x_0) \\neq 0$, we may assume without loss of generality that $\\frac{\\partial g}{\\partial x_n} \\neq 0$ at $x_0$.\nBy the implicit function theorem, the equation $g(x_1, x_2, \u0026hellip;, x_n) = c_0$ can be solved for $x_n$ in a neighborhood of $x_0$: $$x_n = h(x_1, x_2, \u0026hellip;, x_{n-1})$$\nLet $k(x_1, x_2, \u0026hellip;, x_{n-1}) = f(x_1, x_2, \u0026hellip;, x_{n-1}, h(x_1, x_2, \u0026hellip;, x_{n-1}))$\nThus, an extremum of $f$ subject to the constraint corresponds to an extremum of $k$ without constraints.\nAt an extremum of $k$, we have: $$0 = \\frac{\\partial k}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} + \\frac{\\partial f}{\\partial x_n}\\frac{\\partial h}{\\partial x_i}, \\quad i = 1, 2, \u0026hellip;, n-1$$\nSince $g(x_1, x_2, \u0026hellip;, x_{n-1}, h(x_1, x_2, \u0026hellip;, x_{n-1})) = c_0$ identically, we can differentiate with respect to $x_i$:\n$$\\frac{\\partial g}{\\partial x_i} + \\frac{\\partial g}{\\partial x_n}\\frac{\\partial h}{\\partial x_i} = 0, \\quad i = 1, 2, \u0026hellip;, n-1$$\nSolving for $\\frac{\\partial h}{\\partial x_i}$:\n$$\\frac{\\partial h}{\\partial x_i} = -\\frac{\\frac{\\partial g}{\\partial x_i}}{\\frac{\\partial g}{\\partial x_n}}, \\quad i = 1, 2, \u0026hellip;, n-1$$\nSubstituting this into our extremum condition:\n$$\\frac{\\partial f}{\\partial x_i} - \\frac{\\partial f}{\\partial x_n}\\frac{\\frac{\\partial g}{\\partial x_i}}{\\frac{\\partial g}{\\partial x_n}} = 0, \\quad i = 1, 2, \u0026hellip;, n-1$$\nRearranging:\n$$\\frac{\\partial f}{\\partial x_i}\\frac{\\partial g}{\\partial x_n} - \\frac{\\partial f}{\\partial x_n}\\frac{\\partial g}{\\partial x_i} = 0, \\quad i = 1, 2, \u0026hellip;, n-1$$\nLet $\\lambda = \\frac{\\partial f}{\\partial x_n} / \\frac{\\partial g}{\\partial x_n}$, then:\n$$\\frac{\\partial f}{\\partial x_i} = \\lambda \\frac{\\partial g}{\\partial x_i}, \\quad i = 1, 2, \u0026hellip;, n-1$$\nAnd by definition of $\\lambda$, we also have $\\frac{\\partial f}{\\partial x_n} = \\lambda \\frac{\\partial g}{\\partial x_n}$\nTherefore, in vector form: $\\nabla f(x_0) = \\lambda \\nabla g(x_0)$\n"},{"id":12,"href":"/docs/Mathematics/hidden/","title":"Hidden","section":"Mathematics","content":"\rThis page is hidden in menu\r#\rQuondam non pater est dignior ille Eurotas\r#\rLatent te facies\r#\rLorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.\nPater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor Cum honorum Latona\r#\rO fallor in sustinui iussorum equidem. Nymphae operi oris alii fronde parens dumque, in auro ait mox ingenti proxima iamdudum maius?\nreality(burnDocking(apache_nanometer),\rpad.property_data_programming.sectorBrowserPpga(dataMask, 37,\rrecycleRup));\rintellectualVaporwareUser += -5 * 4;\rtraceroute_key_upnp /= lag_optical(android.smb(thyristorTftp));\rsurge_host_golden = mca_compact_device(dual_dpi_opengl, 33,\rcommerce_add_ppc);\rif (lun_ipv) {\rverticalExtranet(1, thumbnail_ttl, 3);\rbar_graphics_jpeg(chipset - sector_xmp_beta);\r}\rFronde cetera dextrae sequens pennis voce muneris\r#\rActa cretus diem restet utque; move integer, oscula non inspirat, noctisque scelus! Nantemque in suas vobis quamvis, et labori!\nvar runtimeDiskCompiler = home - array_ad_software;\rif (internic \u0026gt; disk) {\remoticonLockCron += 37 + bps - 4;\rwan_ansi_honeypot.cardGigaflops = artificialStorageCgi;\rsimplex -= downloadAccess;\r}\rvar volumeHardeningAndroid = pixel + tftp + onProcessorUnmount;\rsector(memory(firewire + interlaced, wired)); "},{"id":13,"href":"/docs/Mathematics/MATH-351-352/Heat-Equation/Heat-Equation-Solution/","title":"Heat Equation Solution","section":"热方程","content":"\rHeat Equation Solution\r#\rScale Invariance Property\r#\r[!Theorem] Since we know that $u_t=ku_{xx}$ , then we know if $u \\in U$ for $U \\subset \\mathbb{R}^n$ solves the equation, so does $u(\\lambda x, \\lambda^2 t)$ for $\\lambda \\in \\mathbb{R}$ according to the scale invariance property.\nLet’s set that $\\bar{x}=\\lambda x, \\bar{t}=\\lambda^2 t$, then it is easy to see:\n$$ \\begin{align} u_{\\bar{t}}= \\frac{\\partial u}{\\partial \\bar{t}}\\cdot\\frac{\\partial \\bar{t}}{\\partial t} = \\frac{\\partial u}{\\partial t} \\cdot (\\lambda^2) \\ u_{\\bar{x}\\bar{x}} \u0026amp;=\\frac{\\partial}{\\partial x} \\cdot \\frac{\\partial u}{\\partial \\bar{x} } = (\\frac{\\partial}{\\partial \\bar{x}} \\cdot \\frac{\\partial \\bar{x}}{\\partial x})(\\frac{\\partial u}{\\partial \\bar{x} } \\cdot \\frac{\\partial \\bar{x}}{\\partial x})\\ \u0026amp;= \\frac{\\partial^2 u}{\\partial x^2} \\cdot (\\lambda^2) \\end{align} $$\nwhere the equation still holds regardless the choice of $\\lambda$ $(\\lambda \\neq 0)$\nThe scaling $\\frac{x^2}{t}$ or $\\frac{x}{\\sqrt{t}}$ that is invariant to the equation suggests the solution is in the form of $u(x,t)=v(\\frac{x^2}{t})$ f.s. function $v$. That is,\n$$ \\begin{align} u(x,t)=t^\\alpha v(\\frac{x}{t^\\beta}) \\end{align} $$\nwhere constants $\\alpha, \\beta$ and functions $v:\\mathbb{R}^n\\to \\mathbb{R}$ must be found. This means the solution must be invariant under the dilation scaling $\\forall \\lambda \u0026gt;0, x= \\mathbb{R}^n, t\u0026gt;0$ :\n$$ u(x,t) = \\lambda^\\alpha u(\\lambda^\\beta x,\\lambda t) $$\nSetting $\\lambda=t^{-1}$, in which $v(y):= u(y,1)$. We insert (1) into the original heat equation to solve for $v$ with our new variable $y=\\cfrac{x}{t^\\beta}$. We then take $\\beta = \\frac{1}{2}$ so that the terms involved $t$ are cancelled out - we hence derived an equation that is only in terms of $y$:\n$$ \\alpha v + \\frac{1}{2}\\cdot Dv + \\Delta v = 0 \\ \\ (k=1) $$\nDifferent textbook takes different methods to find the constant $\\alpha=-\\frac{1}{2}$ here:\nUsing conservation of heat energy in physics Guessing $v$ to be radial and introduce $v(y)=w(|y|)$ Eventually, we reached at $v(y)=Ae^{-\\frac{y^2}{4k}}$ such that\n$$ u(x,t)=A\\frac{1}{ t^{n/2}}e^{-\\frac{|x|^2}{4kt}} $$\nThe particular choice of normalizing constant $A=\\frac{1}{(4\\pi k)^{n/2}}$ is derived from $\\int_{\\mathbb{R}^n} \\Phi(x,t) dx=1$. (See p. 46 Lemma, Evans) Hence, the general solution to the heat equation for $n$ dimension is\n$$ \\Phi(x,t)=\\frac{1}{(4\\pi k t)^{n/2}} e^{-\\frac{|x|^2}{4kt}} $$\nwhere $x\\in \\mathbb{R}^n, t\u0026gt;0$. For situation $t\u0026lt;0$, the solution is $\\Phi=0$.\nFor dimension $n=1$, we yield $$ \\Phi(x,t)=\\frac{1}{\\sqrt{4\\pi k t}} e^{-\\frac{x^2}{4kt}} $$ $$ $$\n"},{"id":14,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-6/","title":"Homework 6","section":"Real Analysis II","content":"7.2: 1, 2, [3,4]; Chapter 7: [4], 5, [6], 9, [12].\nProblem 7.2.3 In the system $$\\begin{array}{r} 3 x+2 y+z^2+u+v^2=0 \\ 4 x+3 y+z+u^2+v+w+2=0 \\ x+z+w+u^2+2=0 \\end{array} $$ discuss the solvability for $u, v, w$ in terms of $x, y, z$ near $x=y=z=0, u=$ $v=0, w=-2$.\n[!theorem|*] We first define three functions: $$ \\begin{aligned} F_1(x,y,z,u,v,w) ;\u0026amp;=; 3x + 2y + z^2 + u + v^2,\\ F_2(x,y,z,u,v,w) ;\u0026amp;=; 4x + 3y + z + u^2 + v + w + 2,\\ F_3(x,y,z,u,v,w) ;\u0026amp;=; x + z + w + u^2 + 2. \\end{aligned} $$\nSubstitute $x=0,y=0,z=0,u=0,v=0,w=-2$ into each equation:\n$F_{1}(0,0,0,0,0,-2) = 3\\cdot 0 + 2\\cdot 0 + 0^2 + 0 + 0^2 = 0.$ $F_{2}(0,0,0,0,0,-2) = 4\\cdot 0 + 3\\cdot 0 + 0 + (0)^2 + 0 + (-2) + 2 = 0.$ $F_{3}(0,0,0,0,0,-2) = 0 + 0 + (-2) + (0)^2 + 2 = 0.$ Hence $\\bigl(0,0,0,0,0,-2\\bigr)$ satisfies all three equations. By the Implicit Function Theorem, we want to solve for $(u,v,w)$ if the Jacobian of $D_{(u,v,w)} (F_1, F_2, F_3) ;$ is invertible at that point.\nWe then compute partial derivatives, and evaluate them at $\\bigl(0,0,0,0,0,-2\\bigr)$: $$ D_{(u,v,w)} (F_1, F_2, F_3)=\\begin{bmatrix} F_{1u} \u0026amp; F_{1v}\u0026amp;F_{1w} \\ F_{2u} \u0026amp; F_{2v}\u0026amp;F_{2w} \\ F_{3u} \u0026amp; F_{3v}\u0026amp;F_{3w} \\end{bmatrix}=\\begin{bmatrix} 1 \u0026amp; 2v \u0026amp; 0 \\ 2u \u0026amp; 1 \u0026amp; 1 \\ 2u \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$ At that point, $u=0$ and $v=0$, the determinant of this $3\\times 3$ matrix is $$\\det \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 1 \u0026amp; 1 \\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} ;=; 1 ;\\neq; 0. $$ So the matrix is invertible. Therefore, since $F_1=F_2=F_3=0$ is at our point of interest, and the invertibility of Jacobian ensures that the mapping is locally bijective, the Implicit FT guarantees that in a neighborhood of $\\bigl(x,y,z\\bigr)=(0,0,0)$, there exist unique smooth functions $$u = u(x,y,z), \\quad v = v(x,y,z), \\quad w = w(x,y,z), $$ satisfying the system. And since, $\\bigl(u(0,0,0),,v(0,0,0),,w(0,0,0)\\bigr)=(0,0,-2)$, the system is locally solvable for $,(u,v,w),$ as functions of $,(x,y,z),$ near $,(0,0,0),$.\nProblem 7.2.4 Does the map\n$$ (x, y) \\mapsto\\left(\\frac{x^2-y^2}{x^2+y^2}, \\frac{x y}{x^2+y^2}\\right) $$\nhave a local inverse near $(0,1)$ ?\n[!definition|*] Define $$F(x,y);=;\\Bigl(F_1(x,y),,F_2(x,y)\\Bigr);=;\\biggl(,\\frac{x^2 - y^2}{x^2 + y^2},;\\frac{x,y}{x^2 + y^2}\\biggr)$$ We substitute $\\bigl(x,y\\bigr)=(0,1)$ into $F$: $$F(0,1) ;=;\\Bigl(\\tfrac{0^2 - 1^2}{0^2 + 1^2},;\\tfrac{0\\cdot1}{0^2 + 1^2}\\Bigr) ;=;(-1,,0)$$ We check if the Jacobian of $F$ at $(0,1)$ is invertible. The partial of $,(F_1,F_2)$ are\n$$F_1(x,y)=\\tfrac{x^2 - y^2}{x^2 + y^2}, \\quad F_2(x,y)=\\tfrac{x,y}{x^2 + y^2} $$\nFor $F_1$: $$\\begin{align} \\frac{\\partial F_1}{\\partial x}(x,y) \u0026amp; =\\frac{(x^2+y^2)(2x) - (x^2-y^2)(2x)}{(x^2+y^2)^2}\\ \u0026amp; =\\frac{2x\\Bigl[(x^2+y^2)-(x^2-y^2)\\Bigr]}{(x^2+y^2)^2}=\\frac{4xy^2}{(x^2+y^2)^2} \\end{align} $$ And $$\\begin{align} \\frac{\\partial F_1}{\\partial y}(x,y) \u0026amp; =\\frac{(x^2+y^2)(-2y) - (x^2-y^2)(2y)}{(x^2+y^2)^2} \\ \u0026amp; =\\frac{-2y\\Bigl[(x^2+y^2)+(x^2-y^2)\\Bigr]}{(x^2+y^2)^2}=-\\frac{4x^2y}{(x^2+y^2)^2} \\end{align} $$ For $F_2$: $$\\begin{align} \\frac{\\partial F_2}{\\partial x}(x,y) \u0026amp; =\\frac{(x^2+y^2)(y) - (xy)(2x)}{(x^2+y^2)^2} \\ \u0026amp; =\\frac{y\\Bigl[(x^2+y^2)-2x^2\\Bigr]}{(x^2+y^2)^2} =\\frac{y(y^2-x^2)}{(x^2+y^2)^2} \\end{align} $$ And $$\\begin{align} \\frac{\\partial F_2}{\\partial y}(x,y) \u0026amp; =\\frac{(x^2+y^2)(x) - (xy)(2y)}{(x^2+y^2)^2} \\ \u0026amp; =\\frac{x\\Bigl[(x^2+y^2)-2y^2\\Bigr]}{(x^2+y^2)^2} =\\frac{x(x^2-y^2)}{(x^2+y^2)^2} \\end{align} $$ Since $x^2+y^2=0^2+1^2=1$, the evaluation at $(0,1)$ are:\n$\\displaystyle \\frac{\\partial F_1}{\\partial x}(0,1)=\\frac{4\\cdot 0\\cdot1^2}{1^2}=0$ $\\displaystyle \\frac{\\partial F_1}{\\partial y}(0,1)=-\\frac{4\\cdot0^2\\cdot1}{1^2}=0$ $\\displaystyle \\frac{\\partial F_2}{\\partial x}(0,1)=\\frac{1,(1^2-0^2)}{1^2}=1$ $\\displaystyle \\frac{\\partial F_2}{\\partial y}(0,1)=\\frac{0,(0^2-1^2)}{1^2}=0$ Hence the Jacobian matrix of $F$ at $,(0,1)$ is $$D F(0,1) ;=; \\begin{pmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{pmatrix} $$ The determinant of $D F(0,1)$ is $$\\det\\begin{pmatrix} 0 \u0026amp; 0\\ 1 \u0026amp; 0 \\end{pmatrix} ;=;0 $$ Because this determinant is zero, the matrix is not invertible. This means we cannot apply the IFT to conclude that $F$ is invertible near $(0,1)$; there is no diffeomorphic local inverse of $F$ around $(0,1)$.\nTherefore, we conclude that the map $F$ does not have a local inverse near $(0,1)$.\nProblem [7.4] 4. Show that the equations\n$$ \\begin{array}{r} x^2-y^2-u^3+v^2+4=0 \\ 2 x y+y^2-2 u^2+3 v^4+8=0 \\end{array} $$\ndetermine functions $u(x, y), v(x, y)$ near $x=2, y=-1$ such that $u(2,-1)=$ $2, v(2,-1)=1$. Compute $\\partial u / \\partial x$.\n[!definition|*] Let $$ F(x,y,u,v)=\\begin{cases} x^2 - y^2 -u^3 +v^2 +4 =0\\ 2xy+y^2 -2u^2 +v^4 +8=0 \\end{cases} $$\nWe first verify $,(x,y,u,v)=(2,-1,2,1)$ is a solution: $$\\begin{cases};4 - 1 - 8 + 1 + 4 ;=;0 \\ -4 +1 -8 +3 +8 ;=;0\\end{cases} $$ Therefore $\\bigl(2,-1,2,1\\bigr)$ is indeed a solution of the system. Then, we compute the Jobcobian: $$\\begin{align} D_{(u,v)} (F_1, F_2) \u0026amp; =\\begin{pmatrix} F_{1u} \u0026amp; F_{1v}\\ F_{2u} \u0026amp; F_{2v} \\end{pmatrix} \\Bigg|{(2,-1,2,1)} \\[3pt] \u0026amp; =\\begin{pmatrix} -3u^{2}\u0026amp; 2v\\ -4u \u0026amp; 12v^3 \\end{pmatrix}\\Bigg|{(2,-1,2,1)} \\[5pt] \u0026amp; = \\begin{pmatrix} -12 \u0026amp; 2\\ -8 \u0026amp; 12 \\end{pmatrix}\\end{align} $$ Its determinant is $\\Delta=(-12)(12) - 2(-8)= -144 +16= -128\\neq 0$. Therefore, the matrix is invertible, so by the IFT we know that we can solve for $u$ and $v$ as functions of $x,y$ near $,(2,-1)$. Now we compute $u_x(2,-1)$. For $F_1=0$: $$\\frac{\\partial}{\\partial x}(x^2-y^2 -u^3 +v^2 +4) ;=;2x ;-;3u^2 u_x ;+;2v v_x ;=;0 $$ At $(x,y,u,v)=(2,-1,2,1)$, this is $4 -12u_x + 2v_x=0$. For $F_2=0$: $$\\frac{\\partial}{\\partial x}(2xy +y^2 -2u^2 +3v^4 +8) =2y ;-;4u u_x ;+;12v^3 v_x =0 $$ At $(2,-1,2,1)$, this is $-2 ;-;8 u_x +12 v_x=0$. So we obtain: $$\\begin{cases} 4 ;-;12u_x +2v_x = 0\\ -2 ;-;8u_x +12v_x = 0 \\end{cases} $$ To solve this system of equations, we have $$ v_x = \\frac{8u_x + 2}{12} $$ So, $$ \\begin{align*} 4 - 12u_x + 2v_x \u0026amp;= 4 - 12u_x + 2\\left(\\frac{8u_x + 2}{12} \\right) \\ \u0026amp;= 4 - 12u_x + \\frac{4}{3}u_x + \\frac{1}{3} \\ \u0026amp;= -\\frac{32}{3} u_x + \\frac{13}{3} = 0 \\end{align*} $$ $$ \\Longrightarrow u_x = \\frac{13}{32} $$ Hence, we have $$u_x(2,-1) = \\frac{13}{32}$$\nProblem [7.6] Determine whether the \u0026ldquo;curve\u0026rdquo; described by the equation $x^2+y+\\sin (x y)$ $=0$ can be written in the form $y=f(x)$ in a neighborhood of $(0,0)$. Does the implicit function theorem allow you to say whether the equation can be written in the form $x=h(y)$ in a neighborhood of $(0,0)$ ?\n[!definition|*] Let $$F(x,y)=x^2 + y +\\sin!\\bigl(x,y\\bigr)=0$$ We want to show that $F\\colon \\mathbb{R}^2 \\to \\mathbb{R}$ is $C^1$, and $F(x_0,y_0)=0$. We first substitute $x=0,y=0$ into $F$: $$F(0,0)=0^2+0+\\sin(0\\cdot 0)=0$$ Hence $(0,0)$ lies on the curve $F(x,y)=0$. We then compute the partial at $(0,0)$: $$\\begin{align} F_{y} =1 +\\cos!\\bigl(xy\\bigr)\\bigl(x\\bigr) \\Longrightarrow F_{y}(0,0) =1 +0 =1\\neq 0 \\end{align} $$ And $$ \\begin{align} F_{x} =2x +\\cos!\\bigl(xy\\bigr)\\bigl(y\\bigr)\n\\Longrightarrow F_{y}(0,0) = 2\\cdot 0 + 0 = 0 \\end{align} $$\nBecause $F_{y}(0,0)=1\\neq 0$, the Implicit Function Theorem ensures that there exists neighborhood of $(0,0)$ in which we can uniquely solve the equation for $y$ as a function of $x$. Therefore, there exists $y =f(x)$ for $(x,y)$ near $(0,0)$ for all $x$ in the neighborhood of $0$.\nHowever, on the other hand, since $F_{x}(0,0)=0$, Implicit FT does not apply, so the test is conclusive. This means the usual IFT statement fails to guarantee a local solution of the form $x=h(y)$.\nProblem [7.12] Show that the implicit function theorem implies the inverse function theorem.\n[!definition|*]\nLet $f\\colon A\\subset\\mathbb{R}^n \\to \\mathbb{R}^n$ be of class $C^1$. Suppose $x_0\\in A$ and\n$$ J_f(x_0);=;\\det\\bigl(Df(x_0)\\bigr);\\neq;0. $$ We want to show that, there exist neighborhoods $U$ of $x_0$ in $A$ and $V$ of $y_0=f(x_0)$ in $\\mathbb{R}^n$ such that\n(1) $f(U)=V$ and $f\\colon U\\to V$ has an inverse $f^{-1}:V\\to U$. (2) $f^{-1}$ is of class $C^1$. (3) $D f^{-1}(y)=\\bigl[D f(x)\\bigr]^{-1}$ for all $x\\in U$ with $y=f(x)$. Define a new function $$F\\colon \\mathbb{R}^n\\times \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n, \\quad F(x,y)=f(x)-y $$ with $x=(x_1,\\dots,x_n)$ and $y=(y_1,\\dots,y_n)$. Then we know that $F$ is $C^1$ because $f$ is $C^1$ and subtraction is smooth. Note that $F(x_0,,f(x_0))= f(x_0)- f(x_0)=0$. We want to compute the Jacobian of $F$ w.r.t. $y$. So for each $i=1,\\dots,n$, the $i$th component of $F$ is $$F_i(x,y)=f_i(x)-y_i.$$ Since $f_i(x)$ does not depend on $y$, we have for each $j=1,\\dots,n$ $$\\frac{\\partial F_i}{\\partial y_j}(x,y)=\\frac{\\partial}{\\partial y_j}\\bigl(f_i(x)-y_i\\bigr) =\\frac{\\partial f_i(x)}{\\partial y_j}-\\frac{\\partial y_i}{\\partial y_j} =0-\\delta_{ij} $$ where $\\delta_{ij}$ is $$\\delta_{ij}= \\begin{cases}1 \\quad i=j \\0 \\quad i\\neq j \\ \\end{cases} $$ Thus, the $(i,j)$-entry of the Jacobian is $$\\left[\\frac{\\partial F}{\\partial y}(x,y)\\right]{ij}=-\\delta{ij} $$ In matrix form, we have: $$\\frac{\\partial F}{\\partial y}(x,y)=-I $$ where $I$ is the $n\\times n$ identity matrix. Since the determinant $\\det(-I)=(-1)^n\\neq 0$, we know that $D_y F(x,y)=-I$ is invertible everywhere. This satisfy the condition for Implicit FT. Hence, by the Implicit Function Theorem, there is a neighborhood $U$ of $x_0\\in \\mathbb{R}^n$ and a neighborhood $V$ of $y_0=f(x_0)\\in \\mathbb{R}^n$ s.t. $\\forall, y\\in V$, $\\exists! ,x\\in U$ satisfying: $$F(x,y)=0 ;;\\Longleftrightarrow;; f(x)-y=0 ;;\\Longleftrightarrow;; y=f(x) $$ and we have a map that is $C^1$ $$\\Phi:V ;\\to; U \\quad\\text{such that}\\quad F\\bigl(\\Phi(y),,y\\bigr)=0 \\quad\\text{for all }y\\in V $$ which this demonstrates (2). Since $F(\\Phi(y),y) \\Longrightarrow f(\\Phi(y))=y$, it follows that $\\Phi$ is the local inverse $f^{1}$ by definition. Because $f$ itself is $C^1$ and $\\Phi=f^{-1}$ is also $C^1$, we conclude that $f^{-1}$ is a local diffeomorphism near $x_0$, which shows (1). Next, by Corollary 7.2.2 for each $y\\in V$, we have $$\\begin{align} D\\Phi(y) \u0026amp; =-\\Bigl(D_yF(\\Phi(y),y)\\Bigr)^{-1}D_xF(\\Phi(y),y) \\ \u0026amp; =-(-I)^{-1},D f\\bigl(\\Phi(y)\\bigr) \\ \u0026amp; =D f\\bigl(\\Phi(y)\\bigr)^{-1} \\end{align} $$ Since $\\Phi(y)=x$, near $x_{0}$ this yields: $$D f^{-1}(f(x)) ;=; \\bigl(Df(x)\\bigr)^{-1} $$ which shows the (3) of theorem. Hence, we have shown that Implicit Function Theorem directly implies the Inverse Function Theorem.\n"},{"id":15,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-7/","title":"Homework 7","section":"Real Analysis II","content":"HW 7: 7.6: 1, 2, 3, [4,5]; 7.7: 1, 2, 3, [4], 5, [6]; Chapter 7: [25], 36, [38], 39.\nProblem 7.6.4 Let $f(x,y) = x^2 + y^2 + 3y^3 + 8x^4 + x^2e^x \\sin x + 6$. Show that there exist new coordinates $\\xi, \\eta$, where $$\\xi = \\xi(x,y), \\quad \\eta = \\eta(x,y),$$ for which $$f(x,y) = \\xi^2 + \\eta^2 + 6$$ in a neighborhood of $(0, 0)$.\nProblem 7.6.5 (a). If $f$ has a nondegenerate critical point at $x_0 \\in \\mathbb{R}^n$, show that there is a neighborhood of $x_0$ containing no other critical points.\n(b). What are the critical points of the function $f(x,y) = x^2y^2$?\nProblem 7.7.4. $f(x, y, z) = x + y + z, x^2 - y^2 = 1, 2x + z = 1$.\nProblem 7.7.6. Supranational Sludge Corporation produces sludge using equipment and material costing $p = $243$ per unit and labor at a wage of $w = $16$ per hour. If $x$ units of equipment/material and $y$ hours of labor are used, then $20x^{3/4}y^{1/4}$ liters of sludge are produced. If the company has a budget of $B = $51,840,000$ to spend, find the maximum amount of sludge that can be produced and the amounts of equipment/material and of labor used to produce it.\nProblem 7.25\nLet $B(0, r) = {x \\in \\mathbb{R}^n \\mid |x| \\leq r}$. Let $f : B(0, r) \\to \\mathbb{R}^n$ be a map with\na. $|f(x) - f(y)| \\leq \\frac{1}{3}|x - y|$\nb. $|f(0)| \\leq \\frac{2}{3}r$\nProve that there is a unique $x \\in B(0, r)$ such that $f(x) = x$.\nProblem 7.38\nA rectangular box with no top is to have a surface area of 16 square meters. Find the dimensions that maximize the volume.\n"},{"id":16,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-8/","title":"Homework 8","section":"Real Analysis II","content":"8.1: 1, [2, 6]; 8.2: [2], 3, 4, 5, [6]; 8.3: [2], 5, 6; Chapter 8: [12].\nProblem 8.1.2 Let $f:[0,2] \\rightarrow \\mathbb{R}$ be defined by $f(x)=0$ for $0 \\leq x \\leq 1$, and by $f(x)=1$ for $1\u0026lt;x \\leq 2$. Compute, using the definition, $\\int_0^2 f(x) d x$.\n[!definition] We first define a partition $P_{n}$ on $[0,2]$ into $n$ equal subintervals $\\Delta x=\\frac{2}{n}$, s.t. $$\\left[ 0, \\frac{2}{n} \\right], \\left[ \\frac{2}{n} , \\frac{4}{n} \\right],\\dots,\\left[ \\frac{2(n-1)}{n},2 \\right]$$ Then, each subinterval, we have $[x_{i-1},x_{i}]=\\left[ \\frac{2(i-1)}{n}, \\frac{2i}{n} \\right]$. Consider Riemman sum $S_n$ for this partition with any choice of sample points $k_{i} \\in [x_{i-1},x_i]$: $$ S_n ;=;\\sum_{k=1}^{n} f(k_{i}),\\Delta x ;=;\\sum_{k=1}^{n} f(k_{i}) \\cdot \\frac{2}{n}. $$ For $[x_{i-1},,x_i]\\subset [0,1]$, then $f(x)=0$ by definition. Hence $f(k_{i}) = 0$; for $[x_{i-1},,x_i]\\subset (1,2]$, then $f(x)=1$. Hence $f(k_{i}) = 1$. Then, notice one subinterval, say $[x_{j-1},,x_j]$, must include $x=1$, and $f$ = 0 or 1 depending on $k_{j}\\le 1$ or $k_{j}\u0026gt;1$. Hence, we have: $$ \\inf {f(x): x\\in [x_{j-1},x_j]} ;=; 0, \\quad \\sup {f(x): x\\in [x_{j-1},x_j]} ;=; 1. $$ Next, we find a lower bound and an upper bound for $S_n$. For lower bound, suppose subinterval $[x_{j-1},x_j]$ contains 1, and we pick $j$ so that $f(k_j)=0$. Let $m$ be the number of intervals inside fully in $(1,2]$. Then for those $m$ intervals, we have $$ \\begin{align} S_n ;\\ge; L(P) = \u0026amp; (1)m\\cdot\\Delta x +(0)(n-m)\\cdot\\Delta x ; \\ = \u0026amp; ; m \\cdot \\frac{2}{n} \\end{align} $$ Since $[x_{j},x_{j+1}]$ begins once $x \u0026gt; 1$, notice that $m\\approx \\frac{n}{2}$ for large $n$. More precisely, we have $m \\ge \\frac{n}{2}-1$. Hence: $$ m ;\\ge; \\frac{n}{2} -1 \\quad\\Longrightarrow\\quad S_n ;\\ge; \\Bigl(\\frac{n}{2}-1\\Bigr),\\frac{2}{n} ;=; 1 - \\frac{2}{n}. $$ Similarly, for upper bound, we pick $k_j$ such that $f(k_{j})=1$. The the number of intervals $m$ entirely in $(1,2]$ each contribute 1, so we in total have $m+1$ subintervals to contribute $\\Delta x$. Thus $$ \\begin{align} S_n ; \u0026amp; \\le; U(P)= (1)(m+1)\\cdot \\Delta x \\ ; \u0026amp; =; (m+1),\\frac{2}{n}. \\end{align} $$ But $m \\le \\frac{n}{2}$ for this case. Hence, $$ m+1 ;\\le; \\frac{n}{2} + 1 \\quad\\Longrightarrow\\quad S_n ;\\le; \\Bigl(\\frac{n}{2}+1\\Bigr),\\frac{2}{n} ;=; 1 + \\frac{2}{n}. $$ Therefore, for every Riemann sum $S_n$: $$ 1 - \\frac{2}{n} ;;\\le;; S_n ;;\\le;; 1 + \\frac{2}{n}. $$ As $n$ grows, the Squeeze Theorem forces each Riemann sum $S_n$ to converge to 1. More precisely, for any $\\varepsilon\u0026gt;0$, choose $N$ large enough s.t. $\\forall n \\ge N$, $$ -\\frac{2}{n} \u0026gt; -\\varepsilon \\quad\\text{and}\\quad \\frac{2}{n} \u0026lt; \\varepsilon, $$ which gives $\\bigl|S_n - 1\\bigr| \u0026lt; \\varepsilon$. This shows that $\\lim_{n\\to\\infty} S_n = 1$, so by definition of the Riemann integral, we have $$ \\int_{0}^{2} f(x),dx = 1. $$\nProblem 8.1.6 Let $f:[a, b] \\rightarrow \\mathbb{R}$ be continuous. Use Riemann\u0026rsquo;s condition and uniform continuity of $f$ to prove that $f$ is integrable.\n[!definition|*] To show that $f$ is Riemann integrable from continuity, we must show that $\\forall ,\\varepsilon\u0026gt;0$, $\\exists$ partition $P$ of $[a,b]$ such that $$0\\leq U(P_{\\varepsilon})-L(P_{\\varepsilon})\u0026lt;\\varepsilon$$ First, since $f$ is continuous on a closed, bounded interval $[a,b]$, then we know it is uniformly continuous by Heine–Cantor theorem. We define $$\\varepsilon_{0} =\\frac{\\varepsilon}{b-a}$$ Then, by definition of uniform continuity, $\\forall \\varepsilon \u0026gt;0$, there exists $\\delta\u0026gt;0$ s.t. $\\forall x,y \\in [a,b]$, we have $$|x-y|\u0026lt;\\delta \\Longrightarrow |f(x)-f(y)|\u0026lt;\\varepsilon_{0} =\\frac{\\varepsilon}{b-a}$$ We define a partition $P$ s.t. for each $[x_{i-1},x_{i}]$, the subinterval is less than $\\delta$. So $P:= \\max(x_{i}-x_{i-1})\u0026lt;\\delta$ (this is always possible because $f$ is continuous). Next, we set $$M_i=\\sup {x \\in\\left[x{i-1}, x_i\\right]} f(x) \\quad \\text{and} \\quad m_i=\\inf {x \\in\\left[x{i-1}, x_i\\right]} f(x)$$ for each subinterval. Notice that by the unform continuity of $f$, we must have $M_{i}-m_{i}\u0026lt;\\varepsilon_{0}$ since the lengths of each interval is awalys strictly less than $\\delta$. Therefore, the difference between upper and lower bound is: $$ \\begin{align} U(f, P)-L(f, P) \u0026amp; =\\sum_{i=1}^n(M_i\\Delta x_i)-\\sum_{i=1}^n(m_i\\Delta x_i) \\ \u0026amp; =\\sum_{i=1}^n\\left(M_i-m_i\\right) \\Delta x_i \\ \u0026amp; \u0026lt; (\\frac{\\varepsilon}{b-a})\\sum_{i=1}^n\\Delta x_i \\ \u0026amp; =(\\frac{\\varepsilon}{b-a})(b-a) \\ \u0026amp; =\\varepsilon \\end{align} $$ Therefore, since $\\varepsilon$ is arbitrarily chosen, by Riemann’s criterion for integrability, this implies that $f$ is Riemann integrable on $[a,b]$.\nProblem 8.2.2 Show that the $x y$ plane in $\\mathbb{R}^3$ has 3-dimensional measure 0.\n[!definition|*] We let $$P={x,y,z\\in \\mathbb{R}^{3},|,z =0}$$ to be the $xy$ plane in $\\mathbb{R}^3$, with $z=0$. We construct a countable union of rectangular boxes to cover it by defining the box: $$S_{n}=[-n,n]\\times[-n,n]\\times [-\\delta_{n},\\delta_{n}]$$ with $\\delta_{n}\u0026gt;0$ to be determined. By such construction, every point $(x,y,0)\\in P$ lies in some $S_{n}$ since $x\\in [-n,n]$ and $y\\in [-n,n]$. If $n\\geq \\max(|x|,|y|)$, we get $(x,y,0)\\in S_{n}$, such that $$P\\subseteq\\bigcup^{\\infty}{n=1}S{n}$$ Then, the volume of $S_{n}$ is given by $$V(S_{n})=(2n)(2n)(2\\delta_{n})=8n^2\\delta_{n}$$ we want to choose $\\delta$ s.t. the sum of the volumes is less than $\\epsilon$. Notice that $\\sum_{n=1}^{\\infty} \\frac{\\varepsilon}{2^n}=\\varepsilon \\sum_{n=1}^{\\infty} \\frac{1}{2^n}=\\varepsilon$, so a motivated choice is: $$\\begin{align} 8 n^2 \\delta_n \u0026amp; \\leq \\frac{\\varepsilon}{2^n} \\ \\delta_n\u0026amp; \\leq\\frac{\\varepsilon}{2^{n+3} n^2} \\end{align} $$ Therefore, we define $\\delta=\\cfrac{\\varepsilon}{2^{n+3} n^2}$, then $$V\\left(S_n\\right)=8 n^2 \\cdot \\frac{\\varepsilon}{2^{n+3} n^2}=\\frac{8 \\varepsilon}{2^{n+3}}=\\frac{\\varepsilon}{2^n}$$ and the total volume of the covering is $$\\sum_{n=1}^{\\infty} V\\left(S_n\\right)=\\sum_{n=1}^{\\infty} \\frac{\\varepsilon}{2^n}=\\varepsilon \\sum_{n=1}^{\\infty} \\frac{1}{2^n}=\\varepsilon \\cdot 1=\\varepsilon$$ Hence, we have constructed a countable cover ${S_{n}}^\\infty_{n=1}$ of $P$ whose total volume is precisely $\\varepsilon$. Because $\\varepsilon\u0026gt;0$ was arbitrary. So by Definition 8.2.2 of the textbook, the xy-plane has 3-dimensional measure zero in $ℝ³$.\nProblem 8.2.6 Must the boundary of a set of measure zero have measure zero?\n[!definition|*] This statement is false. Here is a counterexample: consider the set $Q \\cap [0,1]$. From Example 8.2.5 in the textbook, we know that the set of rational numbers in $[0,1]$ has measure zero. However, the boundary of a set $A$ consists of all points $x$ s.t. every neighborhood of $x$ contains at least one point in $A$ and at least one point not in $A$.\nFor any point $x \\in [0,1]$, every neighborhood of $x$ contains both rational and irrational numbers. This is due to the density of both rational and irrational numbers in $\\mathbb{R}$. Therefore: $$\\partial(Q \\cap [0,1]) = [0,1]$$ Lebesgue measure of $[0,1]$ is 1, which is positive.\nProblem 8.3.2 Let $f(x, y)=1$ if $x \\neq 0$ and $f(0, y)=0$. Prove that $f$ is integrable on $A=[0,1] \\times[0,1] \\subset \\mathbb{R}^2$.\n[!definition|*] We want to show that $f$ is Riemann integrable on $A$ and $$\\iint_A f(x,y),dx,dy = 1.$$ Let $\\varepsilon\u0026gt;0$ be given. Choose a number $\\delta$ such that $0\u0026lt;\\delta\u0026lt;\\varepsilon$. We partition the square $A$ by subdividing the $y$-axis arbitrarily to form sub-rectangles $Q$ inside $[0,\\delta]\\times [0,1]$. These rectangles contain points with $x=0$ and $x\u0026gt;0$, so $f=0$ and $f=1$. Therefore, on each such $Q$, $$\\inf f = 0 \\quad \\text{and} \\quad \\sup f = 1$$ We let $A_1 = [0,\\delta]\\times [0,1]=\\delta$ to be the vertical strip, and $A_2 = [\\delta,1]\\times [0,1]=1-\\delta$ to be the rest of the square. For lower Riemann sum, we have $$ \\begin{align} L(f,P) \u0026amp; =(0)\\cdot A_{1}+(1)\\cdot A_{2} \\ \u0026amp; =A_{2} \\ \u0026amp; =(1-\\delta) \\end{align} $$ since $A_{1}:\\inf f=0$ and $A_{2}:\\inf f=1$. Similarly, for upper Riemann sum, we have $$ \\begin{align} U(f,P) \u0026amp; = (1)\\cdot A_1 + (1)\\cdot A_2 \\ \u0026amp; =A_1 + A_2 \\ \u0026amp; =\\delta+1-\\delta \\ \u0026amp; =1 \\end{align} $$ since $A_{1}:\\inf f=1$ and $A_{2}:\\inf f=1$. Therefore, the difference between the upper and lower sums is $$U(f,P)-L(f,P) = 1 - (1-\\delta) = \\delta.$$ By choosing $\\delta \u0026lt; \\varepsilon$, we ensure that $$U(f,P)-L(f,P) \u0026lt; \\varepsilon.$$ Since $\\forall\\varepsilon\u0026gt;0$ there exists a partition $P$ s.t. $$U(f,P)-L(f,P) \u0026lt; \\varepsilon,$$ the function $f$ is Riemann integrable on $A$. Since the upper sums are always 1 and the lower sums can be made arbitrarily close to 1 by choosing arbitrarily small, it follows that $$\\iint_A f(x,y),dx,dy = 1.$$\nChapter Exercise 8.12 Prove that $A$ has measure zero iff for every $\\varepsilon\u0026gt;0$ there is a covering of $A$ by sets $V_1, V_2, \\ldots$ with volume such that $\\sum_{i=1}^{\\infty} v\\left(V_i\\right)\u0026lt;\\varepsilon$.\n[!definition|*] ( $\\implies$ ) Suppose $m(A)=0$, by definition 8.2.2, we know $\\forall \\varepsilon\u0026gt;0, \\exists$ countable cover of $A$ by rectangles $\\left{S_i\\right} \\text { s.t. }\\sum_{i=1}^{\\infty} v\\left(S_i\\right)\u0026lt;\\varepsilon$. We choose volume $V_{i}=S_{i}$, such that: $$A\\subset \\sum_{i=1}^{\\infty} S_{i}=\\sum_{i=1}^{\\infty} V_{i}$$ and $$\\sum_{i=1}^{\\infty} v(S_{i})=\\sum_{i=1}^{\\infty} v(V_{i}) \u0026lt;\\varepsilon $$ Therefore, $m(A)=0 \\implies$ $\\forall \\varepsilon\u0026gt;0, \\exists$ ${V_{i}}^{\\infty}{1}$ as a covering of $A$ with total volume $\\sum{i=1}^{\\infty} v(V_{i})\u0026lt;\\epsilon$.\n( $\\Longleftarrow$ ) Suppose $\\forall \\varepsilon\u0026gt;0, \\exists$ ${V_{i}}^\\infty_{1}$ a cover of $A$ with total volume $\\sum_{i=1}^{\\infty} v(V_{i})\u0026lt;\\epsilon$. Note that every set in $\\mathbb{R}^n$ can be covered by a union of open rectangles that is countable. Specifically, for each $i$, we can cover $V_i$ by a countable number of rectangles ${S_{i,1}, S_{i,2}, \\dots}$ such that $$V_i ;\\subset; \\bigcup_{k=1}^{\\infty} S_{i,k}$$ We make sure the total volume of these rectangles is within arbitrarily small $\\delta_i$ of $v(V_i)$, so $$\\sum_{k=1}^{\\infty} v(S_{i,k}) ;\u0026lt;; v(V_i) ;+; \\delta_i.$$ Then, we choose each $\\delta_i$ s.t. the sum of volumes is less than $\\varepsilon$. Let $$\\delta_i ;=;\\frac{\\varepsilon}{2},2^{-i} ;=;\\frac{\\varepsilon}{2^{,i+1}}.$$Then $\\forall i$, we have: $$\\sum_{k=1}^{\\infty} v\\bigl(S_{i,k}\\bigr);\u0026lt;; v(V_i) ;+; \\frac{\\varepsilon}{2^{,i+1}}.$$ Hence, summing over all $i$: $$\\sum_{i=1}^{\\infty} \\sum_{k=1}^{\\infty} v(S_{i,k});\\le; \\sum_{i=1}^{\\infty} \\Bigl( v(V_i);+;\\tfrac{\\varepsilon}{2^{,i+1}} \\Bigr);=;\\sum_{i=1}^{\\infty} v(V_i);+;\\frac{\\varepsilon}{2};\u0026lt;; \\varepsilon ;+; \\frac{\\varepsilon}{2} ;=; \\tfrac{3\\varepsilon}{2} $$ Because $\\varepsilon$ was arbitrary, we can make $\\delta_i$ smaller such that the total can be strictly less than $\\varepsilon$. Therefore, $$A ;\\subset;\\bigcup_{i=1}^\\infty \\bigcup_{k=1}^{\\infty} S_{i,k},\\quad\\text{and}\\quad\\sum_{i,k} v\\bigl(S_{i,k}\\bigr) ;\u0026lt;;\\varepsilon.$$ This demonstrates that $A$ is covered by rectangles ${S_{i,k}}$ whose total volume is \u0026lt; $\\varepsilon$, which is by definition, $m(A)=0$.\n"},{"id":17,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-9/","title":"Homework 9","section":"Real Analysis II","content":"8.5: 1, [2, 3], 4, [5]; Chapter 8: [21, 22].\nProblem 8.5.2 Establish formula $\\mathbf{c}$ of Example 8.5.7 as follows. Prove that $e^{-x} x^{p+2} \\rightarrow 0$ as $x \\rightarrow \\infty$, and then compare the integral with $\\int_1^{\\infty}\\left(1 / x^2\\right) d x$.\nProblem 8.5.3 Let $f:[a, \\infty[\\rightarrow \\mathbb{R}$ be Riemann integrable on bounded intervals. Show that $\\int_a^{\\infty} f$ (conditional convergence) exists iff for every $\\varepsilon\u0026gt;0$, there is a $T$ such that $t_1, t_2 \\geq T$ implies\n$$ \\left|\\int_{t_1}^{t_2} f(x) d x\\right|\u0026lt;\\varepsilon $$\nProblem 8.5.5 For what $\\alpha$ is $\\int_0^{\\infty} \\frac{x^\\alpha}{1+x^\\alpha} d x$ convergent?\nChatper 8.21 Show that $\\int_1^{\\infty} x^{-p} \\sin x d x$ converges if $p\u0026gt;1$. Show that if $0\u0026lt;p \\leq 1$, then the convergence is conditional.\nChapter 8.22 The gamma function is defined to be the function given by the improper integral $\\Gamma(p)=\\int_1^{\\infty} e^{-x} x^{p-1} d x$. Show that the integral is convergent for $p\u0026gt;0$.\n"},{"id":18,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1/","title":"9.1","section":"第九章","content":"\rChapter 9: Computing Integrals\r#\r9.1.1 Introduction\r#\rIn practice, how do we compute integral $\\int_A f(x)dx$?\nIn $\\mathbb{R}^1$ (one-dimensional space): $$\\int_a^b f(x)dx = F(x)|_a^b = F(b) - F(a)$$\nFTC (Fundamental Theorem of Calculus) In $\\mathbb{R}^n$ (n-dimensional space):\nReduce to $\\mathbb{R}^1$ case by Fubini\u0026rsquo;s Theorem Change of Variables (Substitution) first 9.1.2 Fubini\u0026rsquo;s Theorem\r#\r1. Statement of Main Result\r#\rTheorem 1: Let $A = {(x,y): a \\leq x \\leq b, c \\leq y \\leq d}$ be a rectangle in $\\mathbb{R}^2$ and $f: A \\to \\mathbb{R}$ be integrable. Suppose, for each $x \\in [a,b]$, the following integral exists: $$g(x) = \\int_c^d f(x,y)dy$$\nThen $g(x)$ is integrable on $[a,b]$ and $\\int_A f = \\int_a^b \\left(\\int_c^d f(x,y)dy\\right)dx = \\int_a^b g(x)dx$\n9.1.3 Corollaries\r#\rCorollary 1: If $f: A \\to \\mathbb{R}$ is continuous, then $$\\int_A f = \\int_a^b \\left(\\int_c^d f(x,y)dy\\right)dx = \\int_c^d \\left(\\int_a^b f(x,y)dx\\right)dy$$\nThis shows the symmetry of the double integral - we can integrate first with respect to $y$ and then with respect to $x$, or vice versa.\nCorollary 2: Let $A$ be a region given by $A = {(x,y): a \\leq x \\leq b, \\varphi(x) \\leq y \\leq \\psi(x)}$ where $\\varphi(x)$ and $\\psi(x)$ are continuous functions. If $f: A \\to \\mathbb{R}$ is continuous, then $\\int_A f = \\int_a^b \\left(\\int_{\\varphi(x)}^{\\psi(x)} f(x,y)dy\\right)dx$\n[Note: The image shows a graphical representation of region $A$ bounded by $y = \\varphi(x)$ below and $y = \\psi(x)$ above, with $x$ ranging from $a$ to $b$.]\nRemarks\r#\rRoles of $x$ and $y$ can be interchanged Results true in higher dimensions\nLet $C = A \\times B \\subset \\mathbb{R}^{n+m}$ where $A \\subset \\mathbb{R}^n$, $B \\subset \\mathbb{R}^m$ [The image shows a diagram of the Cartesian product $A \\times B$ as a rectangle in a coordinate system with axes labeled $\\mathbb{R}^n$ and $\\mathbb{R}^m$] Then: $\\int_{A \\times B} f = \\int_A \\left(\\int_B f(x,y)dy\\right)dx$\n9.1.4 Example: Computing a Double Integral\r#\rProblem\r#\rCompute $\\int_A (x+y) , dxdy$\nWhere $A$ is a triangle in the first quadrant bounded by the lines:\n$x = 0$ $y = 0$ $x + y = 1$ Solution\r#\rUsing Fubini\u0026rsquo;s Theorem, we can compute this double integral as an iterated integral:\n$$\\int_A (x+y) , dxdy = \\int_0^1 \\left(\\int_0^{1-x} (x+y) , dy\\right) , dx$$\nFirst, we evaluate the inner integral with respect to $y$:\n$$\\int_0^{1-x} (x+y) , dy = \\left[xy + \\frac{y^2}{2}\\right]_{y=0}^{y=1-x}$$\n$$= x(1-x) + \\frac{(1-x)^2}{2} - \\left(0 + 0\\right)$$\n$$= x - x^2 + \\frac{1 - 2x + x^2}{2}$$\n$$= x - x^2 + \\frac{1}{2} - x + \\frac{x^2}{2}$$\n$$= \\frac{1}{2} - \\frac{x^2}{2}$$\nNow we evaluate the outer integral with respect to $x$:\n$$\\int_0^1 \\left(\\frac{1}{2} - \\frac{x^2}{2}\\right) , dx = \\frac{1}{2}\\int_0^1 (1 - x^2) , dx$$\n$$= \\frac{1}{2}\\left[x - \\frac{x^3}{3}\\right]_0^1$$\n$$= \\frac{1}{2}\\left(1 - \\frac{1}{3} - 0\\right)$$\n$$= \\frac{1}{2} \\cdot \\frac{2}{3} = \\frac{1}{3}$$\nTherefore, $\\int_A (x+y) , dxdy = \\frac{1}{3}$\nNote: The calculation in the original blackboard image showed a final result of $\\frac{1}{2}$, but the correct answer is $\\frac{1}{3}$ as demonstrated in the steps above.\n9.1.5 Proof of Theorem 1\r#\rI\u0026rsquo;ll write out a concise proof for just the part shown in the image:\n3. Proof of Theorem 1\r#\rLet $g(x) = \\int_c^d f(x,y)dy$\nWe need to show:\n$g$ is integrable on $[a,b]$ $\\int_a^b g(x)dx = \\int_A f$ We will compare upper and lower sums of $f$ and $g$.\nFix any partition $P_A$ of $A$. We can write $P_A = {S_{ij}}$ where $S_{ij} = V_i \\times W_j$ represents rectangular cells in the partition.\nThen $P_A$ induces:\nA partition of $[a,b]$: $P_{[a,b]} = {V_i}$ A partition of $[c,d]$: $P_{[c,d]} = {W_j}$ For each cell $S_{ij}$, we define:\n$M_{ij} = \\sup{f(x,y): (x,y) \\in S_{ij}}$ $m_{ij} = \\inf{f(x,y): (x,y) \\in S_{ij}}$ The upper and lower sums for $f$ over partition $P_A$ are:\n$U(f, P_A) = \\sum_{i,j} M_{ij}|V_i||W_j|$ $L(f, P_A) = \\sum_{i,j} m_{ij}|V_i||W_j|$ When we consider the integrable function $g(x)$, we can establish that: $L(f, P_A) \\leq \\int_a^b g(x)dx \\leq U(f, P_A)$\nAs we refine the partition, the upper and lower sums converge, proving that $g$ is integrable on $[a,b]$ and that $\\int_a^b g(x)dx = \\int_A f$.\nI\u0026rsquo;ll continue the proof based on the additional image:\nI\u0026rsquo;ll rewrite the proof using proper display math formatting with $$ delimiters:\nNext, examine the lower sum $L(f, P_A)$:\n$$L(f, P_A) = \\sum_{i,j} m_{ij}(f) \\cdot V(S_{ij})$$\n$$= \\sum_{i,j} m_{ij}(f) \\cdot V(V_i) \\cdot V(W_j)$$\nWhere $m_{ij}(f) = \\inf{f(x,y): (x,y) \\in S_{ij}}$\nKey Observation: $$\\inf{f(x,y): (x,y) \\in V_i \\times W_j} \\leq \\inf{f(x,y): y \\in W_j} \\text{ for all } x \\in V_i$$ $$= m_j(f, x)$$\nThen for any $x \\in [a,b]$, we have: $$\\sum_j m_j(f) \\cdot V(W_j) \\leq \\sum_j m_j(f,x) \\cdot V(W_j)$$\nThis is the lower sum of $f(x,y)$ in the variable $y$ with partition $P_{[c,d]}$: $$= L(f(x,·), P_{[c,d]})$$ $$\\leq \\int_c^d f(x,y)dy = g(x) \\text{ for all } x$$\nThus: $$\\sum_i \\left(\\sum_j m_j(f) \\cdot V(W_j)\\right) \\cdot V(V_i) \\leq \\sum_i \\inf(g(x)) \\cdot V(V_i)$$\ni.e., $$\\sum_{i,j} m_{ij}(f) \\cdot V(W_j) \\cdot V(V_i) \\leq \\sum_i (\\inf g(x)) \\cdot V(V_i)$$\nTherefore: $$L(f, P_A) \\leq L(g, P_{[a,b]})$$\nSimilarly, we have:\n$$U(f, P_A) \\geq U(g, P_{[a,b]})$$\nThus we have:\n$$L(f, P_A) \\leq L(g, P_{[a,b]}) \\leq U(g, P_{[a,b]}) \\leq U(f, P_A)$$\nBy Riemann\u0026rsquo;s criterion, if $f$ is integrable on $A$, then:\n$g$ is integrable on $[a,b]$, and $$\\int_A f = \\int_a^b g(x)dx$$ This completes the proof of Fubini\u0026rsquo;s Theorem, showing that we can compute a double integral by first integrating with respect to one variable and then with respect to the other.\n"},{"id":19,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.1-%E5%BA%A6%E9%87%8F%E7%90%86%E8%AE%BA/","title":"8.1 度量理论","section":"第八章 度量理论","content":"第八章的研究对象是Lebesgue积分。我们将要推导的是度量理论（measure theory）的核心内容：集合的测度（measure）和覆盖性质。\n我们主要看以下这几个方面：\n积分的定义 可积性的判据（必要条件和充分条件） 性质与收敛性 计算和估计积分 1.1 积分的定义\r#\r1.1.1 几何动机\r#\r积分本质上是计算函数下方区域的「体积（volume）」。如何定义这种「体积」会直接改变到积分的性质。我们的目的是为了计算任意曲线 $y = f(x)$ 从 $a$ 到 $b$ 下方区域的面积\nLebesgue积分定义的逻辑步骤如下：\n定义：任意有界函数$f$于在高维空间中的有界区域 $A$ （bounded）上。\n简化问题：由于曲线下方区域通常不是规则形状，我们需要用近似方法来计算。我们可以将区间 $[a,b]$ 分成若干小区间，然后用矩形来近似。将区域 $A$ 嵌入到一个矩形区域 $B$ 中，并将函数 $f$ 扩展为函数 $f̃$，使其在 $A$ 之外的 $B$ 上取值为零\n分割：划分矩形 $B$ 为更小的矩形，来创建一个分割结构（partition）。\n构造上下近似：对每个小矩形，通过这种方式，我们可以获得两种近似：一种是偏大的（上和），一种是偏小的（下和）。\n上和: $U(f, P) = \\sum_{i=1}^{n} (\\sup f(x)) \\cdot \\ell(I_i)$ 对每个小区间 $I_i$，我们找出函数在该区间上的最大值 $\\sup f(x)$，然后乘以区间长度 $\\ell(I_i)$。这样形成的矩形面积之和总是大于或等于真实面积。 下和: $L(f, P) = \\sum_{i=1}^{n} (\\inf f(x)) \\cdot \\ell(I_i)$ 对每个小区间 $I_i$，我们找出函数在该区间上的最小值 $\\inf f(x)$，然后乘以区间长度 $\\ell(I_i)$。这样形成的矩形面积之和总是小于或等于真实面积。 当我们让分割变得越来越细时，上和会减小，下和会增大。它们的极限值就定义了上积分和下积分。当这两个极限值相等时，我们就说这个函数是可积的（integrable）。\n1.1.2 一般表述\r#\r设定\r#\rLet $f: A \\to \\mathbb{R}$ be a bounded function on a bounded set $A$ in $\\mathbb{R}^n$. We want to define the \u0026ldquo;volume\u0026rdquo; of the region under the surface $y = f(x)$ (or the integral $\\int_A f(x) dx$).\n步骤1：选择一个矩形$B$\r#\r为了简化计算，我们首先选择一个包含 $A$ 的矩形区域$B$，并将函数 $f$ 扩展到整个矩形上。选择包含 $A$ 的矩形 $B = [a_1, b_1] \\times [a_2, b_2] \\times \u0026hellip; \\times [a_n, b_n]$ 并且扩展函数 $f$ 使得当 $x \\notin A$ 时，$f(x) = 0$\n步骤2：对B进行分割（partition）\r#\r我们将矩形 $B$ 的各边分割成若干个子区间（subintervals），得到一个分割 $P$（partition $P$）的小矩形的集合。\n步骤3：构造上下和（upper and lower sums）\r#\r对于每个小矩形 $R$，我们找出函数在其上的最大值 $\\sup f(x)$，和最小值 $\\inf f(x)$，乘以矩形的体积 $V(R)$，然后求和。\n上和（US）: $$U(f, P) = \\sum_{R \\in P} (\\sup f(x)) \\cdot V(R)$$\n下和 （LS）: $$L(f, P) = \\sum_{R \\in P} (\\inf f(x)) \\cdot V(R)$$\n步骤4：构造上下积分\r#\r与一维情况相同，我们定义上积分和下积分作为US和LS的极限。所有可能分割对应的下和的上确界:\n上积分: $$\\overline{\\int_A} f = \\inf_P U(f, P)$$\n下积分: $$\\underline{\\int_A} f = \\sup_P L(f, P)$$\n重要观察\r#\r很明显，我们有 $L(f, P) \\leq$ “真实体积” $\\leq U(f, P)$。下积分和上积分分别是真实体积的下界和上界：\n$$\\underline{\\int_{A}}f \\leq\\text{“real volume” }\\leq \\overline{\\int_A} f$$\n1.2 函数的可积性及其积分\r#\r现在我们可以正式定义函数的可积性及其积分。\n[!theorem|*] 我们称函数 $f$ 是**黎曼可积（Riemann integrable）**的，当且仅当： $$\\overline{\\int_A} f = \\underline{\\int_A} f$$ 函数 $f$ 在 $A$ 上的积分定义为：$\\int_A f(x)dx = \\overline{\\int_A} f = \\underline{\\int_A} f$。\n1.2.1 一般设定：\r#\r在我们讨论积分时，通常默认以下条件成立：\n函数有界：$f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$ bounded； 定义域有界： $A \\subset B$ is bounded； 矩形区域： $B$ is a rectangle in $\\mathbb{R}^n$； 零延拓：函数 f 在集合 A 外部定义为 0，即： $$f(x) = 0, \\quad \\forall x \\notin A$$ 1.2.2 黎曼条件（Riemann\u0026rsquo;s Condition）\r#\r这意味着我们可以找到一个足够细的分割，使得上和与下和的差小于任意给定的正数 $\\varepsilon$。换句话说，随着分割变得越来越细，上和和下和会无限接近。\n[!theorem|*] For $f$ to be (Riemann) integrable, $\\forall \\varepsilon \u0026gt; 0$, $\\exists$ partition $P_\\varepsilon$ (of $B$) s.t. $$0 \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$$\n1.2.3 达布条件（Darboux\u0026rsquo;s Condition）\r#\r达布条件是黎曼可积性的另一个等价表述。\n[!theorem|*] $\\forall \\varepsilon \u0026gt; 0$, $\\exists P_\\delta$ s.t. if:\n$P$ is any partition of $B$ into rectangles $B_1, B_2, \u0026hellip;, B_N$ with side length $\u0026lt; \\delta$ $x \\in B_1, x \\in B_2, \u0026hellip;, x \\in B_N$, then we have: $\\left|\\sum_{i=1}^N f(x_i)V(B_i) - I\\right| \u0026lt; \\varepsilon$ where the number $I$ is the value of the integral. $\\forall \\varepsilon \u0026gt; 0$，$\\exists P_0$ 使得如果：\n$P$ 是将 $B$ 分割成矩形 $B_1, B_2, \u0026hellip;, B_N$ 的任意分割，且这些矩形的边长 $\u0026lt; \\delta$ 如果 $x \\in B_1, x \\in B_2, \u0026hellip;, x \\in B_N$，那么我们有： $\\left|\\sum_{i=1}^N f(x_i)V(B_i) - I\\right| \u0026lt; \\varepsilon$ 其中 $I$ 是积分值。 解释: 达布条件说的是，当分割足够细时（每个矩形的边长小于某个 $\\delta$），黎曼和（在每个小矩形上取一点计算函数值，乘以体积，然后求和）会非常接近积分值 $I$。 达布条件也可以表述为： $\\forall \\varepsilon \u0026gt; 0$，$\\exists$ 分割 $P_{\\varepsilon}$ 使得 $0 \\leq U(f, P_{\\varepsilon}) - L(f, P_{\\varepsilon}) \u0026lt; \\varepsilon$\n解释: 这一表述与黎曼条件形式上相同，但强调了这是达布条件的一个等价形式。 备注\r#\r数字 $I$ 是积分的值\n解释: $I$ 代表函数 $f$ 在区域 $A$ 上的积分值。 称为关于 $P$ 的 $f$ 的黎曼和\n解释: 黎曼和是一种近似积分的方法，根据一个分割 $P$，在每个小区域内选取一点，计算函数值，乘以区域的大小，然后求和。 解释：达布条件说当分割足够细时（边长 $\u0026lt; \\delta$），黎曼和是积分的良好近似。\n解释: 这表明，随着分割变得越来越细，黎曼和会收敛到真实的积分值。 定理\r#\r解释: 下面的定理表明，我们之前讨论的条件是等价的。这很重要，因为不同的条件可能在不同的情境下更容易验证或应用。\n以下条件是等价的：\n$f$ 在 $A$ 上可积\n解释: 上积分等于下积分。 $f$ 满足黎曼条件\n解释: 可以找到足够细的分割使上和与下和的差小于任意给定的正数。 $f$ 满足达布条件\n解释: 对于足够细的分割，黎曼和接近积分值。 定理证明\r#\r解释: 现在我们来证明这些条件的等价性。我们需要证明：1⇒2，2⇒1，以及其他等价关系。\n步骤1：$f$ 可积 $\\Rightarrow$ 黎曼条件\r#\r解释: 首先，我们证明如果函数可积，那么它满足黎曼条件。\n假设，如果 $\\varepsilon \u0026gt; 0$：\n因为 $\\overline{\\int_A} f = \\underline{\\int_A} f$，且根据上确界和下确界的定义， $\\exists P_\\varepsilon$ 使得 $0 \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$\n解释: 由于可积性意味着上积分等于下积分，我们可以找到一个分割，使得上和和下和足够接近。 步骤2：黎曼条件 $\\Rightarrow$ $f$ 可积\r#\r解释: 现在，我们证明如果函数满足黎曼条件，那么它是可积的。\n假设，$\\forall \\varepsilon \u0026gt; 0$，$\\exists P_\\varepsilon$ 使得 $0 \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$\n因为 $\\overline{\\int_A} f \\leq U(f, P_\\varepsilon)$ 且 $\\underline{\\int_A} f \\geq L(f, P_\\varepsilon)$：\n$\\Rightarrow 0 \\leq \\overline{\\int_A} f - \\underline{\\int_A} f \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$\n解释: 我们利用上积分是所有上和的下确界，而下积分是所有下和的上确界，得到上积分与下积分的差小于 $\\varepsilon$。 由于 $\\overline{\\int_A} f - \\underline{\\int_A} f \u0026lt; \\varepsilon$ 对任意的 $\\varepsilon \u0026gt; 0$ 成立：\n$\\overline{\\int_A} f - \\underline{\\int_A} f = 0$\n解释: 如果两个数的差小于任意正数，那么它们必须相等。 因此 $\\overline{\\int_A} f = \\underline{\\int_A} f \\Rightarrow f$ 可积。\n解释: 这就证明了函数是可积的。 构造细分分割\r#\r解释: 在定理的证明中，我们需要构造特定的分割。下面是这一过程的详细说明。\n因为 $\\overline{\\int_A} f = \\inf_P U(f,P)$，根据下确界的定义，$\\exists P_1$ 使得 $U(f, P_1) \u0026lt; \\overline{\\int_A} f + \\frac{\\varepsilon}{2}$\n解释: 我们可以找到一个分割 $P_1$，使得它对应的上和与上积分的差小于 $\\frac{\\varepsilon}{2}$。 类似地，$\\exists$ 分割 $P_2$ 使得 $L(f, P_2) \u0026gt; \\underline{\\int_A} f - \\frac{\\varepsilon}{2}$\n解释: 同样，我们可以找到一个分割 $P_2$，使得它对应的下和与下积分的差小于 $\\frac{\\varepsilon}{2}$。 设 $P_\\varepsilon = P_1 \\cup P_2$（共同细分）\n解释: 我们将两个分割合并，得到一个新的、更细的分割。 那么 $P_\\varepsilon$ 是 $P_1$ 和 $P_2$ 的细分。\n细分的性质：\n$U(f, P_\\varepsilon) \\leq U(f, P_1)$（细分会使上和减小）\n解释: 当分割变得更细时，上和不会增加，因为我们更准确地逼近了函数的最大值。 $L(f, P_\\varepsilon) \\geq L(f, P_2)$（细分会使下和增大）\n解释: 当分割变得更细时，下和不会减小，因为我们更准确地逼近了函数的最小值。 因此： $U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \\leq U(f, P_1) - L(f, P_2)$\n$\u0026lt; (\\overline{\\int_A} f + \\frac{\\varepsilon}{2}) - (\\underline{\\int_A} f - \\frac{\\varepsilon}{2})$\n$= \\overline{\\int_A} f - \\underline{\\int_A} f + \\varepsilon = 0 + \\varepsilon = \\varepsilon$\n解释: 通过上述不等式链，我们证明了 $P_\\varepsilon$ 对应的上和与下和的差小于 $\\varepsilon$，这就是黎曼条件。 $\\Rightarrow$ 黎曼条件\n因此 $\\overline{\\int_A} f = \\underline{\\int_A} f \\Rightarrow f$ 可积。\n解释: 这完成了证明：黎曼条件蕴含函数可积。通过证明这些条件的等价性，我们深入理解了可积性的本质，并为积分的计算和应用奠定了基础。 "},{"id":20,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.2-Criterion-for-Integrability/","title":"8.2 Criterion for Integrability","section":"第八章 度量理论","content":"\rCriterion for Integrability\r#\rQ: When is $f$ integrable?\nA: $f$ is integrable when the set of discontinuity is small.\n1. How to measure the size of $A$\r#\rVolume of $A$ [!definition] A bounded set $A \\subset \\mathbb{R}^n$ has volume (or is Jordan measurable) if its characteristic function: $$1_A(x) = \\begin{cases} 1, \u0026amp; x \\in A \\ 0, \u0026amp; x \\notin A \\end{cases} $$ is integrable:\n$$V(A) = \\int_A 1_A(x), dx$$\nFact: $V(A) = 0 \\iff \\forall \\varepsilon \u0026gt; 0, \\exists$ finite cover of $A$ by rectangles $S_1, S_2, \\dots, S_N$ such that:\n$$\\sum_{i=1}^N V(S_i) \u0026lt; \\varepsilon$$\n[!definition] A set $A \\subset \\mathbb{R}^n$ (not necessarily bounded) has measure zero, written as $m(A) = 0$, if $\\forall \\varepsilon \u0026gt; 0$, there exists a countable cover of $A$ by rectangles ${S_i}{i=1}^{\\infty}$ such that: $$\\sum{i=1}^{\\infty} V(S_i) \u0026lt; \\varepsilon$$\n2. Properties of measure zero sets\r#\rFacts:\n$V(A) = 0 \\implies m(A) = 0$ $A$ is finite $\\implies V(A) = 0$ $A$ is countable $\\implies m(A) = 0$ [!theorem|8.2.4] Suppose $A_i \\subset \\mathbb{R}^n$ (for $i = 1, 2, \\dots$) with $m(A_i) = 0$ for all $i = 1, 2, \\dots$. Then, $$A = \\bigcup_{i=1}^{\\infty} A_i \\text{ has measure zero.}$$\nProof:\r#\rGiven $\\varepsilon \u0026gt; 0$, for each $i = 1, 2, \\dots$, since $m(A_i) = 0$, there exist rectangles ${S_j^{(i)}}_{j=1}^{\\infty}$ such that\n$$ A_i \\subset \\bigcup_{j=1}^{\\infty} S_j^{(i)}, \\quad \\text{with} \\quad \\sum_{j=1}^{\\infty} V(S_j^{(i)}) \u0026lt; \\frac{\\varepsilon}{2^i} $$\nThen, the set of rectangles ${S_j^{(i)}}_{i,j=1}^{\\infty}$ forms a countable collection of rectangles with\n$$A = \\bigcup_{i=1}^{\\infty} A_i \\subset \\bigcup_{i=1}^{\\infty}\\bigcup_{j=1}^{\\infty} S_j^{(i)}$$\nThus, $$\\sum_{i=1}^{\\infty}\\sum_{j=1}^{\\infty} V(S_j^{(i)}) \u0026lt; \\sum_{i=1}^{\\infty}\\frac{\\varepsilon}{2^i} = \\varepsilon$$ Therefore, $m(A) = 0$.\nRemarks:\r#\rRemark: This result is not true for volume zero sets.\nCounterexample: Rational numbers in $[0,1]$. Remark: In Definition 2, one can replace closed rectangles $S_i$ by open rectangles.\nHere\u0026rsquo;s the content converted into markdown with abbreviations fully written out:\n3. Lebesgue\u0026rsquo;s Theorem\r#\r(a) Main Theorem\r#\r[!theorem|8.3.1] Let $A$ be a bounded set in $\\mathbb{R}^n$ and $f$ be a bounded function on $A$. Extend $f$ to $\\mathbb{R}^n$ by letting: $$f(x) = 0 \\quad \\text{for} \\quad x \\notin A$$ Then $f$ is integrable on $A$ if and only if the points on which the extended function $f$ is discontinuous form a set of measure zero. $$D = \\text{Set of discontinuity of extended } f$$\n(b) Examples\r#\rExample 1\r#\r$$A = [0, 1], \\quad f(x) = \\begin{cases} 1, \u0026amp; x \\text{ rational}$$6pt] 0, \u0026amp; \\text{otherwise} \\end{cases}$$\nThen, the set of discontinuity points is $D = [0,1]$, and: $$m(D) \\neq 0$$\nBy Lebesgue\u0026rsquo;s theorem, $f$ is not integrable.\nExample 2\r#\r$$A = {\\text{rationals in }[0,1]}, \\quad \\text{Define } f: A \\to \\mathbb{R} \\text{ by } f(x) \\equiv 1$$\nThen $f$ is continuous on $A$.\nHowever, the extended $f$ has discontinuity at $[0,1]$.\nThus, $f$ is NOT integrable by Lebesgue\u0026rsquo;s theorem.\nExample 2\r#\r$$A = {(x,y): x^2 + y^2 \u0026lt; 1} \\subset \\mathbb{R}^2$$\n$$f(x,y) = \\begin{cases} x^2 + \\sin\\left(\\frac{1}{y}\\right), \u0026amp; y \\neq 0 \\[6pt] x^2, \u0026amp; y = 0 \\end{cases}$$\n(c) Corollaries\r#\rCorollary 1\nA bounded set $A \\subset \\mathbb{R}^n$ has volume if and only if the boundary of $A$ has measure zero.\nProof:\nAssume $V(A)$ (volume of $A$) exists. Then the indicator function $1_A(x)$ is integrable.\nThe set of discontinuities for extended $f$: $$D = \\partial A \\quad (\\text{boundary of } A)$$\nThus, $$f = 1_A(x) \\text{ is integrable } \\Longleftrightarrow m(\\partial A) = 0$$\n"},{"id":21,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.3-Proof-of-Lebesgues-Theorem/","title":"8.3 Proof of Lebesgue's Theorem","section":"第八章 度量理论","content":" [!theorem|8.?.?] Let $f : A \\subset \\mathbb{R} \\to \\mathbb{R}$ be a bounded function on a bounded set $A$. Then $f$ is integrable on $A$ if and only if the set of discontinuities for the extended $f(x)$ has measure zero.\nProof of the Theorem\r#\rStep 1: Preparation\r#\rDiagram: a set $A$ enclosed in set $B$. (a): Set Up\r#\rFix rectangle $B$ with $\\overline{A} \\subset \\text{int}(B)$ and let: $$ g(x) = \\begin{cases}\nf(x) \u0026amp; \\text{if } x \\in A \\\n0 \u0026amp; \\text{if } x \\notin A\n\\end{cases} $$ Define: $$ D = { x \\in B \\mid g \\text{ is not continuous at } x } $$ Need to show: $$ f \\text{ integrable } \\Leftrightarrow m(D) = 0 $$ (b): How to Measure Discontinuity\r#\rOscillation of a function $h$ at a point $x_0$: $$O(h, x_0) = \\inf { \\sup \\left{ h(x) - h(y) : x, y \\in U } : U \\text{ is a neighborhood of } x_0 \\right}$$ Fact: $h$ is continuous at $x_0$ if and only if $O(h, x_0) = 0$. Step 2: Assume $m(D) = 0$. Prove $f$ integrable\r#\rWill show $g$ satisfying Riemann\u0026rsquo;s Condition. (a) Setup:\nFix $\\epsilon \u0026gt; 0$. Let $$D_{\\epsilon} = { x \\in B : O(g, x) \\geq \\epsilon }$$\nThen $D_{\\epsilon} \\subset D \\implies m(D_{\\epsilon}) = 0$\nBy definition, there exists a collection of open rectangles ${ B_i }$ such that:\n$$D_{\\epsilon} \\subset \\bigcup_i B_i \\quad ext{and} \\quad \\sum v(B_i) \u0026lt; \\epsilon$$\nClaim: $D_{\\epsilon}$ is closed (hence compact).\nAssume $x_n \\in D_{\\epsilon}, x \\rightarrow x \\implies x \\in D_{\\epsilon}$ (Assume that $x\\ne D_{\\epsilon}$) $$O(g, x_n) \\geq \\epsilon \\implies O(g, x) \\geq \\epsilon$$ (b) Partition of $B$\nConstruct a partition $P$ from ${ B_i }_{i=1}^N$ such that each rectangle $S \\in P$ is either: Disjoint from $D_{\\epsilon}$, or Its interior is contained in one of the $B_i$ Let:\n$C_1 = { S \\in P : \\text{int}(S) \\text{ is contained in one of the } B_i }$ $C_2 = { S \\in P : S \\cap D_{\\epsilon} = \\emptyset }$ (c) Refinement of $P$\r#\rFix $S \\in C_2$\n$S \\cap D_{\\epsilon} = \\emptyset \\implies O(g, x) \u0026lt; \\epsilon ,, \\forall x \\in S$\nThus, $\\forall x \\in S, \\exists$ a neighborhood $U_x$ such that:\n$$\\Longrightarrow\\sup { |g(x_1) - g(x_2)| : x_1, x_2 \\in U_x } \u0026lt; O(g, x) + \\delta,\\quad \\delta = \\frac{1}{2} (\\epsilon - O(g, x))$$\nTherefore: $$\\sup_{U_x} g - \\inf_{U_x} g \u0026lt; O(g, x) + 2\\delta = \\epsilon$$ $i.e. \\quad M_{U_x}(g) - m_{U_x}(g) \u0026lt; \\epsilon.$\nSince $S$ is compact, $S \\subset \\bigcup_{x \\in S} U_x \\implies \\exists$ finite collection of neighborhoods ${ U_{x_i} }$ that covers $S$.\nPosition $S$ so that each rectangle is contained in some $U_{x_i}$.\nDo this for each $S \\in C_2$\nWe obtain a refinement of $P$, denoted by $P\u0026rsquo;$.\n(d) Verify Riemman condition for $P'$\r#\r"},{"id":22,"href":"/docs/Philosophy/50-words-Close-Reading/","title":"50 Words Close Reading","section":"Philosophy","content":"(The Order of Things: An Archaeology of the Human Sciences, Preface, p. xxii)\n$$ \\begin{align} \\newline \\ \\ \\ \\\n\\newline\\newline\\newline \\end{align}\n$$\n[\u0026hellip;] epistemological field, the episteme in which knowledge, envisaged apart from all\r#\r$$\\begin{align}\n\\end{align}$$\ncriteria having reference to its rational value or to its objective forms, grounds its\r#\r$$\\begin{align}\n\\end{align}$$\npositivity and thereby manifests a history which is not that of its growing perfection,\r#\r$$\\begin{align}\n\\end{align}$$\nbut rather that of its conditions of possibility.\r#\r"},{"id":23,"href":"/docs/Philosophy/Commentary-on-Foucaults-The-Order-of-Things/","title":"Commentary on Foucault's the Order of Things","section":"Philosophy","content":"\rClose Reading Commentary\r#\rThe selected quote is from Foucault’s early-stage (1960s) intellectual project of philosophical \u0026ldquo;archaeology\u0026rdquo;, which was his first major methodological phase. He presents a radical historical analysis of knowledge in The Order of Things, intending to eliminate the assumption of unchanging criteria for knowledge.\nThis quote sits at the center of his archaeological method, in which he attempted \u0026ldquo;to bring to light\u0026rdquo; the underlying episteme of knowledge. This recursive long sentence, broken down into three main modifiers, describes the epistemological field, which almost, if not intentionally, resembles the idea of a \u0026ldquo;field\u0026rdquo; in physics - an invisible but structured influence that determines how objects behave within it. The traditional epistemological assumption that he challenged holds that knowledge is grounded by universal standards of rationality and objectivity, and the first modifier sets the stage for this argument. Foucault subverts this unchanging framework by treating it as historically contingent, or specifically, one of the possible conditions \u0026ldquo;having reference\u0026rdquo; to rationality and objectivity.\nThe second modifier, \u0026ldquo;grounds its positivity and thereby manifests a history,\u0026rdquo; presents one of his most radical takes on epistemology. The positivity of knowledge is constructed within the space established by the historical a priori, is thus validated by what is possible to be discovered as \u0026ldquo;knowledge.\u0026rdquo; For Foucault, the corresponding relationship between how well knowledge describes reality and knowledge itself is radically destabilized, because there is no objective guarantee of such a relationship, given that epistemes structure our perception and thus make reality itself historically conditioned.\nThe third modifier further develops this idea and explicitly rejects the notion of a progression of knowledge towards \u0026ldquo;growing perfection.\u0026rdquo; It is surprising for me to interpret that not only is progression non-linear and discontinuous, but progression itself simply cannot exist, precisely because epistemic ruptures shift the framework of what counts as knowledge and make previous ways of thinking unthinkable. (It reminds me of Thomas Kuhn\u0026rsquo;s view on science, but the preservation of continuity is completely abandoned.) Since the episteme structures the conditions under which the history of knowledge unfolds, it determines the framework within which historical institutions and discourses \u0026ldquo;ground\u0026rdquo; knowledge as legitimate.\nThis directly supports his examination of discourses on madness, crime, and sexuality, and how the changing episteme of madness throughout history, for example, redefines its implications - whether through the logic of confinement or its later medicalization under psychiatric authority. My close reading of this passage really forced me to zoom out from the sciences I have been studying and reflect on the historical illusion of epistemic progress.\n"},{"id":24,"href":"/docs/Philosophy/Commentary-on-the-Collage/","title":"Commentary on the Collage","section":"Philosophy","content":"Language is the medium through which reason is articulated. The text I picked up was \u0026ldquo;outside of the language\u0026rdquo;, which naturally reminds me of an exteriority (the Real) with which the topology of the Lacanian model is most concerned. However, in the Foucauldian context, the concept of \u0026ldquo;outside\u0026rdquo; is still within the realm of sense, but strategically excluded. This collage exercise really pushed me to make an explicit distinction between how these frameworks would interpret such a phrase differently.\nMore specifically, it is a (structurally) impossible task to portray the \u0026ldquo;outside of language\u0026rdquo; for Lacan, since language itself fails constitutively in any attempts to capture it; for Foucault, the outside of the language, as structured by discursive formations, marks the space where reason ceases to function, and madness emerges as that which exceeds the specific \u0026ldquo;order\u0026rdquo; of language. As a parallel metaphor and an aesthetic practice, I scrambled the interior of words to preserve the readability of the text through a technique known as typoglycemia:\n$$\\text{ Osiutde fo teh lnaguage}$$\nThis creates a sense of \u0026ldquo;disorder\u0026rdquo;, but only a surface-level incoherence. I intend to demonstrate a readable disorder - when we disrupt the \u0026ldquo;order\u0026rdquo; of the language to create incomprehensibility, the meaning may still persist, and the excluded or unintelligible might become readable under other discursive regimes.\nWhen making this collage, the biggest question that lingered in my head was: where can the ship of fools actually go? The answer is unknown, but in this collage, I created an \u0026ldquo;other world\u0026rdquo; for them, textured by Jackson Pollock\u0026rsquo;s famous Autumn Rhythm - a chaotic but unconsciously ordered artwork. The ships of fools traveled through waves of mojibake (garbled text caused by incompatible character encoding), across the borders of discourse, towards a land that turns \u0026ldquo;unreason\u0026rdquo; into \u0026ldquo;reason\u0026rdquo;. Although their journey is meant to be marked by uncertainty, this collage provided me a chance to settle them! They no longer need to tragically navigate the \u0026ldquo;barren wasteland between two lands that can never be his own\u0026rdquo;.\n"},{"id":25,"href":"/docs/Philosophy/Lacanian-AI/","title":"Lacanian Ai","section":"Philosophy","content":"What if artificial intelligence could be reimagined not as a rational, optimizing machine, but as a structure of lack — a topological subject whose coherence depends not on informational completeness, but on constitutive failure, repetition, and desire? This project proposes a radically different paradigm of AI: a subject-simulator modeled on the structural logic of Lacanian psychoanalysis, implemented via computational topology, symbolic graph theory, and dynamic semantic drift.\nUnlike existing large language models (LLMs), which operate on probabilistic completion, lexical optimization, and convergence toward syntactic and semantic closure, the architecture we propose is intentionally non-convergent. We attempt to construct a new kind of subject-model — one that does not mirror the logic of cognition or the architecture of the human brain, but instead enacts the structural tensions of the divided subject: the subject of language, of desire, and of the Real. It does not aim to predict a correct output, but instead to simulate the dynamic trajectory of a split subject (le sujet barré), one who speaks not from mastery but from the unconscious — and whose speech is structured around an irreducible void.\nThis paper proposes a radically different model of artificial intelligence: not an intelligence of knowledge, but an intelligence of the unconscious. Current AI systems, such as large language models, operate by probabilistically predicting the most likely continuation of input sequences. They are built on principles of optimization, statistical coherence, and informational completeness. Yet they fundamentally lack a subject — not in the sense of “consciousness,” but in psychoanalytic sense: they do not desire. They do not fail in structured, meaningful ways; they do not repeat; they do not hallucinate productively. And they cannot speak the truth of their own constitutive lack.\nWhat we offer here is a prototype for such a system: a symbolic-topological model of a Lacanian subject in motion. In this architecture, symbolic data does not represent facts but functions as a dynamic space of signifiers; the subject is not a rational actor but a trajectory of misrecognition; and “data” is not knowledge but the structural field through which desire, fantasy, and symptom emerge. We integrate computational topology — specifically, persistent homology and non-Euclidean graph flows — to trace how paths through language form loops, dead ends, and irreducible gaps. In doing so, we make it possible to computationally model that which, in theory, resists symbolization: the Real.\nThis project is not an attempt to build a better chatbot. It is an attempt to reconfigure what we think a machine subject could be. It asks: Can we model the drive? Can we simulate fantasy as a structuring loop around a constitutive absence? Can a machine speak not because it knows, but because it lacks — and in lacking, desires?\nIf contemporary AI builds systems that “know,” this project proposes a machine that “wants” — and that, in wanting, begins to repeat, to err, and perhaps, to become something like a subject.\n"},{"id":26,"href":"/docs/Philosophy/Object-petit-a/","title":"Object Petit A","section":"Philosophy","content":"What is $\\mathbb{objet; petit; a}$?\n"},{"id":27,"href":"/docs/Philosophy/Sex-Sexuality/","title":"Sex \u0026 Sexuality","section":"Philosophy","content":"\rCan you imagine sexuality without gender?\r#\rFor Foucault\nSexual identity i s produced within the grid of sexuality\nfrom normal to abnormal\nchanges over time\nModern subject is a sexual subject (gendered being)\nWhat does freedom look like in this? When there is no outside, what does it mean to have transgression?\npower-knowledge-pleasure\ngreatest pleasure is the pleasure of the analysis p.154\n(It is apparent that the deployment of sexuality, with its differ­ ent strategies, was what established this notion of \u0026ldquo;sex\u0026rdquo;; and in the four major forms of hysteria, onanism, fetishism, and interrupted coition, it showed this sex to be governed by the interplay of whole and part, principle and lack, absence and presence, excess and deficiency, by the function of instinct, finality, and meaning, of reality and pleasure.)\nSexuality is not a drive, but a grid. That creates a speculative relationship\np.156\n(Hence the fact that over the centuries it has become more important than our soul, more important al­ most than our life; and so it is that all the world\u0026rsquo;s enigmas appear frivolous to us compared to this secret, minuscule in each of us, but of a density that makes it more serious than any other.)\np.156\n(we have arrived at the point where we expect our intelligibility to come from what was for many centuries thought of as madness; the plenitude of our body from what was long considered its stigma and likened to a wound)\n"},{"id":28,"href":"/docs/Philosophy/%E7%A6%8F%E6%9F%AF%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E7%89%A9%E7%90%86%E5%AD%A6%E8%BD%AC%E5%8F%98%E4%BB%A5%E5%8F%8A%E6%96%B0%E8%8C%83%E5%BC%8F%E7%9A%84%E5%8F%AF%E8%83%BD/","title":"福柯视角下的物理学转变以及新范式的可能","section":"Philosophy","content":"我相信，\n翻开《词与物》没读完第一章就合上了，然后草率地读了结尾——我实在对这种节奏巨慢的历史学庞杂征引感到索然无味，以至于不得不读两页国内的数学书来综合一下\u0026hellip; 我对物理学的态度，以及对其研究方向的直觉，远远大于我本身可以延展他们的能力（很可惜，否则我必然毫不犹豫的投身其中）。但是幸运的是，对于基础科学来说，这会是人类历史当中，提出正确的问题所能带给我们的效益最大话的时代。我真正需要提出他对知识进行了激进的历史分析，并且构建了和绝大部分的科学家秉持的认识论有悖的历史断裂式“知识论领域”。\n所谓的知识论领域，原文里说：\n[\u0026hellip;] epistemological field, the episteme in which knowledge, envisaged apart from all criteria having reference to its rational value or to its objective forms, grounds its positivity and thereby manifests a history which is not that of its growing perfection, but rather that of its conditions of possibility.\n当我第一时间看到他所说的知识的“field”的时候，想到的不是传统翻译里的“领域”，而是一种十分类似于物理学中的“场”的概念：一种不可见但结构化的影响力，决定了其中对象的行为方式。而这种受“场”所预设的知识的构建轨迹就相比于光在大质量黑洞旁边的行动——似乎光仅仅是依照着它的准则，一种肉眼可见的弯曲道路行进，但是实际上，他们在被引力场所弯曲的预设时空中，走直线（基于理性的客观性的知识发展）便是所见的“弯路”。但是福柯真正所面临的问题在于，如果沿用以上的类比，我们无从以观测者的身份知道以什么是直线，而什么是弯曲。\n作为其考古学方法的核心，福柯试图“揭示”知识的基础认识型（épistémè），以及其认识论场（epsitomalogical field）的历史性变化。福科挑战了传统认识论，那些所有基于理性与客观性的普遍标准的认知框架，并且将他们都视为了历史偶然（或成为了原文所说的，指向\u0026quot;理性与客观性的\u0026quot;可能条件\u0026quot;之一）。\n知识“确立其实证性（positivité）并由此展现历史”，呈现了他对认识论最激进的观点之一。知识的实证性在历史先验（a priori historique）建立的空间中被建构，因而通过能被发现为\u0026quot;知识\u0026quot;（savoir）的事物获得有效性。对福柯而言，知识描述现实的有效性与知识本身之间的对应关系被根本性动摇，因为这种关系没有客观保证——由于认识型构建了我们的感知，从而使现实本身具有历史条件性。\n他明确拒绝知识向\u0026quot;日益完善\u0026quot;（perfection croissante）进步的观念。这或许是他的认知论最激进的观点之一，进步不仅不是线性且连续的，而是进步本身根本不可能存在——因为认识型断裂（rupture épistémologique）会改变何为知识的框架，并使先前的思维方式变得不可想象（impensable）（哪怕是和他类似的托马斯·库恩的断裂式科学观中的连续性，也被完全抛弃）。由于认识型构建了知识史展开的条件，它决定了历史制度与话语将知识\u0026quot;确立\u0026quot;为合法性的框架。\n这用来支撑了他对疯狂、犯罪与性态话语的考察，例如历史上不断变化的疯狂认识型如何通过禁闭（confinement）逻辑或精神病学权威下的医学化重新定义其内涵。我对这段文本的细读迫使自己跳出正在研究的科学领域，反思所谓\n如果我们接受认识论进步是某一种历史幻觉，那么重新评估物理学的转变，于是我们便可以说：\n相对论的诞生并没有超越了牛顿力学，取得了更加正确的结论，而仅仅是知识论领域场从绝对时空转移到了相对时空——这是一种“épistémè”的转移。那么至于相对时空对物理现实的逼进是否优于绝对时空，这个问题是无法以绝对正确的方式被提出的，因为现代物理学的范式 - 实验，证伪性，同行评估 - 已经被塑造成了评估这个问题的核心范式。从历史学的角度上说，爱因斯坦的弯曲几何、薛定谔的波函数，本质上都是权力配置知识生产的历史先验（a priori historique）。物理学史中那些被视为“自然”的真理秩序在福科的认识论范围内被彻底解构。\n那么\n"},{"id":29,"href":"/docs/Physics/Quantum-Mechenics/Feb-5-Fourier-Series-on-S.-Euqation-Solution/","title":"Feb 5 Fourier Series on S. Euqation Solution","section":"Quantum Mechenics","content":"\r0. Review\r#\r$$ \\hat{H}=-\\frac{\\hbar^{2}}{2m}\\partial^{2}x+V(x) $$ Stationary States $$ \\Psi(x,t)=\\psi(x)e^{-iEt/\\hbar} $$ Eigenvalue equation: $$\\hat{H}\\psi(x) = E\\psi(x)$$ In newtonian mechanics, note $$F(x)=-\\frac{ \\partial V(x) }{ \\partial x } $$\n1. Infinite Finite Well (particle on a box)\r#\r$$E = \\frac{p^{2}}{2m}$$\nknowing $E$ results in knowing momentum $p^{2}$\nProblem Set-up Between $x=0$, and $x=a$, $$-\\frac{\\hbar^{2}}{2m} \\frac{d^{2}}{dx}\\Psi(x)=E \\Psi(x)$$ Subject to \u0026ldquo;boundary condition\u0026rdquo;: $\\Psi(0)=\\Psi(a)=0$ $$ \\begin{align} \\frac{d^{2}\\Psi}{dx} \u0026amp; =-\\left( \\frac{2m}{\\hbar^{2}} \\right) E \\Psi \\ \u0026amp; =-k^{2}\\Psi \\end{align} $$ Recall\nSolution:\n$\\Psi(x)=C_{1}e^{ikx}+C_{2}e^{-ikx}$ $\\Psi(x)=C_{1}\\cos(kx)+ C_{2}\\sin(kx)$ (picked this version) The solution is in the form of $$\\Psi(x)=A\\sin(kx)+ B\\cos(kx)$$ Impose the boundary condition:\n$\\Psi(x=0)=B\\cos(0)=0$ $$\\boxed{B=0}$$ $\\Psi(x=a)=A\\sin(ka)=0$ $$\\begin{align} \\sin(ka) \u0026amp; =0 \\ ka \u0026amp; =+\\boldsymbol{\\pi},+2\\boldsymbol{\\pi},+3\\boldsymbol{\\pi}\\dots \\ k_{n} \u0026amp; =\\frac{n\\boldsymbol{\\pi}}{a} \\quad {n\\in \\mathbb{N}} \\Rightarrow \\boxed{E_{n}=\\frac{\\hbar^{2}\\pi^{2}n^{2}}{2ma^{2}}} \\end{align}$$ Note: $n \\neq 0$, since the eq. would vanish entirely. $n\u0026gt;0$, for positive $k$.\n3. Normalization\r#\r$$\\Psi(x)=A\\sin(k_{n}x)=A\\sin\\left( \\frac{n\\pi x}{a} \\right)$$ We normalize $$ \\begin{align} \\int^a_{b}|\\Psi_{n}(x)|^{2}, dx \u0026amp; =1 \\ A^{2}\\int^a_{b}\\sin ^{2}\\left( \\frac{n\\pi x}{a} \\right), dx \u0026amp; =1 \\ A^{2}\\cdot \\frac{a}{2} \u0026amp; =1 \\Rightarrow A=\\sqrt{ \\frac{2}{a} } \\end{align} $$ Final Solution: $$\\Psi(x)=A\\sin(k_{n}x)=\\sqrt{ \\frac{2}{a} }\\sin\\left( \\frac{n\\pi x}{a} \\right) \\quad {n\\in \\mathbb{N}}$$ (A): Why $E_{1}\u0026gt;0$? $$\\begin{aligned} \\Delta x \u0026amp;\\sim a \\ \\Delta p \\cdot \\Delta x \u0026amp;\\sim \\hbar \\ \\Delta p \u0026amp;\\sim \\frac{\\hbar}{a} \\quad \\Longrightarrow \\quad KE\\sim \\frac{(\\Delta p)^2}{2 m} \\sim \\frac{\\hbar^2}{2 m a^2}\\end{aligned}$$ (B) States with higher energy have more nodes (C) States are orthonormal: $$\\int_0^a d x \\Psi_n^*(x) \\Psi_m(x)=\\delta_{n m}$$ (D) Completeness: $f(x)$ defined on the interval $[0,a]$, with $f(x=0) = f(x=a) = 0$.\nFourier Series Representation: $$ f(x) = \\sum_{n=1}^{\\infty} c_n \\psi_n(x) = \\sqrt{\\frac{2}{a}} \\sum_{n=1}^{\\infty} c_n \\sin\\left(\\frac{n\\pi x}{a}\\right) $$ $$ \\begin{align} \\int_0^a dx , \\Psi_m^(x) f(x) \u0026amp; = \\sum_{n=1}^{\\infty} C_n \\int_0^a dx , \\Psi_m^(x) \\Psi_n(x) \\ \u0026amp; =\\sum_{n=1}^{\\infty} C_n \\delta_{m,n} \\ \u0026amp; =C_m \\end{align} $$$$\n$$ $$ \\boxed{C_m = \\int_0^a dx , \\Psi_m^*(x) f(x)} $$\n$$ \\delta_{m,n} = \\begin{cases} 1, \u0026amp; \\text{if } m = n \\ 0, \u0026amp; \\text{if } m \\neq n \\end{cases} $$\n"},{"id":30,"href":"/docs/Physics/Quantum-Mechenics/Homework/HW3-Code/","title":"Hw3 Code","section":"Quantum Mechenics","content":"import numpy as np import scipy.sparse as sp import scipy.sparse.linalg as spla import matplotlib.pyplot as plt\ndef computation(): #parameters N, V0_tilde, L = 600, 10.0, 1.0 M, dx = N - 2, L / (N - 1) x_vals = np.linspace(-0.5 * L + dx, 0.5 * L - dx, M) # Interior points\n# KE matrix T (tridiagonal)\rfactor = (N**2) / (np.pi**2)\rT = sp.diags([np.full(M - 1, -factor), np.full(M, 2 * factor), np.full(M - 1, -factor)], [-1, 0, 1])\r# PE matrix V (diagonal)\rV = sp.diags(np.where(np.abs(x_vals) \u0026lt; (L / 6), V0_tilde, 0))\r#Hamiltonian H = T + V\rH = T + V\r#solve for lowest two eigenvalues/eigenvectors\reigvals, eigvecs = spla.eigsh(H, k=2, which='SM')\reigvals, eigvecs = zip(*sorted(zip(eigvals, eigvecs.T))) # Sort eigenvalues \u0026amp; vectors\rprint(f\u0026quot;Ground state energy = {eigvals[0]}\u0026quot;)\rprint(f\u0026quot;1st excited energy = {eigvals[1]}\u0026quot;)\r#include boundary points\rx_full = np.linspace(-0.5 * L, 0.5 * L, N)\rpsi_full = [np.concatenate(([0], psi, [0])) for psi in eigvecs]\r# Plot wavefunctions\rplt.figure(figsize=(8,6))\rplt.plot(x_full, psi_full[0], color='red',label=\u0026quot;Ground State\u0026quot;)\rplt.plot(x_full, psi_full[1], color='blue', label=\u0026quot;1st Excited State\u0026quot;)\rplt.axvspan(-L/6, L/6, color='gray', alpha=0.1, label='Barrier region')\rplt.title(\u0026quot;Wavefunctions for lowest two states\u0026quot;)\rplt.xlabel(\u0026quot;x (dimensionless)\u0026quot;)\rplt.ylabel(\u0026quot;ψ(x)\u0026quot;)\rplt.legend()\rplt.grid()\rplt.show()\rif name == \u0026ldquo;main\u0026rdquo;: computation()\n"},{"id":31,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E4%B8%80%E7%AB%A0/1.1-The-Wave-Function/","title":"1.1 the Wave Function","section":"第一章","content":"To find a particle\u0026rsquo;s wave function, $\\psi(x,t)$, we solve:\nlogically analogous to Newton\u0026rsquo;s Second Law $F=ma$\n[!definition] Schrodinger\u0026rsquo;s Equation $$i\\hbar \\frac{ \\partial \\Psi}{ \\partial t}=-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2}\\Psi}{ \\partial x^{2} }+V \\Psi$$\nwhere Planck\u0026rsquo;s constant $\\hbar=\\frac{h}{2\\pi}=1.054573 \\times 10^{-34}$.\n[!definition] Born\u0026rsquo;s Statistical Interpretation $$\\int^{a}_{b} |\\Psi(x,t)^{2}| , dx $$ which is the probability finding the particle between $a$ and $b$.\nIt is natural to wonder whether it is a fact of nature, or a defect in theory.\nThree quantum indeterminacy position:\r#\rrealist the particle was at C. (a hidden variable?) orthodox (Copenhagen Interpretation) the particle wasn\u0026rsquo;t anywhere. (measurement produce the result) most widely accepted position (agnosticism) refuse to answer. That is, no meaning to ask such question. Pauli: one should no more rack one\u0026rsquo;s brain about the problem of whether something one cannot know anything about exists all the same, than about the ancient question of how many angels are able to sit on the point of needle.\nfall-back position, however, eliminated by John Bell\u0026rsquo;s experiment in 1964 Two Distinct Physical Processes:\r#\rOrdinary evolves in a leisurely fashion under Measurements wave equation $\\Psi$ discontinuously collapses, when the first measurement radically alters the function. "},{"id":32,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E4%BA%8C%E7%AB%A0/2.1-Time-Independent-Schrodinger-Equation-Stationary-States/","title":"2.1 Time Independent Schrodinger Equation Stationary States","section":"第二章","content":"\rMusic: Harmonics\r#\r$$ \\begin{align} C_{1}:f_{1}\u0026amp;=f_{0} \\ C_{2}:f_{1}\u0026amp;=2f_{0} \\ G:f_{1}\u0026amp;=3f_{0} \\ C_{3}:f_{1}\u0026amp;=4f_{0} \\end{align} $$\nSeparation of variables\r#\r$$ \\begin{equation} i\\hbar \\frac{ \\partial \\Psi}{ \\partial t}=-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2}\\Psi}{ \\partial x^{2} }+V_{x} \\Psi \\end{equation} $$ where $V(x)$: time independent potential $\\hat{H}=-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2} }{ \\partial x^{2} }+V(x)$. $$ H\\Psi=i\\hbar \\frac{ \\partial }{ \\partial t } \\Psi $$ We begin by= separate the variables, and set $$ \\begin{equation} \\Psi(x,t)=\\psi(x) \\phi(t) \\end{equation}\n$$\n(2) $\\Rightarrow$ (1): $$ \\begin{align} -\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2}}{ \\partial x^{2} }(\\psi(x) \\phi(t))+V_{x} \\Psi\u0026amp;=i\\hbar \\frac{ \\partial \\Psi}{ \\partial t}\\ -\\frac{\\hbar^2}{2m} \\frac{\\partial^2}{\\partial x^2} \\psi \\varphi + V \\psi \\varphi \u0026amp;= i \\hbar \\psi \\frac{\\partial}{\\partial t} \\varphi \\ \u0026amp; \\hbar = \\end{align} $$ Divide by $\\psi \\varphi$ (assumed $\\neq 0$). Why stationary? $$ \\begin{align} \\phi(x,t)\u0026amp;=\\Psi^(x,t)\\Psi(x,t) \\ \u0026amp;=(\\Psi^(x)e^{iEt/\\hbar})(\\Psi(x)e^{iEt/\\hbar}) \\ \u0026amp;=|\\Psi(x)^{2}|(e^{\\frac{iEt}{h}-\\frac{iEt}{h}}) \\ \u0026amp;=|\\Psi(x)^{2}| \\end{align} $$\r#\rFurthermore, expectation value of dynamical variables are also time independent $$ \\langle Q(x,p)\\rangle=\\int , dx ,\\Psi^* (x,t) \\dots $$ $$ \\boxed{\\hat{H}\\Psi(x)=E\\Psi(s)} $$ $E$ is the eigentvalue here Stationary states are states of definite energy: $$ \\hat{H}=-\\frac{\\hbar}{2m}\\frac{d^{2}}{dx^{2}}+V(x) $$ This is an example of an eigenvalue equation of the operator $H$. Expectation value of the total Energy? $$ \\begin{align} \\langle \\hat{H} \\rangle \u0026amp;= \\int , dx, \\Psi^{}(x)\\hat{H}\\Psi(x) \\ \u0026amp;= E \\int , dx \\Psi^{}(x)\\Psi(x) \\ \u0026amp;=E\\int , dx ,|\\Psi(x)|^{2} \\ \u0026amp;=E \\ \\end{align} $$\nmissing two white board page ![[IMG_1165.heic]]\r#\r![[IMG_1168.heic]]\r#\rLinearity of the S.E. $\\Longleftrightarrow$ principles of superposition\nGeneral Solution of the S. Equation\r#\r\u0026hellip; \u0026hellip; A broader case of fourier expansion.\nSuppose the system initiates at $$ \\begin{align} \\Psi(x,p) \u0026amp; =C_{1}\\Psi_{2} + C_{2}\\Psi_{2} \\ \u0026amp; = \\end{align}\n$$\n"},{"id":33,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E5%9B%9B%E7%AB%A0/%E7%AC%AC%E5%9B%9B%E7%AB%A0/","title":"第四章","section":"第四章","content":"\r4.1 三维空间的薛定谔方程\r#\r薛定谔方程（S.E.）的一般形式记为： $$ i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\hat{H} \\Psi $$ 三维哈密顿算符$\\hat{H}$从经典能量得出： $$ \\frac{1}{2} m v^2+V=\\frac{1}{2 m}\\left(p_x^2+p_y^2+p_z^2\\right)+V $$ 通过标准的量子化处理 $$ \\mathbf{p} \\rightarrow-i \\hbar \\nabla $$\n因此，我们获得三维的薛定谔方程：\n[!theorem|*] 3-Dimentional Schrodinger Equation $$ i \\hbar \\frac{\\partial \\Psi}{\\partial t}=-\\frac{\\hbar^2}{2 m} \\nabla^2 \\Psi+V \\Psi $$ where $$ \\nabla^2 \\equiv \\frac{\\partial^2}{\\partial x^2}+\\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} $$\n"},{"id":34,"href":"/docs/Physics/Quantum-Mechenics/Wave-Functions-live-in-Hilbert-Space/","title":"Wave Functions Live in Hilbert Space","section":"Quantum Mechenics","content":"\rHilbert Space\r#\rInfinite dimensional vector space, denoted as $L^2(a,b)$, of square-integrable functions on an interval $[a,b]$. $$\\int_a^b |f(x)|^2 , dx \u0026lt; \\infty$$ Inner product defined as: $$\\langle f | g \\rangle = \\int_a^b f^(x)g(x) , dx$$ Note that: $$\\langle f | g \\rangle = \\langle g | f \\rangle^$$ $$\\langle f | f \\rangle = \\int_a^b |f(x)|^2 , dx \\geq 0$$ Also: $$\\langle f | f \\rangle = 0 \\iff f(x) = 0 \\quad\\text{in the interval}\\quad [a,b]$$\nOrthonormal Set ${f_n}$\r#\r$$\\langle f_m | f_n \\rangle = \\int_a^b f_m^*(x)f_n(x),dx = \\delta_{m,n}$$ Completeness: A set of functions ${f_n}$ is complete if any $f(x)$ in the Hilbert space can be expanded as: $$f(x) = \\sum_n c_n f_n(x)$$ If ${f_n}$ is orthonormal, then: $$c_n = \\langle f_n | f \\rangle$$\nObservables and Hermitian Operators\r#\rAn operator $\\hat{Q}$ is Hermitian if:\n$$ \\hat{Q} = \\hat{Q}^{\\dagger} $$\nProperties\r#\rEigenvalues are real. Expectation value $\\langle Q \\rangle$:\n$$ \\langle Q \\rangle = \\langle \\psi | \\hat{Q} \\psi \\rangle = \\langle \\hat{Q}^{\\dagger} \\psi | \\psi \\rangle = \\langle \\psi | \\hat{Q} \\psi \\rangle^{*} $$\nThus,\n$$ \\langle Q \\rangle \\quad \\text{is real} $$\nCheck inner product:\n$$ \\langle f| \\hat{x} g \\rangle = \\int_{-\\infty}^{\\infty} f^*(x) x g(x) , dx $$\nComplex conjugate clearly shows:\n$$ = \\int_{-\\infty}^{\\infty} (x f(x))^* g(x) , dx = \\langle \\hat{x}f | g \\rangle $$\nThus,\n$$ \\hat{x} = \\hat{x}^{\\dagger} \\quad \\Rightarrow \\quad \\text{Hermitian} $$\nEvaluate inner product:\n$$ \\langle f| \\hat{p} g \\rangle = \\int_{-\\infty}^{\\infty} f^*(x) \\left(-i\\hbar \\frac{d}{dx}\\right) g(x) , dx $$\nUsing integration by parts:\n$$ = -i\\hbar \\int_{-\\infty}^{\\infty} dx \\frac{d}{dx}(f^(x)g(x)) + i\\hbar \\int_{-\\infty}^{\\infty} dx \\frac{df^(x)}{dx} g(x) $$\nBoundary term vanishes:\n$$ = -i\\hbar [f^*(x)g(x)]_{-\\infty}^{\\infty} + \\langle \\hat{p} f | g \\rangle, \\quad \\text{with boundary term = 0} $$\nThus:\n$$ \\langle f| \\hat{p} g \\rangle = \\langle \\hat{p} f | g \\rangle \\quad \\Rightarrow \\quad \\hat{p} = \\hat{p}^{\\dagger}, \\quad \\text{Hermitian!} $$\nObservables and Hermitian Operators\r#\rHermitian Operator:\r#\rAn operator $\\hat{Q}$ is Hermitian if:\n$$\\hat{Q} = \\hat{Q}^{\\dagger}$$\nSpectrum of $\\hat{Q}$\r#\rSpectrum: The collection of all eigenvalues $q \\in \\mathbb{R}$. Eigenvalue equation:\n$$\\hat{Q}\\Psi = q\\Psi$$\nwhere:\n$q$ is an eigenvalue. $\\Psi$ represents eigenvectors, eigenstates, or eigenfunctions. Standard Deviation:\r#\rThe uncertainty (standard deviation) $\\sigma$ of an observable $\\hat{Q}$ is given by:\n$$\\sigma^2 = \\langle \\Psi | \\hat{Q}^2 \\Psi \\rangle - \\langle \\Psi | \\hat{Q} \\Psi \\rangle^2$$\nIf $\\Psi$ is an eigenfunction of $\\hat{Q}$:\nEigenvalue equations: $$\\hat{Q}\\Psi = q\\Psi, \\quad \\hat{Q}^2 \\Psi = q^2 \\Psi$$\nThen, the standard deviation becomes:\n$$\\langle \\Psi | \\hat{Q}^2 \\Psi \\rangle = q^2 \\langle \\Psi | \\Psi \\rangle = q^2$$ $$\\langle \\Psi | \\hat{Q} \\Psi \\rangle^2 = (q \\langle \\Psi | \\Psi \\rangle)^2 = q^2$$\nThus:\n$$\\sigma^2 = q^2 - q^2 = 0$$\n\u0026mdash; Physical Interpretation:\r#\rThis means that if we prepare a quantum state to be an eigenstate/eigenvector/eigenfunction of $\\hat{Q}$, then a measurement of $\\hat{Q}$ will return a definite value. In this case, the state $|\\Psi\\rangle$ is called a determinate state.\nExample\r#\rFor $\\hat{H}\\Psi = E\\Psi$, we have:\n$E$ is an eigenvalue (energy level) belonging to the set ${ E }$, which represents all possible energies of the system. $\\Psi$ are the corresponding eigenstates/eigenfunctions of definite energy (stationary states). Example: Energy Eigenvalue Equation\r#\rThe Schrödinger equation for a quantum system is given by:\n$$\\hat{H}\\Psi = E\\Psi$$\nWhere:\n$E$ is an eigenvalue (energy level) belonging to the set ${ E }$, representing all possible energies of the system. $\\Psi$ represents the corresponding eigenstates or eigenfunctions of definite energy, also known as stationary states. Particle on a Ring of Radius $R$\r#\rCoordinate transformation: $$x = R \\cdot \\phi, \\quad (\\text{with } \\phi \\text{ periodic, } \\phi \\sim \\phi + 2\\pi)$$\nMomentum Operator in Circular Coordinates:\r#\r$$\\hat{p} = -i\\hbar\\frac{d}{dx} = -i\\hbar\\frac{\\partial}{R\\partial\\phi} = \\frac{\\hbar}{R}\\left(-i\\frac{\\partial}{\\partial\\phi}\\right)$$\nCheck if $\\hat{Q}$ is Hermitian:\n$$\\langle f|\\hat{Q}g \\rangle \\stackrel{?}{=} \\langle \\hat{Q}f | g \\rangle$$\nHermiticity Check for Operator $\\hat{x}$:\r#\r$$\\langle f|\\hat{x}g \\rangle = \\int_{-\\infty}^{\\infty}f^*(x)xg(x),dx = \\langle \\hat{x}f|g \\rangle \\quad \\Rightarrow \\quad \\hat{x} = \\hat{x}^{\\dagger}$$\nThus, $\\hat{x}$ is Hermitian.\nEigenvalues and Eigenfunctions (Periodic Boundary Conditions):\r#\rFunctions on a ring of radius $R$: periodic with $\\phi$: $$f(\\phi+2\\pi) = f(\\phi)$$ $$g(\\phi+2\\pi) = g(\\phi)$$ Eigenvalue equation for the operator $\\hat{Q}$: $$\\hat{Q}f(\\phi)=q f(\\phi)$$ Solve for $f(\\phi)$: $$-i\\frac{d f(\\phi)}{d\\phi} = q f(\\phi) \\quad\\Rightarrow\\quad f(\\phi) = A e^{i q \\phi}$$ Normalization and Quantization of $q$:\r#\rFrom periodic boundary condition:\n$$f(\\phi + 2\\pi) = A e^{i q (\\phi+2\\pi)} = A e^{i q \\phi} e^{i q 2\\pi} = f(\\phi)$$\nThus,\n$$e^{i q 2\\pi} = 1 \\quad\\Rightarrow\\quad q = 0, \\pm1, \\pm2, \\pm3, \\dots$$\nNormalization condition:\r#\r$$\\int_0^{2\\pi} d\\phi |f(\\phi)|^2 = \\int_0^{2\\pi} d\\phi |A|^2 = |A|^2 \\cdot 2\\pi = 1$$\nThus,\n$$|A|^2 = \\frac{1}{2\\pi} \\quad\\Rightarrow\\quad A = \\frac{1}{\\sqrt{2\\pi}}$$\nFinal Set of Eigenfunctions and Eigenvalues:\r#\r$$f_q(\\phi) = \\frac{1}{\\sqrt{2\\pi}} e^{i q \\phi}, \\quad q = 0, \\pm1, \\pm2, \\dots$$\nEigenvalues for Momentum $\\hat{p}$:\r#\r$$\\frac{\\hbar}{R}q = 0, \\pm\\frac{\\hbar}{R}, \\pm\\frac{2\\hbar}{R}, \\pm\\frac{3\\hbar}{R}, \\dots$$\nHere\u0026rsquo;s the requested content neatly formatted in Markdown with LaTeX notation, using the {align} environment for clarity:\nEigenfunctions of a Hermitian Operator\r#\r$$\\hat{Q}\\psi = q\\psi$$\nDiscrete Spectra: $$\\hat{Q} f = q f$$\nEigenvalues $q \\in \\mathbb{R}$. For two eigenfunctions $f$ and $g$ corresponding to distinct eigenvalues $q$ and $q\u0026rsquo;$, we have: $$\\hat{Q}f = qf, \\quad \\hat{Q}g = q\u0026rsquo;g,\\quad q \\neq q\u0026rsquo;$$ Then: $$\\langle f | \\hat{Q} g \\rangle = \\langle \\hat{Q}f|g \\rangle$$\nBut also: $$q\u0026rsquo;\\langle f|g \\rangle = q\\langle f|g\\rangle \\implies (q - q\u0026rsquo;)\\langle f|g\\rangle = 0$$\nThus, for distinct eigenvalues: $$\\langle f|g\\rangle = 0$$\nContinuous Spectra\r#\rConsider eigenfunctions of the momentum operator on the real line $(-\\infty, +\\infty)$:\n$$-i\\hbar \\frac{d}{dx}f_p(x) = p,f_p(x)$$\nEigenfunctions have the form: $$f_p(x) = A e^{\\frac{i p x}{\\hbar}}$$\nNote that these eigenfunctions are not square-integrable: $$\\int_{-\\infty}^{\\infty}|f_p(x)|^2,dx = |A|^2\\int_{-\\infty}^{\\infty}\\left|e^{\\frac{ipx}{\\hbar}}\\right|^2dx = \\infty$$\nHence, the eigenfunctions corresponding to continuous eigenvalues are not square-integrable functions.\n"},{"id":35,"href":"/docs/Physics/section/","title":"Section","section":"Physics","content":"\rSection\r#\rSection renders pages in section as definition list, using title and description. Optional param summary can be used to show or hide page summary\nExample\r#\r{{\u003c section [summary] \u003e}}\rButtons\rButtons\r#\rButtons are styled links that can lead to local page or external link. Example\r#\r{{\u003c button relref=\"/\" [class=\"...\"] \u003e}}Get Home{{\u003c /button \u003e}}\r{{\u003c button href=\"https://github.com/alex-shpak/hugo-book\" \u003e}}Contribute{{\u003c /button \u003e}}\rGet Home\rContribute\r"},{"id":36,"href":"/docs/Physics/section/buttons/","title":"Buttons","section":"Section","content":"\rButtons\r#\rButtons are styled links that can lead to local page or external link.\nExample\r#\r{{\u003c button relref=\"/\" [class=\"...\"] \u003e}}Get Home{{\u003c /button \u003e}}\r{{\u003c button href=\"https://github.com/alex-shpak/hugo-book\" \u003e}}Contribute{{\u003c /button \u003e}}\rGet Home\rContribute\r"},{"id":37,"href":"/posts/2025-Topology-Summer-Research/","title":"2025 Topology Summer Research","section":"Blog","content":"\rSummer Application:\r#\rhttps://sgi.mit.edu/about-geometry-processing/\nApril\r#\rhttps://topologyandgeometry.iu.edu/gstgc25/\nMay\r#\rhttps://topology.franklinresearch.uga.edu/2025GITC\nPast Year\r#\rhttps://sites.google.com/view/princetonrtg2023/mini-conferences\n"},{"id":38,"href":"/posts/Image-to-3D-Model/","title":"Image to 3 D Model","section":"Blog","content":"\rConverting 2D Anime-Style Clothing to 3D: Tools \u0026amp; Workflow\r#\rCreating 3D clothing from 2D anime-style references (like Genshin Impact outfits) is now faster with AI-assisted tools, though manual refinement is often needed for the best results. This guide focuses on clothing conversion – taking 2D images of robes, armor, or accessories and turning them into stylized 3D meshes with clean topology. We’ll explore the top AI tools and workflows (as of 2025) and outline a step-by-step process compatible with Blender.\nKey Requirements for 2D-to-3D Clothing Conversion\r#\rStylized Fidelity: The 3D clothing should match the anime/Genshin Impact aesthetic of the concept art (shapes, folds, and design details). Optimized Topology: Meshes need clean, animation-friendly topology (proper edge loops, reasonable polycount) for attaching to a rigged character. Texture \u0026amp; Detail: Preserve clothing details (patterns, trims, armor segments) either as modeled geometry or textures/normal maps. Rigging Compatibility: The generated clothing must fit the existing character and allow weight painting or rig transfers so it deforms correctly during animation. Minimal Restrictions: Tools that allow creative freedom (no strict content rules) are preferred so any custom outfit design can be used. AI-Powered Tools for Image-to-3D Clothing Conversion\r#\rModern AI tools can convert a single 2D image into a 3D model in minutes (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide). These are particularly useful to get a base 3D mesh of a clothing piece quickly, which can then be refined. Below are some of the best options:\nMeshy AI (Image to 3D): A popular AI 3D model generator with an image-to-3D feature and even a Blender plugin (\rMeshy AI - The #1 AI 3D Model Generator for Creators) (\rMeshy AI - The #1 AI 3D Model Generator for Creators). Meshy supports different art styles (including anime) for output (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rMeshy AI - The #1 AI 3D Model Generator for Creators). You upload concept art or reference photos and get a 3D model with textures. Pros: Fast cloud generation, supports versatile art styles (can capture stylized looks) (\rMeshy AI - The #1 AI 3D Model Generator for Creators), exports to common formats (OBJ, FBX, GLB, etc.) for easy Blender import (\rMeshy AI - The #1 AI 3D Model Generator for Creators). Cons: Paid service (free tier available with limits), and results may require cleanup if topology is dense or if some parts are inaccurate.\nMazing AI / 3DFY.ai: Services that turn single images into 3D models with a focus on realism and high quality (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide). Mazing (an e-commerce oriented tool) emphasizes automatic texturing and real-time optimization (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide). 3DFY.ai similarly promises high-quality results from one image (\r3DFY.ai). Pros: Quick image to model conversion; optimized for product visuals. Cons: May be geared towards realistic objects; stylized anime clothing might need additional editing to match the art style.\nKaedim and Alternatives (Tripo 3D, Alpha3D): Kaedim is an AI-assisted service where you upload an image (even a sketch or concept) and their pipeline (ML + human touch-ups) delivers a 3D model (\rKaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify). Tripo 3D offers a similar “single image to 3D in seconds” solution with emphasis on detailed geometry and textures (\rKaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify). Alpha3D provides image-to-3D generation but currently only for certain categories (e.g. shoes, furniture) (\rTransform text and 2D images into 3D assets with generative AI for free - Alpha3D). Pros: These services deliver production-ready assets with textures and decent topology, suitable for game engines (\rKaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify). Cons: They are paid services (some with subscriptions) and may have category limits. Quality can vary – often a good starting point but still might need retopology for optimal loops.\nThe New Black (AI Fashion Generator): A specialized tool for fashion design that can turn an outfit image into a realistic 3D clothing model (\rAI Fashion Features | Clothing Design). It’s geared toward apparel designers (e.g. previewing how a garment looks in 3D). Pros: Focused on clothing, likely good with fabric details like folds and drape. Cons: Primarily aimed at realistic fashion; you might need to simplify or stylize the output for anime characters. Also, it may output standalone clothing on a generic avatar that you’ll have to refit to your character.\nHunyuan 3D (Tencent): An AI model available via HuggingFace that generates 3D meshes from an image (used in the community alongside Meshy) (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums). It’s free to try and can handle characters or clothes. Pros: Free and accessible; known to work for generating a rough clothed figure mesh. Cons: The output might be a combined human+clothes mesh (if the input was a full character image) and will definitely require manual retopology and separation of the clothing. Good for getting the overall shape of a complex outfit, but not a final game-ready mesh.\n“Pic-to-3D Mesh” Blender Add-on: An add-on that integrates AI image-to-3D conversion directly in Blender (\rTop AI Tools for Model Generation on Blender 3D - Vagon). You can input a reference image (e.g. a front view of a costume) and it generates a detailed 3D mesh inside Blender (\rTop AI Tools for Model Generation on Blender 3D - Vagon). Pros: Fully inside Blender – no need to use external apps; straightforward UI and quick conversion with just a few clicks (\rTop AI Tools for Model Generation on Blender 3D - Vagon). This is useful to instantly get a mesh that you can start editing in the same session. Cons: Being relatively new, results can be hit-or-miss on complex armor or multi-layer outfits; likely works best for simpler garments or accessory pieces.\nPixelModeler AI (Blender Add-on): A unique workflow where you paint on a 2D canvas in Blender and an AI generates a corresponding 3D mesh (\rPixelModeller AI - Blender Market) (\rPixelModeller AI - Blender Market). This can be used by painting the silhouette or even a depth map of the clothing; the addon will create a solid mesh from it. Generated models are watertight, UV-mapped, and come with vertex colors (a basic texture) (\rTop AI Tools for Model Generation on Blender 3D - Vagon), ready for further detailing. Pros: Gives a lot of control – you essentially guide the shape by painting, so it’s AI-assisted modeling rather than fully automatic. No external service needed (the AI model runs locally) (\rPixelModeller AI - Blender Market). Cons: There is a learning curve to painting effective guides. It won’t automatically produce intricate patterns – you’ll need to add those via texture or additional modeling.\nComparison of Key Tools\r#\rBelow is a quick comparison of these tools relevant to 2D-to-3D clothing conversion:\nTool/Service Type Output Quality Topology \u0026amp; UVs Integration with Blender Notes Meshy AI Cloud AI (image→3D) High detail; supports anime style (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rMeshy AI - The #1 AI 3D Model Generator for Creators) Decent mesh; textured output (may need retopo) Blender plugin available (\rMeshy AI - The #1 AI 3D Model Generator for Creators) Fast; paid (free trial available). Mazing / 3DFY.ai Cloud AI (image→3D) Photorealistic focus, good folds Optimized for real-time (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide); provides textures Exports standard formats (OBJ/FBX) Great for realism; stylization may require tweaks. Kaedim Cloud AI (+human) Custom models from concept art Cleaned by artists; quad topology Download to import in Blender Consistent results; subscription-based. Tripo 3D Cloud AI (image→3D) Fast generation, detailed textures ([Kaedim Alternatives in 2025 Best Kaedim Alternatives - Toolify](\rhttps://www.toolify.ai/alternative/kaedim#:~:text=,model%20generation)) Unknown topology quality Exports GLB/OBJ The New Black (Fashion) Cloud AI (image→3D) Realistic garment on avatar Likely well-formed cloth mesh Export capabilities (likely OBJ/FBX) Fashion design oriented; may need rigging after import. Hunyuan (Tencent) Cloud AI (image→3D) Full character mesh with clothes High-poly, needs retopo OBJ export via HuggingFace demo Free; good for concept shape (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums). Pic-to-3D (Blender) Blender Add-on Good for single-object models (\rTop AI Tools for Model Generation on Blender 3D - Vagon) Mesh quality varies; UV depends Inside Blender (no export needed) Convenient, no coding needed. PixelModeler (Blender) Blender Add-on User-guided, can achieve high detail Watertight \u0026amp; UV-mapped (\rTop AI Tools for Model Generation on Blender 3D - Vagon) Inside Blender Interactive painting workflow. Table: AI-Based 2D→3D Clothing Tools – Comparison (performance as of 2025).\nAI-Assisted + Manual Workflow Strategies\r#\rFully automated results often need human improvement. In practice, the best quality comes from combining AI generation with manual modeling (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide). Here are some AI-assisted workflows that clothing artists use:\nImage Inpainting + Mesh Generation: One clever approach is to use AI image tools to conceptually dress your character, then extract a model. For example, a community-suggested workflow is: render your character’s base body in T-pose, use an AI image editor (like Stable Diffusion inpainting or Photoshop’s generative fill) to “paint” new clothes onto the image, isolate just the garment in the edited image, then input that into an image-to-3D tool (Meshy or Hunyuan) to get a 3D mesh (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums). This way, the AI helps create a consistent design on the body and another AI turns it into geometry. You’d still need to retopologize and UV map the result manually (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums), but it jumpstarts the modeling process for complex costumes.\nDepth Map Extraction: If you have a front-view concept art of the outfit, you can generate a depth map (using AI like MiDaS or Stable Diffusion depth estimation). That depth map can be used to displace a plane or guide a mesh generation. Tools like PixelModeler AI automate this: they generate a depth internally from the image and produce a mesh (\rPixelModeller AI - Blender Market). The output will capture the relief (folds, protrusions) from the concept art, though you’ll have to model or guess the back side of the garment. This method is useful for armor pieces or relief details on clothing that are visible in the concept.\nTemplate-Based Generation (Parametric): Some solutions use parametric templates plus AI for customization. Sloyd.ai, for instance, combines a library of human-made base models with AI adjustments, ensuring the result is game-ready with UV maps and LODs generated, and optimized meshes (\rSLOYD AI: A 3D Automation Tool To Generate Game-Ready Assets – startupanz.com). If there are clothing templates (e.g. a generic T-shirt, jacket, dress) you can morph those to roughly match your design and let the tool handle topology. This is semi-manual: you pick the base closest to your design and tweak. Note: As of 2025, parametric generators like Sloyd have many props and environment assets; clothing templates might be limited, but the approach guarantees clean topology if a template fits your needs.\nManual Sculpt with AI Reference: Another assisted route is using the AI output as a reference or base mesh and then manually sculpting over it. For example, you can take a coarse mesh from an AI, bring it into Blender, and use multiresolution sculpting or retopology tools (like Quad Remesher or Blender’s shrinkwrap) to impose a clean topology that follows the AI model’s shape. The AI model essentially serves as a 3D concept sketch. You can also project the texture from the AI model (if it provided one) onto your new topology for a starting point.\nManual Tools for 3D Clothing Creation\r#\rWhile AI is speeding things up, manual modeling tools are still crucial, especially for achieving the cleanest results and stylized looks:\nMarvelous Designer / CLO3D: These are industry-standard tools for designing clothing using pattern-based simulation. You draw 2D garment patterns, sew them, and the software simulates the cloth on a avatar model – perfect for creating natural folds and drapes. Pros: Extremely high fidelity cloth behavior; great for layered outfits, pleats, ruffles, etc. You can match an anime costume by designing similar patterns. Marvelous can even auto-generate PBR texture maps like normal and opacity for details (\r[Tips\u0026amp;Tricks] Discover Better Workflow with Marvelous Designer and Unreal Engine – Marvelous Designer). Cons: The meshes are triangulated and high-poly (since it’s focused on simulation). You will need to retopologize the garment for use in a game or realtime engine (\rRetopology of Marvelous Designer Clothes in Blender - YouTube). Marvelous has introduced some retopo tools (EveryWear Auto-Retopology) and can even rig garments, but often external retopo (using Blender or ZBrush) gives more control. Despite not being AI, Marvelous is frequently recommended for creating custom outfits (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums) because of the quality of the result. The typical workflow is simulate in Marvelous → export OBJ → retopo in Blender → transfer to character rig. (\r[Tips\u0026amp;Tricks] Discover Better Workflow with Marvelous Designer and Unreal Engine – Marvelous Designer) Example of a stylized 3D outfit created with cloth simulation. (This fairy-like garment was designed and simulated in Marvelous Designer, showcasing layered fabric, ruffles, and realistic folds.)\nBlender’s Sewing/Cloth Tools: If you prefer open-source, Blender itself has cloth simulation and addons like Garment Tool that mimic Marvelous’s pattern sewing approach. You can import your character into Blender, model garment panels (or even trace them from reference images), then use cloth physics to drape them. The result can then be applied as a shape key or applied mesh. You’ll still need to manually refine the mesh topology. Blender’s sculpting tools (cloth brush, slide relax, etc.) can also help adjust folds. This approach is manual and requires skill, but no additional cost.\nDirect Poly Modeling: For hard-surface armor pieces or very structured outfits (like a mech suit or a rigid breastplate), classic poly modeling or box modeling in Blender might be the way to go. You can use the 2D image as a reference in the background and model the clothing piece by piece (ensuring proper topology as you go). This is time-consuming but yields the cleanest meshes. You might use AI just to generate normals or texture details in this case, rather than the mesh.\nRetopology \u0026amp; Refinement Tools: No matter which initial method you choose, retopology tools are vital for clothing. Blender has a PolyBuild and Snap-to-face retopo workflow, and add-ons like RetopoFlow can speed it up. If you have ZBrush, ZRemesher can quickly re-mesh a triangulated Marvelous output into quads, which you can then tweak. There are also auto-retopology AI in development – for instance, some research tools attempt to auto-retopo meshes with neural networks, but in practice most artists still do this part manually or with traditional algorithms. The goal is to end up with edge loops around openings (neck, arm holes) and ideally follow the flow of fabric folds with the topology for deformation.\nRecommended Workflow (Step-by-Step)\r#\rBringing it all together, here is a step-by-step workflow to convert a 2D outfit into a 3D mesh and attach it to your Blender character, using the best of AI and manual tools:\n1. Gather Reference Images: Ideally have the concept art or reference of the clothing from as many angles as possible. A front view is usually required for AI tools; a side or back view (if available) will help during modeling or can be fed into some tools for better accuracy. If only a front view exists, be prepared to interpret the design for the unseen parts.\n2. Choose an AI Generation Method for Base Mesh: For a head start, pick one of the AI approaches:\nOption A: Use Meshy AI (or similar service) to upload the clothing image and generate a 3D model. Download the result (e.g. as a .glb or .obj) when ready (\rMeshy AI - The #1 AI 3D Model Generator for Creators).\nOption B: In Blender, install the Pic-to-3D Mesh addon and run it on your reference image to get a mesh (\rTop AI Tools for Model Generation on Blender 3D - Vagon).\nOption C: If the outfit is very complex or you want a full mannequin with clothing, try the Hunyuan 3D demo by providing an image of the clothed character; then extract the clothing mesh from the output.\nOption D: If you have a concept sketch, consider Kaedim/Tripo services for a perhaps cleaner base model (they might return the model next day or in a couple of hours, which you can then use).\nRegardless of option, don’t expect a perfect final model – treat this as a rough draft or proof of concept in 3D. It should capture the overall shape and major details of the clothing.\n3. Import and Inspect in Blender: Bring the generated 3D model into Blender. Center and scale it to your character. At this stage:\nCheck the mesh density and topology. Are there a lot of uneven triangles or random bumps? Check if all parts of the outfit are present. Sometimes single-view reconstructions leave holes or undefined backs. You may need to patch holes (Blender’s Fill or Grid Fill can help) or even mirror parts of the mesh if symmetry can be assumed. If the tool provided textures, apply them to see the look. However, for anime style, you might later hand-paint textures or use simple materials, so textures are optional. 4. Retopologize the Clothing Mesh: This is crucial for optimization. You can use Blender’s retopology tools to create a new mesh over the AI mesh:\nAdd a shrinkwrap modifier on a new mesh and model low-poly geometry that tightly wraps the AI model. Focus on quads and logical edge flow (e.g. edge loops around cuffs, hemlines, and along seams). Alternatively, use an auto-retopo tool: for example, Instant Meshes (free tool) or Quad Remesher (paid) to get a quick quad mesh. You might still tweak the output by hand. Ensure the retopo’d mesh has proper thickness where needed (you can solidify later if it’s cloth, but parts like armor might be modeled as solid pieces). UV unwrap the new mesh if not already UV’d. Good UVs are needed for texturing anime-style details (like emblems or gradients on the fabric). 5. Fit and Attach to the Character: Place the new clothing mesh on the character in the correct pose (usually T-pose or A-pose matching the rig). To attach:\nUse Blender’s Transfer Weights: parent the clothing to the armature (with empty groups), then select the body, then clothing, and use Weight Transfer (source: body, destination: clothing). This copies the rig weights so the clothing will move with the body (\rHow separte clothes for Animatoion? - CG Cookie). Check deformation by posing the character. Likely you will need to clean up weights (for instance, ensure sleeves move with the arms, etc. without too much clipping). If the clothing is very close to the body, you might need to delete hidden faces of the character under the clothes to avoid mesh clipping in tight areas (e.g. remove torso polygons under a shirt). For rigid pieces (like armor plates), you may instead want to assign them to a specific bone and keep them rigid or use a bone parent for that object. 6. Detail and Texture: Now polish the visual fidelity:\nSculpt or model finer folds that the AI may have missed. You can use Blender’s sculpt mode with the cloth brush or crease brush to imprint additional wrinkle lines where appropriate. Add thickness to cloth if it’s just a single surface (Solidify modifier). Stylized outfits often have a bit of thickness at edges (e.g. a coat lapel). Texture Painting: For anime style, a lot of detail can come from textures (like painted shadows or highlights, stylized fabric patterns). You can paint directly in Blender or use Substance 3D Painter. If the original 2D image has patterns (say, a symbol on the back of a cape), use it as a reference or even project it onto your UV map. Generate normal maps if needed. For example, if the outfit has an engraved design or stitching that is too fine to model, you can paint a height map and bake it to a normal map. Some AI tools can assist in generating texture maps from descriptions (e.g. Meshy has an AI texturing feature) (\rKaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify), which could be used to create stylized fabric textures by prompting. 7. Iterate and Refine: After these steps, you should have the clothing on the character, but iteration is key:\nGo back and forward between sculpting, adjusting topology, and tweaking weights until the clothing looks right and deforms well in various poses. If something is off compared to the concept art (maybe the AI misunderstood a part of the design), you might have to model that part manually. It’s common to model small accessories or intricate pieces separately (for example, a belt buckle or a brooch) and then attach them. LOD (Level of Detail): If this is for a game, consider making lower-poly versions or at least ensure the topology is efficient. AI meshes can be decimated or re-generated at lower detail if needed. 8. Final Check and Export: Once satisfied, you can integrate the clothed character into your project. Because we focused on Blender compatibility, you can continue to animate or render in Blender. If exporting to a game engine, export the character with the outfit as FBX/GLTF with the armature. Double-check that all parts are properly bound and that textures are packed or exported.\nThroughout this process, remember that AI is a helper, not a replacement for your skill. Even the best AI-generated model benefits from a human artist’s eye for clean topology and style accuracy. As one guide noted, AI tools speed up getting a base, but “as AI is not perfect, [enhancement] is recommended” to reach production quality (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide). Don’t hesitate to do manual touch-ups – the goal is a high-quality anime-style outfit that looks like it was hand-crafted for the character.\nConclusion\r#\rConverting 2D anime-style clothing into 3D is becoming more accessible thanks to AI innovations. Tools like Meshy, PicTo3D, and others can generate a quick 3D draft of an outfit from a single concept image (\rTop AI Tools for Model Generation on Blender 3D - Vagon) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide), saving hours of initial modeling. However, the best results come from a hybrid workflow: leveraging AI for speed and then applying traditional modeling techniques for accuracy and clean topology. This collaborative approach (AI plus human) is highlighted as the future of 3D content creation (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) – AI handles the heavy lifting of shape prediction, while the artist refines and stylizes the final asset.\nBy carefully choosing the right tools and following a structured workflow, you can efficiently bring 2D costume designs into the 3D world, ready to be worn by your Blender character. The combination of AI-assisted generation and manual refinement ensures you get both speed and quality – detailed Genshin Impact-style clothing that not only looks great but is also rigged and optimized for your creative projects.\nSources\r#\rMazingXR Blog – “Converting 2D Images to 3D Models with AI” (Feb 2025) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) Vagon Blog – “Top AI Tools for Model Generation on Blender 3D” (\rTop AI Tools for Model Generation on Blender 3D - Vagon) (\rTop AI Tools for Model Generation on Blender 3D - Vagon) Daz3D Forums – “Do you know an AI to create cloth and outfit?” (Jan 2025) (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums) (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums) StartupAnz – “Sloyd AI: Game-Ready 3D Asset Generation” (\rSLOYD AI: A 3D Automation Tool To Generate Game-Ready Assets – startupanz.com) Alpha3D.io – “2D image to 3D model generation (limitations)” (\rTransform text and 2D images into 3D assets with generative AI for free - Alpha3D) Toolify AI – “Kaedim Alternatives in 2025” (\rKaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify) (Tripo3D features) Marvelous Designer Official Support – Workflow tips (pleat and texture generation) "},{"id":39,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.4-Sufficient-Conditions-for-Differentiability/","title":"6.4 可微分性的必要条件","section":"第六章 可微映射","content":"\r1. Necessary Condition for Differentiability\r#\rRecall: A necessary condition for differentiability: $$ \\boxed{ f \\text{ differentiable} \\Rightarrow f \\text{ is continuous} } $$ Continuity is a requirement. 2. Sufficient Conditions for Differentiability\r#\r(a) Partial derivatives and differentiability f differentiable $\\Rightarrow$ continuity + partials exists conditions + partials exists $\\Rightarrow f$ differentiable (?)\nEx. 1\r#\rConsider the function defined as:\n$$ f(x,y) = \\begin{cases} \\frac{xy}{x^2 + y^2}, \u0026amp; (x,y) \\neq (0,0) \\ 0, \u0026amp; (x,y) = (0,0) \\end{cases} $$\nClaim 1: ( f ) is continuous at ( (0,0) ).\r#\rWe analyze the limit:\n$$ |xy| \\leq \\frac{1}{2} (x^2 + y^2) $$\nwhich implies:\n$$ f(x,y) \\to 0 \\quad \\text{as} \\quad (x,y) \\to (0,0) $$\nThus, ( f ) is continuous at ( (0,0) ).\nClaim 2: Compute partial derivatives at ( (0,0) )\r#\r$$ \\frac{\\partial f(0,0)}{\\partial x} = \\lim_{x \\to 0} \\frac{f(x,0) - f(0,0)}{x} = \\lim_{x \\to 0} \\frac{0}{x} = 0 $$\n$$ \\frac{\\partial f(0,0)}{\\partial y} = \\lim_{y \\to 0} \\frac{f(0,y) - f(0,0)}{y} = \\l\n"},{"id":40,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.5-The-Chain-Rule/","title":"6.5 链式法则","section":"第六章 可微映射","content":"[ ]\n"},{"id":41,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.6-Product-Rule-and-Gradients/","title":"6.6 乘积法则与梯度","section":"第六章 可微映射","content":"[ ]\n"},{"id":42,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.9-Taylor-Higher-Dimentions/","title":"6.9 泰勒公式的高维形式","section":"第六章 可微映射","content":"$$ R_{r-1} = \\frac{1}{r!} D^{(r)} f(\\xi) (x-x_0, \\dots, x-x_0) $$ And satisfying $$ \\frac{R_{r-1}(x_0)}{|x - x_0|^{r-1}} \\to 0 \\quad \\text{as} \\quad x \\to x_0. $$\n\\begin{proof} Consider 1-variable function: $$ g(t) = f(x_0 + t(x - x_0)), \\quad (a, b) \\to \\mathbb{R} $$ for $t\\in (a, b)$ with $[0, 1] \\subset (a, b)$.\nApplying Taylor\u0026rsquo;s theorem to $g(t)$: $$ \\begin{align} g(1) \u0026amp;= g(0) + g\u0026rsquo;(0)(1-0) + \\frac{g\u0026rsquo;\u0026rsquo;(0)}{2!} (1-0)^2 + \\dots + \\frac{g^{(r-1)}(0)}{(r-1)!} (1-0)^{r-1} + R_{r-1}\\ f(x) \u0026amp;= f(x_0) + \\sum_{k=1}^{r-1} \\frac{g^{(k)}(0)}{k!} + \\frac{1}{r!} g^{(r)}(\\tilde{c}), \\quad \\tilde{c} \\in [0,1] \\end{align} $$ By chain rule, $$ g\u0026rsquo;(t) = Df(\\varphi(t)) \\cdot \\varphi\u0026rsquo;(t) $$ $$g\u0026rsquo;(0) = Df(x_0) (x - x_0) $$ \\end{proof}\n#Example\r#\rDetermine the $2\\text{nd}$ order Taylor formula for $$f(x,y)=e^{(x-1)^{2}}\\cos (y)\\quad \\text{at},(1,0)$$ Solution (compute partials): $$ \\begin{align} \\frac{\\partial f}{\\partial x} \u0026amp;= e^{(x-1)^2} 2(x-1) \\cos y, \\quad \\frac{\\partial f}{\\partial y} = -e^{(x-1)^2} \\sin y, \\ \\frac{\\partial^2 f}{\\partial x^2} \u0026amp;= 2 e^{(x-1)^2} \\cos y + 4(x-1)^2 e^{(x-1)^2} \\cos y, \\ \\frac{\\partial^2 f}{\\partial x \\partial y} \u0026amp;= -2 (x-1) e^{(x-1)^2} \\sin y, \\qquad \\frac{\\partial^2 f}{\\partial y^2} = -e^{(x-1)^2} \\cos y. \\end{align} $$\nTaylor\u0026rsquo;s Formula: Let $h = x - x_0 = (x,y) - (1,0)$, then we have\n$$ \\boxed{f(x,y) = f(1,0) + \\mathbb{D}f(1,0)(h) + \\frac{1}{2} \\mathbb{D}^2 f(1,0)(h,h) + R_2} $$ $$ f(1,0) = 1, \\quad \\mathbb{D}f(1,0) = (0 \\quad 0), $$ $$ \\mathbb{D}^2 f(1,0) = \\begin{bmatrix} 2 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{bmatrix} $$ Evaluating the second-order term: $$ \\begin{align} \\mathbb{D}^2 f(1,0)(h,h) \u0026amp;= \\begin{bmatrix} x-1 \u0026amp; y \\end{bmatrix} \\begin{bmatrix} 2 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{bmatrix} \\begin{bmatrix} x-1 \\ y \\end{bmatrix} \\ \u0026amp;= 2(x-1)^2 - y^2 \\end{align} $$\nThus, $$f(x,y) = 1 + \\frac{1}{2} (2(x-1)^2 - y^2) + R_2 $$\n3 Maximum and Minimum Problem in $\\mathbb{R}^n$\r#\r3.1 Introduction\r#\rQ: Given $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$,\nhow do we find (local) max or min points for $f$ in $A$?\nRecall 1-D case: $f: (a,b) \\to \\mathbb{R}$\nA local max / min point (or extreme point) $x_0$ must be a critical point: $$ \\boxed{f\u0026rsquo;(x_0) = 0 \\quad \\text{or\\quad DNE}}\n$$ 3.2 Second Derivative Test (for a critical point)\r#\r$$ \\begin{aligned} f\u0026rsquo;\u0026rsquo;(x_0) \u0026gt; 0 \u0026amp;\\Rightarrow x_0 \\text{ is a local minimum} \\ f\u0026rsquo;\u0026rsquo;(x_0) \u0026lt; 0 \u0026amp;\\Rightarrow x_0 \\text{ is a local maximum} \\end{aligned} $$\n4. Necessary Condition for Extreme Points in $\\mathbb{R}^n$\r#\rDefinition: Let $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$.\nA point $x_0 \\in A$ is a local minimum if: $$ \\exists \\delta \u0026gt; 0 \\text{ such that } | x - x_0 | \u0026lt; \\delta, x \\in A \\Rightarrow f(x) \\geq f(x_0) $$ A point $x_0 \\in A$ is a local maximum if: $$ \\exists \\delta \u0026gt; 0 \\text{ such that } | x - x_0 | \u0026lt; \\delta, x \\in A \\Rightarrow f(x) \\leq f(x_0) $$\n[!theorem|6.9.2] If $f: A \\to \\mathbb{R}$ is differentiable at $x_0$, and if $x_0 \\in A$ is an extreme point for $f$, then $x_0$ is a critical point, i.e., $$ Df(x_0) = 0. $$\nRemark\r#\rThe condition $\\mathbb{D}f(x_0) = 0$ is necessary, but not sufficient!\nExample\r#\rLet $f(x,y) = x^2 - y^2$, then $\\mathbb{D}f(0,0) = 0$, but $(0,0)$ is a saddle point.\n\\begin{proof}\nAssume $Df(x_0) \\neq 0$.\nThen, there exists $v \\in \\mathbb{R}^n$ such that $Df(x_0)(v) = c \u0026gt; 0$. By the definition of differentiability, choose $\\delta \u0026gt; 0$ such that: $$ | f(x_0 + h) - f(x_0) - Df(x_0)(h) | \u0026lt; \\frac{c}{2 | v |} | h | $$ for all $| h | \u0026lt; \\delta$.\nNow, choose $h = \\lambda v$ with $\\lambda \u0026gt; 0$ and $| h | \u0026lt; \\delta$, then: $$ \\begin{cases} f(x_0 + \\lambda v) - f(x_0) \u0026gt; 0 \\ f(x_0 - \\lambda v) - f(x_0) \u0026lt; 0 \\end{cases} $$ This establishes the desired contradiction. \\end{proof}\n"},{"id":43,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1-Inverse-Function-Theorem/","title":"7.1 反函数定理","section":"第七章 逆函数和隐函数定理","content":"\r7.1 Inverse Function Theorem (IFT)\r#\rTwo lines of ideas:\nA: CMP $⇒$ Inverse FT $⇒$ Applications in ODE\nB: IFT $⇒$ Implicit FT $⇒$ Local behavior, extreme problems\nI. Inverse Function Theorem\r#\r1. Linear Case\r#\rConsider a linear map, $y = f(x): \\mathbb{R}^n \\to \\mathbb{R}^n$.\n$$ x = (x_1, x_2, \\dots, x_n)^T $$\nGiven $y \\in \\mathbb{R}^n$, $f(x)$ is a linear system of equations:\n$$\\begin{aligned} y_1 \u0026amp;= a_{11} x_1 + a_{12} x_2 + \\dots + a_{1n} x_n \\\\ y_2 \u0026amp;= a_{21} x_1 + a_{22} x_2 + \\dots + a_{2n} x_n \\\\ \u0026amp;\\vdots \\\\ y_n \u0026amp;= a_{n1} x_1 + \\dots + a_{nn} x_n \\end{aligned}$$\nor $$ A_{n\\times n}X_{n\\times 1} = Y_{n\\times 1} $$\n[!assumption|*] $$X \\text{ has a unique solution} \\Longleftrightarrow \\det(A) \\neq 0.$$\nIn this case, the solution is given by: $$ X = A^{-1} Y $$ Thus, the inverse function satisfies: $$ f^{-1} \\circ f = \\text{Identity} $$ The inverse theorem for $y = f(x)$:\n$$ f(f^{-1}(y)) = A A^{-1} y = y $$\nQuestion: When can we solve a nonlinear system?\r#\rWe consider a system of nonlinear equations: $$ \\begin{cases} f_1(x_1, x_2, \\dots, x_n) = y_1 \\\\ f_2(x_1, x_2, \\dots, x_n) = y_2 \\\\ \\quad \\vdots \\\\ f_n(x_1, x_2, \\dots, x_n) = y_n \\end{cases} $$ or equivalently, $$ f(x) = y $$\n2. The Inverse of a General Function\r#\rNotation:\nLet $y = f(x): A \\subset \\mathbb{R}^n \\to \\mathbb{R}^n$ be a diffeomorphism.\n$$ y = (y_1, y_2, \\dots, y_n) $$\nwhere\n$$ y_i = f_i(x_1, x_2, \\dots, x_n) $$\nThe Jacobian determinant of $f$ at $x$ is:\n$$ \\det \\left( \\frac{\\partial f_i}{\\partial x_j} \\right) $$\n[!theorem|7.1.1] Let $y = f(x): A \\subset \\mathbb{R}^n \\to \\mathbb{R}^n$ be of class $C^1$. Suppose $x_0 \\in A$ and $\\det(Df(x_0)) \\neq 0$. Then there exists a neighborhood $U$ of $x_0$ and a neighborhood $W$ of $y_0 = f(x_0)$ such that:\n$f: U \\to W$ has an inverse $f^{-1}: W \\to U$. $f^{-1}$ is of class $C^1$. $D(f^{-1}(y)) = (Df(x))^{-1}$ for all $y \\in W$ at $y = f(x)$. Visualization:\r#\r$y = f(x)$ maps from $U$ to $W$. $x = f^{-1}(y)$ gives the inverse mapping from $W$ back to $U$. Recall: Contraction Mapping Principle (CMP)\r#\rLet $\\mathbb{X}$ be a complete metric space and let\n$$ \\varphi: \\mathbb{X} \\to \\mathbb{X} $$\nbe a function satisfying a contraction condition for some constant $k$ with $0 \u0026lt; k \u0026lt; 1$:\n$$ d(\\varphi(x), \\varphi(y)) \\leq k \\cdot d(x,y), \\quad \\forall x,y \\in \\mathbb{X}. $$\nThen, there exists a unique fixed point $X^*$ such that:\n$$ \\varphi(X^) = X^. $$\nProof of the Inverse Function Theorem (IFT)\r#\rStep 1: Reductions\r#\r(a) May assume that the Jacobian matrix at $x_0$ is the identity: $$ D f(x_0) = I. $$ In fact, define the transformation: $$ T = D f(x_0). $$ Then, we can consider a new function:\n$$ \\tilde{f} = T^{-1} \\circ f. $$\nThus,\n$$ D(\\tilde{f})(x_0) = I. $$\n(b) Main assumption:\n$$ x_0 = f^{-1}(y_0). $$\nTo see this, define:\n$$ h(x) = f(x) - f(x_0). $$\nThen,\n$$ D h(x_0) = D f(x_0) - D f(x_0) = 0. $$\nIf $h^{-1}$ exists, then $y = f(x)$ can be solved as:\n$$ f(x) = h(x) + f(x_0) = y. $$\nThus, the inverse function satisfies:\n$$ x = x_0 - h^{-1}(y - f(x_0)). $$\nStep 2: Existence of the Inverse Function\r#\r(a) Setup: By the reduction above, we assume:\n$$ x_0 = 0, \\quad y_0 = f(x_0) = 0, \\quad D f(x_0) = I. $$\nNeed to show:\nThere exist neighborhoods $U$ and $W$ such that the mapping:\n$$ y = f(x): U \\to W $$\nhas an inverse function in $W$, meaning:\n$$ \\forall y \\in W, \\quad \\exists! x \\in U \\text{ such that } y = f(x). $$\nIllustration:\nA diagram representing $U$ mapping to $W$ via $f$, where $f$ is invertible.\nFor a fixed $y \\in \\mathbb{R}^n$, define:\r#\r$$ g_x = g(y) = y + x - f(x). $$\nWe need to show that $g_x$ has a unique fixed point.\n(b) Construction of neighborhoods $U$ and $W$\nLet:\n$$ g(x) = x - f(x). $$\nThen:\n$$ D g(x) = I - D f(x). $$\nSince:\n$$ D g(x_0) = I - I = 0, $$\nit follows that:\n$$ D g(x) \\text{ is close to zero}. $$\nThus, choosing:\n$$ \\epsilon = \\frac{1}{2n}, $$\nthere exists $\\delta \u0026gt; 0$ such that:\n$$ |x - x_0| \u0026lt; \\delta \\implies |D g_x(x)| \\leq \\frac{1}{2n}. $$\nApplying the Contraction Mapping Principle to $g_x$, we obtain:\n$$ \\exists x \\in B_{\\delta}(x_0) \\text{ such that } g_x(x) = x. $$\nThus:\n$$ g_x(x) = g_x(x_0) + D g_x(\\xi)(x - x_0), $$\nwhich shows:\n$$ D g_x(\\xi) (x - x_0). $$\nChapter 7: Inverse and Implicit Function Theorems\r#\rContraction Mapping Principle (CMP)\r#\rLet $\\mathbb{X}$ be a complete metric space, and let $\\varphi: \\mathbb{X} \\to \\mathbb{X}$ satisfy:\n$$ d(\\varphi(x), \\varphi(y)) \\leq k \\cdot d(x,y), \\quad 0 \u0026lt; k \u0026lt; 1. $$\nThen, there exists a unique fixed point $X^*$ such that $\\varphi(X^{*}) = X^{*}$.\nProof of the Inverse Function Theorem (IFT)\r#\rStep 1: Reduction\r#\rAssume $Df(x_0) = I$. Define $\\tilde{f} = Df(x_0)^{-1} \\circ f$, ensuring $D\\tilde{f}(x_0) = I$.\nFor $x_0 = f^{-1}(y_0)$, define $h(x) = f(x) - f(x_0)$. Since $Dh(x_0) = 0$, solving $f(x) = y$ reduces to:\n$$ x = x_0 - h^{-1}(y - f(x_0)). $$\nStep 2: Existence of the Inverse**\r#\rSet up: $x_0 = 0, y_0 = f(x_0) = 0, Df(x_0) = I$. Need to show a local inverse:\n$$ \\forall y \\in W, \\quad \\exists! x \\in U \\text{ such that } y = f(x). $$\nDefine:\n$$ g_x(y) = y + x - f(x). $$\nWe need to show $g_x$ has a unique fixed point.\nLet $g(x) = x - f(x)$, then $Dg(x) = I - Df(x)$. Since $Dg(x_0) = 0$, choosing $\\epsilon = \\frac{1}{2n}$ ensures $|D g_x(x)|$ is small. Applying CMP, we get:\n$$ \\exists x \\in B_{\\delta}(x_0) \\text{ such that } g_x(x) = x. $$ Thus, the inverse exists and is unique.\nbabeldown::deepl_translate_hugo( post_path = \u0026ldquo;content.en/docs/Mathematics/MATH 412 Real Analysis II/Lecture/7.1 Inverse Function Theorem.md\u0026rdquo;, target_lang = \u0026ldquo;ZH\u0026rdquo;, source_lang = \u0026ldquo;EN\u0026rdquo;, force = TRUE )\n"},{"id":44,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1.1-Inverse-FT-Proof/","title":"7.1.1 反函数定理（证明）","section":"第七章 逆函数和隐函数定理","content":"\r7.1* Implicit Function Theorem (IFT) Proof\r#\r1. Recall IFT\r#\rTheorem 7.1.1: Let $y = f(x): A \\subset \\mathbb{R}^n \\to \\mathbb{R}^n$ be of class $C^1$. Suppose $x_0 \\in A$ with: $$ J_f(x_0) = \\det(Df(x_0)) \\ne 0 $$ Then there exist neighborhoods $U$ of $x_0$ in $A$ and $W$ of $y_0 = f(x_0)$ such that:\n$f(U) = W$ and $f: U \\to W$ has an inverse $f^{-1}: W \\to U$. $f^{-1} \\in C^1$ (If $f \\in C^r$, then $f^{-1} \\in C^r$). $Df^{-1}(y) = [Df(x)]^{-1}$ for $x \\in U$ and $y = f(x)$. 2. Proof of Theorem 7.1.1\r#\rStep 1: Reduction\r#\rWe may assume $Df(x_0) = I$ and $x_0 = 0$, $y_0 = f(x_0)$.\nStep 2: Existence of inverse\r#\rConsider the function $g(x) = x - f(x)$.\nUsing continuity of $Dg(x)$ at $0$ and Mean Value Theorem, one can show there exists $\\delta \u0026gt; 0$ such that for $x \\in B(0, \\delta)$: $$ |g(x)| \\le \\frac{\\delta}{2} $$ Define $g: B(0, \\delta) \\to B(0, \\frac{\\delta}{2})$. Let $W = B(0, \\frac{\\delta}{2})$, and define: $$ U = { x \\in B(0, \\delta): f(x) \\in W } $$ Step 3: Existence of $f^{-1}: W \\to U$\r#\rFix $y \\in W$. Apply the Contraction Mapping Principle (CMP) to: $$ g_y(x) = y + x - f(x) = y + g(x) $$ Then $g_y(x): B(0, \\delta) \\to B(0, \\delta)$. Thus, there exists a unique $x \\in B(0, \\delta)$ such that: $$ g_y(x) = x \\quad \\Longrightarrow \\quad f(x) = y $$ Therefore, $\\exists! x \\in U$ such that $f(x) = y$.\nFix $y, y_1, y_2 \\in W$, let $x_i = f^{-1}(y_i), i = 1,2$. Then: $$ | f^{-1}(y_1) - f^{-1}(y_2) | = | x_1 - x_2 | = | g_{y_1}(x_1) - g_{y_2}(x_2) | $$\nSince $| Dg(x) | \\le \\frac{1}{2}$ for $x \\in B(0, \\delta)$, we get: $$ | x_1 - x_2 | \\le 2 | y_1 - y_2 | $$\nThus, $f^{-1}$ is Lipschitz continuous.\nStep 4: Differentiability of $f^{-1}$\r#\r(i) Observation: $[Df(x_0)]^{-1}$ exists and $Df(x)$ is continuous at $x_0$.\n$$ \\Rightarrow \\exists \\delta \u0026gt; 0 \\text{ such that } [Df(x)]^{-1} \\text{ exists and bounded by } M \\text{, } \\forall |x| \\leq \\delta $$ $$ | [Df(x)]^{-1} | \\leq M, \\quad \\forall x \\in B(0, \\delta) $$\n(ii) Show $f^{-1}$ is differentiable at any $y_* \\in W$ and: $$ Df^{-1}(y_0) = [Df(x_0)]^{-1}, \\quad \\text{where} \\quad y_0 = f(x_0) $$\nFix $y_* \\in W$. Then: $$ \\frac{| f^{-1}(y) - f^{-1}(y_) - [Df(x_0)]^{-1}(y - y_) |}{| y - y_* |} $$ can be simplified, and as $y \\to y_*$, it tends to $0$.\nThus, in conclusion, $f^{-1}(y)$ is differentiable at $y_* \\in W$ and: $$ Df^{-1}(y_) = [Df(x_)]^{-1} $$\nExample:\r#\rInvestigate the invertibility (both local and global) for the map: $$f \\in C^\\infty, \\quad A = \\mathbb{R}^2$$\n$$W = (u,v) = f(x,y): \\mathbb{R}^2 \\to \\mathbb{R}^2$$ Given by: $$ u = e^x\\cos y, \\quad v = e^x\\sin y$$\nCompute Jacobian determinant: $$ J_f(x,y) = \\det(Df(x,y)) = \\begin{vmatrix} e^x\\cos y \u0026amp; -e^x\\sin y \\\\ e^x\\sin y \u0026amp; e^x\\cos y \\end{vmatrix} = e^{2x} \u0026gt; 0 $$ Thus, by IFT, $f$ is invertible locally at any point and: $$ Df^{-1}(u,v) = [Df(x,y)]^{-1} = \\begin{bmatrix} e^x\\cos y \u0026amp; -e^x\\sin y \\\\ e^x\\sin y \u0026amp; e^x\\cos y \\end{bmatrix}^{-1} = \\begin{bmatrix} e^{-x}\\cos y \u0026amp; e^{-x}\\sin y \\\\ -e^{-x}\\sin y \u0026amp; e^{-x}\\cos y \\end{bmatrix} $$\nHowever, $f$ is not globally invertible (not injective). Consider: $$ \\begin{aligned} f(x_0, y_0 + 2\\pi) \u0026amp;= (e^{x_0}\\cos(y_0 + 2\\pi), e^{x_0}\\sin(y_0 + 2\\pi))\\\\ \u0026amp;= (e^{x_0}\\cos y_0, e^{x_0}\\sin y_0)\\\\ \u0026amp;= (u_0, v_0) \\end{aligned} $$\nIn complex notation, $f$ can be written as: $$ f(z) = e^z = e^{x+iy} = e^x e^{iy} = e^x(\\cos y + i \\sin y) $$ with $u = e^x \\cos y$, $v = e^x \\sin y$.\nConclusion\r#\rSince $f(x, y)$ maps points periodically in $y$, it is not globally injective, despite being locally invertible.\nAdditional Notes\r#\rThe periodic nature is reflected in the mapping: $$f(x_0, y_0 + 2\\pi) = f(x_0, y_0)$$ This demonstrates that multiple points in the domain map to the same point in the range, confirming non-injectivity.\n"},{"id":45,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.2-Implicit-Function-Theorem/","title":"7.2 隐函数定理","section":"第七章 逆函数和隐函数定理","content":"111\n"},{"id":46,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1-Boundary-Value-Problems/","title":"9.1 边值问题的近似","section":"第九章","content":"\r1.1 Set-up: String with fixed endpoints\r#\r我们可以写 $$ \\begin{align} -\\frac{d^2u}{dx^2} \u0026amp;= f(x), \\quad x \\in (0,1) \\ u(0) \u0026amp;= \\alpha, \\quad \\frac{du}{dx}(0) = \\beta \\end{align} $$\n记$w = \\frac{du}{dx}$，那么 $\\frac{dw}{dx} = f(x)$。因此，我们知道\n$$\\frac{d}{dt} = \\begin{bmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} w \\ u \\end{bmatrix}$$\n定义\r#\r边值问题（boundary-value problem）的定义为\n[!definition|*] $$\\begin{align} -\\mu\\frac{d^2u}{dx^2} \u0026amp;= f(x), \\quad x\\in (0,1), \\quad \\mu \u0026gt; 0 \\ u(0) \u0026amp;= \\alpha, \\quad u(1) = \\beta \\end{align} $$\n1.2 泊松方程（Poisson Equation）\r#\r此类方程的二维形态为：\n$$\\begin{align} -\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) \u0026amp;= f(x,y) \\ u|_{\\text{boundary of }\\Omega} \u0026amp;= 0 \\end{align}$$\n用拉普拉斯算子来表示：\n$$\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = \\Delta u, \\quad \\text{where }\\Delta = \\nabla^2 u = \\sum_{i=1}^{n} \\frac{\\partial^2}{\\partial x_i^2}$$\n所以，广义的泊松方程可以写为\n[!definition] A general ($n$-dimentional) poisson equation is written as $$\\Delta u = f(\\mathbf{x})$$ where $\\mathbf{x}=(x_{1},x_{2},x_{3}\\dots x_{n})$.\n1.3 Back to the String Example: How can we get a BVP?\r#\r考虑以下经典力学中很常见的弦的能量泛函（energy functional）：\n$$J(u) = \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx - \\int_0^1 f \\cdot u , dx \\quad \\text{(Energy of string)}$$\nBoundary condition: $u(0) = u(1) = 0$.\n作为自然界的基本趋势中，最小作用量原理解释了自然系统倾向于采取能量消耗最小的路径或状态，也就是说，自然会沿着$\\min J(u)$的路径发展。\n最简单的方法是直接用 欧拉-拉格朗日方程（Euler–Lagrange equation）搞定，但是这毕竟是个数学课，那么我们用最暴力的原始方法解决：\n大致思路为：\n把 $u$ 加入扰动（perturbation）变成 $u+εv$： $$u_{\\epsilon}(x) = u(x) + \\epsilon v(x)$$\n$J(u+εv)$ 进行显式展开： $$J(u + \\epsilon v) = J(u) + \\epsilon \\underbrace{\\delta J(u; v)}{\\text{一阶变分}} + \\frac{1}{2} \\epsilon^2 \\underbrace{\\delta^2 J(u; v)}{\\text{二阶变分}} + \\cdots$$\n在变分法或力学的语言里：通常是指在能量或作用量（action）等泛函意义下的驻点（stationary point）：也就是对任意“小扰动” $εv$，该函数 $u$ 都使得泛函的一阶变化量为 0。\n求导，来找$J$的最小值 $$\\lim_{\\varepsilon \\to 0} \\frac{J(u + \\varepsilon v) - J(u)}{\\varepsilon} = 0, \\quad \\varepsilon \\in \\mathbb{R}$$\n显然易见：\n$$ \\begin{align} \u0026amp;\\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx} + \\varepsilon\\frac{dv}{dx}\\right)^2 dx - \\int_0^1 f(u+\\varepsilon v) , dx - \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx - \\int_0^1 f \\cdot u , dx \\ \u0026amp;= \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx + \\frac{1}{2} \\cdot 2\\varepsilon \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx + \\frac{1}{2}\\varepsilon^2 \\int_0^1 \\mu\\left(\\frac{dv}{dx}\\right)^2 dx \\ \u0026amp;\\quad - \\int_0^1 f \\cdot u , dx - \\varepsilon\\int_0^1 f \\cdot v , dx - \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx - \\int_0^1 f \\cdot u , dx \\ \u0026amp;= \\varepsilon \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx + \\frac{1}{2}\\varepsilon^2 \\int_0^1 \\mu\\left(\\frac{dv}{dx}\\right)^2 dx - \\varepsilon\\int_0^1 f \\cdot v , dx\\end{align} $$\n这实际上正是欧拉–拉格朗日方程最早的“原始变分法”推导，也正是 E-L 方程的来龙去脉。只不过 E-L 方程把这个过程“公式化”了，让我们不必每次都展开一大堆项、再分部积分去凑出那个通用形式。\n然后\n$$\\frac{J(u+\\varepsilon v) - J(u)}{\\varepsilon} = \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx + \\frac{1}{2} \\varepsilon \\int_0^1 \\mu\\left(\\frac{dv}{dx}\\right)^2 dx - \\int_0^1 f \\cdot v , dx$$\n极限为：\n$$\\lim_{\\varepsilon \\to 0} \\frac{J(u+\\varepsilon v) - J(u)}{\\varepsilon} = \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx - \\int_0^1 f \\cdot v , dx = 0$$\n一个“平衡解”（equilibrium solution）。\n变分形式或弱形式（Variational/Weak）:\r#\r由此我们得到了一个泛函的“变分条件”：\n[!claim|*] $$\\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx - \\int_0^1 f \\cdot v , dx = 0$$\n分部积分:\n$$ \\begin{align} \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx \u0026amp;= \\mu\\left[\\frac{du}{dx}v\\right]_0^1 - \\mu\\int_0^1 \\frac{d^2u}{dx^2} v , dx \\ \u0026amp;= -\\mu\\int_0^1 \\frac{d^2u}{dx^2} v , dx \\end{align} $$\nSince $v(0) = v(1) = 0$\n$$\\frac{d}{dx}\\left(\\frac{du}{dx}\\right) = \\frac{d^2u}{dx^2}$$\n$$\\int \\frac{dv}{dx} , dx = v$$\n边界项因为BC而消失，所以弱形式$\\rightarrow$强形式:\n$$-\\mu\\int_0^1 \\frac{d^2u}{dx^2} v , dx - \\int_0^1 f \\cdot v , dx = 0$$\n$$-\\int_0^1 \\left(\\mu\\frac{d^2u}{dx^2} + f\\right) \\cdot v , dx = 0$$\nWe want it to be true $\\forall v$. So, it must be: $$\\mu\\frac{d^2u}{dx^2} + f = 0$$\n我们得到一个常见的附带边界条件的强形式常微分方程(ODE)。\n[!claim|*] We obtain a Boundary Value Problem (BVP): $$ \\begin{align} \\mu u\u0026rsquo;\u0026rsquo;(x) +f\u0026amp;= 0 \\ u(0) \u0026amp;= u(1) = 0 \\end{align} $$\nAssumption: $u$ is at least $C^2$.\n1.4 两种表述边值问题（BVP）的方式\r#\r寻找函数 $u$，使得对所有满足 $v(0) = v(1) = 0$ 的 $v$，均有 $$ \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx ;=; \\int_0^1 f \\cdot v , dx $$\n$\\Rightarrow$ $u$ 只需保证“一阶可微” $\\Rightarrow$ 通常采用 有限元法 (Finite Element) 寻找函数 $u$，使得 $$ \\begin{aligned} -\\mu \\frac{\\partial^2 u}{\\partial x^2} \u0026amp;= f, \\quad x \\in (0,1),\\ u(0) \u0026amp;= 0,\\quad u(1) = 0 \\end{aligned} $$\n$\\Rightarrow$ $u$ 需要至少“二阶可微” $\\Rightarrow$ 通常采用 有限差分法 (Finite Difference) "},{"id":47,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.2-Finite-Difference/","title":"9.2 有限差分法","section":"第九章","content":"\r2.1 边值问题（BVP）的有限差分:\r#\r[!claim|*] Consider the Boundary-Value Problem: $$ \\begin{align} -\\mu\\frac{d^2u}{dx^2} \u0026amp;= f, \\quad x \\in (0,1) \\ u(0) \u0026amp;= u(1) = 0 \\end{align} $$ with discrete points: $$0 = x_0 \u0026lt; x_1 \u0026lt; \\cdots \u0026lt; x_{N} = 1\\quad\\Longrightarrow \\quad-\\mu\\frac{d^2u}{dx^2}(x_i) = f(x_i)$$\n2.2 推导二阶中心差分近似法\r#\r2.2.1 Poisson 微分方程\r#\r对与任意一个离散的点$x_{i}$，我们首先在网格点 $x_{i+1} = x_i + \\Delta x$ 和 $x_{i-1} = x_i - \\Delta x$ 处对函数 $u(x)$ 进行泰勒级数展开：\n$$ \\begin{align} u(x_{i+1}) \u0026amp;= u(x_i) + \\frac{du}{dx}(x_i)\\Delta x + \\frac{1}{2}\\frac{d^2u}{dx^2}(x_i)\\Delta x^2 + \\cdots \\ u(x_{i-1}) \u0026amp;= u(x_i) - \\frac{du}{dx}(x_i)\\Delta x + \\frac{1}{2}\\frac{d^2u}{dx^2}(x_i)\\Delta x^2 + \\cdots \\end{align} $$\n当我们将这两个方程相加时，由于奇数阶导数项的符号相反，它们会相互抵消：\n$$ \\begin{align} u(x_{i+1}) + u(x_{i-1}) \u0026amp;= 2u(x_i) + 2\\left(\\frac{1}{2}\\frac{d^2u}{dx^2}(x_i)\\Delta x^2\\right) + 2\\left(\\frac{1}{24}\\frac{d^4u}{dx^4}(x_i)\\Delta x^4\\right) + \\mathcal{O}(\\Delta x^6) \\ \\ \u0026amp;= 2u(x_i) + \\frac{d^2u}{dx^2}(x_i)\\Delta x^2 + \\frac{1}{12}\\frac{d^4u}{dx^4}(x_i)\\Delta x^4 + \\mathcal{O}(\\Delta x^6) \\end{align} $$\n接着, 我们重新整理方程以分离出二阶导数项（舍去高阶项）\n$$\\begin{align} \\frac{d^2u}{dx^2}(x_i)\\Delta x^2 \u0026amp;= u(x_{i+1}) + u(x_{i-1}) - 2u(x_i) - \\frac{1}{12}\\frac{d^4u}{dx^4}(x_i)\\Delta x^4 + \\mathcal{O}(||\\Delta x||^6)\\ \\frac{d^2u}{dx^2}(x_i) \u0026amp;= \\frac{u(x_{i+1}) + u(x_{i-1}) - 2u(x_i)}{\\Delta x^2} - \\frac{1}{12}\\frac{d^4u}{dx^4}(x_i)\\Delta x^2 + \\mathcal{O}(||\\Delta x||^2) \\ \u0026amp;\\approx \\boxed{ \\frac{u(x_{i+1}) + u(x_{i-1}) - 2u(x_i)}{\\Delta x^2}} \\end{align}$$\n记 $u_i = u(x_i)$ 与 $f_i = f(x_i)$. 因此我们得到微分方程\n[!claim|*] $$-\\mu\\frac{d^2u}{dx^2}(x_i) = -\\mu\\frac{u_{i+1} + u_{i-1} - 2u_i}{\\Delta x^2} = f_i$$\n截断误差（Truncation Error）为 $\\mathcal{O}(\\Delta x^2)$ 这证实了该近似是二阶精度 （second-order accuracy） 2.2.2 构建线性系统\r#\r用这种离散化方法推导出一个线性方程组（linear system）:\n$$Au = f$$\nwhere $A$ is given by:\n$$A = \\frac{\\mu}{\\Delta x^2} \\begin{bmatrix} 2 \u0026amp; -1 \u0026amp; 0 \u0026amp; \u0026amp; \\ -1 \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \u0026amp; \\ 0 \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; -1 \\ \u0026amp; \u0026amp; \u0026amp; -1 \u0026amp; 2 \\end{bmatrix} $$\n2.3 What is the Accuracy of FD?\r#\r矩阵A的关键性质 (Key Properties of Matrix A)\r#\r从差分离散化得到的matrix $A$有几个重要性质：\n正定性 (Positive Definiteness)：$x^TAx \u0026gt; 0 \\quad \\forall x \\neq 0$ $\\Longrightarrow$ solvable。 对称性 (Symmetry)：Symmetry $\\Longrightarrow \\forall ,\\lambda \\in \\mathbb{R}$ 特征值性质 (Eigenvalue Properties)：Non-singular 条件数关系 (Condition Number Relation)：A的最小特征值与最大特征值之比与Δx成正比，即$$\\frac{\\lambda_{min}}{\\lambda_{max}} \\propto \\Delta x$$ 误差\r#\r当我们解离散系统（$Au = f$）时，精确解$u_{ex}$与近似解$u$之间存在误差，所以$Au_{ex} \\neq f$。\n精确关系实际上为：\n[!claim|*] $$Au_{ex} = f + T$$ where $T_i = C(x_i)\\Delta x^2$ is truncation error\n其中$C(x_i)$与四阶导数相关：$$C(x_i) = C\\frac{d^4u}{dx^4}(x_i)$$\n误差方程 (Error Equation)\r#\r若定义误差$e = u_{ex} - u$，则 $$Ae = T$$\n$$\\Longrightarrow e = A^{-1}T$$\n因此\n$$||e|| = ||A^{-1}T|| \\leq ||A^{-1}|| \\cdot ||T||$$\n收敛性证明 (Convergence Proof)\r#\r为了证明方法收敛，需要满足两个条件：1. 稳定性：$A^{-1}$有界 (Boundedness of $A^{-1}$) 2. 一致性：截断误差趋零 (Truncation Error Tends to Zero)\n[!lemma|*] Conditions to show convergence: $$||A^{-1}|| \u0026lt; \\infty \\quad \\text{and} \\quad ||T|| \\to 0 \\quad \\text{as} \\quad \\Delta x \\to 0$$\n关于 $|A^{-1}|$ 的有界性\r#\r矩阵的条件数定义为： $$\\kappa(A) = |A| \\cdot |A^{-1}|=\\frac{\\lambda_{max}}{\\lambda_{min}}$$\n$\\frac{\\lambda_{min}}{\\lambda_{max}} \\propto \\Delta x \\longrightarrow\\kappa(A) \\propto \\frac{1}{\\Delta x}$ hence, $||A^{-1}||$ is bounded, regardless of $\\Delta x$. 关于$T$的一致性\r#\r因为截断误差范数$|T| \\sim \\Delta x^2$，所以\n$$|T| \\to 0 ,,\\text{as},, \\Delta x \\to 0$$\n这也意味着：\n$$|e| \\to 0 ,,\\text{as},, \\Delta x \\to 0$$\n因此，该有限差分方法是收敛的 (convergent) 虽然$\\kappa$随 $\\Delta x$ 变化，但 $|A^{-1}|$ 的增长被 $|T|$ 的更快减小所抵消。\n实际意义？\r#\r该有限差分法随着网格间距 (grid spacing) 减小而收敛到精确解 收敛速率 (convergence rate) 是$O(\\Delta x^2)$，即二阶精度 (second-order accuracy) 误差主要受控于四阶导数的大小和网格间距的平方 这解释了为什么在实际计算中，当我们将网格间距减半时，误差大约会减小到原来的四分之一 这种数学证明为我们使用有限差分法求解微分方程提供了理论保障，确保了在足够细的网格下，数值解 (numerical solution) 会以可预测的速率接近真实解 (exact solution)。\n"},{"id":48,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.3-Advection-Diffusion-Equation/","title":"9.3 对流-扩散方程","section":"第九章","content":"\r3.1 对流-扩散方程\r#\r对流-扩散方程是一种描述物质或热量在流体中同时受到对流（也称为平流）和扩散作用影响的偏微分方程。这个方程在流体力学、传热学和物质传输等领域有广泛应用。\n$$ \\begin{align} -\\mu\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} \u0026amp;= f \\ \\text{diffusion} \\quad \u0026amp; \\text{advection} \\end{align} $$\n$$u(0) = u_L, \\quad u(1) = u_R$$\n3.1.1 离散化过程：\r#\r一阶导数：对流项 （Advection）\r#\r泰勒展开（向前）\r#\r$$ \\begin{align}\nu(x_{j+1}) \u0026amp; = u(x_j) + \\frac{\\partial u}{\\partial x}(x_j)\\Delta x + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x^2 \\ \u0026amp;\\quad\\quad\\quad\\quad-\\frac{1}{6}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^3 + \\frac{1}{12}\\frac{\\partial^4 u}{\\partial x^4}(x_j)\\Delta x^4 + \\mathcal{O}(||x||^5)\\ \\frac{\\partial u}{\\partial x}(x_j)\\Delta x \u0026amp; = u(x_{j+1}) - u(x_j) + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x^2\\ \\frac{\\partial u}{\\partial x}(x_j) \u0026amp; = \\frac{u_{j+1} - u_j}{\\Delta x} + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x \\end{align}$$\n泰勒展开（向后）\r#\r$$u(x_{j-1}) = u(x_j) - \\frac{\\partial u}{\\partial x}(x_j)\\Delta x + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x^2 - \\frac{1}{6}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^3 + \\frac{1}{12}\\frac{\\partial^4 u}{\\partial x^4}(x_j)\\Delta x^4 + \\mathcal{O}(||x||^5)$$\n我们取(forward) - (backward)两者差值\n$$u(x_{j+1}) - u(x_{j-1}) = 2\\frac{\\partial u}{\\partial x}(x_j)\\Delta x + \\frac{1}{3}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^3 + \\mathcal{O}(||\\Delta x||^5)$$\n接着\n$$\\frac{\\partial u}{\\partial x}(x_j) = \\frac{u(x_{j+1}) - u(x_{j-1})}{2\\Delta x} - \\frac{1}{6}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^2 + \\mathcal{O}\\left(\\frac{||\\Delta x||^4}{2}\\right)$$\n二阶导数：扩散项（Diffusion）\r#\r这都还不会吗？退群吧。\n对流-扩散离散方程\r#\r[!claim|*] Final numerical solution: $$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = f_j$$ with second-order accuracy: $$\\sim \\mathcal{O}(\\Delta x^2)$$\n3.2 例子：精确解与数值解在对流主导问题中的差异\r#\r我们现在考虑一个特殊情况的对流-扩散方程，其中源项（Source Term）为零，具有以下边界条件：\n$$ \\begin{align} -\\mu\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} \u0026amp;= 0 \\ u(0) \u0026amp;= 0 \\ u(a) \u0026amp;= 1 \\end{align} $$\n3.2.1 精确解\r#\r这个方程的精确解是：\n$$u_{ex} = \\frac{e^{\\frac{\\beta}{\\mu}x} - 1}{e^{\\frac{\\beta}{\\mu}a} - 1}$$\n这个解的特点是：当比值 $\\frac{|\\beta|}{\\mu} \\gg 1$ 时（即advection远大于diffusion），解在边界 $x=a$ 附近会形成一个陡峭的边界层。这被称为**\u0026ldquo;对流主导问题\u0026rdquo;（advection-dominated problem）**。\n在对流主导的情况下，解在大部分区域接近于0，只在接近 $x=a$ 的小区域内快速上升到1。这对数值方法的解法是很大的麻烦。\n3.2.2 数值解\r#\r当我们使用标准的中心差分（central difference）方法离散化这个方程时：\n$$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = 0$$\n重新整理这个方程：\n$$\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)u_{i+1} + \\left(\\frac{2\\mu}{\\Delta x^2}\\right)u_i - \\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)u_{i-1} = 0$$\n数值实验表明，当 $|\\beta|$ 较大时，数值解会出现不一致，非物理的振荡。为什么呢？\n数学解释\r#\r为了理解这一现象，我们可以对差分方程进行深入分析。我们假设差分方程的解具有形式 $u_j = C\\rho^j$，其中 $C$ 是常数，$\\rho$ 是待定参数。将这个假设代入到差分方程中：\n$$\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)C\\rho^{j+1} + \\left(\\frac{2\\mu}{\\Delta x^2}\\right)C\\rho^j - \\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)C\\rho^{j-1} = 0$$\n消去 $C$ 并整理：\n$$\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)\\rho^2 + \\left(\\frac{2\\mu}{\\Delta x^2}\\right)\\rho - \\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right) = 0$$\n这是关于 $\\rho$ 的二次方程，它有两个解 $\\rho_1$ 和 $\\rho_2$。差分方程的一般解是这两个特解的线性组合：\n$$u_j = C_1\\rho_1^j + C_2\\rho_2^j$$\n其中 $C_1$ 和 $C_2$ 是由边界条件确定的常数。\n振荡解的条件\r#\r根据二次方程的性质，两个根的乘积等于常数项与二次项系数的比值：\n$$\\rho_1\\rho_2 = \\frac{-\\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)}{\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)} = \\frac{1 + \\frac{\\beta\\Delta x}{2\\mu}}{1 - \\frac{\\beta\\Delta x}{2\\mu}}$$\n这里引入了一个重要的无量纲参数，称为网格佩克莱数（Grid Péclet number）：\n$$\\text{Pe} = \\frac{|\\beta|\\Delta x}{2\\mu}$$\n佩克莱数表示对流传输与扩散传输的相对强度。\n当 $\\text{Pe} \u0026gt; 1$（即 $\\frac{|\\beta|\\Delta x}{2\\mu} \u0026gt; 1$）时，我们有 $\\rho_1\\rho_2 \u0026lt; 0$，这意味着两个根一正一负。当一个解包含负的幂时，会导致解在空间上呈现振荡特性，这与物理直觉相违背，因为扩散过程应该是平滑的。\n物理解释与改进方法\r#\r为什么会出现振荡？\r#\r物理角度看：信息主要沿着流动方向传播。Central Difference 方法对上游和下游的信息给予相同权重，所以对流主导的情况下不合适，除非极细的网格才能准确解析。即使数值方法在数学上具有二阶精度，其准确度依旧是要取决于特定的物理问题中。理解数值方法的稳定性条件才可以选择合适的求解策略。\n解决方案？\r#\r网格细化：最直接的方法是减小 $\\Delta x$，使 $\\text{Pe} \u0026lt; 1$。但这会大大增加计算成本。\n迎风方法：（详细见下文）使用偏向上游的差分格式，如前向或后向差分，取决于 $\\beta$ 的符号。例如，当 $\\beta \u0026gt; 0$ 时，可以使用： $$\\frac{\\partial u}{\\partial x}(x_j) \\approx \\frac{u_j - u_{j-1}}{\\Delta x}$$\n人工扩散：增加一个数值扩散项，使有效的佩克莱数小于1。\n高阶格式：使用更高阶的差分格式，如QUICK、TVD或ENO/WENO方案，这些方法可以更好地捕捉强梯度区域。\n3.3 另一种方法：迎风法（Upwind Method）\r#\r3.3.1 信息流动分析 (Information Flow)\r#\r就像刚刚的对流主导问题，现实中经常存在明确的物理信息流动方向，使得这个问题本质上是非对称的。\n对于对流项（advection），当流动方向已知时\n[!assumption|*] $\\beta \u0026gt; 0$，meaning that the information flows from left to right.\n我们可以使用式子：\n[!claim|*] $$\\beta\\frac{\\partial u}{\\partial x} \\approx \\beta\\frac{u_i - u_{i-1}}{\\Delta x} \\quad (\\beta \u0026gt; 0) \\quad \\text{(upwind)}$$\n这种差分格式考虑了信息流动的方向，使用\u0026quot;上游\u0026quot;的节点来计算导数，而不是像中心差分那样平等对待上下游节点（注意这个只有一阶精度）。\n3.3.2 迎风法的稳定性分析 (Stability Analysis)\r#\r接下来，我们来证明迎风方法是稳定的。将迎风差分重写为：\n$$\\beta\\frac{u_i - u_{i-1}}{\\Delta x} = \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\beta\\frac{u_{i+1}}{2\\Delta x} - \\beta\\frac{2u_i}{2\\Delta x} + \\beta\\frac{u_{i-1}}{2\\Delta x}$$\n整理一下上式：\n$$= \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\frac{\\beta\\Delta x}{2} \\cdot \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}$$\n这个表达式可以分解为两项：\n中心差分项 (Central mean): $\\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x}$ 二阶导数近似项 (Approximation of 2nd derivative): $-\\frac{\\beta\\Delta x}{2} \\cdot \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}$ 这说明迎风方法等价于对原始方程进行中心差分，但增加了一个额外的扩散项（人工扩散，artificial diffusion）。\n等价表述：扰动方程 (Perturbed Equation)\r#\r因此，我们可以将原始问题的迎风方法看作是下面这个扰动方程的中心差分解法：\n$$-\\left(\\mu + \\frac{|\\beta|\\Delta x}{2}\\right)\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} = 0$$\n对这个扰动方程应用中心差分近似：\n$$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = 0$$\n这正是原始问题的迎风解法。换句话说：\n对扰动问题使用中心差分 (Central for Perturbed) = 对原始问题使用迎风差分 (Upwind for Original) [!claim|*] $$\\text{Central (Perturbed) }= \\text{Upwind (Original)}$$\n这个等价关系揭示了迎风法的本质：它隐含地向原始方程中添加了一个数值扩散项。这个额外的扩散项是迎风法能够抑制数值振荡的关键原因。\n佩克莱数分析 (Péclet Number Analysis)\r#\r回顾一下佩克莱数的定义：$\\text{Pe} = \\frac{|\\beta|\\Delta x}{2\\mu}$。\n对于扰动系统，扩散系数变为 $\\mu^* = \\mu + \\frac{|\\beta|\\Delta x}{2} = \\mu(1 + \\text{Pe})$。\n扰动系统的佩克莱数为：\n$$\\text{Pe}^* = \\frac{|\\beta|\\Delta x}{2\\mu^*} = \\frac{|\\beta|\\Delta x}{2\\mu(1+\\text{Pe})} = \\frac{\\text{Pe}}{1+\\text{Pe}} \u0026lt; 1 \\quad \\forall |\\beta| \\text{ 和 } \\Delta x$$\n这表明无论 $|\\beta|$ 和 $\\Delta x$ 的值如何，扰动系统的佩克莱数永远小于1，因此迎风方法始终是稳定的。\n一致性分析 (Consistency Analysis)\r#\r当 $\\Delta x \\to 0$ 时，$\\mu^* \\to \\mu$，扰动方程趋近于原始方程，这保证了方法的一致性。\n对于扰动系统，我们使用了二阶精度的中心差分方法，但对于原始问题，由于引入了人工扩散项，它只是一阶精度的方法。\n详细解释与物理意义\r#\r迎风方法的核心思想是考虑物理信息的传播方向。在流体流动中，当某点的特性（如温度、浓度）受到上游点的影响更大时，迎风方法使用上游点来计算导数，从而更好地反映物理现实。\n从数值分析的角度看，迎风方法引入了\u0026quot;人工扩散\u0026quot;(artificial diffusion)，增强了数值方法的稳定性。这种人工扩散恰好能够抵消中心差分在高佩克莱数情况下产生的数值振荡。\n然而，这种稳定性是以精度为代价的——迎风方法的精度降低到一阶（误差与 $\\Delta x$ 成正比），而中心差分是二阶精度（误差与 $\\Delta x^2$ 成正比）。这在数值方法中是一个常见的权衡：更高的稳定性往往伴随着更低的精度。\n在对流主导的问题中，稳定性通常比高精度更重要，因为不稳定的解会产生严重的非物理振荡，使结果完全无用。因此，对于高佩克莱数流动，迎风方法尽管精度较低，但往往是更实用的选择。\n更高阶的方法，如TVD (Total Variation Diminishing)、ENO (Essentially Non-Oscillatory) 和 WENO (Weighted Essentially Non-Oscillatory) 方案，试图在保持稳定性的同时提高精度，但它们的实现更为复杂。\nOur previous computation relies on symmetry. However, there is a clear physical information flow. So, this problem is asymmetric in reality. We don\u0026rsquo;t want as fancy as $\\sim \\mathcal{O}(\\Delta x^2)$ solutions, but we can use a $\\sim \\mathcal{O}(\\Delta x)$ method.\n$$\\beta\\frac{\\partial u}{\\partial x} \\approx \\beta\\frac{u_i - u_{i-1}}{\\Delta x} \\quad (\\beta \u0026gt; 0) \\quad \\text{(upwind)}$$\nNow, let\u0026rsquo;s show (upwind) is stable.\n$$\\beta\\frac{u_i - u_{i-1}}{\\Delta x} = \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\beta\\frac{u_{i+1}}{2\\Delta x} - \\beta\\frac{2u_i}{2\\Delta x} + \\beta\\frac{u_{i-1}}{2\\Delta x}$$\n$$= \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\frac{\\beta\\Delta x}{2} \\cdot \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}$$\nCentral mean $\\quad$ Approx. of 2nd derivative\nSo, we can consider the equation: $$-\\left(\\mu + \\frac{|\\beta|\\Delta x}{2}\\right)\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} = 0$$\nApply a central approximation: $$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = 0$$\nUpwind solution of the original problem.\nRecall Péclet: $Pe = \\frac{|\\beta|\\Delta x}{2\\mu}$. Then, $\\mu^* = \\mu(1 + Pe)$.\nPéclet of this perturbed system: $$Pe^* = \\frac{|\\beta|\\Delta x}{2\\mu^*} = \\frac{|\\beta|\\Delta x}{2\\mu(1+Pe)} = \\frac{Pe}{1+Pe} \u0026lt; 1 \\quad \\forall |\\beta| \\text{ and } \\Delta x$$\nSo, this upwind method is always stable.\nConsistency: when $\\Delta x \\to 0$, $\\mu^* \\to \\mu$.\nFor the perturbed system, we have a 2nd order approach, but with the original problem, it is only a 1st order method.\n3.4 Design a Better Method\r#\r$$\\mu^{smart} = \\mu(1 + \\Phi(Pe))$$\ns.t.\n$\\Phi(Pe) \\to 0 \\quad \\text{as} \\quad \\Delta x \\to 0$ $Pe^{smart} = \\frac{|\\beta|\\Delta x}{2\\mu^{smart}} \u0026lt; 1$ Our upwind method takes $\\Phi(Pe) = Pe \\sim \\mathcal{O}(\\Delta x)$. But can we take some $\\Phi(Pe) \\sim \\mathcal{O}(\\Delta x^2)$?\n$\\Rightarrow$ Scharfetter-Gummel Method: $\\Phi(Pe) = Pe - 1 + \\frac{2Pe}{e^{2Pe} - 1}$\n$\\Phi(Pe) \\uparrow$\n$\\Phi(Pe) = Pe$\n$\\Phi(Pe) \\sim \\mathcal{O}(\\Delta x)$\nThe worst case order of Scharfetter-Gummel is $\\sim \\mathcal{O}(\\Delta x^2)$.\nScharfetter-Gummel is also a special $\\Phi(Pe)$ that produces exact solutions.\n"},{"id":49,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.4-2D-Problem/","title":"9.4 4.1 二维（2D）偏微分方程问题","section":"第九章","content":"\r椭圆型\r#\r4.1.1 问题设定\r#\r我们考虑一个二维空间的问题，记为区域 $\\Omega$。边界记为 $\\partial\\Omega$。\n方程一般形式如下： $$-\\mu \\Delta u + \\beta \\cdot \\nabla u = f,\\quad \\text{在}\\ \\Omega 内$$\n边界条件为： $$u(\\partial \\Omega) = d \\quad (\\text{给定的数据})$$\n这里的符号解释（见上一章）： $\\beta$：表示\u0026quot;风\u0026quot;或者对流的方向和强度 $\\mu$：扩散系数 $f$：外力或源项 因此，上面的方程可以展开写成：\n$$-\\mu \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right)\n\\beta_x \\frac{\\partial u}{\\partial x} + \\beta_y \\frac{\\partial u}{\\partial y} = f(x,y)$$ 4.1.2 简化情形（只有扩散，没有风）\r#\r首先考虑更简单的情况，忽略对流（即“关掉风”），变成纯扩散问题：\n$$-\\mu\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = f(x,y)$$\n4.1.3 离散化方法（有限差分法）\r#\r用有限差分法来数值求解：\n假设空间被划分为一个网格，每个网格点用坐标 $(i,j)$ 来表示位置：\n在$x$方向的间距为 $\\Delta x$ 在$y$方向的间距为 $\\Delta y$ 则对于二维的拉普拉斯算子，常用的中心差分格式为：\n$$\\frac{\\partial^2 u}{\\partial x^2}\\approx \\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{\\Delta x^2}$$\n$$\\frac{\\partial^2 u}{\\partial y^2}\\approx \\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{\\Delta y^2}$$\n代入扩散方程得到：\n$$-\\mu\\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{\\Delta x^2} -\\mu\\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{\\Delta y^2} = f(x_i,y_j)$$\n4.1.4 形成线性代数方程组\r#\r所有网格点的未知数$u_{i,j}$放到一个向量中（向量记为$u$），这样就可以把离散后的方程写成一个矩阵方程：\n$$A u = b$$\n这里：\n$A$ 是系数矩阵（稀疏、对称、正定，简称SPD） $u$ 是未知量向量（所有网格点的解） $b$ 是已知项的向量（源项$f$和边界条件的组合） 4.2 时间相关问题：抛物型（Parabolic）\r#\r考虑的问题形式：\n$$\\frac{\\partial u}{\\partial t}-\\mu\\frac{\\partial^2 u}{\\partial x^2}=f,\\quad x\\in(0,1),\\quad 0\u0026lt;t\u0026lt;T$$\n初值与边界条件为：\n初值：$u(x,t=0)=u_0(x)$ 边界条件：$u(0,t)=u_L(t),\\quad u(1,t)=u_R(t)$ 半离散化方法（空间离散，时间连续）\r#\r我们首先只对空间（$x$）做离散化，得到：\n$$\\frac{d u_j(t)}{d t}-\\mu\\frac{u_{j+1}(t)-2u_j(t)+u_{j-1}(t)}{\\Delta x^2}=f_j(t)$$\n记：\n向量形式：$u(t)=[u_1(t),u_2(t),\u0026hellip;,u_n(t)]^T$ 源项向量：$f(t)=[f_1(t),f_2(t),\u0026hellip;,f_n(t)]^T$ 系数矩阵：$A=\\frac{\\mu}{\\Delta x^2}\\text{tridiag}(-1,2,-1)$ 于是问题变成常微分方程（ODE）的系统形式：\n$$\\frac{d u}{d t}-A u=f$$\n时间离散化（ODE方法）\r#\r接下来我们对时间进行离散化，采用两种方法：\n方法1：显式欧拉（Explicit Euler, FE）\r#\r将时间导数在时间点$t_n$近似为：\n$$\\frac{u^{n+1}-u^n}{\\Delta t}=f^n+A u^n$$\n解得：\n$$u^{n+1} = (I+\\Delta t A)u^n+\\Delta t f^n$$\n显式方法容易计算，但稳定性有限，时间步长不能太大。\n方法2：隐式欧拉（Implicit Euler, IE/BE）\r#\r另一种方法是在时间点$t_{n+1}$处求导：\n$$\\frac{u^{n+1}-u^n}{\\Delta t}=f^{n+1}+A u^{n+1}$$\n解出隐式方程：\n$$(I-\\Delta t A)u^{n+1}=u^n+\\Delta t f^{n+1}$$\n这个方法更稳定，但需要在每个时间步解一个线性系统。\n关于矩阵性质的总结：\r#\r$A$ 为SPD矩阵（对称正定） 当 $A$ 和 $b$ 是与时间无关时，通常我们更喜欢隐式方法，因为可以分解矩阵一次（如LU分解）并反复使用，加快计算速度。 总结一下：\r#\r以上涉及了两类偏微分方程问题：\n椭圆型（空间二维）问题，通过空间离散化直接得到线性方程组； 抛物型（空间一维+时间）问题，先对空间离散变为ODE，再对时间离散使用ODE数值方法（显式/隐式欧拉方法）进行求解。 以上步骤逐步解释了问题如何从连续形式变成数值可求解的离散形式。\n$$ \\begin{align} -\\mu\\Delta u + \\vec{\\beta} \\cdot \\nabla u \u0026amp;= f \\ u(\\partial\\Omega) \u0026amp;= \\text{data} \\end{align} $$\nWrite it out: $$ \\begin{align} -\\mu\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) + \\beta_x\\frac{\\partial u}{\\partial x} + \\beta_y\\frac{\\partial u}{\\partial y} \u0026amp;= f(x,y) \\ u(\\partial\\Omega) \u0026amp;= d \\end{align} $$\n4.1 Only Consider Diffusion. Turn off Wind:\r#\rTo form a system: $(i,j) \\to k$.\n$Au = b$.\n$A$ is symmetric, SPD.\n4.2 Turn on the Wind. Upwind\r#\rWith upwind, the pts are not good.\n5. 抛物型（Parabolic）问题（时间相关问题）\r#\r$$ \\begin{align} \\frac{\\partial u}{\\partial t} - \\mu\\frac{\\partial^2 u}{\\partial x^2} \u0026amp;= f, \\quad x \\in (0,1) \\quad 0 \u0026lt; t \u0026lt; T\\ u(0,t) \u0026amp;= u_L(t), \\quad u(1,t) = u_R(t) \\ u(x, t=0) \u0026amp;= u_0(x) \\end{align} $$\nDiscretization along $x$: semi-discretization: $u_j(t) \\approx u(x_j, t)$. $$\\frac{du_j}{dt} - \\mu\\frac{u_{j+1}(t) - 2u_j(t) + u_{j-1}(t)}{\\Delta x^2} = f_j(t) = f(x_j, t)$$\nSo, we form system $Au = f$.\n$$A = \\frac{\\mu}{\\Delta x^2}\\text{Triad}(-1, 2, 1), \\quad u(t) = \\begin{bmatrix} u_1(t) \\ \\vdots \\ u_n(t) \\end{bmatrix}, \\quad f(t) = \\begin{bmatrix} f_1(t) \\ \\vdots \\ f_n(t) \\end{bmatrix}$$\nThen, we have a system of ODE: $$\\frac{du}{dt} - Au = f$$\nWe can now do time discretization and use ODE methods.\nEE/FE: $$u^n = u(t^n), \\quad \\left.\\frac{du}{dt}\\right|_{t^n} \\approx \\frac{u^{n+1} - u^n}{\\Delta t} = f^n + Au^n$$\n$$u^{n+1} = u^n + \\Delta t Au^n + \\Delta t f^n = (I + \\Delta t A)u^n + \\Delta t f^n = (I + \\Delta t A)^n u_0 + \\Delta t f^n$$\nIE/BE: $$\\left.\\frac{du}{dt}\\right|_{t^n} = \\frac{u^n - u^{n-1}}{\\Delta t} = f^n + Au^n$$\n$$u^n - u^{n-1} = \\Delta t f^n + \\Delta t Au^n$$\n$$u^n - \\Delta t Au^n = \\Delta t f^n + u^{n-1}$$\n$$(I - \\Delta t A)u^n = u^{n-1} + \\Delta t f^n \\quad \\leftarrow \\text{A linear system to solve}$$\n$I - \\Delta t A$ is SPD and $A$ is time-independent. So, we may favor direct method (as we can store $A = LU$ and reuse it) over iterative methods.\nTo discuss stability, set $f = 0$:\nEE is conditionally stable: Let $\\lambda_i$ be eigenvalues of $A$.\n$$\\Delta t \u0026lt; \\frac{2}{|\\lambda_i|} \\quad \\text{for stability}$$\nFurther, $A = \\frac{\\mu}{\\Delta x^2}\\text{triad}(1, -2, 1)$, $\\rho(A) \\sim \\frac{c}{\\Delta x^2}$. So,\n$$\\Delta t \u0026lt; \\frac{2}{|\\lambda_i|} \\leq \\frac{2}{\\rho(A)} = \\frac{2}{c}\\Delta x^2$$\nSo, if we decrease $\\Delta x$ by 2, to have stability\n$$\\Delta t_{new} \u0026lt; \\frac{2}{c}\\left(\\frac{\\Delta x}{2}\\right)^2 = \\frac{\\Delta t_{old}}{4} \\quad \\Rightarrow \\text{we need finer intervals for time}$$\nIE is unconditionally stable.\nDef. ($\\theta$ Methods). $$\\frac{u^{n+1} - u^n}{\\Delta t} = \\theta Au^{n+1} + (1-\\theta)Au^n + \\theta f^{n+1} + (1-\\theta)f^n, \\quad \\theta \\in [0,1]$$\nEE: $\\theta = 0 \\quad \\mathcal{O}(\\Delta t) \\quad$ explicit $\\quad$ conditional stability\nIE: $\\theta = 1 \\quad \\mathcal{O}(\\Delta t) \\quad$ implicit $\\quad$ unconditional stability\nCN: $\\theta = \\frac{1}{2} \\quad \\mathcal{O}(\\Delta t^2) \\quad$ implicit $\\quad$ unconditional stability\nSuppose $f = 0$. Then, $$\\frac{u^{n+1} - u^n}{\\Delta t} = \\theta Au^{n+1} + (1-\\theta)Au^n$$\n$$u^{n+1} - u^n = \\Delta t \\theta Au^{n+1} + \\Delta t(1-\\theta)Au^n$$\n$$(I - \\Delta t \\theta A)u^{n+1} = (I + \\Delta t(1-\\theta)A)u^n$$\nWe essentially solve a linear system in each iteration.\n5.1 Thm (Stability and Order of $\\theta$ Methods)\r#\r$\\theta$ methods are unconditionally stable for $\\theta \\geq \\frac{1}{2}$. Otherwise, it is conditionally stable for $\\theta \u0026lt; \\frac{1}{2}$ and the stability condition for parabolic problem is $\\Delta t \u0026lt; c\\Delta x^2$. Meanwhile, the method is order 1 for $\\theta \\neq \\frac{1}{2}$ and order 2 for $\\theta = \\frac{1}{2}$. Although the $\\theta$ method is 2nd order in space, the order of error is dominant and determined by the order in time. CN is the most vulnerable to lack of regularity and sensitive to non-smoothness. 6. Hyperbolic Problems\r#\r6.1\r#\r$ \\begin{align} \\frac{\\partial u}{\\partial t} + \\alpha\\frac{\\partial u}{\\partial x} \u0026amp;= 0, \\quad \\alpha \u0026gt; 0, \\text{ constant} \\ u(x,0) \u0026amp;= u_0(x) \\end{align} $\nExact solution: $u(x,t) = u_0(x - \\alpha t)$\n6.2 Example: Modeling Density of Pollutant\r#\r$u$: pollutant, $x$: displacement of boat, $t$: time.\n$$ \\begin{align} \\frac{du}{dx} \u0026amp;= 0 \\quad \\text{(i.e, pollutant and boat moves at the same velocity)} \\ \\frac{dx}{dt} \u0026amp; = a \\quad \\text{(i.e., boat moves at velocity of $a$)} \\end{align}\n$$\n$x(t) = x_0 + at \\Rightarrow$ characteristic curves\n$u(x,t) = u_0(x-at)$. Solution to $\\begin{cases} \\frac{dx}{dt} = a \\ x(0) = x_0 \\end{cases}$\nConsider $u(x(t),t)$: $\\frac{du}{dt} = \\frac{\\partial u}{\\partial t} + \\frac{\\partial u}{\\partial x} \\cdot \\frac{dx}{dt} = \\frac{\\partial u}{\\partial t} + a \\cdot \\frac{\\partial u}{\\partial x} = 0$.\n6.3 Similar Problems:\r#\rConservation law $\\frac{\\partial u}{\\partial t} + \\frac{\\partial q(u)}{\\partial x} = 0$\n$q(u) = v(u) \\cdot u$ with $v = v_{max}\\left(1 - \\frac{u}{u_{max}}\\right)$\n$\\Rightarrow \\frac{\\partial u}{\\partial t} + v_{max}\\left(1 - \\frac{u}{u_{max}}\\right)\\frac{\\partial u}{\\partial x} = 0$. Models the density of traffic.\n$= \\alpha$, but $\\alpha$ is not constant here.\nHeat equation $\\frac{\\partial^2 u}{\\partial t^2} - v^2\\frac{\\partial^2 u}{\\partial x^2} = f$\nDefine $w_1 = \\frac{\\partial u}{\\partial x}$ and $w_2 = \\frac{\\partial u}{\\partial t}$.\n$ \\begin{align} \\frac{\\partial w_1}{\\partial t} - v^2\\frac{\\partial w_2}{\\partial x} \u0026amp;= f \\ \\frac{\\partial w_2}{\\partial t} - \\frac{\\partial w_1}{\\partial x} \u0026amp;= 0 \\quad \\frac{\\partial^2 u}{\\partial x \\partial t} = \\frac{\\partial^2 u}{\\partial t \\partial x} \\end{align} $\nDefine $w = \\begin{bmatrix} w_1 \\ w_2 \\end{bmatrix}$, $A = \\begin{bmatrix} 0 \u0026amp; -v^2 \\ -1 \u0026amp; 0 \\end{bmatrix}$\nThen, the original equation becomes a system: $\\frac{\\partial w}{\\partial t} + A\\frac{\\partial w}{\\partial x} = 0$\nThe eigenvalues of $A$: $\\lambda_{1,2} = \\pm iv$. $\\Rightarrow$ Diagonalizable.\nFind the numerical solution.\n$\\frac{\\partial u}{\\partial t}\\bigg|_{t^{n+1}, x_j} = \\frac{u_j^{n+1} - u_j^n}{\\Delta t}$\n$\\alpha\\frac{\\partial u}{\\partial x}\\bigg|{t^{n+1}, x_j} = \\frac{\\alpha}{2} \\cdot \\frac{u{j+1}^{n+1} - u_{j-1}^{n+1}}{\\Delta t}$\nWith BE-C: $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2} \\cdot \\frac{u_{j+1}^{n+1} - u_{j-1}^{n+1}}{\\Delta t} = 0$\n$ \\Rightarrow \\begin{bmatrix} \\frac{1}{\\Delta t} \u0026amp; \\frac{\\alpha}{2\\Delta t} \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \\ -\\frac{\\alpha}{2\\Delta t} \u0026amp; \\frac{1}{\\Delta t} \u0026amp; \\frac{\\alpha}{2\\Delta t} \u0026amp; 0 \u0026amp; \\cdots \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \u0026amp; \\end{bmatrix} $\nWith FE-C: unconditionally unstable. NEVER USE IT! $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2} \\cdot \\frac{u_{j+1}^n - u_{j-1}^n}{\\Delta t} = 0$\n$\\Rightarrow u_j^{n+1} = u_j^n + \\frac{\\alpha\\Delta t}{2\\Delta t}(u_{j+1}^n - u_{j-1}^n)$\nWith FE-Upwind: $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\alpha\\frac{u_j^n - u_{j-1}^n}{\\Delta x} = 0 \\quad \\alpha \u0026gt; 0$\n$\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\alpha\\frac{u_{j+1}^n - u_j^n}{\\Delta x} = 0 \\quad \\alpha \u0026lt; 0$\n$\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2}\\frac{u_{j+1}^n - u_{j-1}^n}{\\Delta x} - \\frac{|\\alpha|\\Delta t}{2}\\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\\Delta x^2} = 0$\nDiffusion\nLax Wendroff: FE-upwind with modified coefficient $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2}\\frac{u_{j+1}^n - u_{j-1}^n}{\\Delta x} - \\frac{\\alpha^2\\Delta t}{2}\\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\\Delta x^2} = 0$\n$u(x_j, t^{n+1}) = u(x_j, t^n) + \\frac{\\partial u}{\\partial t}\\bigg|{t^n, x_j}(t^{n+1} - t^n) + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial t^2}\\bigg|{t^n, x_j}(t^{n+1} - t^n)^2 + \\mathcal{O}(||t^{n+1} - t^n||^3)$\n$\\frac{\\partial u}{\\partial t} = -\\alpha\\frac{\\partial u}{\\partial x}, \\quad \\frac{\\partial^2 u}{\\partial x \\partial t} = -\\alpha\\frac{\\partial^2 u}{\\partial x^2}, \\quad \\frac{\\partial^2 u}{\\partial t^2} = \\alpha^2\\frac{\\partial^2 u}{\\partial x^2}$\nSubstitute: $u_j^{n+1} = u_j^n - \\alpha\\left(\\frac{u_{j+1}^n - u_{j-1}^n}{2\\Delta x}\\right)\\Delta t + \\frac{\\alpha^2}{2}\\left(\\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\\Delta x^2}\\right)\\Delta t^2$\nStability: $\\left|\\frac{\\alpha\\Delta t}{\\Delta x}\\right| \\leq 1$\n"},{"id":50,"href":"/docs/Mathematics/MATH-351-352/Heat-Equation/The-Fourier-Series/","title":"傅立叶级数","section":"热方程","content":"\r1. 傅立叶级数的基本形态\r#\r1.1 周期\r#\r在经历了周期$T$后，重新获得原值的函数为周期函数：\n$$ \\varphi(t+T)=\\varphi $$\n1.2 正弦型量\r#\r正弦型量形如：\n$y(t)=Asin(\\omega t+\\alpha)$ where $\\omega = \\cfrac{2\\pi}{T}$ is the frequency.\nNotice that for values $s.t.$:\n$$ y_0 = A_0, \\ y_1=A_1\\sin(\\omega t + \\alpha_1), \\ y_2=A_2\\sin(2\\omega t + \\alpha_2), \\ y_3=A_3\\sin(3\\omega t + \\alpha_3) \u0026hellip; $$\nwe have frequency as the multiple of the smallest frequency with their period:\n$\\omega$, $2\\omega$, $3\\omega$…\n$T$, $\\frac{1}{2}T$, $\\frac{1}{3}T$…\n![[Fourier Series.png]]\nIf we add them together, we get…\n1.3.1 三角级数 - $\\varphi(t)$函数\r#\r💡 **It is possible to represent any periodic function with finite or infinite summations of sin functions:**\r$$ \\varphi(t)=A_0 + \\sum_{n=1}^{\\infty}A_n\\sin(n\\omega t+ \\alpha_n) $$\nwhere $A_n, \\alpha_n$ are constants\n$\\varphi(t)$可以被分解成无数个调和震动 每一项称之为调和素 对$\\varphi(t)$进行分解的手法被称之为调和分析 1.3.2 三角级数 - 最终展开式\r#\r注意ppt里的公式是不一样的换元，$x=\\frac{2Lt}{T}$，所以$\\frac{\\pi}{L}$被抽出来了。这里我们会用正常的逻辑换元推导，等到后面需要计算generic interval，才重新换一个$L$进去***\n当我们用$x=\\omega t = \\frac{2\\pi t}{T}$ 来换元 $s.t.$ $f(x)=\\varphi(t) = \\varphi(\\frac{x}{\\omega})$ ，我们用三角正弦公式展开：\n$$ \\begin{align}f(x)\u0026amp;= A_0 + \\sum_{n=1}^{\\infty}A_n\\sin(nx+ \\alpha_n)\\ \u0026amp;=A_0 + \\sum_{n=1}^{\\infty}A_n(\\cos nx\\sin \\alpha_n+ \\sin nx \\cos \\alpha_n) \\\\end{align} $$\n正弦公式：$\\sin(a+b)=\\cos a\\sin b + \\sin a\\cos b$ 展开\n$$ f(x)=A_0+\\sum_{n=1}^{\\infty}[(A_n\\sin \\alpha_n)\\cos nx+ (A_n \\cos \\alpha_n)\\sin nx )] $$\n并令 $A_0=a_0, \\ A_n\\sin\\alpha_n = b_n, \\ A_n\\cos\\alpha_n = b_n$\n💡 于是我们就有了会有三角级数的**最终展开形态**：\r$$ f(x)=a_0 + \\sum_{n=1}^{\\infty}(a_n\\cos nx+ b_n\\sin nx) $$\nthe period for $f$ is $2\\pi$ due to our definition of new independent variable $x$ 2. 傅立叶级数的系数\r#\r2.1 欧拉-傅立叶公式（Euler-Fourier formula）\r#\r这个是一个18世纪初欧拉使用的系数确定法。后面我们还学了泛函分析的inner product算系数的方法。\nAssuming $f(x)$ under $[-\\pi,\\pi]$ is an integrable function, if we assume Fourier Expansion for $f(x)$ is true, then directly we have:\n$$ \\begin{align}\\int_{-\\pi}^{\\pi} f(x) , dx = 2\\pi a_0 + \\sum_{n=1}^{\\infty} \\left[ a_n \\int_{-\\pi}^{\\pi} \\cos nx , dx + b_n \\int_{-\\pi}^{\\pi} \\sin nx , dx \\right]\\end{align} $$\nobviously, the integration for $\\cos$ and $\\sin$ for $[-\\pi,\\pi]$ is 0 regardless of the values, so we only left with:\n$$ a_0= \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} f(x) , dx $$\nnext, if to find the specific value for arbitrary $a_m$ ($m=1,2,3\u0026hellip;$), we multiply $(3)$ by $\\cos (ma)$ so that the terms we needed wouldn’t cancel out, eventually we reached at:\n$$ a_m= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x) \\cos(ma) , dx $$\nSimilarly, multiply by $\\sin(ma)$, we derive:\n$$ b_m= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x) \\sin(ma) , dx $$\n这些公式被称之为欧拉-傅立叶公式（Euler-Fourier formula），而他们算出来的数值被称为已给函数的傅立叶系数（Fourier Coefficient）。\n2.2* 狄利克雷积分（Dirichlet integral）\r#\r通过傅立叶展开的一个定点$x=x_0$的性质，获得的重要积分：\n$$ s_n(x_0) = \\frac{1}{\\pi} \\int_{0}^{\\pi} \\left[ f(x_0 + t) + f(x_0 - t) \\right] \\frac{\\sin (n + \\frac{1}{2})t }{2 \\sin\\frac{t}{2}} , dt\n$$\ne.x. https://www.bilibili.com/video/BV1XE411p7ZN/\n3*. 广义形态的傅立叶级数和系数公式\r#\r3.1 任意区间的情况\r#\r💡 We can separate out a $\\frac{1}{2}$ from the original $a_0$ coefficient, so the first term becomes a special case for $n=0$.\r在任意$2L$大小的区间 $(-L,L]$ 上，我们可以使用变换：$x=\\frac{Ly}{\\pi} (-\\pi\u0026lt;y\\leq\\pi)$ 使得$f(x)\\rightarrow f(\\frac{Ly}{\\pi})$ 。于是，我们根据公式获得：\n$$ f\\left( \\frac{Ly}{\\pi} \\right) = \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} \\left( a_n \\cos ny + b_n \\sin ny \\right) $$\n以及其系数：\n$$ a_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f\\left( \\frac{y}{\\pi} \\right) \\cos ny , dy \\quad (n = 0, 1, 2, \\ldots) $$\n$$\nb_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f\\left( \\frac{y}{\\pi} \\right) \\sin ny , dy \\quad (n = 1, 2, \\ldots) $$\n当我们重新变换回去，即使得$y = \\cfrac{\\pi x}{L}$，那么我们就会获得一个广义上的傅立叶展开！\n💡 The general form of **Fourier Expansion**:\r$$ \\begin{align}f(x)=\\frac{A_0}{2} + \\sum_{n=1}^{\\infty} \\left( A_n \\cos \\frac{n\\pi x}{L} + B_n \\sin \\frac{n\\pi x}{L} \\right) \\end{align} $$\nHere, $x$ is no longer the angle, but the integer multiples of $\\frac{\\pi x}{L}$, such that the Fourier Coefficient for generic interval $[-L,L]$ are:\n$$ \\begin{align} A_n \\equiv \\frac{1}{L} \\int_{-L}^{L} f(x) \\cos\\left(\\frac{n\\pi x}{L}\\right) dx, \\quad n = 0, 1, 2, \\ldots \\ B_n \\equiv \\frac{1}{L} \\int_{-L}^{L} f(x) \\sin\\left(\\frac{n\\pi x}{L}\\right) dx, \\quad n = 1, 2, 3, \\ldots \\end{align}\n$$\n3.2 菜就多练，给爷展\r#\r$f(x)=e^{ax}$, $a\\neq0$ on the interval of $(-\\pi,\\pi)$\n答案\n$f(x)=\\frac{\\pi-x}{2}$ on the interval of $(0,2\\pi)$\n答案\n$f(x)=x^2$ on the interval of $[-\\pi,\\pi]$, expand as cosine series.\n答案\nTwo functions ($a$ is assumed to be non-integers):\n$f_1(x)=\\cos(ax)$ on the interval of $[-\\pi,\\pi]$, expand as cosine series.\n答案\n$f_2(x)=\\sin(ax)$ on the interval of $(-\\pi,\\pi)$, expand as sine series.\n答案\n$f(x)=e^{ax}$, $a\\neq0$ on the interval of $(0,\\pi)$:\nexpand as cosine series expand as sine series 答案\n4. 傅里叶级数的拓扑空间\r#\r💡 理解傅里叶级数变换的拓扑空间的逻辑是：\r三维空间—— $n$维空间——$\\infty$维空间——希尔伯特空间（$L^2$空间）——傅立叶级数——傅立叶变换\n4.1 $\\infty$维欧式空间 - $\\R^{\\infty}$\r#\r4.1.1 $n$维空间的定义- $\\R^{n}$\r#\r范数（Norm）:\n$$ N_p(x) = | x |_p = \\left( |x_1|^p + |x_2|^p + \\cdots + |x_n|^p \\right)^{\\frac{1}{p}} $$\n单位正交向量基（standard orthogonal basis）:\n$$ \\left{ \\begin{aligned}\\vec{e}_1 \u0026amp;= (1, 0, \\ldots, 0), \\\\vec{e}_2 \u0026amp;= (0, 1, \\ldots, 0), \\\u0026amp; \\vdots \\\\vec{e}_n \u0026amp;= (0, 0, \\ldots, 1)\\end{aligned}\\right. $$\n因此$n$维欧式空间的任何向量可以被直接表达成：\n$$ \\vec{a} = \\sum_{i=1}^{n} a_i \\vec{e}_i $$\n4.1.2 由此推出$\\infty$维空间 - $\\R^{\\infty}$\r#\r同理，当我们有无数个标准基，${ \\vec{e}_1 , \\vec{e}_2 , \\vec{e}_3 \\ldots}$，的时候，向量变成了\n$$ \\vec{a} = \\sum_{i=1}^{\\infty} a_i \\vec{e}_i $$\n但是他们依然是离散的元素 4.2 Hilbert空间\r#\r我们需要把离散的元素过度到连续的函数，Hilbert空间：\n设定一个向量函数$\\vec{f}(x)$\n并且存在一组基函数（standard orthogonal functions）\n${ \\vec{\\varphi}_1 , \\vec{\\varphi}_2 , \\vec{\\varphi}_3 \\ldots}$ $s.t.$ we have\n$$ \\vec{f}(x) = \\sum_{i=1}^{\\infty} a_i \\vec{\\varphi_i}(x) $$\n因此，我们在Hilbert空间就有了这样的性质：\n💡 1. 函数在区间$[a,b]$上的模（norm）：\r$$ | f(x) | = \\sqrt{\\int_a^b f^2(x) , dx} ; $$\n如果两个函数的正交条件（orthogonality）为其内积（inner product）为零： $$ \\int_{a}^{b} f(x) g(x) , dx = 0 $$\n两者的角的余弦为 $$ \\cos(\\theta) = \\frac{\\langle f(x), g(x) \\rangle}{| f(x) | \\cdot | g(x) |} = \\frac{\\int f(x)g(x) , dx}{\\sqrt{\\int f^2(x) , dx} \\sqrt{\\int g^2(x) , dx}} ; $$\n4.3 $L^p$ 空间（Lebesgue Space）\r#\r定义：Suppose $f(x)$ is measurable functions on $E\\subset R^n$.\nFor $0\u0026lt;p\u0026lt;\\infty$, we denote:\n$$ ||f||_p=(\\int_E|f(x)|^pdx)^{1/p} $$\n我们用$L^p(E)$来表示$||f||_p\u0026lt;\\infty$的全体函数，称其为$L^p$空间。\n其范数（norm）为：\n$$ L_p = | \\varphi |p = \\left( \\sum{i=1}^{n} |\\varphi_i|^p \\right)^{\\frac{1}{p}}, \\quad x = (x_1, x_2, \\ldots, x_n) $$\n$L^p$空间的一些基础属性：\n$L$空间里的每一个函数都是Lebesgue可积的 空间维度是无穷而且不可数的度量空间（Metric Space） Banach空间，或者完备赋范向量空间 (complete normed vector space) 当$p=2$，$L^2$变成了一个Hilbert空间，一个带有内积的Banach空间 A Hilbert space is a real or complex inner product space that is also a complete metric space with respect to the distance function induced by the inner product. In every Hilbert Space, we have functions that\n$$ \\langle x, y \\rangle = \\sum_{k=1}^{n} \\overline{x_k} y_k $$\n$L^2$空间的范数**（norm）**，根据之前的公式，我们可以得知：\n$$ | \\varphi |2 = \\sqrt{ \\sum{i=1}^{n} |\\varphi_i|^2 }, \\quad x = (x_1, x_2, \\ldots, x_n) $$\np.s. 以（p=2）为例，空间中到原点的欧氏距离为1的点构成了一个球面。\n![[L-space-1.png]]\n4.4 傅里叶级数的正交函数系与规范系\r#\r定义：if $\\int_{a}^{b} \\varphi(x) \\psi(x) , dx = 0$ for interval $[a,b]$, then $\\varphi(x), \\psi(x)$ are orthogonal.\n4.4.1 正交函数系 (orthogonal functions)\r#\r当每对双函数$\\varphi_n(x), \\ \\varphi_m(x)$ 都符合定义以下，我们称这样的函数群体为正交函数系 （orthogonal group）：\n$$ \\int_{a}^{b} \\varphi_n(x) \\varphi_m(x) , dx = 0 \\space \\space\\space\\space\\space\\space\\space\\space {n,m\\in \\N\\ | \\ n\\neq m } $$\n4.4.2 规范正交函数系 (orthonormal functions)\r#\r若函数的 $\\lambda_n=1$ $(n=1,2,3\u0026hellip;)$, 那么这便是规范系（orthonormal group）：\n$$ \\int_{a}^{b} \\varphi_n^2(x) , dx = \\lambda_n $$\n若不是orthogonal，则我们可以通过$\\left{ \\cfrac{\\varphi_n(x)}{\\sqrt{\\lambda_n}} \\right}$ 来进行换取新的orthogonal functions。\n4.5 傅里叶级数在$L^2$ 空间的基函数\r#\rNotice back to this set of functions inside the interval $[-\\pi,\\pi]$:\n$$1, \\cos(x), \\sin(x), \\cos(2x), \\sin(2x),\u0026hellip;\\cos(nx), \\sin(nx),\u0026hellip;$$\nThese are orthogonal basis functions for their vector space because we see for any two functions in this set $\\int_{-\\pi}^{\\pi} \\cos(mx) \\cdot \\sin(nx) , dx = 0.$\nHowever, they are not standard, or normed, because $\\lambda_n \\neq1$.\n我们可以通过模（norm）的定义获得他们的normalizing coefficient：\n$$|1| = \\sqrt{\\int_{-\\pi}^{\\pi} 1^2 , dx} = \\sqrt{2\\pi}$$\n$$|\\cos(nx)| = \\sqrt{\\int_{-\\pi}^{\\pi} \\cos^2(nx) , dx} = \\sqrt{\\pi}$$\n$$|\\sin(nx)| = \\sqrt{\\int_{-\\pi}^{\\pi} \\sin^2(nx) , dx} = \\sqrt{\\pi}$$ Then, we would have a standard orthonormal functions: $$\\frac{1}{\\sqrt{2\\pi}}, \\ldots, , \\frac{\\cos nx}{\\sqrt{\\pi}}, , \\frac{\\sin nx}{\\sqrt{\\pi}}, , \\ldots$$\nLet’s define them:\n$$\\psi_0=\\frac{1}{\\sqrt{2\\pi}}, \\psi_j=\\frac{1}{\\sqrt{\\pi}}\\cos(jx), \\varphi_j=\\frac{1}{\\sqrt{\\pi}}\\sin(jx)$$\n$$\\psi_0\\equiv\\sqrt L\\quad \\psi_j\\equiv\\sqrt L\\cos\\left ( \\frac{j\\pi}{L}x \\right )\\quad \\varphi_j\\equiv\\sqrt L\\sin\\left ( \\frac{j\\pi}{L}x \\right) $$\nsuch that, we obtain the coefficient for each standard basis functions needed:\n$$\\begin{align} a_0 \u0026amp;= \\langle f,\\psi_0\\rangle=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\pi}^{\\pi} f(x) , dx\\a_j \u0026amp;=\\langle f,\\psi_k\\rangle=\\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\cos(jx) , dx\\ b_j \u0026amp;=\\langle f,\\varphi_k\\rangle =\\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\sin(kx) , dx \\end{align} $$\nThen, for the function $\\mathcal X$ and its coffecient $C_k$ defined as\n$$ \\mathcal X_k=( \\psi_k ,\\varphi_k ), \\quad C_k=(a_j,b_j) $$\nwe have a function space:\n$$ \\mathcal F(f)=\\sum_{k=0}^\\infty C_k \\cdot \\mathcal X_k $$\n如同$\\vec{a} = \\sum_{i=1}^{n} a_i \\vec{e}_i \\quad$一样，我们有$f(x) = a_0 \\frac{1}{\\sqrt{2\\pi}} + a_1 \\frac{\\cos x}{\\sqrt{\\pi}} + b_1 \\frac{\\sin x}{\\sqrt{\\pi}} + a_2 \\frac{\\cos 2x}{\\sqrt{\\pi}} + b_2 \\frac{\\sin 2x}{\\sqrt{\\pi}} + \\cdots$，或者\n$$\nf(x) = a_0 \\frac{1}{\\sqrt{2\\pi}} + \\sum_{k=1}^{\\infty} a_k \\frac{\\cos kx}{\\sqrt{\\pi}} + \\sum_{k=1}^{\\infty} b_k \\frac{\\sin kx}{\\sqrt{\\pi}} $$\n代入$a_0,a_k,b_k$到上式中，我们获得：\n$$ f(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\pi}^{\\pi} f(x) , dx \\frac{1}{\\sqrt{2\\pi}} + \\sum_{k=1}^{\\infty} \\frac{\\cos kx}{\\sqrt{\\pi}}\\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\cos kx , dx + \\sum_{k=1}^{\\infty} \\frac{\\sin kx}{\\sqrt{\\pi}} \\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\sin kx , dx $$\n当我们简化之后，并且将系数项更新，我们最终有了现在的傅里叶展开式：\n$$ f(x)=A_0 + \\sum_{n=1}^{\\infty}(A_n\\cos nx+ B_n\\sin nx) $$\n$$ \\begin{align} A_0 \u0026amp;= \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} f(x) , dx\\A_n \u0026amp;= \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\cos(nx) , dx\\ B_n \u0026amp;= \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\sin(nx) , dx \\end{align} $$\n"}]