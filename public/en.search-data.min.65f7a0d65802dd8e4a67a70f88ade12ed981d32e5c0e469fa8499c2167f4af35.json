[{"id":0,"href":"/posts/Short-Cut-for-GIST/","title":"Short Cut for Gist","section":"Blog","content":"Enter\nssh astrogroup@170.140.162.12\rPassword:\nNGC6814\rRunning\r#\rgistPipeline --config configFiles/MasterConfig --default-dir configFiles/defaultDir\rUpload directly from PowerShell\r#\rscp \"C:\\\\Users\\\\19175\\\\Desktop\\\\TNG Research\\\\GIST\\\\gistTutorial.tar.gz\" astrogroup@170.140.162.12:~/Erise/\rscp \u0026ldquo;D:\\illustrateTNG Files\\SLOAN_MUSE.py‚Äù astrogroup@170.140.162.12:~/miniconda3/envs/gist/lib/python3.6/site-packages/gistPipeline/readData\nscp \u0026ldquo;D:\\illustrateTNG Files\\SLOAN_LR.py‚Äù astrogroup@170.140.162.12:~/miniconda3/envs/gist/lib/python3.6/site-packages/gistPipeline/readData\nThe Directory for Read-File\n~/miniconda3/envs/gist/lib/python3.6/site-packages/gistPipeline/readData\rCD command\nGo to gistTutorial cd ~/Erise/gistTutorial\rOpen LogFile\nnano ~/Erise/gistTutorial/results/Test/LOGFILE\rOpen masterConfig\nnano ~/Erise/gistTutorial/configFiles/MasterConfig\rExtract at your folder in Linux server:\ntar -xzvf gistTutorial.tar.gz\r~/miniconda3/envs/gist/lib/python3.6/site-packages/vorbin/voronoi_2d_binning.py\nunzip the gz file:\ngzip -d -k TNG50-reds-0.035-angle-010-FOV-61-re_kpc-10-snap-98-460756.cube.fits.gz\rFinding\nfind ~/Erise/gistTutorial -name _______\rRemove File\nrm\rRemove Dir\nrmdir\rC:\\Users\\19175\\Desktop\\TNG Research\\YinziyuanResults\\TNG50-reds-0.06-angle-010-FOV-61-re_kpc-15-snap-98-460756.cube.fits.gz\nscp \u0026ldquo;C:\\Users\\19175\\Desktop\\TNG Research\\YinziyuanResults\\TNG50-reds-0.06-angle-010-FOV-61-re_kpc-15-snap-98-460756.cube.fits.gz\u0026rdquo; astrogroup@170.140.162.12:~/Erise/gistTutorial/inputData\nC:\\Users\\19175\\Desktop\\TNG Research\\YinziyuanResults\\TNG50-reds-0.06-angle-010-FOV-61-re_kpc-15-snap-98-460756.cube.fits.gz\nUpload SAURON_LR\ngistpipline\nNGC0000Example\nQuestion\n"},{"id":1,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/01-Mid-Point-Method/","title":"01 Mid Point Method","section":"Êï∞ÂÄºÊñπÊ≥ï","content":"\rFormula Derivation\r#\rThe derivative approximation:\r$$ \\begin{aligned} f(t_i, u_i) \u0026amp;= \\frac{dy}{dt} \\Big|{t_i} \\approx \\frac{y(t{i+1}) - y(t_{i-1})}{2\\Delta t} + \\mathcal{O}(\\Delta t^2) \\end{aligned} $$\nUpdate formula:\n$$ \\begin{aligned} u_{i+1} \u0026amp;= u_{i-1} + 2\\Delta t f(t_i, u_i) \\end{aligned} $$\nInitial Conditions\r#\rInitial Condition Calculation: Given $y_0$ $$ \\begin{aligned} u_2 \u0026amp;= y_0 + 2\\Delta t f(t_1, u_1) \\end{aligned} $$\nPredictor-Corrector Steps:\n$$ \\begin{aligned} u_1^* \u0026amp;= y_0 + \\Delta t f(t_0, y_0), \\ u_1 \u0026amp;= y_0 + \\frac{\\Delta t}{2} (f(t_0, y_0) + f(t_1, u_1^*)) \\end{aligned} $$\nTwo-Step Method\r#\rHigher-order method that improves accuracy using information from two previous time steps. The method achieves second-order accuracy due to the central difference approximation. Absolute Stability\r#\rWe consider the test equation:\n$$ \\begin{aligned} \\frac{dy}{dt} \u0026amp;= -\\lambda y, \\quad y(0) = y_0 \\end{aligned} $$\nwhere $\\lambda \u0026gt; 0$.\nDiscretization\r#\rThe numerical update formula is:\n$$ \\begin{aligned} u_{n+1} \u0026amp;= u_{n-1} - 2\\Delta t \\lambda u_n \\end{aligned} $$\nRearranging:\n$$ \\begin{aligned} u_{n+1} + 2\\Delta t \\lambda u_n - u_{n-1} \u0026amp;= 0 \\end{aligned} $$\nIterative Computation\r#\rStarting with initial conditions ($y_0$, $y_1$):\n$$ \\begin{aligned} u_2 \u0026amp;= y_0 - 2\\Delta t \\lambda y_1, \\ u_3 \u0026amp;= y_1 - 2\\Delta t \\lambda u_2, \\ u_4 \u0026amp;= u_2 - 2\\Delta t \\lambda u_3, \\ u_5 \u0026amp;= u_3 - 2\\Delta t \\lambda u_4 \\end{aligned} $$\nThis formulation helps analyze the stability of the numerical scheme by checking whether the sequence $u_n$ grows or decays as $n \\to \\infty$.\nStability Analysis of the Numerical Scheme\r#\rWe assume a solution of the form:\n$$ \\begin{aligned} u_i \u0026amp;= C \\beta^i \\end{aligned} $$\nSubstituting into the Recurrence Relation\r#\r$$ \\begin{aligned}\n\\end{aligned} $$ $$ \\begin{aligned} C \\beta^{i+1} + C 2\\Delta t \\lambda \\beta^i - C \\beta^{i-1} \u0026amp;= 0\\ \\beta^2 + 2\\Delta t \\lambda \\beta - 1 \u0026amp;= 0 \\quad \\text{devided by $C \\beta^{i-1}$.} \\end{aligned} $$ This is a characteristic equation for the recurrence relation. And solving for $\\beta$,\n$$ \\begin{aligned} \\beta \u0026amp;= \\frac{-2\\Delta t \\lambda \\pm \\sqrt{(2\\Delta t \\lambda)^2 + 4}}{2} \\end{aligned} $$ To ensure stability, we require:\n$$ \\begin{aligned} |\\beta_0|, |\\beta_1| \u0026amp;\\leq 1. \\end{aligned} $$\nGeneral Solution\r#\rSince the recurrence relation is second-order, the general solution is: $$ \\begin{aligned} u_i \u0026amp;= C_0 \\beta_0^i + C_1 \\beta_1^i. \\end{aligned} $$ From the initial conditions:\n$$ \\begin{aligned} u_0 \u0026amp;= C_0 + C_1 = y_0, \\\n\\nu_1 \u0026amp;= C_0 \\beta_0 + C_1 \\beta_1 = y_1 \\end{aligned} $$ which can be solved for $C_0$ and $C_1$.\nStability Condition\r#\rFor stability, the roots $\\beta_0, \\beta_1$ must satisfy:\n$$ \\begin{aligned}|\\beta_0 \\beta_1| \u0026amp;= 1.\\end{aligned} $$\nFrom the characteristic equation:\n$$ \\begin{aligned}\\beta_0 \\beta_1 \u0026amp;= -\\frac{1}{\\beta_1}. \\end{aligned} $$\nEnsuring $|\\beta| \\leq 1$ determines the absolute stability region.\nFinite Difference Approximation and Stability Analysis\r#\rFinite Difference Approximation\r#\r$$ \\begin{aligned} \\alpha u_{i+1} + \\beta u_i + \\sigma u_{i-1} + \\delta u_{i-2} \u0026amp;= (\\alpha + \\beta + \\sigma + \\delta) u_i + \\mathcal{O}(\\Delta t^5) \\end{aligned} $$\nTime Discretization\r#\r$$ \\begin{aligned} \\frac{du}{dt} \\Big|{t_i} \u0026amp;= \\frac{u_i - u{i-1}}{\\Delta t} + \\mathcal{O}(\\Delta t^2) \\end{aligned} $$\nStability Analysis\r#\r$$ \\begin{aligned} \\alpha + 4\\beta \u0026amp;= 0, \\ \\alpha \u0026amp;= -4\\beta, \\ 4\\beta - 2\\beta \u0026amp;= 1 \\Rightarrow \\beta = \\frac{1}{2}, \\quad \\alpha = -2. \\end{aligned} $$\nThese constraints ensure numerical stability and proper convergence of the finite difference scheme.\nTaylor Expansions\r#\rApplying Taylor expansions to express the function values at different time steps: $$ \\begin{aligned} \u0026amp; \\alpha\\left[u_{i+1}=u_i-\\left.\\frac{d u}{d t}\\right|{t_i} \\Delta t+\\ldots\\right] \\ \u0026amp; \\beta\\left[u{i+2}=u_i-\\left.\\frac{d u}{d t}\\right|{t_i} 2 \\Delta t+\\ldots\\right] \\ \u0026amp; \\gamma\\left[u{i+3}=u_i-\\left.\\frac{d u}{d t}\\right|{t_i} 3 \\Delta t+\\ldots\\right] \\ \u0026amp; \\delta\\left[u{i+4}=u_i-\\left.\\frac{d u}{d t}\\right|_{t_i} 4 \\Delta t+\\ldots\\right] \\end{aligned} $$ Summing these expansions, we obtain the system of equations,\n$$ \\left{\\begin{aligned} -\\alpha-2 \\beta-3 \\gamma-4 \\delta \u0026amp; =1 \\ \\alpha+4 \\beta+8 \\gamma+16 \\delta \u0026amp; =0 \\end{aligned}\\right. $$\n"},{"id":2,"href":"/docs/Mathematics/MATH-351-352/Heat-Equation/","title":"ÁÉ≠ÊñπÁ®ã","section":"ÂÅèÂæÆÂàÜÊñπÁ®ã","content":"The general form of heat equation (diffusion equation) is represented as\n$$ u_t-k\\Delta u=0 $$\nwhere $x\\in U$ for $U \\in \\R^n$ (n-dimentional) s.t we have $u:\\bar{U} \\times [0,\\infty) \\to \\R$.\n$k$ is the thermal diffusivity of the material. prototype of parabolic PDEs normalized heat equation where $k=1$ is specified for theoretical studies (focusing on mathematical analysis) For 1-dimentional special case:\n$$ u_t-ku_{xx}=0 $$\nwhere ${x,t\\in(-\\infty,\\infty), [0, \\infty)}$.\nLecture 10\r#\r‰∏ãÈù¢ÁöÑÊéíÂ∫èÊòØÊåâÁÖßÊé®ÂØºÁÉ≠ÊñπÁ®ãËß£ÊûêËß£ÁöÑÈ°∫Â∫èÊù•ÁöÑ„ÄÇ\nÂü∫Êú¨ÊÄùË∑ØÔºö\nÊàë‰ª¨ÂÖàÈÄöËøá\rÊ†áÂ∫¶‰∏çÂèòÊÄßÔºàscale invarianceÔºâÊâæÂà∞‰∏Ä‰∏™ÊâÄÊúâËß£ÈÄöÁî®ÁöÑÂΩ¢ÂºèÔºåÂπ∂‰∏îÂµåÂÖ•ÂéüÊñπÁ®ãËøõË°åËøêÁÆó„ÄÇ\nÊúÄÁªàÔºåÊàë‰ª¨ÂæóÂà∞‰∏ÄÁª¥ÁöÑÔºö\n10.1 ÁÉ≠‰º†ÂØºÊñπÁ®ãÁöÑÂü∫Êú¨Ëß£\r#\rThe Fundamental Solution to Heat Equation\n$$ \\Phi(x,t)=\\frac{1}{\\sqrt{4\\pi k t}} e^{-\\frac{x^2}{4kt}} $$\n10.1.1 Ê≠£ÊÄÅÂàÜÂ∏ÉÂáΩÊï∞ÁöÑÂ±ûÊÄß\r#\r‰ª•Âèä‰∏éÂÆÉ‰Ωú‰∏∫‰∏Ä‰∏™Ê≠£ÊÄÅÂàÜÂ∏ÉÂáΩÊï∞ÁöÑÂ±ûÊÄßÔºö\nProperties of The Fundamental Solution\n10.2 ÂØπ‰∫éÊüØË•øÈóÆÈ¢òÁöÑËß£\r#\rÊé•ÁùÄÔºåÊàë‰ª¨ÂÜçÁî®ËøáÂπ≥Áßª‰∏çÂèòÊÄß **Ôºà**translation invarianceÔºâÂíåÂç∑ÁßØÊù•Ëøõ‰∏ÄÊ≠•Á°ÆÂÆöÊüØË•øÈóÆÈ¢òÔºà$t=0$ÔºâÁöÑËß£„ÄÇ\nThe Solution to Cauchy Problem\n$$ u(x,t) = \\frac{1}{(4\\pi k t)^{1/2}} \\int^{+\\infty}_{-\\infty} e^{-\\frac{(x-y)^2}{4kt}} g(y) , dy $$\nÂπ∂‰∏îÂèØ‰ª•Áî®ËØØÂ∑ÆÂáΩÊï∞Êù•Ë°®Á§∫Ëøô‰∏™Ëß£\n$$ u(x, t) = \\lim_{x \\to \\infty} \\text{erf}(x\\sqrt{4\\pi kt}) $$\nLecture 11\r#\r11.1 ‰∏âÁßç‰∏çÂêåÁöÑËæπÁïåÊù°‰ª∂\r#\rDifferent Types of Boundary Conditions\n11.2 ÂàÜÁ¶ªÂèòÈáèÊ≥ï\r#\rSeparation of Variables Lecture 12\r#\r12.1 ÂÇÖÈáåÂè∂Â±ïÂºÄÂíåÂèòÊç¢\r#\rThe Fourier Series\nLecture 13\r#\r13.1 ÁÉ≠‰º†ÂØºÂÖ¨ÂºèËß£\r#\rThe Fourier Expansion for Heat Equation Solution\n13.2 DirichletÈóÆÈ¢òÁöÑËß£\r#\rNon-Homogenous Dirichlet problem\nLecture 14\r#\r14.1 Ëß£ÁöÑÂîØ‰∏ÄÊÄß\r#\rUniqueness of Solution\n14.2 ÁîµÊ¢ØÊñπÁ®ã\r#\rLifting Function\nüí° the transformed coefficient is defined as\r$$ \\hat \\mu =\\dfrac{\\mu}{(b-a)^2} $$\nLecture 15\r#\r15.1 ÊñΩÂõæÂßÜ-ÂàòÁª¥Â∞îÁêÜËÆ∫\r#\rSturm Liouville Theory (SLE)\nLecture 16\r#\r16.1 ÊûÅÂ§ßÂÄºÂéüÁêÜ\r#\rThe Principle of Maximum\n"},{"id":3,"href":"/posts/Integrate-DeepL-Translation-Instruction/","title":"Integrate Deep L Translation Instruction","section":"Blog","content":"\r1. Install R and Babeldown\r#\r1.1 Install R\r#\rhttps://cran.r-project.org/.\n(The following task is using R console)\n1.2 Install Babeldown\r#\rMore specific instruction, check here: https://docs.ropensci.org/babeldown/\n1.2.1 This command install \u0026lsquo;remotes\u0026rsquo; from CRAN if not already installed:\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\rinstall.packages(\"remotes\")\r}\r1.2.2 Uses the ‚Äòremotes‚Äô package to install the ‚Äòbabeldown‚Äô package from its GitHub repo:\ninstall.packages('babeldown', repos = c('https://ropensci.r-universe.dev', 'https://cloud.r-project.org'))\r2. Set up DeepL API (Inside of R console)\r#\rGo to DeepL\u0026rsquo;s website and get an API key: https://www.deepl.com/en/your-account/keys 3. Connect Babeldown to DeepL API\r#\rBabeldown uses the DeepL¬†free¬†API URL by default (no need to set up unless pro API).\n3.1 Download a keyring package (for secure API key retrieval)\r#\rinstall.packages(\"keyring\")\r3.2 Keyring requests your API key\r#\rlibrary(keyring)\rkeyring::key_set(\"deepl\", prompt = \"API key:\")\rEnter your API key and then in any script you use babeldown, you‚Äôd retrieve the key like so:\nSys.setenv(DEEPL_API_KEY = keyring::key_get(\"deepl\"))\r4. (optional) Set up working directory\r#\rIn R, use getwd() to check current working directory, and you may use setwd(\u0026quot;your absolute path\u0026quot;) to move your working directory for convenience.\nMy setup is:\nsetwd(\"/Users/erisehe/Documents/GitHub/erisehe.github.io\")\r4. Translates\r#\rSince my working directory is at (\u0026quot;\u0026hellip;/erisehe.github.io\u0026quot;), so I runs relative path. The commands, for example, is:\nbabeldown::deepl_translate_hugo(\rpost_path = \"content.en/docs/Mathematics/MATH 412 Real Analysis II/Lecture/name.md\",\rtarget_lang = \"ZH\",\rsource_lang = \"EN\",\rforce = TRUE\r)\rIt translate only one file at a time.\nHow to Translate an Entire Folder\r#\rYou can use base R functions (or packages like purrr) to list the files and apply the translation function.\n# Define the source folder containing your markdown files\rsource_dir \u003c- \"path/to/your/source_folder\"\r# Define the target folder where you want to save the translated files\rtarget_dir \u003c- \"path/to/your/target_folder\"\rif (!dir.exists(target_dir)) {\rdir.create(target_dir)\r}\r# List all markdown files in the source directory\rfiles \u003c- list.files(source_dir, pattern = \"\\\\.md$\", full.names = TRUE)\r# Loop through each file and translate it\rfor (f in files) {\r# Translate the file using babeldown's deepl_translate_hugo function\rbabeldown::deepl_translate_hugo(\rpost_path = f,\rtarget_lang = \"ZH\",\rsource_lang = \"EN\",\rforce = TRUE\r)\r# If the function writes the output file in a default location or with a predictable name,\r# you can move or copy it to your target directory. For example:\routput_file \u003c- file.path(dirname(f), paste0(\"translated_\", basename(f)))\rif (file.exists(output_file)) {\rfile.copy(output_file, file.path(target_dir, basename(output_file)))\r}\r}\r"},{"id":4,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/02-Three-Point-Backward-Differentiation-Formula/","title":"02 Three Point Backward Differentiation Formula","section":"Êï∞ÂÄºÊñπÊ≥ï","content":"\r1. Ok Honestly I Have No Idea Where He Started\r#\rMATH 212 is useless - Alessandro Veneziani\nGiven the population problem #Implicit:\n$$ \\dfrac{d y}{d x} = A\\left( 1 - \\dfrac{y}{B}\\right)y $$\r#\rNumerically, the problem is: $$ \\frac{u_{i+1} - u_i}{\\Delta t} = A \\left(1 - \\frac{u_{i+1}}{B} \\right) u_{i+1} $$ To solve this numerically, we rewrite the equation: $$ \\begin{align} x - u_i \u0026amp;= A \\left(1 - \\frac{x}{B} \\right) x \\ \u0026amp;= A x - \\frac{A}{B} x^2 \\end{align} $$\nusing $f(x)=0$, and we iterate using Newton\u0026rsquo;s method:\nyes he changed notation again\n[!remark|*] Newton\u0026rsquo;s Method $$\\underbrace{x^{(u+1)}}{y{\\text{new}}}=\\underbrace{x^{(u)}}{y{\\text{old}}}-\\frac{f(x^{(u)})}{f\u0026rsquo;(x^{(u)})}$$ We have $|y_{\\text{new}}-y_{\\text{old}}|\\leq \\text{tol}.$\n$$ \\begin{align} x - u_i \u0026amp; = \\Delta t A \\left(1 - \\frac{x}{B} \\right) x \\ \u0026amp; = \\Delta t A x - \\Delta t \\frac{A}{B} x^2 \\\n\\Longrightarrow \\quad x - u_{i}-\\Delta t A x + \\Delta t \\frac{A}{B} x^{2}\u0026amp;=0 \\end{align} $$ Define a function $g(x)$ from above: $$ \\begin{align} g(x) \u0026amp; = x - u_i - \\Delta t A x + \\Delta t \\frac{A}{B} x^{2}\\ \\end{align} $$ Taking the derivative: $$ \\begin{align} g\u0026rsquo;(x) \u0026amp;= \\frac{d}{dx} \\left( x - u_i - \\Delta t A x + \\Delta t \\frac{A}{B} x^2 \\right) \\ \u0026amp;= 1 - \\Delta t \\left(A - \\frac{2A}{B} x \\right) \\ \u0026amp;= 1 - \\Delta t f\u0026rsquo;(x). \\end{align} $$ For $f^{\\prime}(x)=A-\\frac{2 A}{B} x$.\n2. Approximate $\\frac{d y}{d x}$ Using a Three-Point Backward Differentiation Formula (BDF)\r#\rtracing back to last lecture on determination of coefficients $A, B, C$\n$$ \\begin{aligned} f(t_{i},y_{i})=\\left. \\frac{dy}{dx} \\right|{x_i} \u0026amp;\\approx \\frac{3}{2 \\Delta x} y_i - \\frac{4}{2 \\Delta x} y{i-1} + \\frac{1}{2 \\Delta x} y_{i-2} \\ \\ \\text{Taylor Expansion}\\Longrightarrow\\quad \u0026amp;\\left{ \\begin{aligned} y_{i-1} \u0026amp;= y_i - \\left. \\frac{dy}{dx} \\right|{x_i} \\Delta x \\dots \\ y{i-2} \u0026amp;= y_i - \\left. \\frac{dy}{dx} \\right|{x_i} 2 \\Delta x \\dots \\end{aligned} \\right. \\end{aligned}$$ we yield: $$ \\begin{align} \\frac{3}{2 \\Delta x} u_i\u0026amp;-\\frac{4}{2 \\Delta x} u{i-1}+\\frac{1}{2 \\Delta x} u_{i-2}=f\\left(t_i , u_i\\right)\\ \\end{align} $$\nImplicit formula: $$ u_i=\\frac{4}{3} u_{i-1}-\\frac{1}{3} u_{i-2}+\\frac{2}{3} \\Delta x\\left(t_i, u_i\\right) $$ we substitute $f(t_{i},u_{i})=\\lambda u_{i}$ to derive explicitly:\n$$\\begin{align} u_i - \\frac{4}{3} u_{i-1} + \\frac{1}{3} u_{i-2} - \u0026amp;\\frac{2}{3} \\Delta x f(t_i, u_i) = 0 \\ u_i - \\frac{4}{3} u_{i-1} + \\frac{1}{3} u_{i-2} - \u0026amp;\\frac{2}{3} \\Delta x \\lambda u_{i} = 0 \\ \\end{align} $$ Explicit formula: $$\n\\boxed{u_i = \\frac{1}{1 - \\frac{2}{3} \\Delta x\\lambda} \\left( \\frac{4}{3} u_{i-1} - \\frac{1}{3} u_{i-2} \\right) }\n$$\n[!definition|*] Generalized p-step BDF Form $$\\boxed{u_i-\\sum_{j=1}^{p} \\alpha_j u_{i-j}=\\Delta x \\beta_{-1} f\\left(t_i, u_i\\right)} $$ Generalized Implicit Multistep Method: $$\\boxed{u_{i+1}-\\sum_{j=1}^p \\alpha_j u_{i-j}=\\Delta x \\sum_{j={-1}}^p \\beta_j f\\left(t_{i-j}, u_{i+1-j}\\right)}$$\n"},{"id":5,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/03-Numerical-Solution-of-Ordinary-Differential-Equations/","title":"Â∏∏ÂæÆÂàÜÊñπÁ®ãÁöÑÊï∞ÂÄºËß£","section":"Êï∞ÂÄºÊñπÊ≥ï","content":"\rIntroduction\r#\rThe subject of this Chapter is the numerical approximation of the Cauchy problem:\n$$(1) \\quad \\frac{dy}{dt} = f(t,y) \\quad \\text{in } t \u0026gt; 0 \\quad (I.C.)$$\nwith $$y(0) = y_0 \\text{ given}.$$\nor, more in general, a system:\n$$(2) \\quad \\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(t,\\mathbf{y}) \\quad \\text{in } t \u0026gt; 0$$\nwith $$\\mathbf{y}(0) = \\mathbf{y}_0.$$\nLet\u0026rsquo;s recall some basic results on the function $f$. It is said to be Lipschitz continuous in an interval $I$ if there exists a positive $L \u0026gt; 0$ s.t.\n$$|f(t,y_1) - f(t,y_2)| \u0026lt; L |y_1 - y_2|.$$\nClearly, if $f$ is differentiable with continuity in $I$, it is also Lipschitz continuous with $L \\leq \\max_I \\left|\\frac{\\partial f}{\\partial y}\\right|$.\nLocal Theorem\r#\rIf $f$ is Lipschitz continuous in a range $t \\in I_1$ and $y \\in I \\subseteq \\mathbb{R}$, then $\\exists$ an interval $\\hat{I} \\subseteq I$ where the solution to $(1)$ exists and is unique.\nGlobal Theorem\r#\rIf the $f$ is Lipschitz continuous $\\forall t \\in I$ and $y \\in \\mathbb{R}$, then the solution $\\exists$ uniquely in $I$.\nStability Definitions\r#\rFrom the practical point of view, it is important to consider also the perturbed case:\n$$(1_\\epsilon): \\quad \\frac{dy_\\epsilon}{dt} = f(t, y_\\epsilon) + \\delta(t) \\quad t \\geq 0$$\n$$y^\\epsilon(0) = y_0 + \\epsilon$$\nwith $|\\delta(t)| \\leq \\epsilon \\quad \\forall t \\geq 0$\nIf there exists a finite constant $C$ such that\n$$|y - y_\\epsilon| \u0026lt; C\\epsilon \\quad (*)$$\nthen we say that the solution is Lyapunov stable.\nIn general, $C$ may depend on $t$, so that $(*)$ may hold on finite intervals (and not over the entire $[0,\\infty)$ axis).\nTo have a stronger concept, we advocate the ASYMPTOTIC STABILITY:\n$$\\lim_{t \\to \\infty} |y(t) - y_\\epsilon(t)| = 0.$$\nFrom the engineering point of view, this is a central concept, as the theory of control (Automatical Control, Feedback control) relies on this definition.\nRemark\r#\rThe Cauchy problem has a formal (quite useless) solution:\n$$y(t) = y_0 + \\int_0^t f(\\tau, y(\\tau))d\\tau$$\nconnecting the solution in $t$ with its past.\nThis is basically an alternative formulation of the problem and, in fact, will be used to formulate possible numerical approximation alternative to the ones based on $(1)$.\nSome Simple Examples\r#\rTo begin with, we can split the time interval in subintervals, for instance with a uniform reticulation with step $\\Delta t$.\n![Time discretization with points at 0, Œît, 2Œît, 3Œît\u0026hellip;]\nThen, we can use the formula:\n$$\\frac{dy}{dt}(t_i) \\simeq \\frac{y(t_{i+1}) - y(t_i)}{\\Delta t}$$\nthat we know is accurate with an error scaling with $\\Delta t$. In this way, we have:\n[From now on, the approximation of the solution $y(t)$ in $t_i$ will be denoted with $u_i$]\n$$\\frac{u_{i+1} - u_i}{\\Delta t} = f(t_i, u_i)$$\nIn practice, starting at $t = 0$: $$\\begin{align} u_1 \u0026amp;= u_0 + \\Delta t , f(t_0, u_0) \\quad \\rightsquigarrow \\quad (u_0 = y_0) \\ u_2 \u0026amp;= u_1 + \\Delta t , f(t_1, u_1) \\end{align}$$\nWe can easily compute the approximation $u_i$ of $y(t_i)$.\nOn the other hand, we could do:\n$$\\frac{u_i - u_{i-1}}{\\Delta t} = f(t_i, u_i)$$\nleading to:\n$$u_1 = u_0 + \\Delta t , f(t_1, u_1) \\quad (u_0 = y_0)$$\nThis is not as easy as before: it\u0026rsquo;s a NONLINEAR (ALGEBRAIC) EQUATION; we know how to solve it (first Chapter), but it is a computational additional cost. Then, similarly, we have:\n$$u_2 = u_1 + \\Delta t , f(t_2, u_2)$$\nWe have also another option:\n$$\\frac{dy}{dt}(t_i) \\simeq \\frac{u_{i+1} - u_{i-1}}{2\\Delta t}$$\nIn this case, the error scales with $O(\\Delta t^2)$. So, in practice we have:\n$$u_{i+1} = 2\\Delta t , f(t_i, u_i) + u_{i-1}$$\nor specifically: $u_2 = 2\\Delta t , f(t_1, u_1) + u_0 \\quad (u_0 = y_0)$\nI need to know $u_1$, not just $u_0$, then we can use the method.\nWith these three examples, we have already found many possible types of methods:\nImplicit vs Explicit\nImplicit: Solve a non-linear equation Explicit: No need of solving equations One Step vs Multistep\nOne Step: $u_{i+1} = g(\\Delta t, u_i)$ Multistep: $u_{i+1} = g(\\Delta t, u_i, u_{i-1}, u_{i-2}\u0026hellip;)$ At the top of this, we have the problem of the accuracy and - even most importantly- of the CONVERGENCE.\nIn fact, the basic requirement we need is that the method is convergent:\n$$\\lim_{\\Delta t \\to 0} |y(t_i) - u_i| = 0$$\nThen, if we find that $|y(t_i) - u_i| \\sim O(\\Delta t^p)$ then the accuracy or the order of the method is $p$.\nNature (implicit/explicit), Number of steps and most important CONVERGENCE and accuracy (order) are the categories for analyzing and ranking different methods.\nBefore we embark ourselves in a general analysis, however, let\u0026rsquo;s focus on a specific case, where important concepts will be highlighted.\nAnalysis of Forward Euler\r#\rThe method:\n$$\\frac{u_{i+1} - u_i}{\\Delta t} = f(t_i, u_i)$$\nis called Forward Euler.\nLet\u0026rsquo;s consider it in detail.\nTo start with, let\u0026rsquo;s introduce the distinction of \u0026ldquo;consistency\u0026rdquo; and truncation error.\nIf we have the exact solution $y_{ex}(t)$ it is easily realized that\n$$\\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\\Delta t} \\neq f(t_i, y_{ex}(t_i))$$\nFor instance,\n$$\\frac{dy}{dt} = \\lambda y \\quad y(0) = 1 \\implies y_{ex} = e^{\\lambda t}$$\nthen\n$$\\frac{e^{\\lambda(t_i+\\Delta t)} - e^{\\lambda t_i}}{\\Delta t} = e^{\\lambda t_i} \\frac{e^{\\lambda \\Delta t} - 1}{\\Delta t} \\neq \\lambda e^{\\lambda t_i} \\quad (\\lambda y(t_i))$$\nWe can be more specific:\n$$y_{ex}(t_{i+1}) = y_{ex}(t_i) + \\frac{dy_{ex}}{dt}(t_i)\\Delta t + \\frac{1}{2}\\frac{d^2y_{ex}}{dt^2}(t_i)\\Delta t^2 + \u0026hellip;$$\nNow:\n$$\\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\\Delta t} = \\frac{dy_{ex}(t_i)}{dt} + \\frac{1}{2}\\frac{d^2y_{ex}(t_i)}{dt^2}\\Delta t + \u0026hellip;$$\nThis gives us:\n$$\\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\\Delta t} = f(t_i, y_{ex}(t_i)) + \\left[\\frac{\\Delta t}{2}\\frac{d^2y_{ex}}{dt^2}\\right]$$\nForward Euler $\\quad \\quad \\quad$ Local Truncation Error (LTE)\nIn some sense, the truncation error is the residual we get when we pretend the exact solution to solve the numerical scheme.\nNow, to investigate how the error of Forward Euler works, let\u0026rsquo;s consider the following picture:\n![Error propagation diagram showing exact solution trajectory and numerical approximation]\nFrom the picture it is evident that the error:\n$$e_{i+1} = y_{ex}(t_{i+1}) - u_{i+1}$$\nis the result of two contributions:\n$$e_{i+1} = \\underbrace{y_{ex}(t_{i+1}) - u^{i+1}}{\\text{generated at the local step}} + \\underbrace{u^{i+1} - u{i+1}}_{\\text{propagated from previous steps}}$$\nFrom the previous definition:\n$$y_{ex}(t_{i+1}) = y_{ex}(t_i) + \\Delta t , f(t_i, y_{ex}(t_i)) + \\Delta t , \\tau_{i+1}$$\nwhere $\\tau_{i+1} = \\frac{\\Delta t}{2}\\frac{d^2y_{ex}(t_i)}{dt^2}$ is the local TE.\n$$u^*{i+1} = y{ex}(t_i) + \\Delta t , f(t_i, y_{ex}(t_i))$$\n$$y_{ex}(t_{i+1}) - u^*_{i+1} = \\Delta t , \\tau_i$$\nNow, let\u0026rsquo;s focus on the other part.\nThis second component $u^*{i+1} - u{i+1}$ is inherited from the previous errors.\n$$u^*{i+1} = y{ex}(t_i) + \\Delta t , f(t_i, y_{ex}(t_i))$$ $$u_{i+1} = u_i + \\Delta t , f(t_i, u_i)$$\nUsing the Lipschitz assumption $|f(t,y) - f(t,u)| \\leq L |y-u|$ we have:\n$$|u^*{i+1} - u{i+1}| \\leq |e_i| + \\Delta t , L |e_i| = (1 + \\Delta t , L)|e_i|$$\nwhere $e_i = y_{ex}(t_i) - u_i$\nAll together, we have:\n$$|e_{i+1}| \\leq \\Delta t |\\tau_i| + (1 + \\Delta t , L)|e_i|$$\nNow, take $|\\tau^*| = \\max_i |\\tau_i| = \\text{GLOBAL TRUNCATION ERROR}$.\nThen:\n$$|e_{i+1}| \\leq \\underbrace{\\Delta t |\\tau^*|}{\\text{local}} + \\underbrace{(1 + \\Delta t , L)}{\\text{propagated}}|e_i|$$\nAssume that $e_0 = 0$ (no errors on the initial conditions). Then we have:\n$$|e_1| \\leq \\Delta t |\\tau^|$$ $$|e_2| \\leq \\Delta t |\\tau^| + (1 + \\Delta t , L)|e_1| \\leq \\Delta t |\\tau^|(1 + (1+\\Delta t , L))$$ $$|e_3| \\leq \\Delta t |\\tau^| + (1+\\Delta t , L)|e_2| \\leq \\Delta t |\\tau^*|(1 + (1+\\Delta t , L) + (1+\\Delta t , L)^2)$$\nWe infer:\n$$|e_K| \\leq \\Delta t |\\tau^| \\sum_{j=0}^{K-1}(1+\\Delta t , L)^j = \\Delta t |\\tau^|\\frac{(1+\\Delta t , L)^K - 1}{1 - 1 - \\Delta t , L} = \\frac{|\\tau^*|}{L}((1+\\Delta t , L)^K - 1)$$\nNotice that:\n$$(1 + x)^K \\leq \\exp(xK)$$\nSo:\n$$|e_K| \\leq \\frac{|\\tau^|}{L}(\\exp(LKh) - 1) = \\frac{|\\tau^|}{L}(\\exp(Lt_K) - 1)$$\nwhere $K\\Delta t = t_K$\nNow, if we want to have a bound on the error in the interval $[0,T]$, we have:\n$$|e| \\leq \\frac{|\\tau^*|}{L}(\\exp(LT) - 1)$$\nWe have proved the following Theorem:\nConvergence of Forward Euler\r#\rIf the initial data are exact, and $f$ is Lipschitz continuous with respect to the $y$-argument, with a solution $\\in C^2(0,T)$, then FE converges.\nIn fact:\n$$|\\tau^*| = \\frac{1}{2}\\Delta t \\max_i |y^{\u0026rsquo;\u0026rsquo;}| \\xrightarrow{\\Delta t \\to 0} 0$$\nand $|e| \\leq |\\tau^*|\\frac{\\exp(LT) - 1}{L} \\xrightarrow{\\Delta t \\to 0} 0$ is bounded.\nFE is convergent with order 1.\nBeyond the Theorem per se (we will generalize it to other methods), it is important to notice that the convergence is the consequences of two peculiarities:\n(1) $\\tau^* \\xrightarrow{\\Delta t \\to 0} 0$ the LTE/GTE vanishes as $\\Delta t \\to 0$, so locally the error is under control.\nThis property is called CONSISTENCY.\n(2) The factor $\\frac{\\exp(LT) - 1}{L}$ is independent of $\\Delta t$ (or, in general, doesn\u0026rsquo;t blow up for $\\Delta t \\to 0$). This is related to the way the error propagates so it is a \u0026ldquo;global\u0026rdquo; property through the constant $L$.\nThe control of the error in time is called STABILITY.\nIn some sense, we can say that:\nCONVERGENCE = CONSISTENCY + STABILITY\nIn spite of the intuitive arguments we used here, we will see that this is a good representation of general results (Lax-Richtmyer equivalence Theorem) holding for method using linear combinations of the function $f$ evaluated in the points $(t_i, u_i)$.\nOther One-Step Methods\r#\rSo far, we have two one-step methods (forward and backward Euler). Let\u0026rsquo;s see other two. It is instructive to see how they are devised.\nCrank-Nicolson\r#\rFrom $y(t) = y_0 + \\int_0^t f(\\tau, y(\\tau))d\\tau$ we can organize the following method:\n![Timeline showing points $t_0, t_1, t_2, t_3, t_4, t_5, t_6, t_7$]\nLocalize: $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(\\tau, y(\\tau)) d\\tau$\nDiscretize: approximate the integral with the trapezoidal rule:\n$$u_{n+1} = u_n + \\frac{f(t_{n+1}, u_{n+1}) + f(t_n, u_n)}{2} \\Delta t$$\nThis is a Second Order (EXERCISE) one-step implicit method.\nHeun\r#\rLet\u0026rsquo;s start from Crank-Nicolson and make it \u0026ldquo;explicit\u0026rdquo;.\nOn the right hand side of CN we set:\n$$u_{n+1} \\simeq u_n + \\Delta t , f(t_n, u_n) \\quad \\text{(Explicit Euler)}$$\nWe obtain the scheme:\n$$u_{n+1} = u_n + \\Delta t \\frac{f(t_{n+1}, u_n+\\Delta t f(t_n, u_n)) + f(t_n, u_n)}{2}$$\nThis is called Heun. It is One-step, still 2nd order (EXERCISE).\nThe Concept of Zero-Stability\r#\rLet\u0026rsquo;s start applying the concept to one-step methods in the form:\n$$u_{n+1} = u_n + \\Delta t , \\Phi(u_n, t_n, f_n; \\Delta t)$$\nLet\u0026rsquo;s consider the perturbed scheme:\n$$\\begin{cases} w_{n+1} = w_n + \\Delta t (\\Phi(z_n, t_n, f(t_n, z_n); \\Delta t) + \\delta_n) \\ w_0 = y_0 + \\delta_0 \u0026amp; \\text{with } |\\delta_i| \\leq \\varepsilon \\end{cases}$$\nWe say that the method is zero-stable if for $\\Delta t \u0026lt; \\Delta t_0$, there exists a constant $C \u0026gt; 0$ such that\n$$|u_n - z_n| \\leq C\\varepsilon$$\nfor $\\varepsilon \u0026gt; 0$ sufficiently small. ($C$ and $\\Delta t_0$ depend on problem data, $T_{fin}$, $f$).\nIt is possible to prove the following theorem:\nTheorem: If $\\Phi$ is Lipschitz continuous with respect to the dependence on $u_n$ ($\\exists \\Delta \u0026gt; 0$: $|\\Phi(u_n) - \\Phi(z_n)| \\leq \\Delta |u_n - z_n|$), then the One-step method is zero-stable.\nLipschitz continuity in this case is enough for zero-stability (and it is generally a consequence of Lipschitz continuity of $f$).\nThen, we have another theorem. It generalizes the theorem for the explicit Euler.\nTheorem: If $\\Phi$ is like in the previous theorem, then:\n$$|y(t_n) - u_n| \\leq \\left(|y_0 - u_0| + t_n , \\tau(\\Delta t)\\right) e^{L t_n}.$$\nTherefore, if:\n$\\tau(\\Delta t) \\to 0$ with $\\Delta t$ $y_0 - u_0 \\to 0$ with $\\Delta t$ the method is convergent\n(and the order is $\\Delta t^p$, with $p = \\min(p_1, p_2)$ where:\n$\\tau(\\Delta t) \\sim O(\\Delta t^{p_1})$ $y_0 - u_0 \\sim O(\\Delta t^{p_2})$ (generally $p = p_1$). In other terms:\nFor one-step method (not true for multi-step):\n$$\\Phi \\text{ Lipschitz continuous } \\Rightarrow \\text{ Method zero-stable} + \\text{Consistency} \\Rightarrow \\text{CONVERGENCE}$$\n$\\Rightarrow$ If $\\Phi$ is Lipschitz continuous, consistency $\\Rightarrow$ convergence.\nThe Concept of Absolute Stability\r#\rThe zero-stability is a property of the numerical scheme, required by the presence of roundoff errors and other possible perturbations.\nHowever, there is another (maybe more engineering) concept of stability related to the nature of the problem to solve.\nIn simple words, we want that, if a problem is asymptotically stable, the numerical approximation is asymptotically stable too.\nIs this happening? Let\u0026rsquo;s consider the prototype of asymptotically stable problem (Model Problem).\nLet\u0026rsquo;s consider the Cauchy problem:\n$$\\begin{cases} \\frac{dy}{dt} = \\lambda y \u0026amp; t \u0026gt; 0 \\ y(0) = y_0 \\end{cases}$$\nWe know that the solution is asymptotically stable for $\\lambda \u0026lt; 0$ $(y_{ex} = y_0 e^{\\lambda t} \\xrightarrow{t \\to \\infty} 0 \\text{ for } \\lambda \u0026lt; 0)$\nIn fact: $\\frac{dz}{dt} = \\lambda z$ with $z(0) = y_0 + \\varepsilon \\Rightarrow z - y = \\varepsilon e^{\\lambda t} \\xrightarrow{t\\to\\infty} 0$ for $\\lambda \u0026lt; 0$.\nRemark\r#\rFor a system: $\\begin{cases} \\frac{d\\mathbf{y}}{dt} = A\\mathbf{y} \u0026amp; \\mathbf{y} \\in \\mathbb{R}^n \\ \\mathbf{y}(0) = \\mathbf{y}_0 \u0026amp; A \\in \\mathbb{R}^{n \\times n} \\end{cases}$\nthe asymptotic stability is guaranteed if THE REAL PART OF ALL THE EIGENVALUES is negative.\nTo be general, from now on we will consider $\\lambda \\in \\mathbb{C}$ also for the scalar case. In particular, the left-plane $\\text{Real}(\\lambda) \u0026lt; 0$ is the region of the complex plane where the original problem is asymptotically stable.\nQuestion: is the solution of the model problem with Explicit Euler asymptotically vanishing as the exact solution?\r#\rNotice that the question is not related to the behavior of the solution for $\\Delta t \\to 0$, but for $\\Delta t$ given and $t \\to +\\infty$.\nLet\u0026rsquo;s see: $$\\frac{dy}{dt} = \\lambda y \\quad \\text{EE}: \\frac{u_{i+1} - u_i}{\\Delta t} = \\lambda u_i$$\n$$u_{i+1} = (1 + \\lambda \\Delta t) u_i \\quad (\\text{Re}(\\lambda) \u0026lt; 0)$$\n$$|u_{i+1}| = |1 + \\lambda \\Delta t| |u_i| \\Rightarrow |u_{i+1}| = |1 + \\lambda \\Delta t|^{i+1} |u_0|$$\nThe solution asymptotically vanishes if $|1 + \\lambda \\Delta t| \u0026lt; 1$\nIntuitively, if $\\lambda$ is Real and negative:\n$$|1 + \\lambda \\Delta t| \u0026lt; 1$$ $$\\Downarrow$$ $$-1 \u0026lt; 1 + \\lambda \\Delta t \u0026lt; 1$$ $$\\Downarrow$$ $$-2 \u0026lt; \\lambda \\Delta t \u0026lt; 0$$ $$\\Downarrow$$ $$\\Delta t \u0026lt; \\frac{2}{|\\lambda|}$$\nSo, if $\\Delta t \u0026gt; \\frac{2}{|\\lambda|}$ the numerical solution is not stable.\nFor $\\lambda$ complex, we can draw the region of the plane $\\lambda \\Delta t$ where the solution is stable.\n![Complex plane diagram showing unit circle with center at (-1,0)]\nUnit circle with center in $(-1, 0)$\n(In magenta the region of stability of the problem).\nSo, the method is reproducing the asymptotic behavior of the exact solution ONLY UNDER THE CONDITION THAT:\n$$\\lambda \\Delta t \\in \\text{Unit Circle centered in } (-1, 0).$$\nWhat happens with Implicit Euler?\r#\r$$\\frac{1}{|1 - \\lambda \\Delta t|} \u0026lt; 1 \\quad \\forall \\Delta t,$$\nso $u^{n+1} \\xrightarrow{n \\to \\infty} 0 \\quad \\forall \\Delta t$\nThere is a big difference between the two Eulers: one is stable under some conditions, the other is unconditionally stable.\nWhat about Crank-Nicolson?\r#\r$$u^{n+1} = u^n + \\Delta t \\frac{\\lambda u^{n+1} + \\lambda u^n}{2} \\Rightarrow u^{n+1} = \\frac{1 + \\frac{\\Delta t}{2}\\lambda}{1 - \\frac{\\Delta t}{2}\\lambda} u^n$$\nAgain, for $\\lambda \\in \\mathbb{C}$ we have:\n$$\\left|\\frac{1 + \\frac{\\Delta t}{2}\\lambda}{1 - \\frac{\\Delta t}{2}\\lambda}\\right| \u0026lt; 1 \\quad \\forall \\Delta t$$\nso also CN is stable with no condition.\nDEFINITION\r#\rA method is said to be ABSOLUTELY STABLE if the solution of the model problem $\\frac{dy}{dt} = \\lambda y$ vanishes asymptotically when $t \\to +\\infty$.\nWe say that the method is UNCONDITIONALLY ABSOLUTELY STABLE if this happens $\\forall \\Delta t \u0026gt; 0$. On the contrary, it is said to be CONDITIONALLY STABLE if we have limitations on $\\Delta t$.\nAnother Example: Heun\r#\r$$u^{n+1} = u^n + \\frac{\\Delta t}{2}(\\lambda(u^n + \\Delta t \\lambda u^n) + \\lambda u^n) = \\left(1 + \\Delta t \\lambda + \\frac{\\Delta t^2 \\lambda^2}{2}\\right) u^n$$\nNow, consider the curve $\\frac{\\Delta t^2 \\lambda^2}{2} + \\Delta t \\lambda + 1$ for $\\lambda \u0026lt; 0$\nWe see that we need:\n$$0 \u0026lt; \\Delta t \u0026lt; \\frac{2}{|\\lambda|}$$\n(as for Explicit Euler).\nIn the complex plane, the region is slightly larger than for Explicit Euler.\nREMARK: We found that the explicit methods are conditionally stable, while the implicit ones are unconditionally stable.\r#\rIn general, we do not have unconditionally stable explicit methods, but we do have implicit methods that are only conditionally stable.\nThe region of absolute stability is the portion of $\\mathbb{C}^-$ for $\\lambda \\Delta t$ to belong to such that the method is absolutely stable. Unconditional absolute stability (a.k.a A-stability) is when this region is the entire left plane $\\mathbb{C}^-$.\nThe concept of absolute stability somehow clarifies what are the criteria to select a method for a problem.\nIn fact, let\u0026rsquo;s first consider the general case(s):\n$\\frac{dy}{dt} = f(t,y) \\simeq f(t,y_0) + \\frac{\\partial f}{\\partial t}(t-t_0) + \\frac{\\partial f}{\\partial y}(t,y_0)(y-y_0)$\nso we can locally take $\\lambda \\simeq \\frac{\\partial f}{\\partial y}(t,y_0)$\nFor a system: $\\frac{d\\mathbf{y}}{dt} = A \\mathbf{y} \\Rightarrow \\lambda = eig(A)$\n$\\frac{d\\mathbf{y}}{dt} = \\mathbf{F}(t,\\mathbf{y})$ (nonlinear system)\n$\\Rightarrow \\lambda = eig\\left(\\frac{\\partial \\mathbf{F}}{\\partial \\mathbf{y}}(t_0, y_0) \\right)$ - Jacobian\nNow, for a general problem, we have:\nMethod Nature Accuracy Limitations on $\\Delta t$ FE Explicit 1 $\\Delta t \u0026lt; \\frac{2}{ BE Implicit 1 NO CN Implicit 2 NO H Explicit 2 $\\Delta t \u0026lt; \\frac{2}{ Implicit Methods are more computationally expensive.\nIn an extreme synthesis:\n![Comparison of FE and BE with timeline]\nWith FE each step is low-cost (Explicit) but we may need to do many for the conditional stability.\nWith BE each step is more expensive, but we need to do (generally) fewer steps.\n$\\Rightarrow$ The optimal choice is largely problem dependent.\nMultistep Methods\r#\rThe mid-point method is just an example of multi-step methods:\n$$u_{n+1} = u_{n-1} + 2\\Delta t , f(t_n, u_n)$$\nIn general, a multistep method with $p$ steps take the form:\n$$u_{n+1} - \\sum_0^p a_j u_{n-j} = \\Delta t \\sum_{j=-1}^p b_j f(t_{n-j}, u_{n-j})$$\nThe method is IMPLICIT when $b_{-1} \\neq 0$.\nExample:\r#\r$$y(t_{n+1}) = y(t_{n-1}) + \\int_{t_{n-1}}^{t_{n+1}} f(\\tau, y(\\tau)) d\\tau$$\n$$\\Downarrow \\text{ SIMPSON}$$\n$$u_{n+1} = u_{n-1} + 2\\Delta t \\frac{f(t_{n+1}, u_{n+1}) + 4f(t_n, u_n) + f(t_{n-1}, u_{n-1})}{6} =$$\n$$= u_{n-1} + \\frac{\\Delta t}{3}(f(t_{n+1}, u_{n+1}) + 4f(t_n, u_n) + f(t_{n-1}, u_{n-1}))$$\n$$\\mathbf{a} = \\begin{bmatrix} 0 \\ 1 \\end{bmatrix} a_0, a_1 \\quad \\mathbf{b} = \\begin{bmatrix} \\frac{1}{3} \\ \\frac{4}{3} \\ \\frac{1}{3} \\end{bmatrix} b_{-1}, b_0, b_1$$\nIn general, we have two approaches for deriving a Multi-step methods:\nBDF (Backward Difference Formulas)\r#\r$$\\frac{dy}{dt}(t_{n+1}) = f(t_{n+1}, u_{n+1})$$ $$\\downarrow$$ approximate this with a backward finite difference, e.g.,\n$$\\frac{dy}{dt}(t_{n+1}) \\simeq \\frac{\\frac{3}{2}u_{n+1} - 2u_n + \\frac{1}{2}u_{n-1}}{\\Delta t}$$\n$$\\Rightarrow u_{n+1} = \\frac{4}{3}u_n - \\frac{1}{3}u_{n-1} + \\Delta t f(t_{n+1}, u_{n+1})$$\nThey are all in the form:\n$$\\mathbf{b} = \\begin{bmatrix} 1 \\ 0 \\ \\vdots \\ 0 \\end{bmatrix}$$\nAdams\r#\rIn this case, we start from:\n$$y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(\\tau, y(\\tau)) d\\tau$$\nNow, we replace $f$ with an interpolation:\nWe interpolate $f$ over the nodes: $n-p, n-p+1, \u0026hellip; n$ so to have an explicit method (Adams-Bashforth) We interpolate $f$ over the nodes: $n-p, n-p+1, \u0026hellip; n, n+1$ Adams methods have always:\n$$\\mathbf{a} = \\begin{bmatrix} 1 \\ 0 \\ \\vdots \\ 0 \\end{bmatrix}$$\nA Rapid Recall of Difference Equations Theory\r#\rA linear difference equation is an equation in the form:\n$$u_{n+p} + a_{n+p-1}u_{n+p-1} + a_{n+p-2}u_{n+p-2} + \\ldots + a_0u_n = \\varphi_{n+p}$$\nwhere the solution is the set ${u_j}$ and initial conditions $u_0, u_1, \\ldots u_{p-1}$ are given.\nThe theory on difference equations has several similarities with the theory on differential equations. In particular, the general solution of a linear difference equation is the linear combination of a particular solution of the equation and the general solution of the homogeneous one.\nThe general solution of the homogeneous takes the form:\n$$u_n = \\sum_{j=0}^{N}\\left(\\sum_{s=0}^{m_j-1} V_{js}n^s \\right)r_j^n$$\nwhere $r_j$ are the roots of:\n$$r^p + a_{p-1}r^{p-1} + a_{p-2}r^{p-2} + \\ldots + a_0 = 0$$\nand\n$N$ is the number of distinct roots $m_j$ is the multiplicity of $r_j$ We will see a strong connection between this theory and the analysis of the linear multi-step methods.\nIn fact, a LMM reads like:\n$$u_{n+1} - \\sum_{j=0}^p a_j u_{n-j} = \\Delta t \\sum_{j=-1}^p b_j f_{n-j}$$\nIf we consider the model problem, we are lead to the linear difference equation:\n$$u_{n+1} - \\sum_{j=0}^p a_j u_{n-j} - \\Delta t \\lambda \\sum_{j=-1}^p b_j u_{n-j} = 0$$\nso we need to solve exactly a linear difference equation.\nMore precisely, we still define the LTE as the residual of the exact solution in the numerical scheme:\n$$\\Delta t , \\tau_{n+1} = y_{ex}(t_{n+1}) - \\sum_{j=0}^p a_j y_{ex}(t_{n-j}) - \\Delta t \\sum_{j=-1}^p b_j \\frac{dy_{ex}}{dt}(t_{n-j})$$\nThe method is CONSISTENT when $\\tau_{n+1} \\xrightarrow{\\Delta t \\to 0} 0 \\quad \\forall n$\nZero-Stability Definition\r#\rThe definition of zero-stability is similar to the one for One-step Methods.\nAlso, we define:\nFirst characteristic polynomial:\n$$\\rho(z) = z^{p+1} - \\sum_{j=0}^p a_j z^{p-j}$$\nSecond characteristic polynomial:\n$$\\sigma(z) = b_{-1}z^{p+1} - \\sum_{j=0}^p b_j z^{p-j}$$\nand the polynomial: $\\Pi(z) = \\rho(z) - \\Delta t \\lambda \\sigma(z)$ (this is the polynomial found for the model problem)\nBased on this we define:\nRoot Condition: Call $r_i$ the roots of $\\rho(z)$. The polynomial (or the associated LMM) fulfills the root condition when:\n(1) $|r_i| \\leq 1 \\quad \\forall i$ (2) The roots with $|r| = 1$ have multiplicity 1.\nStrong Root Condition: In addition to the R.C., only $r_0$ is s.t. $|r_0| = 1$, all the other roots $|r_j| \u0026lt; 1$ $(j \u0026gt; 1, \\ldots, p)$.\nAbsolute R.C.: $\\exists \\Delta t \\leq \\overline{\\Delta t}$ s.t. all the roots $r_j(\\Delta t)$ of $\\Pi_{\\Delta t}(z)$ are s.t. $|r_j(\\Delta t)| \u0026lt; 1$, $j = 0, \\ldots, p$, $\\Delta t \\leq \\overline{\\Delta t}$.\nAnalysis of Multistep Methods\r#\rWe have a sequence of theorems (no proofs):\nA LMM is consistent if and only if:\n$$\\sum_{j=0}^p a_j = 1 \\quad -\\sum_{j=0}^p j a_j + \\sum_{j=-1}^p b_j = 1$$\nAlso, the method is at least of order $p$ if the solution is $\\in C^{p+1}(I)$ and\n$$(*)\\ \\sum_{j=0}^p (j)^k a_j + k \\sum_{j=-1}^p (j)^{k-1} b_j = 1 \\quad k = 1, 2, \\ldots q$$\nRemark: The condition $\\sum_{j=0}^p a_j = 1$ means that the first characteristic polynomial $\\rho(z)$ has at least one root in 1.\nA consistent method is zero-stable if and only if it fulfills the root condition.\nWith Theorems (1) + (2) we have the convergence and order analysis.\n(1) + (2): If the LMM method falls into (1) and (2) with condition (*) (not true for q+1) and the initial conditions ($u_i \\to y_{ex}(t_i)$ $i = 0, \\ldots, p$) converge to the exact ones with order $q$, the resulting method is convergent with order $q$.\nRemark (First Dahlquist Barrier): There is no zero-stable $p$-LMM with order\n$\u0026gt; p+1$ for $p$ odd $\u0026gt; p+2$ for $p$ even Let\u0026rsquo;s turn now to the Absolute stability.\nThe absolute root condition is necessary and sufficient for the absolute stability. In fact, if $\\overline{\\Delta t} = +\\infty$, the absolute stability is unconditional. The analysis of a LMM is completed by analyzing the coefficients of the method and the roots of the two polynomials $\\rho$ and $\\Pi$.\nThe roots of $\\Pi$ can be analyzed numerically (and are tabulated for most of the methods) by Matlab/Python subroutines.\nSEE EXAMPLES (NODEPY library in Python, QSS in Matlab)\nRemark (Second Dahlquist Barrier): There are no explicit LMM absolutely unconditionally stable. There are no LMM absolutely unconditionally stable with order $q \u0026gt; 2$.\nA Clarification on Stability Concepts\r#\rTo clarify the different stability concepts:\nZero-stability:\n$$|u_j| \\leq C_{T_{fin}} (|u_0| + \\ldots |u_p|)$$\nwhere the $C$ may depend on $T_{fin}$\nAbsolute stability:\n$$C_{T_{fin}} \\xrightarrow{T_{fin} \\to \\infty} 0$$\n$C$ is bounded independently of $T_{fin}$\nWe call this \u0026ldquo;relative stability\u0026rdquo;\n$$\\text{for a consistent scheme} \\quad \\text{R.C.} \\Leftarrow \\text{Strong R.C.} \\Leftarrow\\text{A.R.C.}$$ $$\\text{CONVERGENCE} \\Leftarrow \\text{Zero-Stability} \\Leftarrow \\text{Relative Stability} \\Leftarrow \\text{Absolute Stability}$$\nExample (Extreme)\r#\rMid-point:\n$$u_{n+1} = u_{n-1} + 2\\Delta t , f(t_n, u_n)$$\n$$a_0 = 0 \\quad a_1 = 1 \\quad \\Rightarrow \\rho(z) = z^2 - 1 = 0 \\quad \\Rightarrow \\rho = \\pm 1$$\nR.C.: OK\n$$\\Pi_{\\Delta t}(z) = z^2 - 2\\lambda z - 1$$\nThe product of the two roots is always -1, if one root is \u0026lt; 1 in magnitude, the other is \u0026gt; 1.\nUnconditionally Absolutely UNSTABLE\nPredictor-Corrector Methods\r#\rA clear message from the previous examples is that, in general, explicit methods are less absolutely stable, they are never unconditionally stable and have smaller regions of stability.\nHowever, let\u0026rsquo;s reconsider implicit methods too.\nFor instance, let\u0026rsquo;s consider a generic LMM:\n$$u_{n+1} - \\sum_{j=0}^p a_j u_{n-j} = b_{-1} \\Delta t , f(t_{n+1}, u_{n+1}) + \\Delta t \\sum_{j=0}^p b_j f(t_{n-j}, u_{n-j})$$\nTo solve this nonlinear equation, we could use a Newton method but also a fixed-point method and a natural candidate is the following:\n$$u_{n+1}^{(m)} = \\sum_{j=0}^p a_j u_{n-j} + \\Delta t \\sum_{j=0}^p b_j f_{n-j} + \\Delta t b_{-1} f(t_{n+1}, u_{n+1}^{(m-1)})$$\nNotice that this method converges if:\n$$\\left|\\Delta t , b_{-1} , f\u0026rsquo;(t_{n+1}, u_{n+1}) \\right| \u0026lt; 1$$\nSo we have the condition:\n$$\\Delta t \u0026lt; \\frac{1}{|b_{-1}||f\u0026rsquo;|}$$\nEven if the method is unconditionally stable, we may have a condition on $\\Delta t$ for the convergence of the fixed-point.\nIn any case, a good initial condition is required, because with a good $u^{(0)}$ the number of iterations can be reduced: an explicit method can be used to this.\nThese considerations lead to the design of a new class of methods, Heun being one of the possible examples.\nPredictor [P] is an explicit method of order $q_P$\nCorrector [C] is an implicit method of order $q_C$\nPredictor Corrector method:\nP: do one step of P $\\Rightarrow u_{n+1}^{(0)}$\nE: evaluate $f(t_{n+1}, u_{n+1}^{(0)})$\nC: compute $m$ fixed-point iterations of C\nOptional: E: compute $f_{n+1} = f(t_{n+1}, u_{n+1}^{(m)})$\nThese methods go under the name of $\\text{PEC}^m$ or $\\text{PEC}^m\\text{E}$ if the last step is taken.\nExamples:\r#\rP = Adams-Bashforth of order 2 C = Adams-Moulton of order 3 $m = 1$\nPEC: $$\\begin{cases} u_{n+1}^{(0)} = u_n^{(1)} + \\frac{\\Delta t}{2}(3f_n^{(0)} - f_{n-1}^{(0)}) \\ f_{n+1}^{(0)} = f(t_{n+1}, u_{n+1}^{(0)}) \\ u_{n+1}^{(1)} = u_n^{(1)} + \\frac{\\Delta t}{12}(5f_{n+1}^{(0)} + 8f_n^{(0)} - f_{n-1}^{(0)}) \\end{cases}$$\nPECE: $$\\begin{cases} u_{n+1}^{(0)} = u_n^{(1)} + \\frac{\\Delta t}{2}(3f_n^{(1)} - f_{n-1}^{(1)}) \\ f_{n+1}^{(0)} = f(t_{n+1}, u_{n+1}^{(0)}) \\ u_{n+1}^{(1)} = u_n^{(1)} + \\frac{\\Delta t}{12}(5f_{n+1}^{(0)} + 8f_n^{(1)} - f_{n-1}^{(1)}) \\ f_{n+1}^{(1)} = f(t_{n+1}, u_{n+1}^{(1)}) \\end{cases}$$\nA natural question is: what is the accuracy of $\\text{PEC}^m$ or $\\text{PECE}^m$? What is the region of absolute stability?\nTheorem (Accuracy): $\\text{PECE}$ methods have (under regularity assumptions on the solution) order of accuracy:\n$$q_{PC} = min(q_P + m, q_C)$$\nFor the region of absolute stability, in general:\n$$\\text{Region}(P) \\subseteq \\text{Region}(\\text{PECE}) \\subseteq \\text{Region}(C)$$\nand $\\text{Region}(\\text{PECE}) \\xrightarrow{m \\to +\\infty} \\text{Region}(C)$\nRunge-Kutta Methods\r#\rHeun is also an RK method:\n$$u_{n+1} = u_n + \\Delta t\\left(f_n + f(t_{n+1}, u_n + \\Delta t f_n)\\right)$$\nThis is a 2nd order method but the accuracy is not obtained using more steps, but giving up the linearity of the method.\nIn general, RK are in the form:\n$$u_{n+1} = u_n + \\Delta t , \\mathbf{K} \\cdot \\mathbf{b}$$\nwhere $\\mathbf{K}, \\mathbf{b} \\in \\mathbb{R}^s$\nand $[\\mathbf{K}]_i = f(t_n + c_i \\Delta t, u_n + \\Delta t[A\\mathbf{K}]_i)$\nwhere $[\\mathbf{A}\\mathbf{K}]_i$ is the $i^{th}$ entry of the vector $\\mathbf{A}\\mathbf{K}$\nThe method is characterized by: $s$ (stages), $\\mathbf{b}$, $\\mathbf{c}$ and $\\mathbf{A} \\in \\mathbb{R}^{s \\times s}$\nIn particular, the three \u0026ldquo;ingredients\u0026rdquo; are generally written as:\n$$\\frac{\\mathbf{c} | \\mathbf{A}}{\\mathbf{b}^T} \\quad \\text{(Butcher array)}$$\nIt is assumed that $c_i = \\sum_{j=1}^s a_{ij} \\quad \\forall i = 1, \\ldots s$\nIf $\\mathbf{A}$ is such that $a_{ij} = 0 \\quad \\forall j \\geq i$ then:\n$$K_i = [\\mathbf{K}]i = f(t_n + \\Delta t c_i, u_n + \\Delta t \\sum{j \u0026lt; i} a_{ij} K_j)$$\nso the computation of $K_i$ is immediate. We call this case an explicit RK scheme.\nIf $a_{ij} = 0 \\quad \\forall j \u0026gt; i$, then:\n$$K_i = [\\mathbf{K}]i = f(t_n + \\Delta t c_i, u_n + \\Delta t \\sum{j \u0026lt; i} a_{ij} K_j + \\Delta t a_{ii} K_i)$$\nso we need to solve a non-linear equation to obtain $K_i$. We call this a semi-implicit method.\nIn general, computing $\\mathbf{K}$ requires the solution of a non-linear system (implicit method).\nClearly, the computational cost increases in the three cases.\nDerivation of an Explicit RK Method\r#\rA possible strategy to devise an explicit method is to maximize the order of the LTE with Taylor expansion analysis.\nFor instance, for $s = 2$:\n$$\\begin{pmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\ c_2 \u0026amp; a_{21} \u0026amp; 0 \\ \\hline \u0026amp; b_1 \u0026amp; b_2 \\end{pmatrix}$$\nWe have three parameters, but we set $a_{21} = c_2$.\n$$u_{n+1} = u_n + \\Delta t(b_1 f(t_n, u_n) + b_2 f(t_n + c_2 \\Delta t, u_n + \\Delta t c_2 f(t_n, u_n)))$$\n$$y_{ex}(t_n) = y_{ex}(t_n) + \\Delta t \\frac{dy_{ex}}{dt}(t_n) + \\frac{\\Delta t^2}{2}\\frac{d^2y_{ex}}{dt^2}(t_n) + \\frac{\\Delta t^3}{3!}\\frac{d^3y_{ex}}{dt^3}(t_n) + H.O.T.$$\n$$f(t_n + c_2 \\Delta t, u_n + \\Delta t c_2 f(t_n, u_n)) =$$\n$$= f(t_n, u_n) + \\frac{\\partial f}{\\partial t}(t_n, y_n)c_2 \\Delta t + \\frac{\\partial f}{\\partial y}(t_n, y_n)c_2 \\Delta t f(t_n, u_n) =$$\nNotice that: $$\\frac{d^2y}{dt^2} = \\frac{df}{dt} = \\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{dy}\\frac{dy}{dt} = \\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{\\partial y}f$$\n$$\\Rightarrow f(t_n, u_n) + c_2 \\Delta t \\frac{d^2y}{dt^2}(t_n)$$\nThe Taylor expansion applied to the scheme reads:\n$$u_{n+1} = u_n + \\Delta t(b_1 + b_2)f + c_2 b_2 \\Delta t^2 \\frac{d^2y}{dt^2}$$\nWe match therefore the first terms of the Taylor expansion for:\n$$\\begin{align} b_1 + b_2 \u0026amp;= 1 \\ b_2 c_2 \u0026amp;= \\frac{1}{2} \\end{align}$$\nFor $b_2 = \\frac{1}{2}$ we obtain the Heun method.\nThe L.T.E is $\\Delta t , \\tau \\sim O(\\Delta t^3)$\nso the method is 2nd order.\nImplicit methods can be devised from Gaussian quadratures.\nAnalysis of RK\r#\rCONSISTENCY: As the previous example shows, we need $\\sum b_i = 1$. This is necessary and sufficient for the consistency.\nZERO-STABILITY: RK are One-Step methods, if $f$ is Lipschitz-Continuous, they are zero-stable.\nORDER: In general, the order we can obtain depends on the number of stages in a (non-trivial) way:\nExplicit RK: ORDER 1 2 3 4 5 6 7 8 $s_{min}$ 1 2 3 4 6 7 9 11 $s_{min}$ = minimum number of stages to obtain the corresponding order.\nABSOLUTE STABILITY\r#\rIf we write the method for the model problem, we can write:\n$$u_{n+1} = \\mathcal{R}(\\Delta t \\lambda) u_n$$\nThe region of absolute stability is, in general, the non-trivial region where $|\\mathcal{R}(\\Delta t \\lambda)| \u0026lt; 1$ (in $\\mathbb{C}$).\nWhy are RK so popular?\r#\rRK are extremely popular, because they can be high order with \u0026ldquo;only\u0026rdquo; one-step.\nOne-step means that:\nwe do not need high-order approximation of the initial data needed by LMM (the initial condition is enough) we can easily perform the time-step ADAPTIVITY (much more difficult with LMM). For the adaptivity, there are smart combinations of RK that obtain the adaptivity (i.e. an error estimate to adapt the step) in an efficient way.\nOne of the most popular is: RK45 Fehlberg: smart combination of an explicit method of order 4 and an explicit of order 5 to adapt the step.\nA Final Note on Stiff problems\r#\rMany of the concepts used here can be extended to systems of ODEs:\n$$\\begin{cases} \\frac{d\\mathbf{y}}{dt} = \\mathbf{F}(t, \\mathbf{y}) \\ \\mathbf{y}(0) = \\mathbf{y}_0 \\end{cases}$$\nIn the case of a linear ODE system:\n$$\\mathbf{F} = A , \\mathbf{y} \\quad \\downarrow \\quad \\text{matrix}$$\nThere is, however, an important concept to clarify.\nConsider a simple 2√ó2 problem:\n$$\\frac{d\\mathbf{y}}{dt} = A\\mathbf{y}$$\nwhere $A$ has the two eigenvalues: $\\begin{cases} \\lambda_1 = -10^6 \\ \\lambda_2 = -1 \\end{cases}$\nIf we use Explicit Euler:\n$$\\mathbf{y}^{n+1} = \\mathbf{y}^n + \\Delta t , A \\mathbf{y}^n$$\nthe region of absolute stability is:\n$$\\Delta t \\leq \\min\\left(\\frac{2}{10^6}, \\frac{2}{1}\\right) = 2 \\cdot 10^{-6}$$\nThe solution, on the other hand, is the linear combination of the two functions:\n$$e^{-10^6 t}, e^{-t}$$\n[Graph showing rapid decay of $e^{-10^6 t}$ vs slower decay of $e^{-t}$]\nTo capture the fast dynamics ($\\lambda = 10^{-6}$), that fades away immediately, we need to take $\\Delta t \\sim 10^{-6}$!!!\nAn explicit method is certainly not a good choice here.\nIn general, we say that a problem is \u0026ldquo;stiff\u0026rdquo; when it may require very stringent time-step in a non-efficient way. The name \u0026ldquo;stiff\u0026rdquo; originates from the coupling of springs with different stiffness:\n[Simple diagram of a mass connected to two springs with different spring constants]\nto study the dynamics of the two real balls, one can write an ODE system.\nIf $K_1 \u0026laquo; K_2$ this is a stiff problem.\nEXERCISE on LMM\r#\rConsider the following family of methods (LMM):\n$$u_{n+1} = \\alpha u_n + (1-\\alpha)u_{n-1} + \\Delta t , \\gamma f_{n+1}$$\nInvestigate the convergence properties of the method as function of $\\alpha$ and $\\gamma$. Sol: For $\\alpha \\neq 1$, the method is 2-step. For $\\gamma \\neq 0$, the method is implicit.\nIt\u0026rsquo;s a LMM with: $\\mathbf{a} = \\begin{bmatrix} \\alpha \\ 1-\\alpha \\ 0 \\end{bmatrix} \\begin{bmatrix} 0 \\ 1 \\ 0 \\end{bmatrix}$, $\\mathbf{b} = \\begin{bmatrix} \\gamma \\ 0 \\ 0 \\end{bmatrix} \\begin{bmatrix} -1 \\ 0 \\ 1 \\end{bmatrix}$\nConsistency: $\\sum a_j = 1$: $\\alpha + (1-\\alpha) = 1$ ‚úìOK $-\\sum j a_j + \\sum b_j = 1$: $0 \\cdot \\alpha - 1 \\cdot (1-\\alpha) + \\gamma = 1$ $\\Rightarrow \\gamma = 2 - \\alpha$\nThe methods: $u_{n+1} = \\alpha u_n + (1-\\alpha)u_{n-1} + \\Delta t (2-\\alpha)f_{n+1}$ are consistent.\nOrder: $\\sum (j)^2 a_j + 2\\sum (-j)^1 b_j = 1$ ? $\\Rightarrow (1-\\alpha) + 2(2-\\alpha) = 1 \\Rightarrow \\alpha = \\frac{4}{3}, \\gamma = \\frac{2}{3}$\nInvestigate the absolute stability of the method with order 2. The method: $u_{n+1} = \\frac{4}{3}u_n - \\frac{1}{3}u_{n-1} + \\Delta t \\frac{2}{3}f_{n+1}$\nis of order 2 (it is, in fact, a BDF of order 2).\n$\\rho(z) = z^2 - \\frac{4}{3}z + \\frac{1}{3} = 0 \\quad r_{1,2} = \\frac{\\frac{4}{3} \\pm \\sqrt{\\frac{16}{9} - \\frac{4}{3}}}{2} = \\frac{4 \\pm 2}{6} = \\frac{2 \\pm 1}{3}$\nR.C. ‚úì\n$\\Pi_{\\Delta t}(z) = z^2 - \\frac{4}{3}z + \\frac{1}{3} - \\Delta t \\lambda \\frac{2}{3}z^2 = 0$\nLet\u0026rsquo;s consider $\\lambda \\in \\mathbb{R}^-$:\n$(3 - \\Delta t \\lambda 2)z^2 - 4z + 1 = 0$\n$z^2 - \\frac{4z}{3 + 2\\Delta t|\\lambda|} + \\frac{1}{3 + 2\\Delta t|\\lambda|} = 0$\n$r_1 \\cdot r_2 = \\frac{1}{3 + 2\\Delta t|\\lambda|}$\n$r_1 = \\frac{2 + \\sqrt{1 - 2\\Delta t|\\lambda|}}{3 + 2\\Delta t|\\lambda|} \\xrightarrow{\\Delta t \\to 0} 1$\n$|r_1(\\Delta t)| \u0026lt; 1 \\quad \\forall \\Delta t \u0026gt; 0$\n$r_2 = \\frac{2 - \\sqrt{1 - 2\\Delta t|\\lambda|}}{3 + 2\\Delta t|\\lambda|} \\xrightarrow{\\Delta t \\to 0} \\frac{1}{3}$\n$|r_2(\\Delta t)| \u0026lt; \\frac{1}{3}$\nMethod unconditionally stable (Verify with Python/Matlab).\nSolve $$\\begin{cases} \\frac{dy}{dt} = -(1 + t_g(t))y \u0026amp; t \\in [0, 1] \\ y(0) = 1 \\end{cases}$$ with the BDF2 method found and verify your results, knowing that the exact solution is $y_{ex} = e^{-x}\\cos(x)$.\nUsing MATLAB, the problem is easily solved with the QSS subroutines:\nqssstab.m (draws the region of absolute stability) qssmulti.m (solves with a generic LMM)\nWith Python there are many libraries: SciPy, odeint that uses RK, ode, and allows the selection of BDF methods, but generally with adaptive time step.\nNODEPY is potentially an excellent library but buggy.\nIt\u0026rsquo;s excellent for drawing the stability region (see hmm.py on Canvas)\nI have written a simple LMM solver with fixed-point iterations for implicit methods.\nUsing my-hmm.py you can verify that our method is 2nd order:\nmax error $3 \\cdot 10^{-4}$ $8 \\cdot 10^{-5}$ $2 \\cdot 10^{-5}$ $\\Delta t$ 0.05 0.025 0.0125 [Graph showing exact vs numerical solution]\n[Stability region diagram showing a circle in the complex plane] Red = Region of Stability\n"},{"id":6,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/Exercise/","title":"‰π†È¢ò","section":"Á¨¨‰πùÁ´†","content":"\rProblem 1: Finite-Difference Solution\r#\rWe wish to discretize and solve the boundary-value problem\n$$-\\mu\\frac{d^2u}{dx^2} + \\beta\\frac{du}{dx} = f(x),\\quad x\\in(0,1),\\quad u(0)=u(1)=0$$\nOr equivalently:\n$$\\begin{cases} -\\mu u\u0026rsquo;\u0026rsquo;(x) + \\beta u\u0026rsquo;(x) = f(x), \u0026amp; 0\u0026lt;x\u0026lt;1,\\ u(0)=0,; u(1)=0. \\end{cases}$$\nWhere $\\mu\u0026gt;0$ and $\\beta$ is a constant (possibly negative), and $f\\in C^0(0,1)$.\n1. Finite-Difference Discretization\r#\rDivide $[0,1]$ into $N$ equal subintervals so that $\\Delta x = \\frac{1}{N}$. Let\n$$x_j = j\\Delta x,\\quad j=0,1,2,\\dots,N,$$\nso that $x_0=0$ and $x_N=1$. We approximate $u(x_j)\\approx u_j$. The boundary conditions become $u_0=0$ and $u_N=0$.\n1.1 Central Difference Scheme (for the interior points)\r#\rA standard centered second-difference for $u\u0026rsquo;\u0026rsquo;(x)$ at $x_j$ is:\n$$u\u0026rsquo;\u0026rsquo;(x_j) \\approx \\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2}.$$\nA standard centered first-difference for $u\u0026rsquo;(x)$ at $x_j$ is:\n$$u\u0026rsquo;(x_j) \\approx \\frac{u_{j+1} - u_{j-1}}{2\\Delta x}.$$\nHence, the PDE $-\\mu u\u0026rsquo;\u0026rsquo;(x) + \\beta u\u0026rsquo;(x)=f(x)$ becomes for $j=1,\\dots,N-1$:\n$$-\\mu\\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1} - u_{j-1}}{2\\Delta x} = f(x_j).$$\nWe impose $u_0 = 0$ and $u_N=0$.\n2. Local Truncation Error\r#\rThe second-order central difference for $u\u0026rsquo;\u0026rsquo;$ is $O((\\Delta x)^2)$ accurate. The central difference for $u\u0026rsquo;$ is also $O((\\Delta x)^2)$ accurate. Hence the local truncation error of the combined scheme is $O((\\Delta x)^2)$.\n3. Matrix Form\r#\rCollect unknowns $u_1,u_2,\\dots,u_{N-1}$ into a vector $\\mathbf{u}=(u_1,\\dots,u_{N-1})^T$. The boundary values $u_0=0$ and $u_N=0$ are known.\nRewrite the finite-difference equation for an interior index $j$:\n$$-\\mu\\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1} - u_{j-1}}{2\\Delta x} = f(x_j).$$\nWe can factor out coefficients:\nLet $\\alpha \\equiv \\frac{\\mu}{(\\Delta x)^2}$. Let $\\gamma \\equiv \\frac{\\beta}{2\\Delta x}$. Then the coefficient of $u_j$ is $2\\alpha$, the coefficient of $u_{j+1}$ is $-\\alpha + \\gamma$, and the coefficient of $u_{j-1}$ is $-\\alpha - \\gamma$. Thus, in matrix form:\n$$\\begin{pmatrix} 2\\alpha \u0026amp; -\\alpha+\\gamma \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\ -\\alpha-\\gamma \u0026amp; 2\\alpha \u0026amp; -\\alpha+\\gamma \u0026amp; \\cdots \u0026amp; 0\\ 0 \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\vdots\\ \\vdots \u0026amp; \u0026amp; -\\alpha-\\gamma \u0026amp; 2\\alpha \u0026amp; -\\alpha+\\gamma\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; -\\alpha-\\gamma \u0026amp; 2\\alpha \\end{pmatrix} \\begin{pmatrix} u_1\\ u_2\\ \\vdots \\ u_{N-2}\\ u_{N-1} \\end{pmatrix}\r#\r\\begin{pmatrix} f(x_1)\\ f(x_2)\\ \\vdots \\ f(x_{N-2})\\ f(x_{N-1}) \\end{pmatrix}.$$\nThis is a tridiagonal linear system, solvable by standard methods (e.g., Thomas algorithm).\n4. Quality of the Solution vs. $\\beta/\\mu$\r#\rThe ratio $\\frac{\\beta}{\\mu}$ often plays the role of a P√©clet-type number in advection-diffusion problems.\nIf $\\frac{\\beta}{\\mu}$ is small (diffusion-dominated), the solution is usually smooth and well-behaved under central differencing. If $\\frac{\\beta}{\\mu}$ is large (advection-dominated), pure central differences may produce spurious oscillations unless $\\Delta x$ is refined or upwinding techniques are used to stabilize the discrete solution. 5. Upwind Method (First Order) for $\\beta\u0026lt;0$\r#\rWhen $\\beta\u0026lt;0$, the \u0026ldquo;flow\u0026rdquo; is from right to left, so an upwind difference for the first derivative $\\beta u\u0026rsquo;(x)$ uses values on the \u0026ldquo;right\u0026rdquo; side at each $j$. Concretely, for $\\beta\u0026lt;0$, we replace:\n$$u\u0026rsquo;(x_j) \\approx \\frac{u_{j+1} - u_j}{\\Delta x} \\quad \\text{(a \u0026ldquo;backward\u0026rdquo; upwind if flow is leftward)}.$$\nHence, the difference equation becomes:\n$$-\\mu\\frac{u_{j+1} - 2u_j + u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1} - u_j}{\\Delta x} = f(x_j), \\quad j=1,\\dots,N-1.$$\n(This replaces the central difference in the advective term by a one-sided upwind difference.)\n5.1 No Spurious Oscillations\r#\rThe first-order upwind scheme for linear advection-diffusion is known to be monotone for any $\\Delta x\u0026gt;0$ when $\\beta\u0026lt;0$ (or, more generally, for any sign of $\\beta$ if we choose the correct upwind direction). Monotonicity prevents nonphysical oscillations. In short:\nCentral difference can oscillate if $|\\beta|$ is large relative to $\\mu$. Upwind difference sacrifices some accuracy (only first order in $\\Delta x$ for the advective term) but remains stable and nonoscillatory for any step size $\\Delta x$. Thus, with $\\beta\u0026lt;0$, the upwind approach $u\u0026rsquo;(x_j)\\approx (u_{j+1}-u_j)/\\Delta x$ ensures a stable, physically plausible solution without oscillations.\n6. Summary\r#\rEquation \u0026amp; Discretization\n$$-\\mu u\u0026rsquo;\u0026rsquo; + \\beta u\u0026rsquo; = f(x), \\quad u(0)=u(1)=0 \\longrightarrow \\begin{cases} -\\mu\\frac{u_{j+1}-2u_j+u_{j-1}}{(\\Delta x)^2} +\\beta\\frac{u_{j+1}-u_{j-1}}{2\\Delta x} = f(x_j),\\ u_0=0,;u_N=0. \\end{cases}$$\nLocal Truncation Error is $O((\\Delta x)^2)$ for the centered scheme.\nMatrix Form: A standard tridiagonal system with bands $-\\alpha\\mp \\gamma$, $2\\alpha$, $-\\alpha\\pm \\gamma$.\nEffect of $\\beta/\\mu$: If $|\\beta|$ is large relative to $\\mu$, central differences can produce oscillatory solutions; upwind methods help.\nUpwind Method (for $\\beta\u0026lt;0$): $$-\\mu\\frac{u_{j+1}-2u_j+u_{j-1}}{(\\Delta x)^2} + \\beta\\frac{u_{j+1}-u_j}{\\Delta x} = f(x_j)$$ prevents oscillations for any $\\Delta x$.\nProblem 2\r#\rSolution Outline\nWe have the linear advection equation\n$$\\frac{\\partial u}{\\partial t} ;+; a,\\frac{\\partial u}{\\partial x} ;=; 0,\\quad x\\in \\mathbb{R},;t\u0026gt;0,$$ with initial condition $u(x,0) = u_0(x)$. We wish to:\nDerive the Lax‚ÄìWendroff scheme for this PDE. Determine the CFL condition for stability, i.e.\\ find the condition on $\\displaystyle \\frac{|a|\\Delta t}{\\Delta x}$. 1) Derivation of the Lax‚ÄìWendroff Scheme\r#\rA succinct way to derive Lax‚ÄìWendroff is via a second-order Taylor expansion in time about $t^n$:\n$$u^{n+1}i ;\\approx; u(x_i,,t_n + \\Delta t) ;=; u(x_i,t_n) ;+;\\Delta t,\\frac{\\partial u}{\\partial t} ;+;\\tfrac{(\\Delta t)^2}{2},\\frac{\\partial^2 u}{\\partial t^2};\\bigg|{(x_i,t_n)}.$$\nFrom the PDE $\\partial_t u = -,a,\\partial_x u$, we can replace time-derivatives by spatial derivatives:\nFirst derivative in time: $$\\frac{\\partial u}{\\partial t} ;=; -,a,\\frac{\\partial u}{\\partial x}.$$\nSecond derivative in time: $$\\frac{\\partial^2 u}{\\partial t^2} ;=; \\frac{\\partial}{\\partial t}\\Bigl(-,a,\\frac{\\partial u}{\\partial x}\\Bigr) ;=; -,a,\\frac{\\partial}{\\partial x}\\Bigl(\\frac{\\partial u}{\\partial t}\\Bigr) ;=; -,a,\\frac{\\partial}{\\partial x}\\Bigl(-,a,\\frac{\\partial u}{\\partial x}\\Bigr) ;=; a^2,\\frac{\\partial^2 u}{\\partial x^2}.$$\nHence,\n$$u^{n+1}_i ;\\approx; u^{n}i ;-; a,\\Delta t ,\\frac{\\partial u}{\\partial x} ;+; \\frac{a^2(\\Delta t)^2}{2},\\frac{\\partial^2 u}{\\partial x^2} ;\\Bigg|{(x_i,t_n)}.$$\nDiscretizing the spatial derivatives\r#\rWe replace the first and second spatial derivatives by standard centered finite differences:\n$$\\frac{\\partial u}{\\partial x}\\bigg|{x_i} \\approx \\frac{u{i+1}^n - u_{i-1}^n}{2,\\Delta x}, \\qquad \\frac{\\partial^2 u}{\\partial x^2}\\bigg|{x_i} \\approx \\frac{u{i+1}^n - 2,u_i^n + u_{i-1}^n}{(\\Delta x)^2}.$$\nPutting this all together:\n$$u_{i}^{n+1} ;=; u_{i}^{n} ;-; a,\\Delta t ,\\frac{u_{i+1}^{n} - u_{i-1}^{n}}{2,\\Delta x} ;+; \\frac{a^2,(\\Delta t)^2}{2}, \\frac{u_{i+1}^{n} - 2,u_{i}^{n} + u_{i-1}^{n}}{(\\Delta x)^2}.$$\nIt is common to set $\\displaystyle \\nu ;=;\\frac{a,\\Delta t}{\\Delta x}$. Then the scheme reads\n$$\\boxed{ u_{i}^{n+1} ;=; u_{i}^{n} ;-;\\frac{\\nu}{2},\\bigl(u_{i+1}^{n} - u_{i-1}^{n}\\bigr) ;+;\\frac{\\nu^{2}}{2}, \\bigl(u_{i+1}^{n} ;-;2,u_{i}^{n} ;+;u_{i-1}^{n}\\bigr). }$$\nThis is the Lax‚ÄìWendroff scheme for the linear advection equation.\n2) The CFL Stability Condition\r#\rA standard von Neumann (Fourier) stability analysis, or the usual Lax‚ÄìRichtmyer theory for hyperbolic PDEs, shows that Lax‚ÄìWendroff is stable if and only if the Courant number satisfies\n$$\\bigl|,\\nu,\\bigr| ;=; \\biggl|\\frac{a,\\Delta t}{\\Delta x}\\biggr| ;\\le; 1.$$\nTherefore among the multiple-choice options, the correct condition is\n$$\\boxed{;; \\bigl|\\tfrac{a,\\Delta t}{\\Delta x}\\bigr| ;\\le; 1.}$$\n3) Motivation\r#\rDomain of dependence argument. For the PDE $\\partial_t u + a,\\partial_x u = 0,$ characteristics travel with speed $a$. Numerically, we must ensure that information from these characteristics is captured on the grid from one time step to the next; that is the essence of the CFL condition. If $|a|\\Delta t \u0026gt; \\Delta x$, the method ‚Äújumps over‚Äù grid cells and fails to remain stable.\nVon Neumann analysis. Substituting $u_i^n = \\lambda^n e^{ikx_i}$ into the scheme shows that the amplification factor $|\\lambda|$ is $\\le 1$ if and only if $\\bigl|\\frac{a\\Delta t}{\\Delta x}\\bigr|\\le 1.$\nHence the short answer is:\nLax‚ÄìWendroff has the form $$u_{i}^{n+1}\r#\ru_{i}^{n}\r#\r\\frac{\\nu}{2},(u_{i+1}^{n} - u_{i-1}^{n}) + \\frac{\\nu^{2}}{2},(u_{i+1}^{n}-2u_{i}^{n}+u_{i-1}^{n}),$$ The method is stable if and only if $\\displaystyle \\bigl|\\frac{a\\Delta t}{\\Delta x}\\bigr|\\le 1.$ This condition is exactly the usual CFL requirement ensuring the numerical domain of dependence covers the PDE‚Äôs domain of dependence. Problem 3\r#\rBelow is a fairly detailed derivation and discussion of the Crank‚ÄìNicolson (CN) method (the $\\theta$-method with $\\theta = \\tfrac12$) for the heat equation\n$$\\frac{\\partial u}{\\partial t};-;\\frac{\\partial^2 u}{\\partial x^2};=;f, \\quad x\\in(0,1),;t\u0026gt;0, \\quad u(0,t) ;=;u(1,t);=;0, \\quad u(x,0);=;u_0(x).$$\nSince the PDE can be written as $$u_t ;=; u_{xx} + f,$$ we will discretize in both space and time.\n1. The Crank‚ÄìNicolson Discretization\r#\rLet $\\Delta x = \\frac{1}{M}$ partition $[0,1]$ into $M+1$ grid points $x_i = i,\\Delta x$, $i=0,\\dots,M$, and let $\\Delta t$ be a time step, so $t^n = n,\\Delta t$. We write $u_i^n\\approx u(x_i,t^n)$. The standard second‚Äêorder central difference for $u_{xx}$ is\n$$u_{xx}(x_i,t^n) ;\\approx; \\frac{u_{i+1}^n - 2,u_i^n + u_{i-1}^n}{(\\Delta x)^2}.$$\nA $\\theta$‚Äêmethod (also called the $\\theta$-scheme) for $u_t = u_{xx} + f$ in time is:\n$$\\frac{u_i^{n+1} - u_i^n}{\\Delta t} ;=; \\theta\\Bigl[\\underbrace{\\tfrac{u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}}{(\\Delta x)^2} ;+; f_i^{n+1}}{\\text{‚Äúimplicit‚Äù part}}\\Bigr] ;+; \\bigl(1-\\theta\\bigr)\\Bigl[\\underbrace{\\tfrac{u{i+1}^{n} - 2u_i^{n} + u_{i-1}^{n}}{(\\Delta x)^2};+; f_i^{n}}_{\\text{‚Äúexplicit‚Äù part}}\\Bigr].$$\nCrank‚ÄìNicolson is the special case $\\theta = \\tfrac12$. Substituting $\\theta=\\tfrac12$, we get\n$$\\frac{u_i^{n+1} - u_i^n}{\\Delta t} ;=; \\tfrac12\\Bigl[\\tfrac{u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}}{(\\Delta x)^2} + f_i^{n+1}\\Bigr] ;+; \\tfrac12\\Bigl[\\tfrac{u_{i+1}^{n} - 2u_i^{n} + u_{i-1}^{n}}{(\\Delta x)^2} + f_i^n\\Bigr].$$\nRearranging terms gives $$u_i^{n+1} ;-; \\frac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}\\bigr) ;=; u_i^{n} ;+; \\frac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n} - 2u_i^n + u_{i-1}^{n}\\bigr) ;+; \\frac{\\Delta t}{2},\\bigl(f_i^{n+1} + f_i^n\\bigr).$$ One may think of this as the linear system $$\\bigl[I + \\tfrac{\\Delta t}{2,(\\Delta x)^2},A\\bigr],\\mathbf{u}^{n+1} ;=; \\bigl[I - \\tfrac{\\Delta t}{2,(\\Delta x)^2},A\\bigr],\\mathbf{u}^{n} ;+;\\frac{\\Delta t}{2},\\bigl(\\mathbf{f}^{n+1} + \\mathbf{f}^n\\bigr),$$ where $A$ is the usual tridiagonal matrix corresponding to the second‚Äêdifference operator (and where $\\mathbf{u}^n$ is the vector of $u_i^n$). After applying boundary conditions $u_0^n = u_M^n = 0$, one solves this tridiagonal system at each time step.\nBoundary Conditions\r#\rBecause $u(0,t)=u(1,t)=0$, we set $u_0^n=0$ and $u_M^n=0$ for all $n$. The updates are applied only for $i=1,\\dots,M-1$.\nSummary of the CN Update\r#\rIn ‚Äúindex form,‚Äù the Crank‚ÄìNicolson scheme is: $$\\boxed{ \\begin{aligned} \u0026amp;\\text{For }i=1,\\dots,M-1:\\quad u_i^{n+1} ;-; \\tfrac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}\\bigr) \\ \u0026amp;\\qquad\\quad;=; u_i^{n} ;+; \\tfrac{\\Delta t}{2,(\\Delta x)^2},\\bigl(u_{i+1}^{n} - 2u_i^n + u_{i-1}^{n}\\bigr) ;+;\\frac{\\Delta t}{2},\\bigl(f_i^{n+1} + f_i^n\\bigr). \\end{aligned} }$$ This is solved simultaneously for all $i$, respecting $u_0^{n+1}=u_M^{n+1}=0$.\n2. Properties: Stability \u0026amp; Accuracy\r#\rStability:\nUnconditional stability for the heat equation. In other words, there is no restriction on $\\Delta t$ relative to $\\Delta x$ needed solely for stability (unlike the explicit forward‚ÄêEuler method, which requires $\\Delta t \\le \\tfrac12 (\\Delta x)^2$). More precisely, CN is A‚Äêstable as an ODE solver applied to the linear diffusion operator. One can show by a von Neumann analysis or standard Lax‚ÄìRichtmyer theory that errors do not grow unboundedly for any $\\Delta t\u0026gt;0$. Accuracy:\nIn time, Crank‚ÄìNicolson is second‚Äêorder accurate, because it is essentially the trapezoidal rule in time (it uses $\\frac12$ of the ‚Äúnew‚Äù time‚Äêlevel‚Äôs spatial derivative plus $\\frac12$ of the ‚Äúold‚Äù time‚Äêlevel‚Äôs spatial derivative). In space, if we use the standard second‚Äêdifference approximation, the scheme is also second‚Äêorder in $\\Delta x$. Overall, we often say ‚ÄúCN is second‚Äêorder in both space and time (for sufficiently smooth solutions).‚Äù 3. BONUS: Discontinuous Initial Condition\r#\rEven if $u_0(x)$ is not continuous, the heat equation itself is smoothing: for $t\u0026gt;0$, the exact solution becomes infinitely differentiable in $x$. Numerically:\nThe scheme remains stable and convergent. Because it is a diffusion‚Äêtype PDE, any jump discontinuity in the initial data gets smoothed out instantly as $t$ increases. Crank‚ÄìNicolson will faithfully capture that smoothing. You may see large gradients at early time steps near the discontinuity, but the method will not become unstable. Hence having a discontinuous initial condition does not cause instability for the heat equation with Crank‚ÄìNicolson. The scheme still converges (second‚Äêorder in both space and time) to the unique smooth solution that the parabolic PDE defines for $t\u0026gt;0$.\nProblem 4 Ê±ÇËß£‰∫åÁª¥ÊãâÊôÆÊãâÊñØÊñπÁ®ãÁöÑ‰∫îÁÇπÂ∑ÆÂàÜÊ†ºÂºè\r#\rÊàë‰ª¨ËÄÉËôëÂ¶Ç‰∏ãÁöÑÊ§≠ÂúÜÊñπÁ®ãÔºàPoisson ÂûãÔºâÔºö $$-,\\Delta u ;=; f, \\quad (x,y)\\in [0,1] \\times [0,1].$$\nÂÖ∂‰∏≠ $$\\Delta ;=;\\frac{\\partial^2}{\\partial x^2} ;+;\\frac{\\partial^2}{\\partial y^2}$$ ÊòØ‰∫åÁª¥ÊãâÊôÆÊãâÊñØÁÆóÂ≠êÔºå$f$ ÊòØÂ∑≤Áü•ÁöÑÂáΩÊï∞„ÄÇ\n1. Âª∫Á´ãÁΩëÊ†º\r#\r‰ª§ $N_x$ Âíå $N_y$ ÂàÜÂà´Ë°®Á§∫Âú® $x$ Âíå $y$ ÊñπÂêë‰∏äÁöÑÁΩëÊ†ºÂàíÂàÜÊï∞ÁõÆÔºà‰ªÖÊåáÂÜÖÈÉ®ËäÇÁÇπÊï∞Ôºå‰∏çÂê´ËæπÁïåÔºâÔºåÂàôÊ≠•Èïø‰∏∫ $$\\delta x ;=;\\frac{1}{N_x+1}, \\quad \\delta y ;=;\\frac{1}{N_y+1}.$$ Êàë‰ª¨Âú®Âå∫Èó¥ $[0,1]\\times[0,1]$ ÂÜÖÂèñÁ¶ªÊï£ÁΩëÊ†ºÁÇπ $$x_i ;=; i,\\delta x, \\quad y_j ;=; j,\\delta y,$$ ÂÖ∂‰∏≠ $i=0,1,2,\\dots,N_x+1$, $j=0,1,2,\\dots,N_y+1$„ÄÇ\nÂú®ÂÜÖÈÉ®ËäÇÁÇπ $(x_i,y_j)$ ‰∏äÔºåÊàë‰ª¨Áî® $u_{i,j}$ Ë°®Á§∫ÂØπÁúüËß£ $u(x_i,y_j)$ ÁöÑÊï∞ÂÄºËøë‰ºº„ÄÇ\n2. ‰∫îÁÇπÂ∑ÆÂàÜÊ†ºÂºè\r#\rÊñπÁ®ã $-\\Delta u = f$ ÂèØÂÜôÊàê $$-\\left( \\frac{\\partial^2 u}{\\partial x^2} ;+; \\frac{\\partial^2 u}{\\partial y^2} \\right) ;=; f,$$ Âç≥ $$\\frac{\\partial^2 u}{\\partial x^2} ;+; \\frac{\\partial^2 u}{\\partial y^2} ;=; -,f.$$\nÂú®ÁΩëÊ†º‰∏äÔºå‰∫åÈò∂ÂØºÊï∞ÁöÑ‰∏≠ÂøÉÂ∑ÆÂàÜËøë‰ººÂàÜÂà´‰∏∫Ôºö\nÂú® $x$ ÊñπÂêëÔºö $$\\frac{\\partial^2 u}{\\partial x^2}\\bigg|{(x_i,y_j)} ;\\approx; \\frac{u{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2},$$ Âú® $y$ ÊñπÂêëÔºö $$\\frac{\\partial^2 u}{\\partial y^2}\\bigg|{(x_i,y_j)} ;\\approx; \\frac{u{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2}.$$ Âõ†Ê≠§Ôºå$\\Delta u$ Âú®Á¶ªÊï£ÂåñÂêéÂèØÂÜô‰∏∫ $$\\Delta u_{i,j} ;=; \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2}.$$\nÁî±‰∫éÊñπÁ®ãÊòØ $-\\Delta u = f$ÔºåÂàôÂØπÂ∫îÁöÑ‰∫îÁÇπÂ∑ÆÂàÜÊ†ºÂºè‰∏∫\n$$-\\left[, \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2} \\right] ;=; f_{i,j},$$ ÊàñÁ≠â‰ª∑Âú∞ÂÜôÊàê $$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2} ;=; -,f_{i,j}.$$\nËøôÈáå $f_{i,j} = f(x_i,y_j)$ Ë°®Á§∫Âú®ÁΩëÊ†ºÁÇπÂ§ÑÁöÑÂáΩÊï∞ÂèñÂÄº„ÄÇ\n3. Á≤æÂ∫¶Èò∂Ê¨°\r#\r‰∏äËø∞‰∏≠ÂøÉÂ∑ÆÂàÜÊ†ºÂºèÂØπ‰∫åÈò∂ÂØºÊï∞Âú®Á©∫Èó¥Ê≠•Èïø‰∏äÂÖ∑Êúâ ‰∫åÈò∂Á≤æÂ∫¶„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÂ¶ÇÊûúÊàë‰ª¨Â∞Ü $\\delta x$ Âíå $\\delta y$ ÂêåÊ≠•Áº©Â∞èÔºàÂÅáËÆæÁΩëÊ†ºÁ≠âË∑ùÔºâÔºåÂàôÁ¶ªÊï£Ëß£Áõ∏ÂØπ‰∫éÁúüËß£ÁöÑËØØÂ∑ÆÂú® $\\delta x, \\delta y \\to 0$ Êó∂Êª°Ë∂≥ $$\\mathcal{O}\\bigl((\\delta x)^2 + (\\delta y)^2\\bigr).$$\nÁÆÄËÄåË®Ä‰πãÔºåÂØπÊãâÊôÆÊãâÊñØÊñπÁ®ãÈááÁî®ËøôÁßç‰∫îÁÇπ‰∏≠ÂøÉÂ∑ÆÂàÜÊ†ºÂºèÔºåÂú®ÂùáÂåÄÁΩëÊ†º‰∏ãÊòØ‰∫åÈò∂Á≤æÂ∫¶„ÄÇ\n4. ÊúÄÁªàÂΩ¢ÊàêÁöÑÁ∫øÊÄßÊñπÁ®ãÁªÑ\r#\rÂØπÊâÄÊúâÂÜÖÈÉ®ËäÇÁÇπ $(i,j)$ÔºàÂç≥ $1\\le i\\le N_x$, $1\\le j\\le N_y$ÔºâÂ∫îÁî®‰∏äËø∞Á¶ªÊï£ÊñπÁ®ãÔºåÊàë‰ª¨‰æøÂæóÂà∞‰∏Ä‰∏™ÂÖ≥‰∫éÊâÄÊúâÊú™Áü•Èáè ${u_{i,j}}$ ÁöÑÁ∫øÊÄßÊñπÁ®ãÁªÑ„ÄÇËã•ÂÜçÁªìÂêàËæπÁïåÊù°‰ª∂Ôºà‰æãÂ¶ÇÂ∑≤Áü•ËæπÁïå‰∏äÁöÑ $u_{0,j},u_{N_x+1,j},u_{i,0},u_{i,N_y+1}$ÔºâÔºåÂç≥ÂèØÂÆåÂÖ®Ê±ÇËß£„ÄÇ\nÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÂèØ‰ª•Áî®ÂêÑÁßçËø≠‰ª£Ê≥ïÔºàÂ¶Ç Jacobi„ÄÅGauss-Seidel„ÄÅSOR Á≠âÔºâÊàñÁõ¥Êé•Ê≥ïÔºàÂ¶Ç LU ÂàÜËß£Á≠âÔºâÊù•Ê±ÇËß£Ëøô‰∏™Á¶ªÊï£ÊñπÁ®ãÁªÑ„ÄÇ\n5. ÊÄªÁªì\r#\r‰∫îÁÇπÂ∑ÆÂàÜÊ†ºÂºèÁöÑÁ¶ªÊï£ÊñπÁ®ãÔºàÂú®‰∫åÁª¥ÊÉÖÂÜµ‰∏ãÔºâÊòØÔºö $$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\\delta x)^2} ;+; \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\\delta y)^2} ;=; -,f_{i,j}.$$ Âú®Ê†áÂáÜÁöÑÔºàÂùáÂåÄÔºâÁΩëÊ†º‰∏ãÔºåËØ•ÊñπÊ≥ïÁöÑÁ©∫Èó¥Á¶ªÊï£Á≤æÂ∫¶ÊòØ‰∫åÈò∂„ÄÇ "},{"id":7,"href":"/posts/creating-a-new-theme/","title":"Creating a New Theme","section":"Blog","content":"\rIntroduction\r#\rThis tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\nWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\nIn this tutorial, commands that you enter will start with the \u0026ldquo;$\u0026rdquo; prompt. The output will follow. Lines that start with \u0026ldquo;#\u0026rdquo; are comments that I\u0026rsquo;ve added to explain a point. When I show updates to a file, the \u0026ldquo;:wq\u0026rdquo; on the last line means to save the file.\nHere\u0026rsquo;s an example:\n## this is a comment\r$ echo this is a command\rthis is a command\r## edit the file\r$ vi foo.md\r+++\rdate = \"2014-09-28\"\rtitle = \"creating a new theme\"\r+++\rbah and humbug\r:wq\r## show it\r$ cat foo.md\r+++\rdate = \"2014-09-28\"\rtitle = \"creating a new theme\"\r+++\rbah and humbug\r$\rSome Definitions\r#\rThere are a few concepts that you need to understand before creating a theme.\nSkins\r#\rSkins are the files responsible for the look and feel of your site. It‚Äôs the CSS that controls colors and fonts, it‚Äôs the Javascript that determines actions and reactions. It‚Äôs also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors.\nYou have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you don‚Äôt have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin.\nYour second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. It‚Äôs extra work, though, so why bother with it?\nThe difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ can‚Äôt be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it.\nThe rest of this tutorial will call a skin created in the themes/ directory a theme.\nNote that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you won‚Äôt need to update the site‚Äôs configuration file to use a theme.\nThe Home Page\r#\rThe home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html.\nSite Configuration File\r#\rWhen Hugo runs, it looks for a configuration file that contains settings that override default values for the entire site. The file can use TOML, YAML, or JSON. I prefer to use TOML for my configuration files. If you prefer to use JSON or YAML, you‚Äôll need to translate my examples. You‚Äôll also need to change the name of the file since Hugo uses the extension to determine how to process it.\nHugo translates Markdown files into HTML. By default, Hugo expects to find Markdown files in your content/ directory and template files in your themes/ directory. It will create HTML files in your public/ directory. You can change this by specifying alternate locations in the configuration file.\nContent\r#\rContent is stored in text files that contain two sections. The first section is the ‚Äúfront matter,‚Äù which is the meta-information on the content. The second section contains Markdown that will be converted to HTML.\nFront Matter\r#\rThe front matter is information about the content. Like the configuration file, it can be written in TOML, YAML, or JSON. Unlike the configuration file, Hugo doesn‚Äôt use the file‚Äôs extension to know the format. It looks for markers to signal the type. TOML is surrounded by ‚Äú+++‚Äù, YAML by ‚Äú---‚Äù, and JSON is enclosed in curly braces. I prefer to use TOML, so you‚Äôll need to translate my examples if you prefer YAML or JSON.\nThe information in the front matter is passed into the template before the content is rendered into HTML.\nMarkdown\r#\rContent is written in Markdown which makes it easier to create the content. Hugo runs the content through a Markdown engine to create the HTML which will be written to the output file.\nTemplate Files\r#\rHugo uses template files to render content into HTML. Template files are a bridge between the content and presentation. Rules in the template define what content is published, where it\u0026rsquo;s published to, and how it will rendered to the HTML file. The template guides the presentation by specifying the style to use.\nThere are three types of templates: single, list, and partial. Each type takes a bit of content as input and transforms it based on the commands in the template.\nHugo uses its knowledge of the content to find the template file used to render the content. If it can‚Äôt find a template that is an exact match for the content, it will shift up a level and search from there. It will continue to do so until it finds a matching template or runs out of templates to try. If it can‚Äôt find a template, it will use the default template for the site.\nPlease note that you can use the front matter to influence Hugo‚Äôs choice of templates.\nSingle Template\r#\rA single template is used to render a single piece of content. For example, an article or post would be a single piece of content and use a single template.\nList Template\r#\rA list template renders a group of related content. That could be a summary of recent postings or all articles in a category. List templates can contain multiple groups.\nThe homepage template is a special type of list template. Hugo assumes that the home page of your site will act as the portal for the rest of the content in the site.\nPartial Template\r#\rA partial template is a template that can be included in other templates. Partial templates must be called using the ‚Äúpartial‚Äù template command. They are very handy for rolling up common behavior. For example, your site may have a banner that all pages use. Instead of copying the text of the banner into every single and list template, you could create a partial with the banner in it. That way if you decide to change the banner, you only have to change the partial template.\nCreate a New Site\r#\rLet\u0026rsquo;s use Hugo to create a new web site. I\u0026rsquo;m a Mac user, so I\u0026rsquo;ll create mine in my home directory, in the Sites folder. If you\u0026rsquo;re using Linux, you might have to create the folder first.\nThe \u0026ldquo;new site\u0026rdquo; command will create a skeleton of a site. It will give you the basic directory structure and a useable configuration file.\n$ hugo new site ~/Sites/zafta\r$ cd ~/Sites/zafta\r$ ls -l\rtotal 8\rdrwxr-xr-x 7 quoha staff 238 Sep 29 16:49 .\rdrwxr-xr-x 3 quoha staff 102 Sep 29 16:49 ..\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\r$\rTake a look in the content/ directory to confirm that it is empty.\nThe other directories (archetypes/, layouts/, and static/) are used when customizing a theme. That\u0026rsquo;s a topic for a different tutorial, so please ignore them for now.\nGenerate the HTML For the New Site\r#\rRunning the hugo command with no options will read all the available content and generate the HTML files. It will also copy all static files (that\u0026rsquo;s everything that\u0026rsquo;s not content). Since we have an empty site, it won\u0026rsquo;t do much, but it will do it very quickly.\n$ hugo --verbose\rINFO: 2014/09/29 Using config file: config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$ The \u0026ldquo;--verbose\u0026rdquo; flag gives extra information that will be helpful when we build the template. Every line of the output that starts with \u0026ldquo;INFO:\u0026rdquo; or \u0026ldquo;WARN:\u0026rdquo; is present because we used that flag. The lines that start with \u0026ldquo;WARN:\u0026rdquo; are warning messages. We\u0026rsquo;ll go over them later.\nWe can verify that the command worked by looking at the directory again.\n$ ls -l\rtotal 8\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\r$\rSee that new public/ directory? Hugo placed all generated content there. When you\u0026rsquo;re ready to publish your web site, that\u0026rsquo;s the place to start. For now, though, let\u0026rsquo;s just confirm that we have what we\u0026rsquo;d expect from a site with no content.\n$ ls -l public\rtotal 16\r-rw-r--r-- 1 quoha staff 416 Sep 29 17:02 index.xml\r-rw-r--r-- 1 quoha staff 262 Sep 29 17:02 sitemap.xml\r$ Hugo created two XML files, which is standard, but there are no HTML files.\nTest the New Site\r#\rVerify that you can run the built-in web server. It will dramatically shorten your development cycle if you do. Start it by running the \u0026ldquo;server\u0026rdquo; command. If it is successful, you will see output similar to the following:\n$ hugo server --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\rServing pages from /Users/quoha/Sites/zafta/public\rWeb Server is available at http://localhost:1313\rPress Ctrl+C to stop\rConnect to the listed URL (it\u0026rsquo;s on the line that starts with \u0026ldquo;Web Server\u0026rdquo;). If everything is working correctly, you should get a page that shows the following:\nindex.xml\rsitemap.xml\rThat\u0026rsquo;s a listing of your public/ directory. Hugo didn\u0026rsquo;t create a home page because our site has no content. When there\u0026rsquo;s no index.html file in a directory, the server lists the files in the directory, which is what you should see in your browser.\nLet‚Äôs go back and look at those warnings again.\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\rThat second warning is easier to explain. We haven‚Äôt created a template to be used to generate ‚Äúpage not found errors.‚Äù The 404 message is a topic for a separate tutorial.\nNow for the first warning. It is for the home page. You can tell because the first layout that it looked for was ‚Äúindex.html.‚Äù That‚Äôs only used by the home page.\nI like that the verbose flag causes Hugo to list the files that it\u0026rsquo;s searching for. For the home page, they are index.html, _default/list.html, and _default/single.html. There are some rules that we\u0026rsquo;ll cover later that explain the names and paths. For now, just remember that Hugo couldn\u0026rsquo;t find a template for the home page and it told you so.\nAt this point, you\u0026rsquo;ve got a working installation and site that we can build upon. All that‚Äôs left is to add some content and a theme to display it.\nCreate a New Theme\r#\rHugo doesn\u0026rsquo;t ship with a default theme. There are a few available (I counted a dozen when I first installed Hugo) and Hugo comes with a command to create new themes.\nWe\u0026rsquo;re going to create a new theme called \u0026ldquo;zafta.\u0026rdquo; Since the goal of this tutorial is to show you how to fill out the files to pull in your content, the theme will not contain any CSS. In other words, ugly but functional.\nAll themes have opinions on content and layout. For example, Zafta uses \u0026ldquo;post\u0026rdquo; over \u0026ldquo;blog\u0026rdquo;. Strong opinions make for simpler templates but differing opinions make it tougher to use themes. When you build a theme, consider using the terms that other themes do.\nCreate a Skeleton\r#\rUse the hugo \u0026ldquo;new\u0026rdquo; command to create the skeleton of a theme. This creates the directory structure and places empty files for you to fill out.\n$ hugo new theme zafta\r$ ls -l\rtotal 8\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\rdrwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes\r$ find themes -type f | xargs ls -l\r-rw-r--r-- 1 quoha staff 1081 Sep 29 17:31 themes/zafta/LICENSE.md\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/archetypes/default.md\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html\r-rw-r--r-- 1 quoha staff 93 Sep 29 17:31 themes/zafta/theme.toml\r$ The skeleton includes templates (the files ending in .html), license file, a description of your theme (the theme.toml file), and an empty archetype.\nPlease take a minute to fill out the theme.toml and LICENSE.md files. They\u0026rsquo;re optional, but if you\u0026rsquo;re going to be distributing your theme, it tells the world who to praise (or blame). It\u0026rsquo;s also nice to declare the license so that people will know how they can use the theme.\n$ vi themes/zafta/theme.toml\rauthor = \"michael d henderson\"\rdescription = \"a minimal working template\"\rlicense = \"MIT\"\rname = \"zafta\"\rsource_repo = \"\"\rtags = [\"tags\", \"categories\"]\r:wq\r## also edit themes/zafta/LICENSE.md and change\r## the bit that says \"YOUR_NAME_HERE\"\rNote that the the skeleton\u0026rsquo;s template files are empty. Don\u0026rsquo;t worry, we\u0026rsquo;ll be changing that shortly.\n$ find themes/zafta -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html\r$\rUpdate the Configuration File to Use the Theme\r#\rNow that we\u0026rsquo;ve got a theme to work with, it\u0026rsquo;s a good idea to add the theme name to the configuration file. This is optional, because you can always add \u0026ldquo;-t zafta\u0026rdquo; on all your commands. I like to put it the configuration file because I like shorter command lines. If you don\u0026rsquo;t put it in the configuration file or specify it on the command line, you won\u0026rsquo;t use the template that you\u0026rsquo;re expecting to.\nEdit the file to add the theme, add a title for the site, and specify that all of our content will use the TOML format.\n$ vi config.toml\rtheme = \"zafta\"\rbaseurl = \"\"\rlanguageCode = \"en-us\"\rtitle = \"zafta - totally refreshing\"\rMetaDataFormat = \"toml\"\r:wq\r$\rGenerate the Site\r#\rNow that we have an empty theme, let\u0026rsquo;s generate the site again.\n$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$\rDid you notice that the output is different? The warning message for the home page has disappeared and we have an additional information line saying that Hugo is syncing from the theme\u0026rsquo;s directory.\nLet\u0026rsquo;s check the public/ directory to see what Hugo\u0026rsquo;s created.\n$ ls -l public\rtotal 16\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:56 css\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:56 index.html\r-rw-r--r-- 1 quoha staff 407 Sep 29 17:56 index.xml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:56 js\r-rw-r--r-- 1 quoha staff 243 Sep 29 17:56 sitemap.xml\r$\rNotice four things:\nHugo created a home page. This is the file public/index.html. Hugo created a css/ directory. Hugo created a js/ directory. Hugo claimed that it created 0 pages. It created a file and copied over static files, but didn\u0026rsquo;t create any pages. That\u0026rsquo;s because it considers a \u0026ldquo;page\u0026rdquo; to be a file created directly from a content file. It doesn\u0026rsquo;t count things like the index.html files that it creates automatically. The Home Page\r#\rHugo supports many different types of templates. The home page is special because it gets its own type of template and its own template file. The file, layouts/index.html, is used to generate the HTML for the home page. The Hugo documentation says that this is the only required template, but that depends. Hugo\u0026rsquo;s warning message shows that it looks for three different templates:\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rIf it can\u0026rsquo;t find any of these, it completely skips creating the home page. We noticed that when we built the site without having a theme installed.\nWhen Hugo created our theme, it created an empty home page template. Now, when we build the site, Hugo finds the template and uses it to generate the HTML for the home page. Since the template file is empty, the HTML file is empty, too. If the template had any rules in it, then Hugo would have used them to generate the home page.\n$ find . -name index.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 20:21 ./public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 ./themes/zafta/layouts/index.html\r$ The Magic of Static\r#\rHugo does two things when generating the site. It uses templates to transform content into HTML and it copies static files into the site. Unlike content, static files are not transformed. They are copied exactly as they are.\nHugo assumes that your site will use both CSS and JavaScript, so it creates directories in your theme to hold them. Remember opinions? Well, Hugo\u0026rsquo;s opinion is that you\u0026rsquo;ll store your CSS in a directory named css/ and your JavaScript in a directory named js/. If you don\u0026rsquo;t like that, you can change the directory names in your theme directory or even delete them completely. Hugo\u0026rsquo;s nice enough to offer its opinion, then behave nicely if you disagree.\n$ find themes/zafta -type d | xargs ls -ld\rdrwxr-xr-x 7 quoha staff 238 Sep 29 17:38 themes/zafta\rdrwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes/zafta/archetypes\rdrwxr-xr-x 5 quoha staff 170 Sep 29 17:31 themes/zafta/layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/_default\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/partials\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/static\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/css\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/js\r$ The Theme Development Cycle\r#\rWhen you\u0026rsquo;re working on a theme, you will make changes in the theme\u0026rsquo;s directory, rebuild the site, and check your changes in the browser. Hugo makes this very easy:\nPurge the public/ directory. Run the built in web server in watch mode. Open your site in a browser. Update the theme. Glance at your browser window to see changes. Return to step 4. I‚Äôll throw in one more opinion: never work on a theme on a live site. Always work on a copy of your site. Make changes to your theme, test them, then copy them up to your site. For added safety, use a tool like Git to keep a revision history of your content and your theme. Believe me when I say that it is too easy to lose both your mind and your changes.\nCheck the main Hugo site for information on using Git with Hugo.\nPurge the public/ Directory\r#\rWhen generating the site, Hugo will create new files and update existing ones in the public/ directory. It will not delete files that are no longer used. For example, files that were created in the wrong directory or with the wrong title will remain. If you leave them, you might get confused by them later. I recommend cleaning out your site prior to generating it.\nNote: If you\u0026rsquo;re building on an SSD, you should ignore this. Churning on a SSD can be costly.\nHugo\u0026rsquo;s Watch Option\r#\rHugo\u0026rsquo;s \u0026ldquo;--watch\u0026rdquo; option will monitor the content/ and your theme directories for changes and rebuild the site automatically.\nLive Reload\r#\rHugo\u0026rsquo;s built in web server supports live reload. As pages are saved on the server, the browser is told to refresh the page. Usually, this happens faster than you can say, \u0026ldquo;Wow, that\u0026rsquo;s totally amazing.\u0026rdquo;\nDevelopment Commands\r#\rUse the following commands as the basis for your workflow.\n## purge old files. hugo will recreate the public directory.\r##\r$ rm -rf public\r##\r## run hugo in watch mode\r##\r$ hugo server --watch --verbose\rHere\u0026rsquo;s sample output showing Hugo detecting a change to the template for the home page. Once generated, the web browser automatically reloaded the page. I\u0026rsquo;ve said this before, it\u0026rsquo;s amazing.\n$ rm -rf public\r$ hugo server --watch --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\rWatching for changes in /Users/quoha/Sites/zafta/content\rServing pages from /Users/quoha/Sites/zafta/public\rWeb Server is available at http://localhost:1313\rPress Ctrl+C to stop\rINFO: 2014/09/29 File System Event: [\"/Users/quoha/Sites/zafta/themes/zafta/layouts/index.html\": MODIFY|ATTRIB]\rChange detected, rebuilding site\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 1 ms\rUpdate the Home Page Template\r#\rThe home page is one of a few special pages that Hugo creates automatically. As mentioned earlier, it looks for one of three files in the theme\u0026rsquo;s layout/ directory:\nindex.html _default/list.html _default/single.html We could update one of the default templates, but a good design decision is to update the most specific template available. That\u0026rsquo;s not a hard and fast rule (in fact, we\u0026rsquo;ll break it a few times in this tutorial), but it is a good generalization.\nMake a Static Home Page\r#\rRight now, that page is empty because we don\u0026rsquo;t have any content and we don\u0026rsquo;t have any logic in the template. Let\u0026rsquo;s change that by adding some text to the template.\n$ vi themes/zafta/layouts/index.html\r\u003c!DOCTYPE html\u003e hugo says hello!\n:wq\r$\rBuild the web site and then verify the results.\n$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 78 Sep 29 21:26 public/index.html\r$ cat public/index.html \u003c!DOCTYPE html\u003e hugo says hello!\nLive Reload\r#\rNote: If you\u0026rsquo;re running the server with the --watch option, you\u0026rsquo;ll see different content in the file:\n$ cat public/index.html \u003c!DOCTYPE html\u003e hugo says hello!\nWhen you use --watch, the Live Reload script is added by Hugo. Look for live reload in the documentation to see what it does and how to disable it.\nBuild a \u0026ldquo;Dynamic\u0026rdquo; Home Page\r#\r\u0026ldquo;Dynamic home page?\u0026rdquo; Hugo\u0026rsquo;s a static web site generator, so this seems an odd thing to say. I mean let\u0026rsquo;s have the home page automatically reflect the content in the site every time Hugo builds it. We\u0026rsquo;ll use iteration in the template to do that.\nCreate New Posts\r#\rNow that we have the home page generating static content, let\u0026rsquo;s add some content to the site. We\u0026rsquo;ll display these posts as a list on the home page and on their own page, too.\nHugo has a command to generate a skeleton post, just like it does for sites and themes.\n$ hugo --verbose new post/first.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/first.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/default.md\rERROR: 2014/09/29 Unable to Cast to map[string]interface{}\r$ That wasn\u0026rsquo;t very nice, was it?\nThe \u0026ldquo;new\u0026rdquo; command uses an archetype to create the post file. Hugo created an empty default archetype file, but that causes an error when there\u0026rsquo;s a theme. For me, the workaround was to create an archetypes file specifically for the post type.\n$ vi themes/zafta/archetypes/post.md\r+++\rDescription = \"\"\rTags = []\rCategories = []\r+++\r:wq\r$ find themes/zafta/archetypes -type f | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 21:53 themes/zafta/archetypes/default.md\r-rw-r--r-- 1 quoha staff 51 Sep 29 21:54 themes/zafta/archetypes/post.md\r$ hugo --verbose new post/first.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/first.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md\rINFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/first.md\r/Users/quoha/Sites/zafta/content/post/first.md created\r$ hugo --verbose new post/second.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/second.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md\rINFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/second.md\r/Users/quoha/Sites/zafta/content/post/second.md created\r$ ls -l content/post\rtotal 16\r-rw-r--r-- 1 quoha staff 104 Sep 29 21:54 first.md\r-rw-r--r-- 1 quoha staff 105 Sep 29 21:57 second.md\r$ cat content/post/first.md +++\rCategories = []\rDescription = \"\"\rTags = []\rdate = \"2014-09-29T21:54:53-05:00\"\rtitle = \"first\"\r+++\rmy first post\r$ cat content/post/second.md +++\rCategories = []\rDescription = \"\"\rTags = []\rdate = \"2014-09-29T21:57:09-05:00\"\rtitle = \"second\"\r+++\rmy second post\r$ Build the web site and then verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"category\":\"categories\", \"tag\":\"tags\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$\rThe output says that it created 2 pages. Those are our new posts:\n$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 78 Sep 29 22:13 public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/second/index.html\r$\rThe new files are empty because because the templates used to generate the content are empty. The homepage doesn\u0026rsquo;t show the new content, either. We have to update the templates to add the posts.\nList and Single Templates\r#\rIn Hugo, we have three major kinds of templates. There\u0026rsquo;s the home page template that we updated previously. It is used only by the home page. We also have \u0026ldquo;single\u0026rdquo; templates which are used to generate output for a single content file. We also have \u0026ldquo;list\u0026rdquo; templates that are used to group multiple pieces of content before generating output.\nGenerally speaking, list templates are named \u0026ldquo;list.html\u0026rdquo; and single templates are named \u0026ldquo;single.html.\u0026rdquo;\nThere are three other types of templates: partials, content views, and terms. We will not go into much detail on these.\nAdd Content to the Homepage\r#\rThe home page will contain a list of posts. Let\u0026rsquo;s update its template to add the posts that we just created. The logic in the template will run every time we build the site.\n$ vi themes/zafta/layouts/index.html \u003c!DOCTYPE html\u003e\r{{ range first 10 .Data.Pages }}\r{{ .Title }}\r{{ end }}\r:wq\r$\rHugo uses the Go template engine. That engine scans the template files for commands which are enclosed between \u0026ldquo;{{\u0026rdquo; and \u0026ldquo;}}\u0026rdquo;. In our template, the commands are:\nrange .Title end The \u0026ldquo;range\u0026rdquo; command is an iterator. We\u0026rsquo;re going to use it to go through the first ten pages. Every HTML file that Hugo creates is treated as a page, so looping through the list of pages will look at every file that will be created.\nThe \u0026ldquo;.Title\u0026rdquo; command prints the value of the \u0026ldquo;title\u0026rdquo; variable. Hugo pulls it from the front matter in the Markdown file.\nThe \u0026ldquo;end\u0026rdquo; command signals the end of the range iterator. The engine loops back to the top of the iteration when it finds \u0026ldquo;end.\u0026rdquo; Everything between the \u0026ldquo;range\u0026rdquo; and \u0026ldquo;end\u0026rdquo; is evaluated every time the engine goes through the iteration. In this file, that would cause the title from the first ten pages to be output as heading level one.\nIt\u0026rsquo;s helpful to remember that some variables, like .Data, are created before any output files. Hugo loads every content file into the variable and then gives the template a chance to process before creating the HTML files.\nBuild the web site and then verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:23 public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/second/index.html\r$ cat public/index.html \u003c!DOCTYPE html\u003e\rsecond\rfirst\r$\rCongratulations, the home page shows the title of the two posts. The posts themselves are still empty, but let\u0026rsquo;s take a moment to appreciate what we\u0026rsquo;ve done. Your template now generates output dynamically. Believe it or not, by inserting the range command inside of those curly braces, you\u0026rsquo;ve learned everything you need to know to build a theme. All that\u0026rsquo;s really left is understanding which template will be used to generate each content file and becoming familiar with the commands for the template engine.\nAnd, if that were entirely true, this tutorial would be much shorter. There are a few things to know that will make creating a new template much easier. Don\u0026rsquo;t worry, though, that\u0026rsquo;s all to come.\nAdd Content to the Posts\r#\rWe\u0026rsquo;re working with posts, which are in the content/post/ directory. That means that their section is \u0026ldquo;post\u0026rdquo; (and if we don\u0026rsquo;t do something weird, their type is also \u0026ldquo;post\u0026rdquo;).\nHugo uses the section and type to find the template file for every piece of content. Hugo will first look for a template file that matches the section or type name. If it can\u0026rsquo;t find one, then it will look in the _default/ directory. There are some twists that we\u0026rsquo;ll cover when we get to categories and tags, but for now we can assume that Hugo will try post/single.html, then _default/single.html.\nNow that we know the search rule, let\u0026rsquo;s see what we actually have available:\n$ find themes/zafta -name single.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 132 Sep 29 17:31 themes/zafta/layouts/_default/single.html\rWe could create a new template, post/single.html, or change the default. Since we don\u0026rsquo;t know of any other content types, let\u0026rsquo;s start with updating the default.\nRemember, any content that we haven\u0026rsquo;t created a template for will end up using this template. That can be good or bad. Bad because I know that we\u0026rsquo;re going to be adding different types of content and we\u0026rsquo;re going to end up undoing some of the changes we\u0026rsquo;ve made. It\u0026rsquo;s good because we\u0026rsquo;ll be able to see immediate results. It\u0026rsquo;s also good to start here because we can start to build the basic layout for the site. As we add more content types, we\u0026rsquo;ll refactor this file and move logic around. Hugo makes that fairly painless, so we\u0026rsquo;ll accept the cost and proceed.\nPlease see the Hugo documentation on template rendering for all the details on determining which template to use. And, as the docs mention, if you\u0026rsquo;re building a single page application (SPA) web site, you can delete all of the other templates and work with just the default single page. That\u0026rsquo;s a refreshing amount of joy right there.\nUpdate the Template File\r#\r$ vi themes/zafta/layouts/_default/single.html \u003c!DOCTYPE html\u003e\r{{ .Title }}\r{{ .Title }}\r{{ .Content }}\r:wq\r$\rBuild the web site and verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 94 Sep 29 22:40 public/index.html\r-rw-r--r-- 1 quoha staff 125 Sep 29 22:40 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:40 public/post/index.html\r-rw-r--r-- 1 quoha staff 128 Sep 29 22:40 public/post/second/index.html\r$ cat public/post/first/index.html \u003c!DOCTYPE html\u003e\rfirst\rfirst\rmy first post\n$ cat public/post/second/index.html \u003c!DOCTYPE html\u003e\rsecond\rsecond\rmy second post\n$\rNotice that the posts now have content. You can go to localhost:1313/post/first to verify.\nLinking to Content\r#\rThe posts are on the home page. Let\u0026rsquo;s add a link from there to the post. Since this is the home page, we\u0026rsquo;ll update its template.\n$ vi themes/zafta/layouts/index.html\r\u003c!DOCTYPE html\u003e\r{{ range first 10 .Data.Pages }}\r{{ .Title }}\r{{ end }}\rBuild the web site and verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 149 Sep 29 22:44 public/index.html\r-rw-r--r-- 1 quoha staff 125 Sep 29 22:44 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:44 public/post/index.html\r-rw-r--r-- 1 quoha staff 128 Sep 29 22:44 public/post/second/index.html\r$ cat public/index.html \u003c!DOCTYPE html\u003e\rsecond\rfirst\r$\rCreate a Post Listing\r#\rWe have the posts displaying on the home page and on their own page. We also have a file public/post/index.html that is empty. Let\u0026rsquo;s make it show a list of all posts (not just the first ten).\nWe need to decide which template to update. This will be a listing, so it should be a list template. Let\u0026rsquo;s take a quick look and see which list templates are available.\n$ find themes/zafta -name list.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\rAs with the single post, we have to decide to update _default/list.html or create post/list.html. We still don\u0026rsquo;t have multiple content types, so let\u0026rsquo;s stay consistent and update the default list template.\nCreating Top Level Pages\r#\rLet\u0026rsquo;s add an \u0026ldquo;about\u0026rdquo; page and display it at the top level (as opposed to a sub-level like we did with posts).\nThe default in Hugo is to use the directory structure of the content/ directory to guide the location of the generated html in the public/ directory. Let\u0026rsquo;s verify that by creating an \u0026ldquo;about\u0026rdquo; page at the top level:\n$ vi content/about.md +++\rtitle = \"about\"\rdescription = \"about this site\"\rdate = \"2014-09-27\"\rslug = \"about time\"\r+++\r## about us\ri'm speechless\r:wq\rGenerate the web site and verify the results.\n$ find public -name '*.html' | xargs ls -l\r-rw-rw-r-- 1 mdhender staff 334 Sep 27 15:08 public/about-time/index.html\r-rw-rw-r-- 1 mdhender staff 527 Sep 27 15:08 public/index.html\r-rw-rw-r-- 1 mdhender staff 358 Sep 27 15:08 public/post/first-post/index.html\r-rw-rw-r-- 1 mdhender staff 0 Sep 27 15:08 public/post/index.html\r-rw-rw-r-- 1 mdhender staff 342 Sep 27 15:08 public/post/second-post/index.html\rNotice that the page wasn\u0026rsquo;t created at the top level. It was created in a sub-directory named \u0026lsquo;about-time/\u0026rsquo;. That name came from our slug. Hugo will use the slug to name the generated content. It\u0026rsquo;s a reasonable default, by the way, but we can learn a few things by fighting it for this file.\nOne other thing. Take a look at the home page.\n$ cat public/index.html\r\u003c!DOCTYPE html\u003e\rcreating a new theme\rabout\rsecond\rfirst\rNotice that the \u0026ldquo;about\u0026rdquo; link is listed with the posts? That\u0026rsquo;s not desirable, so let\u0026rsquo;s change that first.\n$ vi themes/zafta/layouts/index.html\r\u003c!DOCTYPE html\u003e\rposts\r{{ range first 10 .Data.Pages }}\r{{ if eq .Type \"post\"}}\r{{ .Title }}\r{{ end }}\r{{ end }}\rpages\r{{ range .Data.Pages }}\r{{ if eq .Type \"page\" }}\r{{ .Title }}\r{{ end }}\r{{ end }}\r:wq\rGenerate the web site and verify the results. The home page has two sections, posts and pages, and each section has the right set of headings and links in it.\nBut, that about page still renders to about-time/index.html.\n$ find public -name '*.html' | xargs ls -l\r-rw-rw-r-- 1 mdhender staff 334 Sep 27 15:33 public/about-time/index.html\r-rw-rw-r-- 1 mdhender staff 645 Sep 27 15:33 public/index.html\r-rw-rw-r-- 1 mdhender staff 358 Sep 27 15:33 public/post/first-post/index.html\r-rw-rw-r-- 1 mdhender staff 0 Sep 27 15:33 public/post/index.html\r-rw-rw-r-- 1 mdhender staff 342 Sep 27 15:33 public/post/second-post/index.html\rKnowing that hugo is using the slug to generate the file name, the simplest solution is to change the slug. Let\u0026rsquo;s do it the hard way and change the permalink in the configuration file.\n$ vi config.toml\r[permalinks]\rpage = \"/:title/\"\rabout = \"/:filename/\"\rGenerate the web site and verify that this didn\u0026rsquo;t work. Hugo lets \u0026ldquo;slug\u0026rdquo; or \u0026ldquo;URL\u0026rdquo; override the permalinks setting in the configuration file. Go ahead and comment out the slug in content/about.md, then generate the web site to get it to be created in the right place.\nSharing Templates\r#\rIf you\u0026rsquo;ve been following along, you probably noticed that posts have titles in the browser and the home page doesn\u0026rsquo;t. That\u0026rsquo;s because we didn\u0026rsquo;t put the title in the home page\u0026rsquo;s template (layouts/index.html). That\u0026rsquo;s an easy thing to do, but let\u0026rsquo;s look at a different option.\nWe can put the common bits into a shared template that\u0026rsquo;s stored in the themes/zafta/layouts/partials/ directory.\nCreate the Header and Footer Partials\r#\rIn Hugo, a partial is a sugar-coated template. Normally a template reference has a path specified. Partials are different. Hugo searches for them along a TODO defined search path. This makes it easier for end-users to override the theme\u0026rsquo;s presentation.\n$ vi themes/zafta/layouts/partials/header.html\r\u003c!DOCTYPE html\u003e\r{{ .Title }}\r:wq\r$ vi themes/zafta/layouts/partials/footer.html\r:wq\rUpdate the Home Page Template to Use the Partials\r#\rThe most noticeable difference between a template call and a partials call is the lack of path:\n{{ template \"theme/partials/header.html\" . }}\rversus\n{{ partial \"header.html\" . }}\rBoth pass in the context.\nLet\u0026rsquo;s change the home page template to use these new partials.\n$ vi themes/zafta/layouts/index.html\r{{ partial \"header.html\" . }}\rposts\r{{ range first 10 .Data.Pages }}\r{{ if eq .Type \"post\"}}\r{{ .Title }}\r{{ end }}\r{{ end }}\rpages\r{{ range .Data.Pages }}\r{{ if or (eq .Type \"page\") (eq .Type \"about\") }}\r{{ .Type }} - {{ .Title }} - {{ .RelPermalink }}\r{{ end }}\r{{ end }}\r{{ partial \"footer.html\" . }}\r:wq\rGenerate the web site and verify the results. The title on the home page is now \u0026ldquo;your title here\u0026rdquo;, which comes from the \u0026ldquo;title\u0026rdquo; variable in the config.toml file.\nUpdate the Default Single Template to Use the Partials\r#\r$ vi themes/zafta/layouts/_default/single.html\r{{ partial \"header.html\" . }}\r{{ .Title }}\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\rGenerate the web site and verify the results. The title on the posts and the about page should both reflect the value in the markdown file.\nAdd ‚ÄúDate Published‚Äù to Posts\r#\rIt\u0026rsquo;s common to have posts display the date that they were written or published, so let\u0026rsquo;s add that. The front matter of our posts has a variable named \u0026ldquo;date.\u0026rdquo; It\u0026rsquo;s usually the date the content was created, but let\u0026rsquo;s pretend that\u0026rsquo;s the value we want to display.\nAdd ‚ÄúDate Published‚Äù to the Template\r#\rWe\u0026rsquo;ll start by updating the template used to render the posts. The template code will look like:\n{{ .Date.Format \"Mon, Jan 2, 2006\" }}\rPosts use the default single template, so we\u0026rsquo;ll change that file.\n$ vi themes/zafta/layouts/_default/single.html\r{{ partial \"header.html\" . }}\r{{ .Title }}\r{{ .Date.Format \"Mon, Jan 2, 2006\" }}\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\rGenerate the web site and verify the results. The posts now have the date displayed in them. There\u0026rsquo;s a problem, though. The \u0026ldquo;about\u0026rdquo; page also has the date displayed.\nAs usual, there are a couple of ways to make the date display only on posts. We could do an \u0026ldquo;if\u0026rdquo; statement like we did on the home page. Another way would be to create a separate template for posts.\nThe \u0026ldquo;if\u0026rdquo; solution works for sites that have just a couple of content types. It aligns with the principle of \u0026ldquo;code for today,\u0026rdquo; too.\nLet\u0026rsquo;s assume, though, that we\u0026rsquo;ve made our site so complex that we feel we have to create a new template type. In Hugo-speak, we\u0026rsquo;re going to create a section template.\nLet\u0026rsquo;s restore the default single template before we forget.\n$ mkdir themes/zafta/layouts/post\r$ vi themes/zafta/layouts/_default/single.html\r{{ partial \"header.html\" . }}\r{{ .Title }}\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\rNow we\u0026rsquo;ll update the post\u0026rsquo;s version of the single template. If you remember Hugo\u0026rsquo;s rules, the template engine will use this version over the default.\n$ vi themes/zafta/layouts/post/single.html\r{{ partial \"header.html\" . }}\r{{ .Title }}\r{{ .Date.Format \"Mon, Jan 2, 2006\" }}\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\rNote that we removed the date logic from the default template and put it in the post template. Generate the web site and verify the results. Posts have dates and the about page doesn\u0026rsquo;t.\nDon\u0026rsquo;t Repeat Yourself\r#\rDRY is a good design goal and Hugo does a great job supporting it. Part of the art of a good template is knowing when to add a new template and when to update an existing one. While you\u0026rsquo;re figuring that out, accept that you\u0026rsquo;ll be doing some refactoring. Hugo makes that easy and fast, so it\u0026rsquo;s okay to delay splitting up a template.\n"},{"id":8,"href":"/posts/migrate-from-jekyll/","title":"Migrating from Jekyll","section":"Blog","content":"\rMove static content to static\r#\rJekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like\n‚ñæ \u0026lt;root\u0026gt;/\r‚ñæ images/\rlogo.png\rshould become\n‚ñæ \u0026lt;root\u0026gt;/\r‚ñæ static/\r‚ñæ images/\rlogo.png\rAdditionally, you\u0026rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.\nCreate your Hugo configuration file\r#\rHugo can read your configuration as JSON, YAML or TOML. Hugo supports parameters custom configuration too. Refer to the Hugo configuration documentation for details.\nSet your configuration publish folder to _site\r#\rThe default is for Jekyll to publish to _site and for Hugo to publish to public. If, like me, you have _site mapped to a git submodule on the gh-pages branch, you\u0026rsquo;ll want to do one of two alternatives:\nChange your submodule to point to map gh-pages to public instead of _site (recommended).\ngit submodule deinit _site\rgit rm _site\rgit submodule add -b gh-pages git@github.com:your-username/your-repo.git public\rOr, change the Hugo configuration to use _site instead of public.\n{\r..\r\u0026quot;publishdir\u0026quot;: \u0026quot;_site\u0026quot;,\r..\r}\rConvert Jekyll templates to Hugo templates\r#\rThat\u0026rsquo;s the bulk of the work right here. The documentation is your friend. You should refer to Jekyll\u0026rsquo;s template documentation if you need to refresh your memory on how you built your blog and Hugo\u0026rsquo;s template to learn Hugo\u0026rsquo;s way.\nAs a single reference data point, converting my templates for heyitsalex.net took me no more than a few hours.\nConvert Jekyll plugins to Hugo shortcodes\r#\rJekyll has plugins; Hugo has shortcodes. It\u0026rsquo;s fairly trivial to do a port.\nImplementation\r#\rAs an example, I was using a custom image_tag plugin to generate figures with caption when running Jekyll. As I read about shortcodes, I found Hugo had a nice built-in shortcode that does exactly the same thing.\nJekyll\u0026rsquo;s plugin:\nmodule Jekyll\rclass ImageTag \u0026lt; Liquid::Tag\r@url = nil\r@caption = nil\r@class = nil\r@link = nil\r// Patterns\rIMAGE_URL_WITH_CLASS_AND_CAPTION =\rIMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;(\\s+)-\u0026gt;((https?:\\/\\/|\\/)(\\S+))(\\s*)/i\rIMAGE_URL_WITH_CAPTION = /((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;/i\rIMAGE_URL_WITH_CLASS = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))/i\rIMAGE_URL = /((https?:\\/\\/|\\/)(\\S+))/i\rdef initialize(tag_name, markup, tokens)\rsuper\rif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK\r@class = $1\r@url = $3\r@caption = $7\r@link = $9\relsif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION\r@class = $1\r@url = $3\r@caption = $7\relsif markup =~ IMAGE_URL_WITH_CAPTION\r@url = $1\r@caption = $5\relsif markup =~ IMAGE_URL_WITH_CLASS\r@class = $1\r@url = $3\relsif markup =~ IMAGE_URL\r@url = $1\rend\rend\rdef render(context)\rif @class\rsource = \u0026quot;\u0026lt;figure class='#{@class}'\u0026gt;\u0026quot;\relse\rsource = \u0026quot;\u0026lt;figure\u0026gt;\u0026quot;\rend\rif @link\rsource += \u0026quot;\u0026lt;a href=\\\u0026quot;#{@link}\\\u0026quot;\u0026gt;\u0026quot;\rend\rsource += \u0026quot;\u0026lt;img src=\\\u0026quot;#{@url}\\\u0026quot;\u0026gt;\u0026quot;\rif @link\rsource += \u0026quot;\u0026lt;/a\u0026gt;\u0026quot;\rend\rsource += \u0026quot;\u0026lt;figcaption\u0026gt;#{@caption}\u0026lt;/figcaption\u0026gt;\u0026quot; if @caption\rsource += \u0026quot;\u0026lt;/figure\u0026gt;\u0026quot;\rsource\rend\rend\rend\rLiquid::Template.register_tag('image', Jekyll::ImageTag)\ris written as this Hugo shortcode:\n\u0026lt;!-- image --\u0026gt;\r\u0026lt;figure {{ with .Get \u0026quot;class\u0026quot; }}class=\u0026quot;{{.}}\u0026quot;{{ end }}\u0026gt;\r{{ with .Get \u0026quot;link\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt;{{ end }}\r\u0026lt;img src=\u0026quot;{{ .Get \u0026quot;src\u0026quot; }}\u0026quot; {{ if or (.Get \u0026quot;alt\u0026quot;) (.Get \u0026quot;caption\u0026quot;) }}alt=\u0026quot;{{ with .Get \u0026quot;alt\u0026quot;}}{{.}}{{else}}{{ .Get \u0026quot;caption\u0026quot; }}{{ end }}\u0026quot;{{ end }} /\u0026gt;\r{{ if .Get \u0026quot;link\u0026quot;}}\u0026lt;/a\u0026gt;{{ end }}\r{{ if or (or (.Get \u0026quot;title\u0026quot;) (.Get \u0026quot;caption\u0026quot;)) (.Get \u0026quot;attr\u0026quot;)}}\r\u0026lt;figcaption\u0026gt;{{ if isset .Params \u0026quot;title\u0026quot; }}\r{{ .Get \u0026quot;title\u0026quot; }}{{ end }}\r{{ if or (.Get \u0026quot;caption\u0026quot;) (.Get \u0026quot;attr\u0026quot;)}}\u0026lt;p\u0026gt;\r{{ .Get \u0026quot;caption\u0026quot; }}\r{{ with .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt; {{ end }}\r{{ .Get \u0026quot;attr\u0026quot; }}\r{{ if .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;/a\u0026gt; {{ end }}\r\u0026lt;/p\u0026gt; {{ end }}\r\u0026lt;/figcaption\u0026gt;\r{{ end }}\r\u0026lt;/figure\u0026gt;\r\u0026lt;!-- image --\u0026gt;\rUsage\r#\rI simply changed:\n{% image full http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg \u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were \u0026quot;having fun\u0026quot; and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; -\u0026gt;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/ %}\rto this (this example uses a slightly extended version named fig, different than the built-in figure):\n{{% fig class=\u0026quot;full\u0026quot; src=\u0026quot;http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg\u0026quot; title=\u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were having fun and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; link=\u0026quot;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/\u0026quot; %}}\rAs a bonus, the shortcode named parameters are, arguably, more readable.\nFinishing touches\r#\rFix content\r#\rDepending on the amount of customization that was done with each post with Jekyll, this step will require more or less effort. There are no hard and fast rules here except that hugo server --watch is your friend. Test your changes and fix errors as needed.\nClean up\r#\rYou\u0026rsquo;ll want to remove the Jekyll configuration at this point. If you have anything else that isn\u0026rsquo;t used, delete it.\nA practical example in a diff\r#\rHey, it\u0026rsquo;s Alex was migrated in less than a father-with-kids day from Jekyll to Hugo. You can see all the changes (and screw-ups) by looking at this diff.\n"},{"id":9,"href":"/docs/Mathematics/AI/","title":"AI","section":"Mathematics","content":"\rEvolution of AI: Foundational Papers and Milestones (Chronological)\r#\rBelow is a chronological list of influential papers that have shaped artificial intelligence ‚Äì from early symbolic reasoning and neural network concepts to the rise of deep learning and large language models. Each entry includes the work‚Äôs main contribution, an influence rating, and a beginner-friendly explanation of its significance.\n1943 ‚Äì McCulloch \u0026amp; Pitts: ‚ÄúA Logical Calculus of the Ideas Immanent in Nervous Activity‚Äù\nContribution \u0026amp; Impact: Proposed the first mathematical model of how networks of artificial neurons could represent logical computations (\rA Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia). This seminal work showed that simple on/off neurons with weighted inputs can compute any logical function, laying the groundwork for neural networks (\rA Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia).\nInfluence (1‚Äì10): 10/10\nBeginner-Friendly Explanation: McCulloch and Pitts imagined the brain as a network of simple switches (neurons) that could be either on or off. They proved a bunch of these neuron-like switches could be connected to perform logical reasoning (like an electronic circuit). This was a foundational idea: it suggested machines could think by mimicking brain networks.\n1950 ‚Äì Alan Turing: ‚ÄúComputing Machinery and Intelligence‚Äù\nContribution \u0026amp; Impact: Introduced the famous Turing Test as a criterion for machine intelligence (\rAlan Turing\u0026rsquo;s Contributions to Artificial Intelligence : History of Information). Turing argued that instead of asking ‚ÄúCan machines think?‚Äù, we should ask if a machine can imitate a human so well in conversation that an evaluator cannot tell the difference. This paper framed the philosophical and practical challenge of AI for decades.\nInfluence (1‚Äì10): 10/10\nBeginner-Friendly Explanation: Turing basically said: ‚ÄúIf you can chat with a computer and can‚Äôt tell it‚Äôs not human, then for all practical purposes, that computer is ‚Äòthinking‚Äô.‚Äù This idea ‚Äì a computer fooling a person in a conversation ‚Äì became a guiding goal for AI research and popular imagination.\n1956 ‚Äì Newell \u0026amp; Simon: The Logic Theorist (RAND Corporation Report \u0026amp; Dartmoor Demo)\nContribution \u0026amp; Impact: Demonstrated the first AI program deliberately engineered to mimic human problem-solving (\rNewell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information). The Logic Theorist could prove mathematical theorems from Principia Mathematica, even finding an elegant proof for one theorem that was more efficient than the original (\rNewell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information). This ‚Äúheuristic search‚Äù approach showed digital computers can perform symbolic reasoning, launching the field of symbolic AI.\nInfluence (1‚Äì10): 9/10\nBeginner-Friendly Explanation: Newell and Simon built a program that solved logic puzzles (proving math theorems) in a way a person might, by searching through possible steps. It was the first time a computer did something ‚Äúbrainy‚Äù beyond pure calculations. This success convinced people that computers could manipulate symbols and logic to solve problems, not just crunch numbers.\n1958 ‚Äì Frank Rosenblatt: The Perceptron (Psychological Review \u0026amp; Mark I Perceptron)\nContribution \u0026amp; Impact: Introduced the perceptron, a simple neural network that learns from experience. Rosenblatt‚Äôs perceptron machine was the first computer that could learn new skills by trial and error using a neural network modeled on the brain (\rRosenblatt\u0026rsquo;s Perceptron Uses a Type of Neural Network : History of Information). It learned to classify patterns (like distinguishing shapes) by adjusting connection weights based on errors. This work pioneered the field of machine learning and inspired decades of neural network research.\nInfluence (1‚Äì10): 9/10\nBeginner-Friendly Explanation: The perceptron was essentially a mechanical ‚Äústudent.‚Äù It would make a guess about a pattern (for example, is this a picture of a dog or a cat?), then check if it was wrong. If it was wrong, it tweaked its internal settings to do better next time. Over many trials it gradually got more accurate (\rProfessor‚Äôs perceptron paved the way for AI ‚Äì 60 years too soon | Cornell Chronicle). This was the first example of a machine learning from its mistakes ‚Äì a key idea in AI.\n1959 ‚Äì Arthur Samuel: ‚ÄúSome Studies in Machine Learning Using the Game of Checkers‚Äù\nContribution \u0026amp; Impact: Demonstrated one of the first successful self-learning programs. Samuel‚Äôs checkers (draughts) program learned to improve at the game by playing against itself thousands of times. Importantly, it introduced mechanisms for a computer to learn from past games ‚Äì recording positions that led to wins or losses and updating its strategy accordingly (\rThe games that helped AI evolve | IBM). Samuel even coined the term ‚Äúmachine learning‚Äù for this approach. In 1962, his program was able to beat a respectable human player, proving that computers can learn complex tasks without being explicitly programmed for all situations.\nInfluence (1‚Äì10): 8/10\nBeginner-Friendly Explanation: Samuel‚Äôs checkers program was like a rookie player that got better by practicing. It kept track of board positions and whether it eventually won or lost from them. Over time it favored moves that led to wins and avoided those leading to losses (\rThe games that helped AI evolve | IBM). This was revolutionary: the computer wasn‚Äôt just following a fixed strategy given by a human ‚Äì it was figuring out a winning strategy by itself through experience.\n1969 ‚Äì Marvin Minsky \u0026amp; Seymour Papert: Perceptrons (MIT Press book)\nContribution \u0026amp; Impact: Delivered a thorough mathematical analysis of perceptrons and famously highlighted their limitations (\rMinsky \u0026amp; Papert‚Äôs ‚ÄúPerceptrons‚Äù ‚Äì Building Babylon). They proved that a single-layer perceptron cannot learn certain simple functions (like the XOR problem ‚Äì determining if an input has an odd number of 1‚Äôs), unless it uses an exponentially large number of features. This critique (published as a book) effectively punctured the hype around neural networks at the time. It led to a significant shift in AI research focus from connectionist (neural network) methods to symbolic AI approaches in the 1970s, contributing to an ‚ÄúAI winter‚Äù for neural nets (\rMinsky \u0026amp; Papert‚Äôs ‚ÄúPerceptrons‚Äù ‚Äì Building Babylon).\nInfluence (1‚Äì10): 9/10\nBeginner-Friendly Explanation: Minsky and Papert showed that the perceptron ‚Äì this early learning neural network ‚Äì was very limited in what it could learn. For example, it couldn‚Äôt correctly learn the simple logic of an ‚Äúeither/or‚Äù (XOR) condition because of its single-layer design (\rMinsky \u0026amp; Papert‚Äôs ‚ÄúPerceptrons‚Äù ‚Äì Building Babylon). Their analysis basically said, ‚ÄúNeural nets are neat, but they can‚Äôt handle some basic problems unless they get much more complex.‚Äù This turned many researchers away from neural networks for years, as they focused instead on logic and rule-based AI.\n1986 ‚Äì Rumelhart, Hinton \u0026amp; Williams: ‚ÄúLearning Representations by Back-Propagating Errors‚Äù\nContribution \u0026amp; Impact: Introduced the backpropagation algorithm for training multi-layer neural networks efficiently (though the method had been conceptually described earlier, this paper popularized it). Backpropagation provided a practical way to adjust the weights in a network with many layers by propagating the error gradient backward from the output layer (\rBackpropagation - Wikipedia). This breakthrough overcame the training difficulty of multi-layer perceptrons and sparked a resurgence of interest in neural network research in the late 1980s (\rBackpropagation - Wikipedia). In short, it enabled ‚Äúdeep‚Äù neural networks to actually learn internal representations from data.\nInfluence (1‚Äì10): 10/10\nBeginner-Friendly Explanation: Backpropagation is an algorithm that finally let multi-layer neural networks learn. Think of it like teaching a multi-step math solution: you check the final answer, see how wrong it is, and then send feedback backward to correct each step. Similarly, backpropagation takes the error at the output and systematically adjusts each connection in all layers to reduce that error. Once this was introduced and shown to work (\rBackpropagation - Wikipedia), researchers could train networks with several layers ‚Äì giving neural nets much more brain-like ability to form complex concepts (like recognizing shapes, then objects, then scenes). This revived neural networks as a viable AI approach.\n1988 ‚Äì Judea Pearl: Probabilistic Reasoning in Intelligent Systems (book)\nContribution \u0026amp; Impact: Established the field of Bayesian networks for reasoning under uncertainty. Pearl introduced a formalism where cause-and-effect relationships and uncertain knowledge could be encoded in a graphical model (a Bayesian network) and updated with probability theory. This was a paradigm shift from rule-based AI to probabilistic AI: instead of logic with strict true/false values, AI systems could handle gray areas and uncertainty in a principled way. Pearl‚Äôs 1988 book became known as the ‚Äúbible‚Äù of probabilistic AI (\rProbabilistic Reasoning (1993‚Äì2011) ‚Äî Making Things Think: How AI and Deep Learning Power the Products We Use) and his techniques for probabilistic inference laid the groundwork for modern AI systems that need to deal with real-world ambiguity (including everything from medical diagnosis expert systems to speech recognition).\nInfluence (1‚Äì10): 10/10\nBeginner-Friendly Explanation: Pearl taught AI how to handle uncertainty. Earlier AI often used rigid rules (e.g., ‚ÄúIF X then Y‚Äù), but real life is full of maybes. Pearl‚Äôs Bayesian networks let a computer draw a graph of causes and effects (say, symptoms and diseases) and then reason with probabilities ‚Äì for example, ‚Äúgiven these symptoms, there‚Äôs an 80% chance of flu.‚Äù This made AI much better at dealing with uncertain, real-world information, and it was a huge turning point that influenced everything from machine vision to natural language understanding. (\rProbabilistic Reasoning (1993‚Äì2011) ‚Äî Making Things Think: How AI and Deep Learning Power the Products We Use) (\rProbabilistic Reasoning (1993‚Äì2011) ‚Äî Making Things Think: How AI and Deep Learning Power the Products We Use)\n1989 ‚Äì Yann LeCun et al.: ‚ÄúBackpropagation Applied to Handwritten Zip Code Recognition‚Äù\nContribution \u0026amp; Impact: Demonstrated the first real-world success of a deep neural network (a convolutional neural network, or CNN) trained end-to-end with backpropagation. LeCun‚Äôs CNN, later known as LeNet-5, could read handwritten digits (like postal ZIP codes) from images with high accuracy ([\nDeep Neural Nets: 33 years ago and 33 years from now (Invited Post) ¬∑ The ICLR Blog Track\n](\rhttps://iclr-blog-track.github.io/2022/03/26/lecun1989/#:~:text=The%20Yann%20LeCun%20et%20al,I%20set%20out%20to%20reproduce)). It introduced the convolutional layer architecture that mimics the visual cortex, extracting features through local receptive fields and shared weights. This work was historically significant as an early proof that multi-layer neural networks can solve practical pattern-recognition problems that other methods struggled with, foreshadowing the deep learning breakthroughs decades later ([\nDeep Neural Nets: 33 years ago and 33 years from now (Invited Post) ¬∑ The ICLR Blog Track\n](\rhttps://iclr-blog-track.github.io/2022/03/26/lecun1989/#:~:text=The%20Yann%20LeCun%20et%20al,I%20set%20out%20to%20reproduce)).\nInfluence (1‚Äì10): 8/10\nBeginner-Friendly Explanation: LeCun and colleagues built a neural network that could read handwritten numbers ‚Äì for example, automatically recognizing zip code digits on mail. They designed special layers (now called convolutional layers) that help the network focus on small patches of an image, just like how our eyes notice local patterns. By training this multi-layer network with backpropagation, it got really good at digit recognition. This was one of the first times deep learning beat other methods on a real task, proving that these layered neural networks weren‚Äôt just academic toys but could actually see things in images and make sense of them.\n1989 ‚Äì Chris Watkins: ‚ÄúLearning from Delayed Rewards‚Äù (PhD thesis introducing Q-Learning) Contribution \u0026amp; Impact: Introduced Q-learning, a foundational algorithm in reinforcement learning. Q-learning provided a model-free way for an agent to learn an optimal action policy by trial-and-error, even when outcomes (rewards) are delayed (\rQ-learning - Wikipedia). Watkins proved that Q-learning converges to the optimal solution given sufficient exploration. This algorithm was crucial because it showed how an AI agent can learn to make sequences of decisions in an unknown environment to maximize reward, without needing a model of the environment‚Äôs dynamics. Q-learning (and the broader reinforcement learning framework) became a major branch of AI, underpinning later successes in game-playing AI, robotics, and beyond.\nInfluence (1‚Äì10): 9/10\nBeginner-Friendly Explanation: Q-learning is like learning to play a game by playing. Imagine you‚Äôre dropped into a video game without instructions. You try moves at random and eventually figure out what gives you points or causes losses. Over time, you assign a value (‚ÄúQ-value‚Äù) to each move in each situation based on how good it turned out to be. Watkins‚Äô Q-learning algorithm formalized this idea ‚Äì the computer updates its estimations of future rewards for actions as it experiments (\rQ-learning - Wikipedia). The beauty is that the AI doesn‚Äôt need to know the rules in advance; it learns what actions are best just from feedback. This became a cornerstone of how we teach AI agents to learn from experience (like teaching a robot to navigate a maze or an AI to play Atari games).\n1995 ‚Äì Cortes \u0026amp; Vapnik: ‚ÄúSupport-Vector Networks‚Äù Contribution \u0026amp; Impact: Introduced Support Vector Machines (SVMs), a new supervised learning approach based on maximizing the margin between classes. SVMs framed learning as finding the optimal separating hyperplane in a high-dimensional space ‚Äì and by using the kernel trick, they could efficiently handle complex, non-linear decision boundaries by implicitly mapping data into higher dimensions (\rSupport vector machine - Wikipedia). SVMs offered strong theoretical guarantees (rooted in Vapnik‚Äôs statistical learning theory) and delivered excellent performance on many tasks in the late 1990s and 2000s. They became one of the most dominant machine learning methods before the deep learning era, widely used in image recognition, text classification, and bioinformatics.\nInfluence (1‚Äì10): 8/10\nBeginner-Friendly Explanation: SVMs were a new way to do pattern recognition with math. Think of drawing a line to separate two groups of points on a paper: an SVM finds the line (or surface in higher dimensions) that not only separates the groups, but is as far away from all points as possible ‚Äì this is the maximum-margin idea (\rSupport vector machine - Wikipedia) (\rSupport vector machine - Wikipedia). If the groups aren‚Äôt linearly separable, SVMs use a clever trick (the kernel) to imagine the data in a higher-dimensional space where a separation is possible, without having to enumerate all those dimensions explicitly. The result was a very powerful and robust classifier that for years was the go-to method when you wanted high accuracy in machine learning tasks.\n1997 ‚Äì Hochreiter \u0026amp; Schmidhuber: ‚ÄúLong Short-Term Memory‚Äù (Neural Computation) Contribution \u0026amp; Impact: Developed the Long Short-Term Memory (LSTM) network, a type of recurrent neural network designed to overcome the vanishing gradient problem for long sequence learning. LSTM introduced gating mechanisms (input, output, and forget gates) that allow the network to maintain information over long time lags (\rLong short-term memory - Wikipedia) (\rLong short-term memory - Wikipedia). This architecture enabled RNNs to retain long-term dependencies in sequence data (e.g. remembering context from far earlier in a text or time series). LSTMs proved enormously successful in the 2000s and 2010s for tasks like speech recognition, language modeling, and translation ‚Äì essentially any task involving sequential data ‚Äì and were a key component in state-of-the-art models until the transformer era.\nInfluence (1‚Äì10): 9/10\nBeginner-Friendly Explanation: LSTMs made neural networks much better at remembering. Standard RNNs have short memories ‚Äì they tend to ‚Äúforget‚Äù things quickly as new inputs come in, partly because the error signal fades over time steps. LSTM units added gates that learn what to keep, what to throw away, and what to output (\rLong short-term memory - Wikipedia) (\rLong short-term memory - Wikipedia). For example, when processing a sentence, an LSTM can learn to carry forward the subject of the sentence so that many words later it still knows who or what the discussion is about. This was a big deal ‚Äì it meant AI could understand sequences (like text, speech, or music) with far better context and consistency.\n2006 ‚Äì Hinton et al.: ‚ÄúA Fast Learning Algorithm for Deep Belief Nets‚Äù (Science) Contribution \u0026amp; Impact: Revived deep neural networks by introducing an effective training strategy using Deep Belief Networks (DBNs). Hinton showed that a deep multi-layer network could be trained by greedily training one layer at a time in an unsupervised fashion (using Restricted Boltzmann Machines), then fine-tuning with supervised learning. This breakthrough in 2006 was the first to successfully train networks with many layers, overcoming previous optimization difficulties (\r[PDF] Why does Unsupervised Pre-training Help Deep Learning?). It proved that unsupervised pre-training could initialize deep networks in a good state, leading to much better results and reigniting research into ‚Äúdeep learning.‚Äù This work directly influenced subsequent deep architectures and is seen as a turning point that led to the deep learning boom.\nInfluence (1‚Äì10): 8/10\nBeginner-Friendly Explanation: By the 2000s, neural nets had mostly one or two hidden layers because training more was too hard ‚Äì the signal just wouldn‚Äôt propagate well. Hinton‚Äôs team figured out a clever solution: train one layer at a time in a sort of self-supervised way (each layer learns to encode the data in a compressed form), then stack them up. With this layer-by-layer pre-training, they could finally train really deep networks (many layers) without things falling apart (\r[PDF] Why does Unsupervised Pre-training Help Deep Learning?). This showed the community that deep networks could work in practice, and it set the stage for the breakthroughs that followed (especially once big data and GPUs became available a few years later).\n2012 ‚Äì Krizhevsky, Sutskever \u0026amp; Hinton: ‚ÄúImageNet Classification with Deep Convolutional Neural Networks‚Äù Contribution \u0026amp; Impact: Better known as AlexNet, this paper rocked the computer vision world by winning the 2012 ImageNet challenge by a huge margin using a deep convolutional neural network (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone). AlexNet was an 8-layer CNN (with ReLU activations, dropout regularization, and GPU training) that achieved a top-5 error of 15.3% on ImageNet, while the next best approach was 26.2% ‚Äì an unprecedented 10% jump in accuracy (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone). It was the first widely acknowledged, practical success of deep learning in a large-scale task (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone), marking the start of the deep learning revolution in computer vision (and soon other fields). After AlexNet, the research community rapidly pivoted to deep neural networks for vision tasks.\nInfluence (1‚Äì10): 10/10\nBeginner-Friendly Explanation: AlexNet‚Äôs victory was like an underdog team winning a championship by a landslide. In a contest of recognizing 1,000 different object types from images (ImageNet), this deep neural network crushed the traditional approaches ‚Äì it was much more accurate (about 16% error vs. 26% for the best non-neural method) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone). People were stunned. This was proof that with enough data (millions of images), compute (GPUs), and a good architecture, neural networks dramatically outperformed other techniques. It instantly made deep learning the focus for anyone working on image recognition, and soon after, for speech and other areas as well.\n2014 ‚Äì Goodfellow et al.: ‚ÄúGenerative Adversarial Networks‚Äù Contribution \u0026amp; Impact: Introduced Generative Adversarial Networks (GANs), a novel framework for training generative models by pitting two neural networks against each other ‚Äì a generator that tries to create fake data, and a discriminator that tries to detect fakes (\r10 AI milestones of the last 10 years | Royal Institution). This adversarial training scheme resulted in generative models that could produce remarkably realistic images (and other data) over time. GANs were a conceptual breakthrough in how to train networks to create rather than just recognize, and they spawned an entire subfield of research. They eventually led to high-fidelity image synthesis, deepfakes, and many creative AI applications.\nInfluence (1‚Äì10): 9/10\nBeginner-Friendly Explanation: A GAN is like having a counterfeiter and a detective who improve together. The counterfeiter (generator network) tries to make fake outputs ‚Äì say, fake images of faces ‚Äì that look real. The detective (discriminator network) looks at images and says ‚Äúreal‚Äù or ‚Äúfake.‚Äù As they train, the counterfeiter gets better at fooling the detective, and the detective gets better at spotting fakes (\r10 AI milestones of the last 10 years | Royal Institution). Eventually, the fake outputs become so realistic that even humans can‚Äôt easily tell they‚Äôre generated by a computer. This idea of two AIs dueling with each other turned out to be a powerful way for machines to imagine and create realistic data.\n2015 ‚Äì Mnih et al.: ‚ÄúHuman-Level Control Through Deep Reinforcement Learning‚Äù Contribution \u0026amp; Impact: Demonstrated the power of deep reinforcement learning by introducing the Deep Q-Network (DQN) agent. This system combined Q-learning with a deep convolutional neural network, enabling an AI to learn to play Atari 2600 video games directly from raw pixel inputs (\rHuman-level control through deep reinforcement learning | Nature). Strikingly, the same DQN algorithm achieved human-level performance (or better) on dozens of Atari games ‚Äì using only the game screen pixels and score as input ‚Äì with no game-specific tweaks (\rHuman-level control through deep reinforcement learning | Nature). This was the first time a single AI agent learned a broad range of complex tasks end-to-end, bridging the gap between sensory perception and decision-making. It reinvigorated research in reinforcement learning and underscored the potential of combining deep learning with trial-and-error learning.\nInfluence (1‚Äì10): 10/10\nBeginner-Friendly Explanation: DeepMind‚Äôs DQN agent was a breakthrough in game-playing AI. They took a neural network and trained it via reinforcement learning to play classic video games like Breakout, Pac-Man, and Space Invaders. The input was just the pixels on the screen and the game score, and it had to figure out what to do from that alone. Amazingly, after training, the AI could play many of these games as well as or better than a human ‚Äì using the same one algorithm for all (\rHuman-level control through deep reinforcement learning | Nature). For example, in Breakout it learned the clever strategy of tunneling around the bricks (a trick human experts use) without ever being told. This result showed that an AI can start from raw perception (seeing the screen) and learn intelligent control (playing the game) purely by trial and error, which was a big step toward more general learning systems.\n2016 ‚Äì Silver et al.: ‚ÄúMastering the Game of Go with Deep Neural Networks and Tree Search‚Äù (AlphaGo) Contribution \u0026amp; Impact: Achieved what was previously thought to be at least a decade away: a computer program defeating a top human professional in the game of Go (\rMastering the game of Go with deep neural networks and tree search | Nature). The AlphaGo system did this by combining deep policy networks (to choose moves) and value networks (to evaluate board positions) with Monte Carlo Tree Search. It learned first from expert human games and then via millions of games of self-play, refining its skills. In the published Nature paper, AlphaGo achieved a 99.8% win rate against other Go programs and beat the European Go champion 5‚Äì0 (\rMastering the game of Go with deep neural networks and tree search | Nature). This was a watershed moment for AI, showcasing the synthesis of deep learning and advanced search to conquer one of the most complex board games. It suggested that similar techniques could tackle other problems of high complexity.\nInfluence (1‚Äì10): 10/10\nBeginner-Friendly Explanation: For decades, Go was the ultimate challenge for AI ‚Äì far more complex than chess. AlphaGo‚Äôs design had two brains: one that suggested likely good moves (a policy neural network) and one that judged board positions (a value neural network) (\rMastering the game of Go with deep neural networks and tree search | Nature). It also simulated future move sequences (tree search) but in a smarter way guided by those neural nets. The result was an AI that plays Go brilliantly. It first learned by studying human games, then got even better by playing against itself millions of times, each time learning from its mistakes. In 2016 it shocked the world by beating one of the best human Go players. This victory was about more than Go ‚Äì it meant AI could tackle extremely complex, subtle problems that were once thought to require human intuition.\n2017 ‚Äì Vaswani et al.: ‚ÄúAttention Is All You Need‚Äù Contribution \u0026amp; Impact: Introduced the Transformer architecture, built entirely on self-attention mechanisms and devoid of recurrence or convolution. This paper showed that attention mechanisms alone can capture relationships in sequential input (like words in a sentence) more efficiently and with better parallelization than previous RNN/CNN approaches. The transformer architecture led to dramatic improvements in machine translation and natural language processing. It is the direct precursor to today‚Äôs large language models. Indeed, this work provided the technological foundation for LLMs, as transformers can read entire sequences and learn contextual dependencies with ease (\r10 AI milestones of the last 10 years | Royal Institution). Subsequent models like BERT and GPT are built on the transformer, validating the paper‚Äôs title that ‚Äúattention is all you need.‚Äù\nInfluence (1‚Äì10): 10/10\nBeginner-Friendly Explanation: The Transformer architecture threw out the old playbook for processing sequences. Instead of reading words one-by-one in order (like an RNN) or focusing only on a fixed-size window (like a CNN), a Transformer looks at all the words at once and learns which words to pay attention to in order to understand the meaning (\r10 AI milestones of the last 10 years | Royal Institution). For example, to translate a sentence or answer a question, it can see how each word relates to every other word (using a mechanism called self-attention). This was revolutionary because it made language models much better at capturing context (who did what to whom, etc.) and it could be massively parallelized (so it could train on huge datasets). Nearly all modern large language models (like GPT-3, BERT) are based on this Transformer design, which shows how impactful this paper was.\n2018 ‚Äì Devlin et al.: ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù Contribution \u0026amp; Impact: Introduced BERT (Bidirectional Encoder Representations from Transformers), which demonstrated the power of pre-training a large transformer on unsupervised language tasks and then fine-tuning it for specific NLP tasks. BERT‚Äôs bidirectional training (masked language modeling and next-sentence prediction objectives) produced deep language representations that achieved state-of-the-art results on a wide array of NLP benchmarks (\r[PDF] BERT: Pre-training of Deep Bidirectional Transformers for Language \u0026hellip;). This paper popularized the paradigm of pre-train then fine-tune in NLP. After BERT, large pre-trained language models became the norm, fundamentally changing NLP research and leading to an explosion of models that improved on various tasks via fine-tuning rather than task-specific architectures from scratch.\nInfluence (1‚Äì10): 9/10\nBeginner-Friendly Explanation: BERT showed that an AI can learn a lot about language just by reading billions of words of text, without any specific task in mind. BERT is a huge Transformer network trained in a clever way: it hides some words in a sentence and forces itself to guess them (that‚Äôs masked language modeling). In doing so, it learns rich knowledge about syntax, semantics, and general language facts. Once trained on this ‚Äúfill-in-the-blanks‚Äù task across Wikipedia and books, BERT can be quickly taught to solve all sorts of language problems (question answering, sentiment analysis, etc.) by fine-tuning on a small task-specific dataset (\r[PDF] BERT: Pre-training of Deep Bidirectional Transformers for Language \u0026hellip;). For NLP, this was a game-changer ‚Äì instead of training separate models from scratch for each little task, we now pre-train one giant model that knows a lot, and then just tweak it for the task at hand.\n2018 ‚Äì Radford et al.: ‚ÄúImproving Language Understanding by Generative Pre-Training‚Äù (GPT-1) Contribution \u0026amp; Impact: This OpenAI work introduced the first Generative Pre-Trained Transformer (GPT) model. GPT-1 demonstrated that a transformer trained as a language model (predicting the next word on a massive corpus of books) could be fine-tuned to perform downstream tasks like question answering with excellent results ‚Äì even though it was not trained on those tasks directly. It established the effectiveness of unsupervised pre-training for transformers (using a left-to-right generative objective) and showed the versatility of the resulting representations. GPT-1 was able to generate coherent text and answer questions in a fluent way after pre-training on ~7000 unpublished books and then fine-tuning (\r10 AI milestones of the last 10 years | Royal Institution). While smaller in scale by today‚Äôs standards, GPT-1 laid the groundwork for the GPT series and the concept of large language models.\nInfluence (1‚Äì10): 7/10\nBeginner-Friendly Explanation: GPT-1 was the first step toward modern chatbots and LLMs. The idea was simple but powerful: train a Transformer to predict the next word in a sentence by feeding it a ton of books. By doing this, the model learns grammar, facts, and some reasoning just from the text. Then the researchers showed that you could take this generatively pre-trained model and fine-tune it on specific tasks (like having it answer questions or analyze sentiment) and it worked really well (\r10 AI milestones of the last 10 years | Royal Institution). In essence, GPT-1 was like teaching a student by letting it read an entire library (with the goal of guessing missing words), and then giving it a short internship for a specific job ‚Äì and it turned out the student had learned enough from reading to do the job impressively well.\n2020 ‚Äì Brown et al.: ‚ÄúLanguage Models are Few-Shot Learners‚Äù (GPT-3) Contribution \u0026amp; Impact: Introduced GPT-3, a 175-billion-parameter transformer that demonstrated astonishing capabilities in natural language generation and few-shot learning. GPT-3 showed that simply by scaling up model size and training on virtually all of the internet‚Äôs text, a language model could perform a wide range of tasks without explicit training for each one ‚Äì given just a few examples in its prompt (a phenomenon called in-context learning). It achieved strong performance on translation, Q\u0026amp;A, and more by prompt alone (\r[2005.14165] Language Models are Few-Shot Learners - arXiv). GPT-3‚Äôs release dazzled the world with its ability to generate human-like essays, code, and dialogues, revealing emergent properties of language models at scale. This paper marked the point where ‚Äúlarge language model‚Äù entered the mainstream vocabulary and led to widespread deployment of LLMs in applications.\nInfluence (1‚Äì10): 10/10\nBeginner-Friendly Explanation: GPT-3 was a quantum leap in scale and ability for AI language models. The OpenAI team scaled up the number of neurons (parameters) in the model to 175 billion (hundreds of times larger than prior models) and trained it on text from the internet and books. The surprising finding was that GPT-3 can learn to do a task from just a few examples given in the prompt, without any further training ‚Äì for instance, give it two English-to-French translation examples in the prompt, and it can translate a new English sentence to French by analogy (\r[2005.14165] Language Models are Few-Shot Learners - arXiv). This ‚Äúfew-shot‚Äù ability felt almost like instant learning. Moreover, GPT-3 could generate paragraphs of coherent, often insightful text on almost any topic you prompted it with. It was the first AI that really felt like a general-purpose language generator, and it set the stage for the chatbots and creative AI that followed.\n2020 ‚Äì Jumper et al.: ‚ÄúHigh Accuracy Protein Structure Prediction Using Deep Learning (AlphaFold)‚Äù Contribution \u0026amp; Impact: Solved a 50-year grand challenge in biology ‚Äì the protein folding problem ‚Äì using deep learning. AlphaFold2 (described in a 2020 Nature paper and CASP competition results) employed an attention-based neural network (in fact, a modified transformer) to predict 3D protein structures from amino acid sequences with atomic-level accuracy (\r10 AI milestones of the last 10 years | Royal Institution). It trained on the sequences and known structures in public databases and dramatically outperformed all prior methods, achieving accuracies comparable to experimental laboratory techniques. This was a milestone not just for AI but for science, showing that AI can make major contributions to scientific problems. AlphaFold‚Äôs success has accelerated research in drug discovery and biology, and it demonstrated the versatility of deep learning beyond traditional ‚ÄúAI tasks.‚Äù\nInfluence (1‚Äì10): 10/10\nBeginner-Friendly Explanation: AlphaFold was like an ‚ÄúAI biochemist‚Äù that figured out how proteins fold into their 3D shapes. Proteins are molecular machines in our bodies, and their function depends on their shape. Determining that shape used to take scientists years in the lab, but AlphaFold learned to predict shapes in hours with incredible accuracy (\r10 AI milestones of the last 10 years | Royal Institution). How? It looked at tens of thousands of known protein shapes and learned patterns. Using a transformer neural network, it could then take a new protein‚Äôs sequence (its amino acid recipe) and compute its likely folded structure. This achievement was considered a major scientific breakthrough ‚Äì something many experts thought AI wouldn‚Äôt crack for a long time. It showed AI can not only play games or chat, but also solve hard scientific puzzles, potentially leading to new medicines and understandings of biology.\n2022 ‚Äì Ouyang et al.: ‚ÄúTraining Language Models to Follow Instructions with Human Feedback‚Äù (InstructGPT) Contribution \u0026amp; Impact: Demonstrated a successful approach to AI alignment by fine-tuning a large language model using human feedback. The paper introduced InstructGPT, a version of GPT-3 fine-tuned with Reinforcement Learning from Human Feedback (RLHF). Human evaluators ranked outputs, and those rankings were used to train a reward model which then guided the policy optimization. The result was an LLM that followed user instructions much more reliably and produced outputs that were more truthful and less toxic than the base GPT-3 (\rTraining language models to follow instructions with human feedback). InstructGPT showed that large models can be steered toward helpful behavior, and its techniques directly underlie OpenAI‚Äôs ChatGPT. This was a key step in making LLMs practically useful and safer for wide deployment.\nInfluence (1‚Äì10): 9/10\nBeginner-Friendly Explanation: InstructGPT took a big language model and basically gave it a manners class with human teachers. Instead of just predicting the next word in general, it was trained to follow instructions. Researchers accomplished this by showing the model many examples of prompts and ideal answers (crafted or rated by humans), and even having humans rank different AI responses to the same prompt. The model was then tweaked (using a reinforcement learning process) to prefer responses that humans liked (\rTraining language models to follow instructions with human feedback). The outcome: compared to the original GPT-3, InstructGPT is much better at doing what you ask ‚Äì if you prompt it to be polite and to avoid certain topics, it will, and it makes fewer factual errors. This approach of using human feedback became crucial for turning big general models into helpful assistants (it‚Äôs essentially how ChatGPT was trained).\n2023 ‚Äì OpenAI: ‚ÄúGPT-4 Technical Report‚Äù Contribution \u0026amp; Impact: Described GPT-4, a large-scale multimodal model that represents the state-of-the-art in 2023. GPT-4 is multimodal, accepting both image and text inputs and generating text outputs (\r[2303.08774] GPT-4 Technical Report - arXiv). It further improved the capability and alignment of LLMs ‚Äì exhibiting more advanced reasoning, fewer mistakes, and the ability to handle much more complex instructions than its predecessors. While many details (like model size) remain unpublished, GPT-4‚Äôs impact has been to push the envelope of what AI systems can do (such as passing standardized tests, coding large programs, or analyzing images in detail) and to highlight new challenges (like hallucination reduction and transparency). GPT-4‚Äôs release solidified that LLMs are here to stay, being integrated into products and society at large, and it raised the urgency of addressing AI‚Äôs open problems.\nInfluence (1‚Äì10): 9/10\nBeginner-Friendly Explanation: GPT-4 is currently the most advanced iteration of the GPT series. One big new feature is that it can accept images as part of the prompt ‚Äì so you can show it a picture and ask questions about it, and it will respond (for example, describing an image or interpreting a meme) (\r[2303.08774] GPT-4 Technical Report - arXiv). It‚Äôs also significantly smarter in many ways: it can handle much longer essays, it‚Äôs harder to trick into giving harmful answers, and it scores impressively on exams in math, law, medicine, etc. People have used GPT-4 to draft legal documents, create apps from scratch, and tutor themselves on complex topics. In short, GPT-4 pushed the boundary of AI capabilities forward, while also making it clear that we need to deal with issues like the model sometimes being confidently wrong (hallucinations) or the need for even better alignment with human values.\nOpen Problems and Future Directions\r#\rDespite these milestones and the immense progress in AI, several fundamental challenges remain unsolved:\nExplainability: Many advanced AI models (especially deep neural networks) operate as ‚Äúblack boxes,‚Äù making it hard to understand or trust their decisions (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Developing AI that can explain why it made a certain decision is an ongoing area of research.\nGeneralization and Robustness: AI systems can be brittle ‚Äì a model trained on certain data may fail in unexpected ways when faced with novel situations or adversarial inputs (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Achieving human-like adaptability and reliability in the open world (not just controlled settings) is still an open problem.\nBias and Fairness: Models often inherit biases present in their training data, which can lead to unfair or incorrect outcomes, especially in sensitive applications (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Eliminating harmful biases and ensuring AI systems treat different groups fairly is crucial and challenging.\nCommon-Sense Reasoning: AI still notably lacks common sense. Tasks that are trivial for humans ‚Äì understanding unstated assumptions, basic physical or social logic ‚Äì can stump AI systems (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). Building AI with a robust common-sense understanding of the world is an important unsolved challenge.\nSafety and Alignment: As AI systems become more powerful, ensuring they align with human values and intent is paramount (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). This involves preventing unintended harmful behavior, controlling misinterpretations of instructions, and, in the long term, making sure AI goals remain tethered to what humans actually want. Techniques like the RLHF used in InstructGPT are initial steps, but deeper solutions in AI safety, robustness, and governance are actively being sought.\nIn summary, the evolution of AI has been driven by a series of conceptual and technical breakthroughs ‚Äì from the theoretical underpinnings of symbolic reasoning and neural networks to the empirical triumphs of deep learning and large language models. Each foundational paper above either opened a new avenue of research or dramatically accelerated progress in an existing one. A reader with a strong analytical background can appreciate how each innovation addressed a core limitation of the previous generation: giving machines the ability to reason with logic, to learn from data, to handle uncertainty, to perceive patterns, to remember sequences, to leverage big data and compute, and to align with human goals. While many ‚Äúimpossible‚Äù feats have now been achieved (playing Go, folding proteins, human-level dialogue), AI is not a solved problem ‚Äì far from it. The community is now wrestling with making AI more understandable, general, fair, and safe. These open problems will define the next chapter of AI research, as we move from simply building powerful systems to ensuring those systems behave intelligently and beneficially in the messy, nuanced world we live in.\nSources: The descriptions above draw from the original papers and subsequent analyses, including historical accounts and summaries from Wikipedia and other secondary sources. Key references include McCulloch \u0026amp; Pitts (1943) (\rA Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia) (\rA Logical Calculus of the Ideas Immanent in Nervous Activity - Wikipedia), Turing (1950) (\rAlan Turing\u0026rsquo;s Contributions to Artificial Intelligence : History of Information), Newell \u0026amp; Simon (1956) (\rNewell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information) (\rNewell, Simon \u0026amp; Shaw Develop the First Artificial Intelligence Program : History of Information), Rosenblatt‚Äôs perceptron (1958) (\rRosenblatt\u0026rsquo;s Perceptron Uses a Type of Neural Network : History of Information) (\rProfessor‚Äôs perceptron paved the way for AI ‚Äì 60 years too soon | Cornell Chronicle), Samuel‚Äôs checkers (1959) (\rThe games that helped AI evolve | IBM), Minsky \u0026amp; Papert (1969) (\rMinsky \u0026amp; Papert‚Äôs ‚ÄúPerceptrons‚Äù ‚Äì Building Babylon) (\rMinsky \u0026amp; Papert‚Äôs ‚ÄúPerceptrons‚Äù ‚Äì Building Babylon), backpropagation (1986) (\rBackpropagation - Wikipedia), Pearl‚Äôs Bayesian networks (1988) (\rProbabilistic Reasoning (1993‚Äì2011) ‚Äî Making Things Think: How AI and Deep Learning Power the Products We Use) (\rProbabilistic Reasoning (1993‚Äì2011) ‚Äî Making Things Think: How AI and Deep Learning Power the Products We Use), LeCun‚Äôs CNN (1989) ([\nDeep Neural Nets: 33 years ago and 33 years from now (Invited Post) ¬∑ The ICLR Blog Track\n](\rhttps://iclr-blog-track.github.io/2022/03/26/lecun1989/#:~:text=The%20Yann%20LeCun%20et%20al,I%20set%20out%20to%20reproduce)), Watkins‚Äô Q-learning (1989) (\rQ-learning - Wikipedia), Cortes \u0026amp; Vapnik (1995) on SVMs (\rSupport vector machine - Wikipedia), Hochreiter \u0026amp; Schmidhuber (1997) on LSTM (\rLong short-term memory - Wikipedia), Hinton‚Äôs deep belief nets (2006) (\r[PDF] Why does Unsupervised Pre-training Help Deep Learning?), AlexNet (2012) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone) (\rAlexNet and ImageNet: The Birth of Deep Learning | Pinecone), GANs (2014) (\r10 AI milestones of the last 10 years | Royal Institution), DeepMind‚Äôs DQN (2015) (\rHuman-level control through deep reinforcement learning | Nature), AlphaGo (2016) (\rMastering the game of Go with deep neural networks and tree search | Nature), the Transformer (2017) (\r10 AI milestones of the last 10 years | Royal Institution), BERT (2018) (\r[PDF] BERT: Pre-training of Deep Bidirectional Transformers for Language \u0026hellip;), GPT-1 (2018) (\r10 AI milestones of the last 10 years | Royal Institution), GPT-3 (2020) (\r[2005.14165] Language Models are Few-Shot Learners - arXiv), AlphaFold (2020) (\r10 AI milestones of the last 10 years | Royal Institution), InstructGPT (2022) (\rTraining language models to follow instructions with human feedback), and GPT-4 (2023) (\r[2303.08774] GPT-4 Technical Report - arXiv), as well as discussions on AI‚Äôs remaining challenges (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium) (\rThe Frontiers of Intelligence: Navigating Open Problems in AI | by Frank Morales Aguilera | AI Simplified in Plain English | Medium). These works collectively chart the conceptual history of AI‚Äôs evolution and the road ahead.\n"},{"id":10,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.5-Lebesgue-Convergence-Theorem/","title":"8.5 Lebesgue Convergence Theorem","section":"Á¨¨ÂÖ´Á´† Â∫¶ÈáèÁêÜËÆ∫","content":"\rMain Question\r#\rWhen do we have: $$\\lim_{n\\to\\infty}\\int_A f_n(x)dx = \\int_A(\\lim_{n\\to\\infty}f_n(x))dx?$$\nLebesgue Monotone Convergence Theorem (Theorem 8.6.1)\r#\rLet $g_n: [0,1] \\to \\mathbb{R}$ be a sequence of non-negative measurable functions such that:\n$g_{n+1}(x) \\leq g_n(x)$ $\\forall x$ (decreasing sequence) $\\lim_{n\\to\\infty} g_n(x) = 0$ $\\forall x \\in [0,1]$ Then: $$\\lim_{n\\to\\infty}\\int_0^1 g_n(x)dx = \\int_0^1 0 dx = 0$$\nProof Details\r#\rFor LMCT to hold, we only need:\n$f_n(x) \\leq f_{n+1}(x) \\leq f(x)$ $\\forall x$ $f_n(x) \\to f(x)$, $\\forall x$ This implies: $f_n(x) \\uparrow f(x)$\nThe assumption $A \\subset [0,1] \\subset \\mathbb{R}$ is not essential. Result is true for any set $A \\subset \\mathbb{R}^n$.\nThe Monotonicity Assumption Cannot Be Removed\r#\rExample\r#\r$g_n(x) = \\begin{cases} n, \u0026amp; 0 \u0026lt; x \u0026lt; \\frac{1}{n} \\ 0, \u0026amp; \\text{else} \\end{cases}$\nNote that:\n$g_n(x) \\to 0$ $\\forall x \\in [0,1]$ $\\int_0^1 g_n(x)dx = 1$ $\\forall n$ Therefore $\\lim_{n\\to\\infty}\\int_0^1 g_n(x)dx = 1 \\neq 0 = \\int_0^1 0 dx$\n2nd Part of LMCT\r#\rLemma\r#\rSuppose $f: [0,1] \\to \\mathbb{R}$ is measurable with $|f| \\leq M$ and $\\int_0^1 f \\geq \\alpha \u0026gt; 0$. Then the set: $E = {x \\in [0,1]: f(x) \\geq \\frac{\\alpha}{2}}$ contains a finite union of disjoint open intervals of total length $\\geq \\frac{\\alpha}{4M}$\nResult\r#\r$0 \\leq \\int_0^1 f - L(f,P) \\leq \\frac{\\alpha}{4}$\nWhere $L$ denotes the total length of intervals $I \\in P$ with $I \\subset E$.\nThen: $$\\frac{3\\alpha}{4} \u0026lt; L(f, P) = \\sum_{I \\in P} \\left(\\inf_{I}(f(x))\\right)\\ell(I)$$\n$$= \\sum_{I \\subset E} + \\sum_{I \\not\\subset E} \\left(\\inf_{I}(f(x))\\right)\\ell(I)$$\n$$\\leq \\sum_{I \\subset E} M\\ell(I) + \\sum_{I} \\frac{\\alpha}{2}\\ell(I)$$\n$$\\leq \\ell M + \\frac{\\alpha}{2} \\cdot 1$$\nSince $\\ell M \u0026gt; \\frac{3\\alpha}{4} - \\frac{\\alpha}{2} = \\frac{\\alpha}{4}$, we can conclude:\n$$\\ell \u0026gt; \\frac{\\alpha}{4M}$$\nThis completes the proof of the lemma, demonstrating that the set $E$ contains intervals with sufficient total length, which is a key component in establishing the Lebesgue Monotone Convergence Theorem.\nThe boxed result at the end confirms that $\\ell \u0026gt; \\frac{\\alpha}{4M}$, which validates our earlier assertion about the minimum total length of the intervals in set $E$.\nProof of Theorem 8.6.1 (LMCT)\r#\rStep 1: Setup\r#\rGiven:\n$0 \\leq g_{n+1} \\leq g_n$ implies $\\int_0^1 g_{n+1}(x) \\leq \\int_0^1 g_n(x)$ This leads to the limit: $\\lim_{n\\to\\infty}\\int_0^1 g_n(x) = \\lambda \\geq 0$ We need to show that $\\lambda = 0$.\nAssuming $\\lambda \u0026gt; 0$ will lead to a contradiction (using our assumption that $g_n(x) \\to 0$ $\\forall x \\in [0,1]$).\nStep 2: Application of Lemma\r#\rWe apply the previously established lemma to the cut-off function:\n$$(g_n)_M = \\begin{cases} g_n(x), \u0026amp; g_n(x) \\leq M \\ M, \u0026amp; g_n(x) \u0026gt; M \\end{cases}$$\nThis implies:\n$$\\int_0^1 g_n(x) dx = \\lim_{M \\to \\infty} \\int_0^1 (g_n)M$$ ÈÄâÊã© $m\u0026gt; \\frac{2\\lambda}{5}$ s.t. $$0 \\leq \\int^{1}{0}(g_{n}-(g_{n}){M})$\\leq \\int^{1}{0}(g_{1}-(g_{1}){M})\\leq \\frac{\\lambda}{5}$$ Êàë‰ª¨ËÆ©$E{n}=\\left{ x\\in[1,0]:g_{n}(x)\\geq \\frac{2\\lambda}{5} \\right}$ÔºåÁÑ∂Âêé\n$E_{n+1}\\subset E_{n}$ ${x \\in [0,1] : (g_n)_M(x) \\geq \\frac{\\alpha}{2}} \\subset E_n$ where we choose $\\alpha$ such that $\\frac{2\\lambda}{5} = \\frac{\\alpha}{2}$, giving us $\\alpha = \\frac{4\\lambda}{5}$ Key Step:\r#\rApplying the lemma to $(g_n)_M$ with $\\alpha = \\frac{4\\lambda}{5}$: this implies $E_n$ contains a finite union of disjoint open intervals of total length: $$\\ell \\geq \\frac{\\alpha}{4M} = \\frac{\\lambda}{5M}$$\nStep 3:\r#\rShow that $\\bigcap_{n=1}^{\\infty} E_n = \\emptyset$\nLet $D = \\bigcap_{n=1}^{\\infty} {x \\in [0,1] : g_n \\text{ not converging to } 0} = \\bigcap_{n=1}^{\\infty} D_n$\nThen $g_{n}$ intagrable $\\Rightarrow m(D_n) = 0 = m(D) = 0$.\nThus $D$ is covered by $U$, a countable union of open intervals of total length $\u0026lt; \\frac{\\lambda}{5M}$.\nBy Step 2,\n$E_n \\subset U$\n"},{"id":11,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.6-Morse-Lemma/","title":"7.6 Ëé´Â∞îÊñØÂºïÁêÜ","section":"Á¨¨‰∏ÉÁ´† ÈÄÜÂáΩÊï∞ÂíåÈöêÂáΩÊï∞ÂÆöÁêÜ","content":"\rMorse Theory: Local Behavior Near a Critical Point\r#\rLet $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$ be of class $C^2$ and $x_0$ is a critical point $\\rightarrow$ one can use $H(x_0)$ to classify critical points.\nMorse theory: makes this classification more precise.\nMorse Lemma\r#\r[!lemma|*] Let $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$ be of class $C^2$ with critical point $x_0 \\in A$. If $H(x_0)$ is non-degenerate, then there exists a neighborhood of $x_0$ and a diffeomorphism $g$ such that the function $f \\circ g$ has the form: $$f \\circ g(y) = f(x_0) + \\sum_{i=1}^{\\lambda} -y_i^2 + \\sum_{i=\\lambda+1}^n y_i^2$$ where $\\lambda$ is an integer called the index of $f$ at $x_0$.\nApplications\r#\r$\\lambda = 0$: $x_0$ is local minimum (paraboloid opens upward) $\\lambda = n$: $x_0$ is local maximum (paraboloid opens downward) $0 \u0026lt; \\lambda \u0026lt; n$: $x_0$ is saddle point (hyperboloid) Idea: Diagonalization of Hessian Matrix\r#\r$H(f) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} \u0026amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \u0026amp; \\cdots \\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} \u0026amp; \\frac{\\partial^2 f}{\\partial x_2^2} \u0026amp; \\cdots \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \\end{pmatrix}$\n$\\lambda$ = # of negative eigenvalues of $H(f)(x_0)$\nExample: Determine the shape of the surface $z = f(x,y)$ near $(0,0)$\r#\rSolution: At $(0,0)$, $f_x = 0$, $f_y = 0$\nCompute eigenvalues of the Hessian matrix. If there is one negative eigenvalue, $\\lambda = 1$, it\u0026rsquo;s a saddle point (hyperboloid).\nConstrained Extremal Problem\r#\rGoal: To maximize or minimize a function $f(x): \\mathbb{R}^n \\to \\mathbb{R}$ under the condition $g(x) = c$.\nLagrange Multiplier Method\r#\rA Necessary Condition\r#\rTheorem: Let $f, g: U \\subset \\mathbb{R}^n \\to \\mathbb{R}$ be of class $C^1$. Assume $g(x_0) = c_0$ with $\\nabla g(x_0) \\neq 0$. If $f$ restricted to the surface $S = {g(x) = c_0}$ has a max or min at $x_0$, then there exists a real number $\\lambda$ such that:\n$$\\nabla f(x_0) = \\lambda \\nabla g(x_0)$$\nGeometric Meaning\r#\r$\\nabla f(x_0)$ points in the direction of steepest ascent At the extremum, this direction is perpendicular to the surface $S$ Geometric Proof\r#\rLet $c(t)$ be a fixed curve on $S = {g(x) = c_0}$ passing through $x_0$ at $t = t_0$. If $f$ restricted to $S$ has a max at $x_0$, then $f(c(t))$ has max at $t_0$.\nTherefore: $\\frac{d}{dt}f(c(t))|_{t=t_0} = 0$\nBy chain rule: $\\nabla f(c(t_0)) \\cdot c\u0026rsquo;(t_0) = 0$\nSince $c\u0026rsquo;(t_0)$ is tangent to $S$ at $x_0$, $\\nabla f(x_0)$ must be perpendicular to the tangent space of $S$ at $x_0$. Therefore, it must be parallel to $\\nabla g(x_0)$, which is normal to the surface.\nProcedure to Solve Extremal Problem\r#\rStep 1: Solve the system of equations $$\\nabla f(x) = \\lambda \\nabla g(x)$$ $$g(x) = c$$\nThis gives $n+1$ equations with $n+1$ variables $(x_1, x_2, \u0026hellip;, x_n, \\lambda)$\nStep 2: Compute values of $f$ at these critical points and determine which are maxima, minima, or saddle points\nExample: Find the extrema of\r#\r$$f(x,y) = x^2 y^2$$ subject to the condition $x^2 + y^2 = 1$\nSolution:\nApplying Lagrange multiplier method: $\\nabla f = (2xy^2, 2x^2y) = \\lambda \\nabla g = \\lambda(2x, 2y)$\nThis gives us:\n$xy^2 = \\lambda x$ $x^2y = \\lambda y$ Analyzing by cases:\nCase 1: If $x = 0$, then $y = \\pm 1$ (from constraint) Case 2: If $y = 0$, then $x = \\pm 1$ (from constraint) Case 3: If $x \\neq 0$ and $y \\neq 0$: From the first equation: $y^2 = \\lambda$ (dividing by $x$) From the second equation: $x^2 = \\lambda$ (dividing by $y$) This gives $x^2 = y^2$ With the constraint $x^2 + y^2 = 1$, we get $2x^2 = 1$, so $x = \\pm\\frac{1}{\\sqrt{2}}$ and $y = \\pm\\frac{1}{\\sqrt{2}}$ Critical points: $(0, \\pm 1), (\\pm 1, 0), (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}), (\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}), (-\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}), (-\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}})$\nEvaluating $f(x,y) = x^2y^2$ at these points:\n$f(0, \\pm 1) = 0$ $f(\\pm 1, 0) = 0$ $f(\\pm\\frac{1}{\\sqrt{2}}, \\pm\\frac{1}{\\sqrt{2}}) = (\\frac{1}{2})^2 = \\frac{1}{4}$ Therefore, the maximum value is $\\frac{1}{4}$ at $(\\pm\\frac{1}{\\sqrt{2}}, \\pm\\frac{1}{\\sqrt{2}})$, and the minimum value is $0$ at $(0, \\pm 1)$ and $(\\pm 1, 0)$.\nExtremal Problem with Multiple Constraints\r#\rMaximize/minimize $f(x)$ with constraints:\n$g_1(x) = c_1$ $g_2(x) = c_2$ \u0026hellip; $g_m(x) = c_m$ Procedure: Solve the system of equations\r#\r$$\\nabla f(x) = \\lambda_1 \\nabla g_1(x) + \\lambda_2 \\nabla g_2(x) + \u0026hellip; + \\lambda_m \\nabla g_m(x)$$ $$g_1(x) = c_1, g_2(x) = c_2, \u0026hellip;, g_m(x) = c_m$$\nWith $m+n$ equations and $m+n$ variables $(x_1,\u0026hellip;,x_n, \\lambda_1,\u0026hellip;,\\lambda_m)$\nAnalytical Proof of the Theorem (Lagrange Multiplier)\r#\rWe want to substitute the condition $g(x) = c_0$ into the function $f(x)$ to eliminate the constraint.\nSince $\\nabla g(x_0) \\neq 0$, we may assume without loss of generality that $\\frac{\\partial g}{\\partial x_n} \\neq 0$ at $x_0$.\nBy the implicit function theorem, the equation $g(x_1, x_2, \u0026hellip;, x_n) = c_0$ can be solved for $x_n$ in a neighborhood of $x_0$: $$x_n = h(x_1, x_2, \u0026hellip;, x_{n-1})$$\nLet $k(x_1, x_2, \u0026hellip;, x_{n-1}) = f(x_1, x_2, \u0026hellip;, x_{n-1}, h(x_1, x_2, \u0026hellip;, x_{n-1}))$\nThus, an extremum of $f$ subject to the constraint corresponds to an extremum of $k$ without constraints.\nAt an extremum of $k$, we have: $$0 = \\frac{\\partial k}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} + \\frac{\\partial f}{\\partial x_n}\\frac{\\partial h}{\\partial x_i}, \\quad i = 1, 2, \u0026hellip;, n-1$$\nSince $g(x_1, x_2, \u0026hellip;, x_{n-1}, h(x_1, x_2, \u0026hellip;, x_{n-1})) = c_0$ identically, we can differentiate with respect to $x_i$:\n$$\\frac{\\partial g}{\\partial x_i} + \\frac{\\partial g}{\\partial x_n}\\frac{\\partial h}{\\partial x_i} = 0, \\quad i = 1, 2, \u0026hellip;, n-1$$\nSolving for $\\frac{\\partial h}{\\partial x_i}$:\n$$\\frac{\\partial h}{\\partial x_i} = -\\frac{\\frac{\\partial g}{\\partial x_i}}{\\frac{\\partial g}{\\partial x_n}}, \\quad i = 1, 2, \u0026hellip;, n-1$$\nSubstituting this into our extremum condition:\n$$\\frac{\\partial f}{\\partial x_i} - \\frac{\\partial f}{\\partial x_n}\\frac{\\frac{\\partial g}{\\partial x_i}}{\\frac{\\partial g}{\\partial x_n}} = 0, \\quad i = 1, 2, \u0026hellip;, n-1$$\nRearranging:\n$$\\frac{\\partial f}{\\partial x_i}\\frac{\\partial g}{\\partial x_n} - \\frac{\\partial f}{\\partial x_n}\\frac{\\partial g}{\\partial x_i} = 0, \\quad i = 1, 2, \u0026hellip;, n-1$$\nLet $\\lambda = \\frac{\\partial f}{\\partial x_n} / \\frac{\\partial g}{\\partial x_n}$, then:\n$$\\frac{\\partial f}{\\partial x_i} = \\lambda \\frac{\\partial g}{\\partial x_i}, \\quad i = 1, 2, \u0026hellip;, n-1$$\nAnd by definition of $\\lambda$, we also have $\\frac{\\partial f}{\\partial x_n} = \\lambda \\frac{\\partial g}{\\partial x_n}$\nTherefore, in vector form: $\\nabla f(x_0) = \\lambda \\nabla g(x_0)$\n"},{"id":12,"href":"/docs/Mathematics/hidden/","title":"Hidden","section":"Mathematics","content":"\rThis page is hidden in menu\r#\rQuondam non pater est dignior ille Eurotas\r#\rLatent te facies\r#\rLorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.\nPater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor Cum honorum Latona\r#\rO fallor in sustinui iussorum equidem. Nymphae operi oris alii fronde parens dumque, in auro ait mox ingenti proxima iamdudum maius?\nreality(burnDocking(apache_nanometer),\rpad.property_data_programming.sectorBrowserPpga(dataMask, 37,\rrecycleRup));\rintellectualVaporwareUser += -5 * 4;\rtraceroute_key_upnp /= lag_optical(android.smb(thyristorTftp));\rsurge_host_golden = mca_compact_device(dual_dpi_opengl, 33,\rcommerce_add_ppc);\rif (lun_ipv) {\rverticalExtranet(1, thumbnail_ttl, 3);\rbar_graphics_jpeg(chipset - sector_xmp_beta);\r}\rFronde cetera dextrae sequens pennis voce muneris\r#\rActa cretus diem restet utque; move integer, oscula non inspirat, noctisque scelus! Nantemque in suas vobis quamvis, et labori!\nvar runtimeDiskCompiler = home - array_ad_software;\rif (internic \u0026gt; disk) {\remoticonLockCron += 37 + bps - 4;\rwan_ansi_honeypot.cardGigaflops = artificialStorageCgi;\rsimplex -= downloadAccess;\r}\rvar volumeHardeningAndroid = pixel + tftp + onProcessorUnmount;\rsector(memory(firewire + interlaced, wired)); "},{"id":13,"href":"/docs/Mathematics/MATH-351-352/Heat-Equation/Heat-Equation-Solution/","title":"Heat Equation Solution","section":"ÁÉ≠ÊñπÁ®ã","content":"\rHeat Equation Solution\r#\rScale Invariance Property\r#\r[!Theorem] Since we know that $u_t=ku_{xx}$ , then we know if $u \\in U$ for $U \\subset \\mathbb{R}^n$ solves the equation, so does $u(\\lambda x, \\lambda^2 t)$ for $\\lambda \\in \\mathbb{R}$ according to the scale invariance property.\nLet‚Äôs set that $\\bar{x}=\\lambda x, \\bar{t}=\\lambda^2 t$, then it is easy to see:\n$$ \\begin{align} u_{\\bar{t}}= \\frac{\\partial u}{\\partial \\bar{t}}\\cdot\\frac{\\partial \\bar{t}}{\\partial t} = \\frac{\\partial u}{\\partial t} \\cdot (\\lambda^2) \\ u_{\\bar{x}\\bar{x}} \u0026amp;=\\frac{\\partial}{\\partial x} \\cdot \\frac{\\partial u}{\\partial \\bar{x} } = (\\frac{\\partial}{\\partial \\bar{x}} \\cdot \\frac{\\partial \\bar{x}}{\\partial x})(\\frac{\\partial u}{\\partial \\bar{x} } \\cdot \\frac{\\partial \\bar{x}}{\\partial x})\\ \u0026amp;= \\frac{\\partial^2 u}{\\partial x^2} \\cdot (\\lambda^2) \\end{align} $$\nwhere the equation still holds regardless the choice of $\\lambda$ $(\\lambda \\neq 0)$\nThe scaling $\\frac{x^2}{t}$ or $\\frac{x}{\\sqrt{t}}$ that is invariant to the equation suggests the solution is in the form of $u(x,t)=v(\\frac{x^2}{t})$ f.s. function $v$. That is,\n$$ \\begin{align} u(x,t)=t^\\alpha v(\\frac{x}{t^\\beta}) \\end{align} $$\nwhere constants $\\alpha, \\beta$ and functions $v:\\mathbb{R}^n\\to \\mathbb{R}$ must be found. This means the solution must be invariant under the dilation scaling $\\forall \\lambda \u0026gt;0, x= \\mathbb{R}^n, t\u0026gt;0$ :\n$$ u(x,t) = \\lambda^\\alpha u(\\lambda^\\beta x,\\lambda t) $$\nSetting $\\lambda=t^{-1}$, in which $v(y):= u(y,1)$. We insert (1) into the original heat equation to solve for $v$ with our new variable $y=\\cfrac{x}{t^\\beta}$. We then take $\\beta = \\frac{1}{2}$ so that the terms involved $t$ are cancelled out - we hence derived an equation that is only in terms of $y$:\n$$ \\alpha v + \\frac{1}{2}\\cdot Dv + \\Delta v = 0 \\ \\ (k=1) $$\nDifferent textbook takes different methods to find the constant $\\alpha=-\\frac{1}{2}$ here:\nUsing conservation of heat energy in physics Guessing $v$ to be radial and introduce $v(y)=w(|y|)$ Eventually, we reached at $v(y)=Ae^{-\\frac{y^2}{4k}}$ such that\n$$ u(x,t)=A\\frac{1}{ t^{n/2}}e^{-\\frac{|x|^2}{4kt}} $$\nThe particular choice of normalizing constant $A=\\frac{1}{(4\\pi k)^{n/2}}$ is derived from $\\int_{\\mathbb{R}^n} \\Phi(x,t) dx=1$. (See p. 46 Lemma, Evans) Hence, the general solution to the heat equation for $n$ dimension is\n$$ \\Phi(x,t)=\\frac{1}{(4\\pi k t)^{n/2}} e^{-\\frac{|x|^2}{4kt}} $$\nwhere $x\\in \\mathbb{R}^n, t\u0026gt;0$. For situation $t\u0026lt;0$, the solution is $\\Phi=0$.\nFor dimension $n=1$, we yield $$ \\Phi(x,t)=\\frac{1}{\\sqrt{4\\pi k t}} e^{-\\frac{x^2}{4kt}} $$ $$ $$\n"},{"id":14,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-6/","title":"Homework 6","section":"Real Analysis II","content":"7.2: 1, 2, [3,4]; Chapter 7: [4], 5, [6], 9, [12].\nProblem 7.2.3 In the system $$\\begin{array}{r} 3 x+2 y+z^2+u+v^2=0 \\ 4 x+3 y+z+u^2+v+w+2=0 \\ x+z+w+u^2+2=0 \\end{array} $$ discuss the solvability for $u, v, w$ in terms of $x, y, z$ near $x=y=z=0, u=$ $v=0, w=-2$.\n[!theorem|*] We first define three functions: $$ \\begin{aligned} F_1(x,y,z,u,v,w) ;\u0026amp;=; 3x + 2y + z^2 + u + v^2,\\ F_2(x,y,z,u,v,w) ;\u0026amp;=; 4x + 3y + z + u^2 + v + w + 2,\\ F_3(x,y,z,u,v,w) ;\u0026amp;=; x + z + w + u^2 + 2. \\end{aligned} $$\nSubstitute $x=0,y=0,z=0,u=0,v=0,w=-2$ into each equation:\n$F_{1}(0,0,0,0,0,-2) = 3\\cdot 0 + 2\\cdot 0 + 0^2 + 0 + 0^2 = 0.$ $F_{2}(0,0,0,0,0,-2) = 4\\cdot 0 + 3\\cdot 0 + 0 + (0)^2 + 0 + (-2) + 2 = 0.$ $F_{3}(0,0,0,0,0,-2) = 0 + 0 + (-2) + (0)^2 + 2 = 0.$ Hence $\\bigl(0,0,0,0,0,-2\\bigr)$ satisfies all three equations. By the Implicit Function Theorem, we want to solve for $(u,v,w)$ if the Jacobian of $D_{(u,v,w)} (F_1, F_2, F_3) ;$ is invertible at that point.\nWe then compute partial derivatives, and evaluate them at $\\bigl(0,0,0,0,0,-2\\bigr)$: $$ D_{(u,v,w)} (F_1, F_2, F_3)=\\begin{bmatrix} F_{1u} \u0026amp; F_{1v}\u0026amp;F_{1w} \\ F_{2u} \u0026amp; F_{2v}\u0026amp;F_{2w} \\ F_{3u} \u0026amp; F_{3v}\u0026amp;F_{3w} \\end{bmatrix}=\\begin{bmatrix} 1 \u0026amp; 2v \u0026amp; 0 \\ 2u \u0026amp; 1 \u0026amp; 1 \\ 2u \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$ At that point, $u=0$ and $v=0$, the determinant of this $3\\times 3$ matrix is $$\\det \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 1 \u0026amp; 1 \\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} ;=; 1 ;\\neq; 0. $$ So the matrix is invertible. Therefore, since $F_1=F_2=F_3=0$ is at our point of interest, and the invertibility of Jacobian ensures that the mapping is locally bijective, the Implicit FT guarantees that in a neighborhood of $\\bigl(x,y,z\\bigr)=(0,0,0)$, there exist unique smooth functions $$u = u(x,y,z), \\quad v = v(x,y,z), \\quad w = w(x,y,z), $$ satisfying the system. And since, $\\bigl(u(0,0,0),,v(0,0,0),,w(0,0,0)\\bigr)=(0,0,-2)$, the system is locally solvable for $,(u,v,w),$ as functions of $,(x,y,z),$ near $,(0,0,0),$.\nProblem 7.2.4 Does the map\n$$ (x, y) \\mapsto\\left(\\frac{x^2-y^2}{x^2+y^2}, \\frac{x y}{x^2+y^2}\\right) $$\nhave a local inverse near $(0,1)$ ?\n[!definition|*] Define $$F(x,y);=;\\Bigl(F_1(x,y),,F_2(x,y)\\Bigr);=;\\biggl(,\\frac{x^2 - y^2}{x^2 + y^2},;\\frac{x,y}{x^2 + y^2}\\biggr)$$ We substitute $\\bigl(x,y\\bigr)=(0,1)$ into $F$: $$F(0,1) ;=;\\Bigl(\\tfrac{0^2 - 1^2}{0^2 + 1^2},;\\tfrac{0\\cdot1}{0^2 + 1^2}\\Bigr) ;=;(-1,,0)$$ We check if the Jacobian of $F$ at $(0,1)$ is invertible. The partial of $,(F_1,F_2)$ are\n$$F_1(x,y)=\\tfrac{x^2 - y^2}{x^2 + y^2}, \\quad F_2(x,y)=\\tfrac{x,y}{x^2 + y^2} $$\nFor $F_1$: $$\\begin{align} \\frac{\\partial F_1}{\\partial x}(x,y) \u0026amp; =\\frac{(x^2+y^2)(2x) - (x^2-y^2)(2x)}{(x^2+y^2)^2}\\ \u0026amp; =\\frac{2x\\Bigl[(x^2+y^2)-(x^2-y^2)\\Bigr]}{(x^2+y^2)^2}=\\frac{4xy^2}{(x^2+y^2)^2} \\end{align} $$ And $$\\begin{align} \\frac{\\partial F_1}{\\partial y}(x,y) \u0026amp; =\\frac{(x^2+y^2)(-2y) - (x^2-y^2)(2y)}{(x^2+y^2)^2} \\ \u0026amp; =\\frac{-2y\\Bigl[(x^2+y^2)+(x^2-y^2)\\Bigr]}{(x^2+y^2)^2}=-\\frac{4x^2y}{(x^2+y^2)^2} \\end{align} $$ For $F_2$: $$\\begin{align} \\frac{\\partial F_2}{\\partial x}(x,y) \u0026amp; =\\frac{(x^2+y^2)(y) - (xy)(2x)}{(x^2+y^2)^2} \\ \u0026amp; =\\frac{y\\Bigl[(x^2+y^2)-2x^2\\Bigr]}{(x^2+y^2)^2} =\\frac{y(y^2-x^2)}{(x^2+y^2)^2} \\end{align} $$ And $$\\begin{align} \\frac{\\partial F_2}{\\partial y}(x,y) \u0026amp; =\\frac{(x^2+y^2)(x) - (xy)(2y)}{(x^2+y^2)^2} \\ \u0026amp; =\\frac{x\\Bigl[(x^2+y^2)-2y^2\\Bigr]}{(x^2+y^2)^2} =\\frac{x(x^2-y^2)}{(x^2+y^2)^2} \\end{align} $$ Since $x^2+y^2=0^2+1^2=1$, the evaluation at $(0,1)$ are:\n$\\displaystyle \\frac{\\partial F_1}{\\partial x}(0,1)=\\frac{4\\cdot 0\\cdot1^2}{1^2}=0$ $\\displaystyle \\frac{\\partial F_1}{\\partial y}(0,1)=-\\frac{4\\cdot0^2\\cdot1}{1^2}=0$ $\\displaystyle \\frac{\\partial F_2}{\\partial x}(0,1)=\\frac{1,(1^2-0^2)}{1^2}=1$ $\\displaystyle \\frac{\\partial F_2}{\\partial y}(0,1)=\\frac{0,(0^2-1^2)}{1^2}=0$ Hence the Jacobian matrix of $F$ at $,(0,1)$ is $$D F(0,1) ;=; \\begin{pmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{pmatrix} $$ The determinant of $D F(0,1)$ is $$\\det\\begin{pmatrix} 0 \u0026amp; 0\\ 1 \u0026amp; 0 \\end{pmatrix} ;=;0 $$ Because this determinant is zero, the matrix is not invertible. This means we cannot apply the IFT to conclude that $F$ is invertible near $(0,1)$; there is no diffeomorphic local inverse of $F$ around $(0,1)$.\nTherefore, we conclude that the map $F$ does not have a local inverse near $(0,1)$.\nProblem [7.4] 4. Show that the equations\n$$ \\begin{array}{r} x^2-y^2-u^3+v^2+4=0 \\ 2 x y+y^2-2 u^2+3 v^4+8=0 \\end{array} $$\ndetermine functions $u(x, y), v(x, y)$ near $x=2, y=-1$ such that $u(2,-1)=$ $2, v(2,-1)=1$. Compute $\\partial u / \\partial x$.\n[!definition|*] Let $$ F(x,y,u,v)=\\begin{cases} x^2 - y^2 -u^3 +v^2 +4 =0\\ 2xy+y^2 -2u^2 +v^4 +8=0 \\end{cases} $$\nWe first verify $,(x,y,u,v)=(2,-1,2,1)$ is a solution: $$\\begin{cases};4 - 1 - 8 + 1 + 4 ;=;0 \\ -4 +1 -8 +3 +8 ;=;0\\end{cases} $$ Therefore $\\bigl(2,-1,2,1\\bigr)$ is indeed a solution of the system. Then, we compute the Jobcobian: $$\\begin{align} D_{(u,v)} (F_1, F_2) \u0026amp; =\\begin{pmatrix} F_{1u} \u0026amp; F_{1v}\\ F_{2u} \u0026amp; F_{2v} \\end{pmatrix} \\Bigg|{(2,-1,2,1)} \\[3pt] \u0026amp; =\\begin{pmatrix} -3u^{2}\u0026amp; 2v\\ -4u \u0026amp; 12v^3 \\end{pmatrix}\\Bigg|{(2,-1,2,1)} \\[5pt] \u0026amp; = \\begin{pmatrix} -12 \u0026amp; 2\\ -8 \u0026amp; 12 \\end{pmatrix}\\end{align} $$ Its determinant is $\\Delta=(-12)(12) - 2(-8)= -144 +16= -128\\neq 0$. Therefore, the matrix is invertible, so by the IFT we know that we can solve for $u$ and $v$ as functions of $x,y$ near $,(2,-1)$. Now we compute $u_x(2,-1)$. For $F_1=0$: $$\\frac{\\partial}{\\partial x}(x^2-y^2 -u^3 +v^2 +4) ;=;2x ;-;3u^2 u_x ;+;2v v_x ;=;0 $$ At $(x,y,u,v)=(2,-1,2,1)$, this is $4 -12u_x + 2v_x=0$. For $F_2=0$: $$\\frac{\\partial}{\\partial x}(2xy +y^2 -2u^2 +3v^4 +8) =2y ;-;4u u_x ;+;12v^3 v_x =0 $$ At $(2,-1,2,1)$, this is $-2 ;-;8 u_x +12 v_x=0$. So we obtain: $$\\begin{cases} 4 ;-;12u_x +2v_x = 0\\ -2 ;-;8u_x +12v_x = 0 \\end{cases} $$ To solve this system of equations, we have $$ v_x = \\frac{8u_x + 2}{12} $$ So, $$ \\begin{align*} 4 - 12u_x + 2v_x \u0026amp;= 4 - 12u_x + 2\\left(\\frac{8u_x + 2}{12} \\right) \\ \u0026amp;= 4 - 12u_x + \\frac{4}{3}u_x + \\frac{1}{3} \\ \u0026amp;= -\\frac{32}{3} u_x + \\frac{13}{3} = 0 \\end{align*} $$ $$ \\Longrightarrow u_x = \\frac{13}{32} $$ Hence, we have $$u_x(2,-1) = \\frac{13}{32}$$\nProblem [7.6] Determine whether the \u0026ldquo;curve\u0026rdquo; described by the equation $x^2+y+\\sin (x y)$ $=0$ can be written in the form $y=f(x)$ in a neighborhood of $(0,0)$. Does the implicit function theorem allow you to say whether the equation can be written in the form $x=h(y)$ in a neighborhood of $(0,0)$ ?\n[!definition|*] Let $$F(x,y)=x^2 + y +\\sin!\\bigl(x,y\\bigr)=0$$ We want to show that $F\\colon \\mathbb{R}^2 \\to \\mathbb{R}$ is $C^1$, and $F(x_0,y_0)=0$. We first substitute $x=0,y=0$ into $F$: $$F(0,0)=0^2+0+\\sin(0\\cdot 0)=0$$ Hence $(0,0)$ lies on the curve $F(x,y)=0$. We then compute the partial at $(0,0)$: $$\\begin{align} F_{y} =1 +\\cos!\\bigl(xy\\bigr)\\bigl(x\\bigr) \\Longrightarrow F_{y}(0,0) =1 +0 =1\\neq 0 \\end{align} $$ And $$ \\begin{align} F_{x} =2x +\\cos!\\bigl(xy\\bigr)\\bigl(y\\bigr)\n\\Longrightarrow F_{y}(0,0) = 2\\cdot 0 + 0 = 0 \\end{align} $$\nBecause $F_{y}(0,0)=1\\neq 0$, the Implicit Function Theorem ensures that there exists neighborhood of $(0,0)$ in which we can uniquely solve the equation for $y$ as a function of $x$. Therefore, there exists $y =f(x)$ for $(x,y)$ near $(0,0)$ for all $x$ in the neighborhood of $0$.\nHowever, on the other hand, since $F_{x}(0,0)=0$, Implicit FT does not apply, so the test is conclusive. This means the usual IFT statement fails to guarantee a local solution of the form $x=h(y)$.\nProblem [7.12] Show that the implicit function theorem implies the inverse function theorem.\n[!definition|*]\nLet $f\\colon A\\subset\\mathbb{R}^n \\to \\mathbb{R}^n$ be of class $C^1$. Suppose $x_0\\in A$ and\n$$ J_f(x_0);=;\\det\\bigl(Df(x_0)\\bigr);\\neq;0. $$ We want to show that, there exist neighborhoods $U$ of $x_0$ in $A$ and $V$ of $y_0=f(x_0)$ in $\\mathbb{R}^n$ such that\n(1) $f(U)=V$ and $f\\colon U\\to V$ has an inverse $f^{-1}:V\\to U$. (2) $f^{-1}$ is of class $C^1$. (3) $D f^{-1}(y)=\\bigl[D f(x)\\bigr]^{-1}$ for all $x\\in U$ with $y=f(x)$. Define a new function $$F\\colon \\mathbb{R}^n\\times \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n, \\quad F(x,y)=f(x)-y $$ with $x=(x_1,\\dots,x_n)$ and $y=(y_1,\\dots,y_n)$. Then we know that $F$ is $C^1$ because $f$ is $C^1$ and subtraction is smooth. Note that $F(x_0,,f(x_0))= f(x_0)- f(x_0)=0$. We want to compute the Jacobian of $F$ w.r.t. $y$. So for each $i=1,\\dots,n$, the $i$th component of $F$ is $$F_i(x,y)=f_i(x)-y_i.$$ Since $f_i(x)$ does not depend on $y$, we have for each $j=1,\\dots,n$ $$\\frac{\\partial F_i}{\\partial y_j}(x,y)=\\frac{\\partial}{\\partial y_j}\\bigl(f_i(x)-y_i\\bigr) =\\frac{\\partial f_i(x)}{\\partial y_j}-\\frac{\\partial y_i}{\\partial y_j} =0-\\delta_{ij} $$ where $\\delta_{ij}$ is $$\\delta_{ij}= \\begin{cases}1 \\quad i=j \\0 \\quad i\\neq j \\ \\end{cases} $$ Thus, the $(i,j)$-entry of the Jacobian is $$\\left[\\frac{\\partial F}{\\partial y}(x,y)\\right]{ij}=-\\delta{ij} $$ In matrix form, we have: $$\\frac{\\partial F}{\\partial y}(x,y)=-I $$ where $I$ is the $n\\times n$ identity matrix. Since the determinant $\\det(-I)=(-1)^n\\neq 0$, we know that $D_y F(x,y)=-I$ is invertible everywhere. This satisfy the condition for Implicit FT. Hence, by the Implicit Function Theorem, there is a neighborhood $U$ of $x_0\\in \\mathbb{R}^n$ and a neighborhood $V$ of $y_0=f(x_0)\\in \\mathbb{R}^n$ s.t. $\\forall, y\\in V$, $\\exists! ,x\\in U$ satisfying: $$F(x,y)=0 ;;\\Longleftrightarrow;; f(x)-y=0 ;;\\Longleftrightarrow;; y=f(x) $$ and we have a map that is $C^1$ $$\\Phi:V ;\\to; U \\quad\\text{such that}\\quad F\\bigl(\\Phi(y),,y\\bigr)=0 \\quad\\text{for all }y\\in V $$ which this demonstrates (2). Since $F(\\Phi(y),y) \\Longrightarrow f(\\Phi(y))=y$, it follows that $\\Phi$ is the local inverse $f^{1}$ by definition. Because $f$ itself is $C^1$ and $\\Phi=f^{-1}$ is also $C^1$, we conclude that $f^{-1}$ is a local diffeomorphism near $x_0$, which shows (1). Next, by Corollary 7.2.2 for each $y\\in V$, we have $$\\begin{align} D\\Phi(y) \u0026amp; =-\\Bigl(D_yF(\\Phi(y),y)\\Bigr)^{-1}D_xF(\\Phi(y),y) \\ \u0026amp; =-(-I)^{-1},D f\\bigl(\\Phi(y)\\bigr) \\ \u0026amp; =D f\\bigl(\\Phi(y)\\bigr)^{-1} \\end{align} $$ Since $\\Phi(y)=x$, near $x_{0}$ this yields: $$D f^{-1}(f(x)) ;=; \\bigl(Df(x)\\bigr)^{-1} $$ which shows the (3) of theorem. Hence, we have shown that Implicit Function Theorem directly implies the Inverse Function Theorem.\n"},{"id":15,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-7/","title":"Homework 7","section":"Real Analysis II","content":"HW 7: 7.6: 1, 2, 3, [4,5]; 7.7: 1, 2, 3, [4], 5, [6]; Chapter 7: [25], 36, [38], 39.\nProblem 7.6.4 Let $f(x,y) = x^2 + y^2 + 3y^3 + 8x^4 + x^2e^x \\sin x + 6$. Show that there exist new coordinates $\\xi, \\eta$, where $$\\xi = \\xi(x,y), \\quad \\eta = \\eta(x,y),$$ for which $$f(x,y) = \\xi^2 + \\eta^2 + 6$$ in a neighborhood of $(0, 0)$.\nProblem 7.6.5 (a). If $f$ has a nondegenerate critical point at $x_0 \\in \\mathbb{R}^n$, show that there is a neighborhood of $x_0$ containing no other critical points.\n(b). What are the critical points of the function $f(x,y) = x^2y^2$?\nProblem 7.7.4. $f(x, y, z) = x + y + z, x^2 - y^2 = 1, 2x + z = 1$.\nProblem 7.7.6. Supranational Sludge Corporation produces sludge using equipment and material costing $p = $243$ per unit and labor at a wage of $w = $16$ per hour. If $x$ units of equipment/material and $y$ hours of labor are used, then $20x^{3/4}y^{1/4}$ liters of sludge are produced. If the company has a budget of $B = $51,840,000$ to spend, find the maximum amount of sludge that can be produced and the amounts of equipment/material and of labor used to produce it.\nProblem 7.25\nLet $B(0, r) = {x \\in \\mathbb{R}^n \\mid |x| \\leq r}$. Let $f : B(0, r) \\to \\mathbb{R}^n$ be a map with\na. $|f(x) - f(y)| \\leq \\frac{1}{3}|x - y|$\nb. $|f(0)| \\leq \\frac{2}{3}r$\nProve that there is a unique $x \\in B(0, r)$ such that $f(x) = x$.\nProblem 7.38\nA rectangular box with no top is to have a surface area of 16 square meters. Find the dimensions that maximize the volume.\n"},{"id":16,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-8/","title":"Homework 8","section":"Real Analysis II","content":"8.1: 1, [2, 6]; 8.2: [2], 3, 4, 5, [6]; 8.3: [2], 5, 6; Chapter 8: [12].\nProblem 8.1.2 Let $f:[0,2] \\rightarrow \\mathbb{R}$ be defined by $f(x)=0$ for $0 \\leq x \\leq 1$, and by $f(x)=1$ for $1\u0026lt;x \\leq 2$. Compute, using the definition, $\\int_0^2 f(x) d x$.\n[!definition] We first define a partition $P_{n}$ on $[0,2]$ into $n$ equal subintervals $\\Delta x=\\frac{2}{n}$, s.t. $$\\left[ 0, \\frac{2}{n} \\right], \\left[ \\frac{2}{n} , \\frac{4}{n} \\right],\\dots,\\left[ \\frac{2(n-1)}{n},2 \\right]$$ Then, each subinterval, we have $[x_{i-1},x_{i}]=\\left[ \\frac{2(i-1)}{n}, \\frac{2i}{n} \\right]$. Consider Riemman sum $S_n$ for this partition with any choice of sample points $k_{i} \\in [x_{i-1},x_i]$: $$ S_n ;=;\\sum_{k=1}^{n} f(k_{i}),\\Delta x ;=;\\sum_{k=1}^{n} f(k_{i}) \\cdot \\frac{2}{n}. $$ For $[x_{i-1},,x_i]\\subset [0,1]$, then $f(x)=0$ by definition. Hence $f(k_{i}) = 0$; for $[x_{i-1},,x_i]\\subset (1,2]$, then $f(x)=1$. Hence $f(k_{i}) = 1$. Then, notice one subinterval, say $[x_{j-1},,x_j]$, must include $x=1$, and $f$ = 0 or 1 depending on $k_{j}\\le 1$ or $k_{j}\u0026gt;1$. Hence, we have: $$ \\inf {f(x): x\\in [x_{j-1},x_j]} ;=; 0, \\quad \\sup {f(x): x\\in [x_{j-1},x_j]} ;=; 1. $$ Next, we find a lower bound and an upper bound for $S_n$. For lower bound, suppose subinterval $[x_{j-1},x_j]$ contains 1, and we pick $j$ so that $f(k_j)=0$. Let $m$ be the number of intervals inside fully in $(1,2]$. Then for those $m$ intervals, we have $$ \\begin{align} S_n ;\\ge; L(P) = \u0026amp; (1)m\\cdot\\Delta x +(0)(n-m)\\cdot\\Delta x ; \\ = \u0026amp; ; m \\cdot \\frac{2}{n} \\end{align} $$ Since $[x_{j},x_{j+1}]$ begins once $x \u0026gt; 1$, notice that $m\\approx \\frac{n}{2}$ for large $n$. More precisely, we have $m \\ge \\frac{n}{2}-1$. Hence: $$ m ;\\ge; \\frac{n}{2} -1 \\quad\\Longrightarrow\\quad S_n ;\\ge; \\Bigl(\\frac{n}{2}-1\\Bigr),\\frac{2}{n} ;=; 1 - \\frac{2}{n}. $$ Similarly, for upper bound, we pick $k_j$ such that $f(k_{j})=1$. The the number of intervals $m$ entirely in $(1,2]$ each contribute 1, so we in total have $m+1$ subintervals to contribute $\\Delta x$. Thus $$ \\begin{align} S_n ; \u0026amp; \\le; U(P)= (1)(m+1)\\cdot \\Delta x \\ ; \u0026amp; =; (m+1),\\frac{2}{n}. \\end{align} $$ But $m \\le \\frac{n}{2}$ for this case. Hence, $$ m+1 ;\\le; \\frac{n}{2} + 1 \\quad\\Longrightarrow\\quad S_n ;\\le; \\Bigl(\\frac{n}{2}+1\\Bigr),\\frac{2}{n} ;=; 1 + \\frac{2}{n}. $$ Therefore, for every Riemann sum $S_n$: $$ 1 - \\frac{2}{n} ;;\\le;; S_n ;;\\le;; 1 + \\frac{2}{n}. $$ As $n$ grows, the Squeeze Theorem forces each Riemann sum $S_n$ to converge to 1. More precisely, for any $\\varepsilon\u0026gt;0$, choose $N$ large enough s.t. $\\forall n \\ge N$, $$ -\\frac{2}{n} \u0026gt; -\\varepsilon \\quad\\text{and}\\quad \\frac{2}{n} \u0026lt; \\varepsilon, $$ which gives $\\bigl|S_n - 1\\bigr| \u0026lt; \\varepsilon$. This shows that $\\lim_{n\\to\\infty} S_n = 1$, so by definition of the Riemann integral, we have $$ \\int_{0}^{2} f(x),dx = 1. $$\nProblem 8.1.6 Let $f:[a, b] \\rightarrow \\mathbb{R}$ be continuous. Use Riemann\u0026rsquo;s condition and uniform continuity of $f$ to prove that $f$ is integrable.\n[!definition|*] To show that $f$ is Riemann integrable from continuity, we must show that $\\forall ,\\varepsilon\u0026gt;0$, $\\exists$ partition $P$ of $[a,b]$ such that $$0\\leq U(P_{\\varepsilon})-L(P_{\\varepsilon})\u0026lt;\\varepsilon$$ First, since $f$ is continuous on a closed, bounded interval $[a,b]$, then we know it is uniformly continuous by Heine‚ÄìCantor theorem. We define $$\\varepsilon_{0} =\\frac{\\varepsilon}{b-a}$$ Then, by definition of uniform continuity, $\\forall \\varepsilon \u0026gt;0$, there exists $\\delta\u0026gt;0$ s.t. $\\forall x,y \\in [a,b]$, we have $$|x-y|\u0026lt;\\delta \\Longrightarrow |f(x)-f(y)|\u0026lt;\\varepsilon_{0} =\\frac{\\varepsilon}{b-a}$$ We define a partition $P$ s.t. for each $[x_{i-1},x_{i}]$, the subinterval is less than $\\delta$. So $P:= \\max(x_{i}-x_{i-1})\u0026lt;\\delta$ (this is always possible because $f$ is continuous). Next, we set $$M_i=\\sup {x \\in\\left[x{i-1}, x_i\\right]} f(x) \\quad \\text{and} \\quad m_i=\\inf {x \\in\\left[x{i-1}, x_i\\right]} f(x)$$ for each subinterval. Notice that by the unform continuity of $f$, we must have $M_{i}-m_{i}\u0026lt;\\varepsilon_{0}$ since the lengths of each interval is awalys strictly less than $\\delta$. Therefore, the difference between upper and lower bound is: $$ \\begin{align} U(f, P)-L(f, P) \u0026amp; =\\sum_{i=1}^n(M_i\\Delta x_i)-\\sum_{i=1}^n(m_i\\Delta x_i) \\ \u0026amp; =\\sum_{i=1}^n\\left(M_i-m_i\\right) \\Delta x_i \\ \u0026amp; \u0026lt; (\\frac{\\varepsilon}{b-a})\\sum_{i=1}^n\\Delta x_i \\ \u0026amp; =(\\frac{\\varepsilon}{b-a})(b-a) \\ \u0026amp; =\\varepsilon \\end{align} $$ Therefore, since $\\varepsilon$ is arbitrarily chosen, by Riemann‚Äôs criterion for integrability, this implies that $f$ is Riemann integrable on $[a,b]$.\nProblem 8.2.2 Show that the $x y$ plane in $\\mathbb{R}^3$ has 3-dimensional measure 0.\n[!definition|*] We let $$P={x,y,z\\in \\mathbb{R}^{3},|,z =0}$$ to be the $xy$ plane in $\\mathbb{R}^3$, with $z=0$. We construct a countable union of rectangular boxes to cover it by defining the box: $$S_{n}=[-n,n]\\times[-n,n]\\times [-\\delta_{n},\\delta_{n}]$$ with $\\delta_{n}\u0026gt;0$ to be determined. By such construction, every point $(x,y,0)\\in P$ lies in some $S_{n}$ since $x\\in [-n,n]$ and $y\\in [-n,n]$. If $n\\geq \\max(|x|,|y|)$, we get $(x,y,0)\\in S_{n}$, such that $$P\\subseteq\\bigcup^{\\infty}{n=1}S{n}$$ Then, the volume of $S_{n}$ is given by $$V(S_{n})=(2n)(2n)(2\\delta_{n})=8n^2\\delta_{n}$$ we want to choose $\\delta$ s.t. the sum of the volumes is less than $\\epsilon$. Notice that $\\sum_{n=1}^{\\infty} \\frac{\\varepsilon}{2^n}=\\varepsilon \\sum_{n=1}^{\\infty} \\frac{1}{2^n}=\\varepsilon$, so a motivated choice is: $$\\begin{align} 8 n^2 \\delta_n \u0026amp; \\leq \\frac{\\varepsilon}{2^n} \\ \\delta_n\u0026amp; \\leq\\frac{\\varepsilon}{2^{n+3} n^2} \\end{align} $$ Therefore, we define $\\delta=\\cfrac{\\varepsilon}{2^{n+3} n^2}$, then $$V\\left(S_n\\right)=8 n^2 \\cdot \\frac{\\varepsilon}{2^{n+3} n^2}=\\frac{8 \\varepsilon}{2^{n+3}}=\\frac{\\varepsilon}{2^n}$$ and the total volume of the covering is $$\\sum_{n=1}^{\\infty} V\\left(S_n\\right)=\\sum_{n=1}^{\\infty} \\frac{\\varepsilon}{2^n}=\\varepsilon \\sum_{n=1}^{\\infty} \\frac{1}{2^n}=\\varepsilon \\cdot 1=\\varepsilon$$ Hence, we have constructed a countable cover ${S_{n}}^\\infty_{n=1}$ of $P$ whose total volume is precisely $\\varepsilon$. Because $\\varepsilon\u0026gt;0$ was arbitrary. So by Definition 8.2.2 of the textbook, the xy-plane has 3-dimensional measure zero in $‚Ñù¬≥$.\nProblem 8.2.6 Must the boundary of a set of measure zero have measure zero?\n[!definition|*] This statement is false. Here is a counterexample: consider the set $Q \\cap [0,1]$. From Example 8.2.5 in the textbook, we know that the set of rational numbers in $[0,1]$ has measure zero. However, the boundary of a set $A$ consists of all points $x$ s.t. every neighborhood of $x$ contains at least one point in $A$ and at least one point not in $A$.\nFor any point $x \\in [0,1]$, every neighborhood of $x$ contains both rational and irrational numbers. This is due to the density of both rational and irrational numbers in $\\mathbb{R}$. Therefore: $$\\partial(Q \\cap [0,1]) = [0,1]$$ Lebesgue measure of $[0,1]$ is 1, which is positive.\nProblem 8.3.2 Let $f(x, y)=1$ if $x \\neq 0$ and $f(0, y)=0$. Prove that $f$ is integrable on $A=[0,1] \\times[0,1] \\subset \\mathbb{R}^2$.\n[!definition|*] We want to show that $f$ is Riemann integrable on $A$ and $$\\iint_A f(x,y),dx,dy = 1.$$ Let $\\varepsilon\u0026gt;0$ be given. Choose a number $\\delta$ such that $0\u0026lt;\\delta\u0026lt;\\varepsilon$. We partition the square $A$ by subdividing the $y$-axis arbitrarily to form sub-rectangles $Q$ inside $[0,\\delta]\\times [0,1]$. These rectangles contain points with $x=0$ and $x\u0026gt;0$, so $f=0$ and $f=1$. Therefore, on each such $Q$, $$\\inf f = 0 \\quad \\text{and} \\quad \\sup f = 1$$ We let $A_1 = [0,\\delta]\\times [0,1]=\\delta$ to be the vertical strip, and $A_2 = [\\delta,1]\\times [0,1]=1-\\delta$ to be the rest of the square. For lower Riemann sum, we have $$ \\begin{align} L(f,P) \u0026amp; =(0)\\cdot A_{1}+(1)\\cdot A_{2} \\ \u0026amp; =A_{2} \\ \u0026amp; =(1-\\delta) \\end{align} $$ since $A_{1}:\\inf f=0$ and $A_{2}:\\inf f=1$. Similarly, for upper Riemann sum, we have $$ \\begin{align} U(f,P) \u0026amp; = (1)\\cdot A_1 + (1)\\cdot A_2 \\ \u0026amp; =A_1 + A_2 \\ \u0026amp; =\\delta+1-\\delta \\ \u0026amp; =1 \\end{align} $$ since $A_{1}:\\inf f=1$ and $A_{2}:\\inf f=1$. Therefore, the difference between the upper and lower sums is $$U(f,P)-L(f,P) = 1 - (1-\\delta) = \\delta.$$ By choosing $\\delta \u0026lt; \\varepsilon$, we ensure that $$U(f,P)-L(f,P) \u0026lt; \\varepsilon.$$ Since $\\forall\\varepsilon\u0026gt;0$ there exists a partition $P$ s.t. $$U(f,P)-L(f,P) \u0026lt; \\varepsilon,$$ the function $f$ is Riemann integrable on $A$. Since the upper sums are always 1 and the lower sums can be made arbitrarily close to 1 by choosing arbitrarily small, it follows that $$\\iint_A f(x,y),dx,dy = 1.$$\nChapter Exercise 8.12 Prove that $A$ has measure zero iff for every $\\varepsilon\u0026gt;0$ there is a covering of $A$ by sets $V_1, V_2, \\ldots$ with volume such that $\\sum_{i=1}^{\\infty} v\\left(V_i\\right)\u0026lt;\\varepsilon$.\n[!definition|*] ( $\\implies$ ) Suppose $m(A)=0$, by definition 8.2.2, we know $\\forall \\varepsilon\u0026gt;0, \\exists$ countable cover of $A$ by rectangles $\\left{S_i\\right} \\text { s.t. }\\sum_{i=1}^{\\infty} v\\left(S_i\\right)\u0026lt;\\varepsilon$. We choose volume $V_{i}=S_{i}$, such that: $$A\\subset \\sum_{i=1}^{\\infty} S_{i}=\\sum_{i=1}^{\\infty} V_{i}$$ and $$\\sum_{i=1}^{\\infty} v(S_{i})=\\sum_{i=1}^{\\infty} v(V_{i}) \u0026lt;\\varepsilon $$ Therefore, $m(A)=0 \\implies$ $\\forall \\varepsilon\u0026gt;0, \\exists$ ${V_{i}}^{\\infty}{1}$ as a covering of $A$ with total volume $\\sum{i=1}^{\\infty} v(V_{i})\u0026lt;\\epsilon$.\n( $\\Longleftarrow$ ) Suppose $\\forall \\varepsilon\u0026gt;0, \\exists$ ${V_{i}}^\\infty_{1}$ a cover of $A$ with total volume $\\sum_{i=1}^{\\infty} v(V_{i})\u0026lt;\\epsilon$. Note that every set in $\\mathbb{R}^n$ can be covered by a union of open rectangles that is countable. Specifically, for each $i$, we can cover $V_i$ by a countable number of rectangles ${S_{i,1}, S_{i,2}, \\dots}$ such that $$V_i ;\\subset; \\bigcup_{k=1}^{\\infty} S_{i,k}$$ We make sure the total volume of these rectangles is within arbitrarily small $\\delta_i$ of $v(V_i)$, so $$\\sum_{k=1}^{\\infty} v(S_{i,k}) ;\u0026lt;; v(V_i) ;+; \\delta_i.$$ Then, we choose each $\\delta_i$ s.t. the sum of volumes is less than $\\varepsilon$. Let $$\\delta_i ;=;\\frac{\\varepsilon}{2},2^{-i} ;=;\\frac{\\varepsilon}{2^{,i+1}}.$$Then $\\forall i$, we have: $$\\sum_{k=1}^{\\infty} v\\bigl(S_{i,k}\\bigr);\u0026lt;; v(V_i) ;+; \\frac{\\varepsilon}{2^{,i+1}}.$$ Hence, summing over all $i$: $$\\sum_{i=1}^{\\infty} \\sum_{k=1}^{\\infty} v(S_{i,k});\\le; \\sum_{i=1}^{\\infty} \\Bigl( v(V_i);+;\\tfrac{\\varepsilon}{2^{,i+1}} \\Bigr);=;\\sum_{i=1}^{\\infty} v(V_i);+;\\frac{\\varepsilon}{2};\u0026lt;; \\varepsilon ;+; \\frac{\\varepsilon}{2} ;=; \\tfrac{3\\varepsilon}{2} $$ Because $\\varepsilon$ was arbitrary, we can make $\\delta_i$ smaller such that the total can be strictly less than $\\varepsilon$. Therefore, $$A ;\\subset;\\bigcup_{i=1}^\\infty \\bigcup_{k=1}^{\\infty} S_{i,k},\\quad\\text{and}\\quad\\sum_{i,k} v\\bigl(S_{i,k}\\bigr) ;\u0026lt;;\\varepsilon.$$ This demonstrates that $A$ is covered by rectangles ${S_{i,k}}$ whose total volume is \u0026lt; $\\varepsilon$, which is by definition, $m(A)=0$.\n"},{"id":17,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/Homework/Homework-9/","title":"Homework 9","section":"Real Analysis II","content":"8.5: 1, [2, 3], 4, [5]; Chapter 8: [21, 22].\nProblem 8.5.2 Establish formula $\\mathbf{c}$ of Example 8.5.7 as follows. Prove that $e^{-x} x^{p+2} \\rightarrow 0$ as $x \\rightarrow \\infty$, and then compare the integral with $\\int_1^{\\infty}\\left(1 / x^2\\right) d x$.\nProblem 8.5.3 Let $f:[a, \\infty[\\rightarrow \\mathbb{R}$ be Riemann integrable on bounded intervals. Show that $\\int_a^{\\infty} f$ (conditional convergence) exists iff for every $\\varepsilon\u0026gt;0$, there is a $T$ such that $t_1, t_2 \\geq T$ implies\n$$ \\left|\\int_{t_1}^{t_2} f(x) d x\\right|\u0026lt;\\varepsilon $$\nProblem 8.5.5 For what $\\alpha$ is $\\int_0^{\\infty} \\frac{x^\\alpha}{1+x^\\alpha} d x$ convergent?\nChatper 8.21 Show that $\\int_1^{\\infty} x^{-p} \\sin x d x$ converges if $p\u0026gt;1$. Show that if $0\u0026lt;p \\leq 1$, then the convergence is conditional.\nChapter 8.22 The gamma function is defined to be the function given by the improper integral $\\Gamma(p)=\\int_1^{\\infty} e^{-x} x^{p-1} d x$. Show that the integral is convergent for $p\u0026gt;0$.\n"},{"id":18,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1/","title":"9.1","section":"Á¨¨‰πùÁ´†","content":"\rChapter 9: Computing Integrals\r#\r9.1.1 Introduction\r#\rIn practice, how do we compute integral $\\int_A f(x)dx$?\nIn $\\mathbb{R}^1$ (one-dimensional space): $$\\int_a^b f(x)dx = F(x)|_a^b = F(b) - F(a)$$\nFTC (Fundamental Theorem of Calculus) In $\\mathbb{R}^n$ (n-dimensional space):\nReduce to $\\mathbb{R}^1$ case by Fubini\u0026rsquo;s Theorem Change of Variables (Substitution) first 9.1.2 Fubini\u0026rsquo;s Theorem\r#\r1. Statement of Main Result\r#\rTheorem 1: Let $A = {(x,y): a \\leq x \\leq b, c \\leq y \\leq d}$ be a rectangle in $\\mathbb{R}^2$ and $f: A \\to \\mathbb{R}$ be integrable. Suppose, for each $x \\in [a,b]$, the following integral exists: $$g(x) = \\int_c^d f(x,y)dy$$\nThen $g(x)$ is integrable on $[a,b]$ and $\\int_A f = \\int_a^b \\left(\\int_c^d f(x,y)dy\\right)dx = \\int_a^b g(x)dx$\n9.1.3 Corollaries\r#\rCorollary 1: If $f: A \\to \\mathbb{R}$ is continuous, then $$\\int_A f = \\int_a^b \\left(\\int_c^d f(x,y)dy\\right)dx = \\int_c^d \\left(\\int_a^b f(x,y)dx\\right)dy$$\nThis shows the symmetry of the double integral - we can integrate first with respect to $y$ and then with respect to $x$, or vice versa.\nCorollary 2: Let $A$ be a region given by $A = {(x,y): a \\leq x \\leq b, \\varphi(x) \\leq y \\leq \\psi(x)}$ where $\\varphi(x)$ and $\\psi(x)$ are continuous functions. If $f: A \\to \\mathbb{R}$ is continuous, then $\\int_A f = \\int_a^b \\left(\\int_{\\varphi(x)}^{\\psi(x)} f(x,y)dy\\right)dx$\n[Note: The image shows a graphical representation of region $A$ bounded by $y = \\varphi(x)$ below and $y = \\psi(x)$ above, with $x$ ranging from $a$ to $b$.]\nRemarks\r#\rRoles of $x$ and $y$ can be interchanged Results true in higher dimensions\nLet $C = A \\times B \\subset \\mathbb{R}^{n+m}$ where $A \\subset \\mathbb{R}^n$, $B \\subset \\mathbb{R}^m$ [The image shows a diagram of the Cartesian product $A \\times B$ as a rectangle in a coordinate system with axes labeled $\\mathbb{R}^n$ and $\\mathbb{R}^m$] Then: $\\int_{A \\times B} f = \\int_A \\left(\\int_B f(x,y)dy\\right)dx$\n9.1.4 Example: Computing a Double Integral\r#\rProblem\r#\rCompute $\\int_A (x+y) , dxdy$\nWhere $A$ is a triangle in the first quadrant bounded by the lines:\n$x = 0$ $y = 0$ $x + y = 1$ Solution\r#\rUsing Fubini\u0026rsquo;s Theorem, we can compute this double integral as an iterated integral:\n$$\\int_A (x+y) , dxdy = \\int_0^1 \\left(\\int_0^{1-x} (x+y) , dy\\right) , dx$$\nFirst, we evaluate the inner integral with respect to $y$:\n$$\\int_0^{1-x} (x+y) , dy = \\left[xy + \\frac{y^2}{2}\\right]_{y=0}^{y=1-x}$$\n$$= x(1-x) + \\frac{(1-x)^2}{2} - \\left(0 + 0\\right)$$\n$$= x - x^2 + \\frac{1 - 2x + x^2}{2}$$\n$$= x - x^2 + \\frac{1}{2} - x + \\frac{x^2}{2}$$\n$$= \\frac{1}{2} - \\frac{x^2}{2}$$\nNow we evaluate the outer integral with respect to $x$:\n$$\\int_0^1 \\left(\\frac{1}{2} - \\frac{x^2}{2}\\right) , dx = \\frac{1}{2}\\int_0^1 (1 - x^2) , dx$$\n$$= \\frac{1}{2}\\left[x - \\frac{x^3}{3}\\right]_0^1$$\n$$= \\frac{1}{2}\\left(1 - \\frac{1}{3} - 0\\right)$$\n$$= \\frac{1}{2} \\cdot \\frac{2}{3} = \\frac{1}{3}$$\nTherefore, $\\int_A (x+y) , dxdy = \\frac{1}{3}$\nNote: The calculation in the original blackboard image showed a final result of $\\frac{1}{2}$, but the correct answer is $\\frac{1}{3}$ as demonstrated in the steps above.\n9.1.5 Proof of Theorem 1\r#\rI\u0026rsquo;ll write out a concise proof for just the part shown in the image:\n3. Proof of Theorem 1\r#\rLet $g(x) = \\int_c^d f(x,y)dy$\nWe need to show:\n$g$ is integrable on $[a,b]$ $\\int_a^b g(x)dx = \\int_A f$ We will compare upper and lower sums of $f$ and $g$.\nFix any partition $P_A$ of $A$. We can write $P_A = {S_{ij}}$ where $S_{ij} = V_i \\times W_j$ represents rectangular cells in the partition.\nThen $P_A$ induces:\nA partition of $[a,b]$: $P_{[a,b]} = {V_i}$ A partition of $[c,d]$: $P_{[c,d]} = {W_j}$ For each cell $S_{ij}$, we define:\n$M_{ij} = \\sup{f(x,y): (x,y) \\in S_{ij}}$ $m_{ij} = \\inf{f(x,y): (x,y) \\in S_{ij}}$ The upper and lower sums for $f$ over partition $P_A$ are:\n$U(f, P_A) = \\sum_{i,j} M_{ij}|V_i||W_j|$ $L(f, P_A) = \\sum_{i,j} m_{ij}|V_i||W_j|$ When we consider the integrable function $g(x)$, we can establish that: $L(f, P_A) \\leq \\int_a^b g(x)dx \\leq U(f, P_A)$\nAs we refine the partition, the upper and lower sums converge, proving that $g$ is integrable on $[a,b]$ and that $\\int_a^b g(x)dx = \\int_A f$.\nI\u0026rsquo;ll continue the proof based on the additional image:\nI\u0026rsquo;ll rewrite the proof using proper display math formatting with $$ delimiters:\nNext, examine the lower sum $L(f, P_A)$:\n$$L(f, P_A) = \\sum_{i,j} m_{ij}(f) \\cdot V(S_{ij})$$\n$$= \\sum_{i,j} m_{ij}(f) \\cdot V(V_i) \\cdot V(W_j)$$\nWhere $m_{ij}(f) = \\inf{f(x,y): (x,y) \\in S_{ij}}$\nKey Observation: $$\\inf{f(x,y): (x,y) \\in V_i \\times W_j} \\leq \\inf{f(x,y): y \\in W_j} \\text{ for all } x \\in V_i$$ $$= m_j(f, x)$$\nThen for any $x \\in [a,b]$, we have: $$\\sum_j m_j(f) \\cdot V(W_j) \\leq \\sum_j m_j(f,x) \\cdot V(W_j)$$\nThis is the lower sum of $f(x,y)$ in the variable $y$ with partition $P_{[c,d]}$: $$= L(f(x,¬∑), P_{[c,d]})$$ $$\\leq \\int_c^d f(x,y)dy = g(x) \\text{ for all } x$$\nThus: $$\\sum_i \\left(\\sum_j m_j(f) \\cdot V(W_j)\\right) \\cdot V(V_i) \\leq \\sum_i \\inf(g(x)) \\cdot V(V_i)$$\ni.e., $$\\sum_{i,j} m_{ij}(f) \\cdot V(W_j) \\cdot V(V_i) \\leq \\sum_i (\\inf g(x)) \\cdot V(V_i)$$\nTherefore: $$L(f, P_A) \\leq L(g, P_{[a,b]})$$\nSimilarly, we have:\n$$U(f, P_A) \\geq U(g, P_{[a,b]})$$\nThus we have:\n$$L(f, P_A) \\leq L(g, P_{[a,b]}) \\leq U(g, P_{[a,b]}) \\leq U(f, P_A)$$\nBy Riemann\u0026rsquo;s criterion, if $f$ is integrable on $A$, then:\n$g$ is integrable on $[a,b]$, and $$\\int_A f = \\int_a^b g(x)dx$$ This completes the proof of Fubini\u0026rsquo;s Theorem, showing that we can compute a double integral by first integrating with respect to one variable and then with respect to the other.\n"},{"id":19,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.1-%E5%BA%A6%E9%87%8F%E7%90%86%E8%AE%BA/","title":"8.1 Â∫¶ÈáèÁêÜËÆ∫","section":"Á¨¨ÂÖ´Á´† Â∫¶ÈáèÁêÜËÆ∫","content":"Á¨¨ÂÖ´Á´†ÁöÑÁ†îÁ©∂ÂØπË±°ÊòØLebesgueÁßØÂàÜ„ÄÇÊàë‰ª¨Â∞ÜË¶ÅÊé®ÂØºÁöÑÊòØÂ∫¶ÈáèÁêÜËÆ∫Ôºàmeasure theoryÔºâÁöÑÊ†∏ÂøÉÂÜÖÂÆπÔºöÈõÜÂêàÁöÑÊµãÂ∫¶ÔºàmeasureÔºâÂíåË¶ÜÁõñÊÄßË¥®„ÄÇ\nÊàë‰ª¨‰∏ªË¶ÅÁúã‰ª•‰∏ãËøôÂá†‰∏™ÊñπÈù¢Ôºö\nÁßØÂàÜÁöÑÂÆö‰πâ ÂèØÁßØÊÄßÁöÑÂà§ÊçÆÔºàÂøÖË¶ÅÊù°‰ª∂ÂíåÂÖÖÂàÜÊù°‰ª∂Ôºâ ÊÄßË¥®‰∏éÊî∂ÊïõÊÄß ËÆ°ÁÆóÂíå‰º∞ËÆ°ÁßØÂàÜ 1.1 ÁßØÂàÜÁöÑÂÆö‰πâ\r#\r1.1.1 Âá†‰ΩïÂä®Êú∫\r#\rÁßØÂàÜÊú¨Ë¥®‰∏äÊòØËÆ°ÁÆóÂáΩÊï∞‰∏ãÊñπÂå∫ÂüüÁöÑ„Äå‰ΩìÁßØÔºàvolumeÔºâ„Äç„ÄÇÂ¶Ç‰ΩïÂÆö‰πâËøôÁßç„Äå‰ΩìÁßØ„Äç‰ºöÁõ¥Êé•ÊîπÂèòÂà∞ÁßØÂàÜÁöÑÊÄßË¥®„ÄÇÊàë‰ª¨ÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫ÜËÆ°ÁÆó‰ªªÊÑèÊõ≤Á∫ø $y = f(x)$ ‰ªé $a$ Âà∞ $b$ ‰∏ãÊñπÂå∫ÂüüÁöÑÈù¢ÁßØ\nLebesgueÁßØÂàÜÂÆö‰πâÁöÑÈÄªËæëÊ≠•È™§Â¶Ç‰∏ãÔºö\nÂÆö‰πâÔºö‰ªªÊÑèÊúâÁïåÂáΩÊï∞$f$‰∫éÂú®È´òÁª¥Á©∫Èó¥‰∏≠ÁöÑÊúâÁïåÂå∫Âüü $A$ ÔºàboundedÔºâ‰∏ä„ÄÇ\nÁÆÄÂåñÈóÆÈ¢òÔºöÁî±‰∫éÊõ≤Á∫ø‰∏ãÊñπÂå∫ÂüüÈÄöÂ∏∏‰∏çÊòØËßÑÂàôÂΩ¢Áä∂ÔºåÊàë‰ª¨ÈúÄË¶ÅÁî®Ëøë‰ººÊñπÊ≥ïÊù•ËÆ°ÁÆó„ÄÇÊàë‰ª¨ÂèØ‰ª•Â∞ÜÂå∫Èó¥ $[a,b]$ ÂàÜÊàêËã•Âπ≤Â∞èÂå∫Èó¥ÔºåÁÑ∂ÂêéÁî®Áü©ÂΩ¢Êù•Ëøë‰ºº„ÄÇÂ∞ÜÂå∫Âüü $A$ ÂµåÂÖ•Âà∞‰∏Ä‰∏™Áü©ÂΩ¢Âå∫Âüü $B$ ‰∏≠ÔºåÂπ∂Â∞ÜÂáΩÊï∞ $f$ Êâ©Â±ï‰∏∫ÂáΩÊï∞ $fÃÉ$Ôºå‰ΩøÂÖ∂Âú® $A$ ‰πãÂ§ñÁöÑ $B$ ‰∏äÂèñÂÄº‰∏∫Èõ∂\nÂàÜÂâ≤ÔºöÂàíÂàÜÁü©ÂΩ¢ $B$ ‰∏∫Êõ¥Â∞èÁöÑÁü©ÂΩ¢ÔºåÊù•ÂàõÂª∫‰∏Ä‰∏™ÂàÜÂâ≤ÁªìÊûÑÔºàpartitionÔºâ„ÄÇ\nÊûÑÈÄ†‰∏ä‰∏ãËøë‰ººÔºöÂØπÊØè‰∏™Â∞èÁü©ÂΩ¢ÔºåÈÄöËøáËøôÁßçÊñπÂºèÔºåÊàë‰ª¨ÂèØ‰ª•Ëé∑Âæó‰∏§ÁßçËøë‰ººÔºö‰∏ÄÁßçÊòØÂÅèÂ§ßÁöÑÔºà‰∏äÂíåÔºâÔºå‰∏ÄÁßçÊòØÂÅèÂ∞èÁöÑÔºà‰∏ãÂíåÔºâ„ÄÇ\n‰∏äÂíå: $U(f, P) = \\sum_{i=1}^{n} (\\sup f(x)) \\cdot \\ell(I_i)$ ÂØπÊØè‰∏™Â∞èÂå∫Èó¥ $I_i$ÔºåÊàë‰ª¨ÊâæÂá∫ÂáΩÊï∞Âú®ËØ•Âå∫Èó¥‰∏äÁöÑÊúÄÂ§ßÂÄº $\\sup f(x)$ÔºåÁÑ∂Âêé‰πò‰ª•Âå∫Èó¥ÈïøÂ∫¶ $\\ell(I_i)$„ÄÇËøôÊ†∑ÂΩ¢ÊàêÁöÑÁü©ÂΩ¢Èù¢ÁßØ‰πãÂíåÊÄªÊòØÂ§ß‰∫éÊàñÁ≠â‰∫éÁúüÂÆûÈù¢ÁßØ„ÄÇ ‰∏ãÂíå: $L(f, P) = \\sum_{i=1}^{n} (\\inf f(x)) \\cdot \\ell(I_i)$ ÂØπÊØè‰∏™Â∞èÂå∫Èó¥ $I_i$ÔºåÊàë‰ª¨ÊâæÂá∫ÂáΩÊï∞Âú®ËØ•Âå∫Èó¥‰∏äÁöÑÊúÄÂ∞èÂÄº $\\inf f(x)$ÔºåÁÑ∂Âêé‰πò‰ª•Âå∫Èó¥ÈïøÂ∫¶ $\\ell(I_i)$„ÄÇËøôÊ†∑ÂΩ¢ÊàêÁöÑÁü©ÂΩ¢Èù¢ÁßØ‰πãÂíåÊÄªÊòØÂ∞è‰∫éÊàñÁ≠â‰∫éÁúüÂÆûÈù¢ÁßØ„ÄÇ ÂΩìÊàë‰ª¨ËÆ©ÂàÜÂâ≤ÂèòÂæóË∂äÊù•Ë∂äÁªÜÊó∂Ôºå‰∏äÂíå‰ºöÂáèÂ∞èÔºå‰∏ãÂíå‰ºöÂ¢ûÂ§ß„ÄÇÂÆÉ‰ª¨ÁöÑÊûÅÈôêÂÄºÂ∞±ÂÆö‰πâ‰∫Ü‰∏äÁßØÂàÜÂíå‰∏ãÁßØÂàÜ„ÄÇÂΩìËøô‰∏§‰∏™ÊûÅÈôêÂÄºÁõ∏Á≠âÊó∂ÔºåÊàë‰ª¨Â∞±ËØ¥Ëøô‰∏™ÂáΩÊï∞ÊòØÂèØÁßØÁöÑÔºàintegrableÔºâ„ÄÇ\n1.1.2 ‰∏ÄËà¨Ë°®Ëø∞\r#\rËÆæÂÆö\r#\rLet $f: A \\to \\mathbb{R}$ be a bounded function on a bounded set $A$ in $\\mathbb{R}^n$. We want to define the \u0026ldquo;volume\u0026rdquo; of the region under the surface $y = f(x)$ (or the integral $\\int_A f(x) dx$).\nÊ≠•È™§1ÔºöÈÄâÊã©‰∏Ä‰∏™Áü©ÂΩ¢$B$\r#\r‰∏∫‰∫ÜÁÆÄÂåñËÆ°ÁÆóÔºåÊàë‰ª¨È¶ñÂÖàÈÄâÊã©‰∏Ä‰∏™ÂåÖÂê´ $A$ ÁöÑÁü©ÂΩ¢Âå∫Âüü$B$ÔºåÂπ∂Â∞ÜÂáΩÊï∞ $f$ Êâ©Â±ïÂà∞Êï¥‰∏™Áü©ÂΩ¢‰∏ä„ÄÇÈÄâÊã©ÂåÖÂê´ $A$ ÁöÑÁü©ÂΩ¢ $B = [a_1, b_1] \\times [a_2, b_2] \\times \u0026hellip; \\times [a_n, b_n]$ Âπ∂‰∏îÊâ©Â±ïÂáΩÊï∞ $f$ ‰ΩøÂæóÂΩì $x \\notin A$ Êó∂Ôºå$f(x) = 0$\nÊ≠•È™§2ÔºöÂØπBËøõË°åÂàÜÂâ≤ÔºàpartitionÔºâ\r#\rÊàë‰ª¨Â∞ÜÁü©ÂΩ¢ $B$ ÁöÑÂêÑËæπÂàÜÂâ≤ÊàêËã•Âπ≤‰∏™Â≠êÂå∫Èó¥ÔºàsubintervalsÔºâÔºåÂæóÂà∞‰∏Ä‰∏™ÂàÜÂâ≤ $P$Ôºàpartition $P$ÔºâÁöÑÂ∞èÁü©ÂΩ¢ÁöÑÈõÜÂêà„ÄÇ\nÊ≠•È™§3ÔºöÊûÑÈÄ†‰∏ä‰∏ãÂíåÔºàupper and lower sumsÔºâ\r#\rÂØπ‰∫éÊØè‰∏™Â∞èÁü©ÂΩ¢ $R$ÔºåÊàë‰ª¨ÊâæÂá∫ÂáΩÊï∞Âú®ÂÖ∂‰∏äÁöÑÊúÄÂ§ßÂÄº $\\sup f(x)$ÔºåÂíåÊúÄÂ∞èÂÄº $\\inf f(x)$Ôºå‰πò‰ª•Áü©ÂΩ¢ÁöÑ‰ΩìÁßØ $V(R)$ÔºåÁÑ∂ÂêéÊ±ÇÂíå„ÄÇ\n‰∏äÂíåÔºàUSÔºâ: $$U(f, P) = \\sum_{R \\in P} (\\sup f(x)) \\cdot V(R)$$\n‰∏ãÂíå ÔºàLSÔºâ: $$L(f, P) = \\sum_{R \\in P} (\\inf f(x)) \\cdot V(R)$$\nÊ≠•È™§4ÔºöÊûÑÈÄ†‰∏ä‰∏ãÁßØÂàÜ\r#\r‰∏é‰∏ÄÁª¥ÊÉÖÂÜµÁõ∏ÂêåÔºåÊàë‰ª¨ÂÆö‰πâ‰∏äÁßØÂàÜÂíå‰∏ãÁßØÂàÜ‰Ωú‰∏∫USÂíåLSÁöÑÊûÅÈôê„ÄÇÊâÄÊúâÂèØËÉΩÂàÜÂâ≤ÂØπÂ∫îÁöÑ‰∏ãÂíåÁöÑ‰∏äÁ°ÆÁïå:\n‰∏äÁßØÂàÜ: $$\\overline{\\int_A} f = \\inf_P U(f, P)$$\n‰∏ãÁßØÂàÜ: $$\\underline{\\int_A} f = \\sup_P L(f, P)$$\nÈáçË¶ÅËßÇÂØü\r#\rÂæàÊòéÊòæÔºåÊàë‰ª¨Êúâ $L(f, P) \\leq$ ‚ÄúÁúüÂÆû‰ΩìÁßØ‚Äù $\\leq U(f, P)$„ÄÇ‰∏ãÁßØÂàÜÂíå‰∏äÁßØÂàÜÂàÜÂà´ÊòØÁúüÂÆû‰ΩìÁßØÁöÑ‰∏ãÁïåÂíå‰∏äÁïåÔºö\n$$\\underline{\\int_{A}}f \\leq\\text{‚Äúreal volume‚Äù }\\leq \\overline{\\int_A} f$$\n1.2 ÂáΩÊï∞ÁöÑÂèØÁßØÊÄßÂèäÂÖ∂ÁßØÂàÜ\r#\rÁé∞Âú®Êàë‰ª¨ÂèØ‰ª•Ê≠£ÂºèÂÆö‰πâÂáΩÊï∞ÁöÑÂèØÁßØÊÄßÂèäÂÖ∂ÁßØÂàÜ„ÄÇ\n[!theorem|*] Êàë‰ª¨Áß∞ÂáΩÊï∞ $f$ ÊòØ**ÈªéÊõºÂèØÁßØÔºàRiemann integrableÔºâ**ÁöÑÔºåÂΩì‰∏î‰ªÖÂΩìÔºö $$\\overline{\\int_A} f = \\underline{\\int_A} f$$ ÂáΩÊï∞ $f$ Âú® $A$ ‰∏äÁöÑÁßØÂàÜÂÆö‰πâ‰∏∫Ôºö$\\int_A f(x)dx = \\overline{\\int_A} f = \\underline{\\int_A} f$„ÄÇ\n1.2.1 ‰∏ÄËà¨ËÆæÂÆöÔºö\r#\rÂú®Êàë‰ª¨ËÆ®ËÆ∫ÁßØÂàÜÊó∂ÔºåÈÄöÂ∏∏ÈªòËÆ§‰ª•‰∏ãÊù°‰ª∂ÊàêÁ´ãÔºö\nÂáΩÊï∞ÊúâÁïåÔºö$f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$ boundedÔºõ ÂÆö‰πâÂüüÊúâÁïåÔºö $A \\subset B$ is boundedÔºõ Áü©ÂΩ¢Âå∫ÂüüÔºö $B$ is a rectangle in $\\mathbb{R}^n$Ôºõ Èõ∂Âª∂ÊãìÔºöÂáΩÊï∞ f Âú®ÈõÜÂêà A Â§ñÈÉ®ÂÆö‰πâ‰∏∫ 0ÔºåÂç≥Ôºö $$f(x) = 0, \\quad \\forall x \\notin A$$ 1.2.2 ÈªéÊõºÊù°‰ª∂ÔºàRiemann\u0026rsquo;s ConditionÔºâ\r#\rËøôÊÑèÂë≥ÁùÄÊàë‰ª¨ÂèØ‰ª•ÊâæÂà∞‰∏Ä‰∏™Ë∂≥Â§üÁªÜÁöÑÂàÜÂâ≤Ôºå‰ΩøÂæó‰∏äÂíå‰∏é‰∏ãÂíåÁöÑÂ∑ÆÂ∞è‰∫é‰ªªÊÑèÁªôÂÆöÁöÑÊ≠£Êï∞ $\\varepsilon$„ÄÇÊç¢Âè•ËØùËØ¥ÔºåÈöèÁùÄÂàÜÂâ≤ÂèòÂæóË∂äÊù•Ë∂äÁªÜÔºå‰∏äÂíåÂíå‰∏ãÂíå‰ºöÊó†ÈôêÊé•Ëøë„ÄÇ\n[!theorem|*] For $f$ to be (Riemann) integrable, $\\forall \\varepsilon \u0026gt; 0$, $\\exists$ partition $P_\\varepsilon$ (of $B$) s.t. $$0 \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$$\n1.2.3 ËææÂ∏ÉÊù°‰ª∂ÔºàDarboux\u0026rsquo;s ConditionÔºâ\r#\rËææÂ∏ÉÊù°‰ª∂ÊòØÈªéÊõºÂèØÁßØÊÄßÁöÑÂè¶‰∏Ä‰∏™Á≠â‰ª∑Ë°®Ëø∞„ÄÇ\n[!theorem|*] $\\forall \\varepsilon \u0026gt; 0$, $\\exists P_\\delta$ s.t. if:\n$P$ is any partition of $B$ into rectangles $B_1, B_2, \u0026hellip;, B_N$ with side length $\u0026lt; \\delta$ $x \\in B_1, x \\in B_2, \u0026hellip;, x \\in B_N$, then we have: $\\left|\\sum_{i=1}^N f(x_i)V(B_i) - I\\right| \u0026lt; \\varepsilon$ where the number $I$ is the value of the integral. $\\forall \\varepsilon \u0026gt; 0$Ôºå$\\exists P_0$ ‰ΩøÂæóÂ¶ÇÊûúÔºö\n$P$ ÊòØÂ∞Ü $B$ ÂàÜÂâ≤ÊàêÁü©ÂΩ¢ $B_1, B_2, \u0026hellip;, B_N$ ÁöÑ‰ªªÊÑèÂàÜÂâ≤Ôºå‰∏îËøô‰∫õÁü©ÂΩ¢ÁöÑËæπÈïø $\u0026lt; \\delta$ Â¶ÇÊûú $x \\in B_1, x \\in B_2, \u0026hellip;, x \\in B_N$ÔºåÈÇ£‰πàÊàë‰ª¨ÊúâÔºö $\\left|\\sum_{i=1}^N f(x_i)V(B_i) - I\\right| \u0026lt; \\varepsilon$ ÂÖ∂‰∏≠ $I$ ÊòØÁßØÂàÜÂÄº„ÄÇ Ëß£Èáä: ËææÂ∏ÉÊù°‰ª∂ËØ¥ÁöÑÊòØÔºåÂΩìÂàÜÂâ≤Ë∂≥Â§üÁªÜÊó∂ÔºàÊØè‰∏™Áü©ÂΩ¢ÁöÑËæπÈïøÂ∞è‰∫éÊüê‰∏™ $\\delta$ÔºâÔºåÈªéÊõºÂíåÔºàÂú®ÊØè‰∏™Â∞èÁü©ÂΩ¢‰∏äÂèñ‰∏ÄÁÇπËÆ°ÁÆóÂáΩÊï∞ÂÄºÔºå‰πò‰ª•‰ΩìÁßØÔºåÁÑ∂ÂêéÊ±ÇÂíåÔºâ‰ºöÈùûÂ∏∏Êé•ËøëÁßØÂàÜÂÄº $I$„ÄÇ ËææÂ∏ÉÊù°‰ª∂‰πüÂèØ‰ª•Ë°®Ëø∞‰∏∫Ôºö $\\forall \\varepsilon \u0026gt; 0$Ôºå$\\exists$ ÂàÜÂâ≤ $P_{\\varepsilon}$ ‰ΩøÂæó $0 \\leq U(f, P_{\\varepsilon}) - L(f, P_{\\varepsilon}) \u0026lt; \\varepsilon$\nËß£Èáä: Ëøô‰∏ÄË°®Ëø∞‰∏éÈªéÊõºÊù°‰ª∂ÂΩ¢Âºè‰∏äÁõ∏ÂêåÔºå‰ΩÜÂº∫Ë∞É‰∫ÜËøôÊòØËææÂ∏ÉÊù°‰ª∂ÁöÑ‰∏Ä‰∏™Á≠â‰ª∑ÂΩ¢Âºè„ÄÇ Â§áÊ≥®\r#\rÊï∞Â≠ó $I$ ÊòØÁßØÂàÜÁöÑÂÄº\nËß£Èáä: $I$ ‰ª£Ë°®ÂáΩÊï∞ $f$ Âú®Âå∫Âüü $A$ ‰∏äÁöÑÁßØÂàÜÂÄº„ÄÇ Áß∞‰∏∫ÂÖ≥‰∫é $P$ ÁöÑ $f$ ÁöÑÈªéÊõºÂíå\nËß£Èáä: ÈªéÊõºÂíåÊòØ‰∏ÄÁßçËøë‰ººÁßØÂàÜÁöÑÊñπÊ≥ïÔºåÊ†πÊçÆ‰∏Ä‰∏™ÂàÜÂâ≤ $P$ÔºåÂú®ÊØè‰∏™Â∞èÂå∫ÂüüÂÜÖÈÄâÂèñ‰∏ÄÁÇπÔºåËÆ°ÁÆóÂáΩÊï∞ÂÄºÔºå‰πò‰ª•Âå∫ÂüüÁöÑÂ§ßÂ∞èÔºåÁÑ∂ÂêéÊ±ÇÂíå„ÄÇ Ëß£ÈáäÔºöËææÂ∏ÉÊù°‰ª∂ËØ¥ÂΩìÂàÜÂâ≤Ë∂≥Â§üÁªÜÊó∂ÔºàËæπÈïø $\u0026lt; \\delta$ÔºâÔºåÈªéÊõºÂíåÊòØÁßØÂàÜÁöÑËâØÂ•ΩËøë‰ºº„ÄÇ\nËß£Èáä: ËøôË°®ÊòéÔºåÈöèÁùÄÂàÜÂâ≤ÂèòÂæóË∂äÊù•Ë∂äÁªÜÔºåÈªéÊõºÂíå‰ºöÊî∂ÊïõÂà∞ÁúüÂÆûÁöÑÁßØÂàÜÂÄº„ÄÇ ÂÆöÁêÜ\r#\rËß£Èáä: ‰∏ãÈù¢ÁöÑÂÆöÁêÜË°®ÊòéÔºåÊàë‰ª¨‰πãÂâçËÆ®ËÆ∫ÁöÑÊù°‰ª∂ÊòØÁ≠â‰ª∑ÁöÑ„ÄÇËøôÂæàÈáçË¶ÅÔºåÂõ†‰∏∫‰∏çÂêåÁöÑÊù°‰ª∂ÂèØËÉΩÂú®‰∏çÂêåÁöÑÊÉÖÂ¢É‰∏ãÊõ¥ÂÆπÊòìÈ™åËØÅÊàñÂ∫îÁî®„ÄÇ\n‰ª•‰∏ãÊù°‰ª∂ÊòØÁ≠â‰ª∑ÁöÑÔºö\n$f$ Âú® $A$ ‰∏äÂèØÁßØ\nËß£Èáä: ‰∏äÁßØÂàÜÁ≠â‰∫é‰∏ãÁßØÂàÜ„ÄÇ $f$ Êª°Ë∂≥ÈªéÊõºÊù°‰ª∂\nËß£Èáä: ÂèØ‰ª•ÊâæÂà∞Ë∂≥Â§üÁªÜÁöÑÂàÜÂâ≤‰Ωø‰∏äÂíå‰∏é‰∏ãÂíåÁöÑÂ∑ÆÂ∞è‰∫é‰ªªÊÑèÁªôÂÆöÁöÑÊ≠£Êï∞„ÄÇ $f$ Êª°Ë∂≥ËææÂ∏ÉÊù°‰ª∂\nËß£Èáä: ÂØπ‰∫éË∂≥Â§üÁªÜÁöÑÂàÜÂâ≤ÔºåÈªéÊõºÂíåÊé•ËøëÁßØÂàÜÂÄº„ÄÇ ÂÆöÁêÜËØÅÊòé\r#\rËß£Èáä: Áé∞Âú®Êàë‰ª¨Êù•ËØÅÊòéËøô‰∫õÊù°‰ª∂ÁöÑÁ≠â‰ª∑ÊÄß„ÄÇÊàë‰ª¨ÈúÄË¶ÅËØÅÊòéÔºö1‚áí2Ôºå2‚áí1Ôºå‰ª•ÂèäÂÖ∂‰ªñÁ≠â‰ª∑ÂÖ≥Á≥ª„ÄÇ\nÊ≠•È™§1Ôºö$f$ ÂèØÁßØ $\\Rightarrow$ ÈªéÊõºÊù°‰ª∂\r#\rËß£Èáä: È¶ñÂÖàÔºåÊàë‰ª¨ËØÅÊòéÂ¶ÇÊûúÂáΩÊï∞ÂèØÁßØÔºåÈÇ£‰πàÂÆÉÊª°Ë∂≥ÈªéÊõºÊù°‰ª∂„ÄÇ\nÂÅáËÆæÔºåÂ¶ÇÊûú $\\varepsilon \u0026gt; 0$Ôºö\nÂõ†‰∏∫ $\\overline{\\int_A} f = \\underline{\\int_A} f$Ôºå‰∏îÊ†πÊçÆ‰∏äÁ°ÆÁïåÂíå‰∏ãÁ°ÆÁïåÁöÑÂÆö‰πâÔºå $\\exists P_\\varepsilon$ ‰ΩøÂæó $0 \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$\nËß£Èáä: Áî±‰∫éÂèØÁßØÊÄßÊÑèÂë≥ÁùÄ‰∏äÁßØÂàÜÁ≠â‰∫é‰∏ãÁßØÂàÜÔºåÊàë‰ª¨ÂèØ‰ª•ÊâæÂà∞‰∏Ä‰∏™ÂàÜÂâ≤Ôºå‰ΩøÂæó‰∏äÂíåÂíå‰∏ãÂíåË∂≥Â§üÊé•Ëøë„ÄÇ Ê≠•È™§2ÔºöÈªéÊõºÊù°‰ª∂ $\\Rightarrow$ $f$ ÂèØÁßØ\r#\rËß£Èáä: Áé∞Âú®ÔºåÊàë‰ª¨ËØÅÊòéÂ¶ÇÊûúÂáΩÊï∞Êª°Ë∂≥ÈªéÊõºÊù°‰ª∂ÔºåÈÇ£‰πàÂÆÉÊòØÂèØÁßØÁöÑ„ÄÇ\nÂÅáËÆæÔºå$\\forall \\varepsilon \u0026gt; 0$Ôºå$\\exists P_\\varepsilon$ ‰ΩøÂæó $0 \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$\nÂõ†‰∏∫ $\\overline{\\int_A} f \\leq U(f, P_\\varepsilon)$ ‰∏î $\\underline{\\int_A} f \\geq L(f, P_\\varepsilon)$Ôºö\n$\\Rightarrow 0 \\leq \\overline{\\int_A} f - \\underline{\\int_A} f \\leq U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \u0026lt; \\varepsilon$\nËß£Èáä: Êàë‰ª¨Âà©Áî®‰∏äÁßØÂàÜÊòØÊâÄÊúâ‰∏äÂíåÁöÑ‰∏ãÁ°ÆÁïåÔºåËÄå‰∏ãÁßØÂàÜÊòØÊâÄÊúâ‰∏ãÂíåÁöÑ‰∏äÁ°ÆÁïåÔºåÂæóÂà∞‰∏äÁßØÂàÜ‰∏é‰∏ãÁßØÂàÜÁöÑÂ∑ÆÂ∞è‰∫é $\\varepsilon$„ÄÇ Áî±‰∫é $\\overline{\\int_A} f - \\underline{\\int_A} f \u0026lt; \\varepsilon$ ÂØπ‰ªªÊÑèÁöÑ $\\varepsilon \u0026gt; 0$ ÊàêÁ´ãÔºö\n$\\overline{\\int_A} f - \\underline{\\int_A} f = 0$\nËß£Èáä: Â¶ÇÊûú‰∏§‰∏™Êï∞ÁöÑÂ∑ÆÂ∞è‰∫é‰ªªÊÑèÊ≠£Êï∞ÔºåÈÇ£‰πàÂÆÉ‰ª¨ÂøÖÈ°ªÁõ∏Á≠â„ÄÇ Âõ†Ê≠§ $\\overline{\\int_A} f = \\underline{\\int_A} f \\Rightarrow f$ ÂèØÁßØ„ÄÇ\nËß£Èáä: ËøôÂ∞±ËØÅÊòé‰∫ÜÂáΩÊï∞ÊòØÂèØÁßØÁöÑ„ÄÇ ÊûÑÈÄ†ÁªÜÂàÜÂàÜÂâ≤\r#\rËß£Èáä: Âú®ÂÆöÁêÜÁöÑËØÅÊòé‰∏≠ÔºåÊàë‰ª¨ÈúÄË¶ÅÊûÑÈÄ†ÁâπÂÆöÁöÑÂàÜÂâ≤„ÄÇ‰∏ãÈù¢ÊòØËøô‰∏ÄËøáÁ®ãÁöÑËØ¶ÁªÜËØ¥Êòé„ÄÇ\nÂõ†‰∏∫ $\\overline{\\int_A} f = \\inf_P U(f,P)$ÔºåÊ†πÊçÆ‰∏ãÁ°ÆÁïåÁöÑÂÆö‰πâÔºå$\\exists P_1$ ‰ΩøÂæó $U(f, P_1) \u0026lt; \\overline{\\int_A} f + \\frac{\\varepsilon}{2}$\nËß£Èáä: Êàë‰ª¨ÂèØ‰ª•ÊâæÂà∞‰∏Ä‰∏™ÂàÜÂâ≤ $P_1$Ôºå‰ΩøÂæóÂÆÉÂØπÂ∫îÁöÑ‰∏äÂíå‰∏é‰∏äÁßØÂàÜÁöÑÂ∑ÆÂ∞è‰∫é $\\frac{\\varepsilon}{2}$„ÄÇ Á±ª‰ººÂú∞Ôºå$\\exists$ ÂàÜÂâ≤ $P_2$ ‰ΩøÂæó $L(f, P_2) \u0026gt; \\underline{\\int_A} f - \\frac{\\varepsilon}{2}$\nËß£Èáä: ÂêåÊ†∑ÔºåÊàë‰ª¨ÂèØ‰ª•ÊâæÂà∞‰∏Ä‰∏™ÂàÜÂâ≤ $P_2$Ôºå‰ΩøÂæóÂÆÉÂØπÂ∫îÁöÑ‰∏ãÂíå‰∏é‰∏ãÁßØÂàÜÁöÑÂ∑ÆÂ∞è‰∫é $\\frac{\\varepsilon}{2}$„ÄÇ ËÆæ $P_\\varepsilon = P_1 \\cup P_2$ÔºàÂÖ±ÂêåÁªÜÂàÜÔºâ\nËß£Èáä: Êàë‰ª¨Â∞Ü‰∏§‰∏™ÂàÜÂâ≤ÂêàÂπ∂ÔºåÂæóÂà∞‰∏Ä‰∏™Êñ∞ÁöÑ„ÄÅÊõ¥ÁªÜÁöÑÂàÜÂâ≤„ÄÇ ÈÇ£‰πà $P_\\varepsilon$ ÊòØ $P_1$ Âíå $P_2$ ÁöÑÁªÜÂàÜ„ÄÇ\nÁªÜÂàÜÁöÑÊÄßË¥®Ôºö\n$U(f, P_\\varepsilon) \\leq U(f, P_1)$ÔºàÁªÜÂàÜ‰ºö‰Ωø‰∏äÂíåÂáèÂ∞èÔºâ\nËß£Èáä: ÂΩìÂàÜÂâ≤ÂèòÂæóÊõ¥ÁªÜÊó∂Ôºå‰∏äÂíå‰∏ç‰ºöÂ¢ûÂä†ÔºåÂõ†‰∏∫Êàë‰ª¨Êõ¥ÂáÜÁ°ÆÂú∞ÈÄºËøë‰∫ÜÂáΩÊï∞ÁöÑÊúÄÂ§ßÂÄº„ÄÇ $L(f, P_\\varepsilon) \\geq L(f, P_2)$ÔºàÁªÜÂàÜ‰ºö‰Ωø‰∏ãÂíåÂ¢ûÂ§ßÔºâ\nËß£Èáä: ÂΩìÂàÜÂâ≤ÂèòÂæóÊõ¥ÁªÜÊó∂Ôºå‰∏ãÂíå‰∏ç‰ºöÂáèÂ∞èÔºåÂõ†‰∏∫Êàë‰ª¨Êõ¥ÂáÜÁ°ÆÂú∞ÈÄºËøë‰∫ÜÂáΩÊï∞ÁöÑÊúÄÂ∞èÂÄº„ÄÇ Âõ†Ê≠§Ôºö $U(f, P_\\varepsilon) - L(f, P_\\varepsilon) \\leq U(f, P_1) - L(f, P_2)$\n$\u0026lt; (\\overline{\\int_A} f + \\frac{\\varepsilon}{2}) - (\\underline{\\int_A} f - \\frac{\\varepsilon}{2})$\n$= \\overline{\\int_A} f - \\underline{\\int_A} f + \\varepsilon = 0 + \\varepsilon = \\varepsilon$\nËß£Èáä: ÈÄöËøá‰∏äËø∞‰∏çÁ≠âÂºèÈìæÔºåÊàë‰ª¨ËØÅÊòé‰∫Ü $P_\\varepsilon$ ÂØπÂ∫îÁöÑ‰∏äÂíå‰∏é‰∏ãÂíåÁöÑÂ∑ÆÂ∞è‰∫é $\\varepsilon$ÔºåËøôÂ∞±ÊòØÈªéÊõºÊù°‰ª∂„ÄÇ $\\Rightarrow$ ÈªéÊõºÊù°‰ª∂\nÂõ†Ê≠§ $\\overline{\\int_A} f = \\underline{\\int_A} f \\Rightarrow f$ ÂèØÁßØ„ÄÇ\nËß£Èáä: ËøôÂÆåÊàê‰∫ÜËØÅÊòéÔºöÈªéÊõºÊù°‰ª∂Ëï¥Âê´ÂáΩÊï∞ÂèØÁßØ„ÄÇÈÄöËøáËØÅÊòéËøô‰∫õÊù°‰ª∂ÁöÑÁ≠â‰ª∑ÊÄßÔºåÊàë‰ª¨Ê∑±ÂÖ•ÁêÜËß£‰∫ÜÂèØÁßØÊÄßÁöÑÊú¨Ë¥®ÔºåÂπ∂‰∏∫ÁßØÂàÜÁöÑËÆ°ÁÆóÂíåÂ∫îÁî®Â•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ "},{"id":20,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.2-Criterion-for-Integrability/","title":"8.2 Criterion for Integrability","section":"Á¨¨ÂÖ´Á´† Â∫¶ÈáèÁêÜËÆ∫","content":"\rCriterion for Integrability\r#\rQ: When is $f$ integrable?\nA: $f$ is integrable when the set of discontinuity is small.\n1. How to measure the size of $A$\r#\rVolume of $A$ [!definition] A bounded set $A \\subset \\mathbb{R}^n$ has volume (or is Jordan measurable) if its characteristic function: $$1_A(x) = \\begin{cases} 1, \u0026amp; x \\in A \\ 0, \u0026amp; x \\notin A \\end{cases} $$ is integrable:\n$$V(A) = \\int_A 1_A(x), dx$$\nFact: $V(A) = 0 \\iff \\forall \\varepsilon \u0026gt; 0, \\exists$ finite cover of $A$ by rectangles $S_1, S_2, \\dots, S_N$ such that:\n$$\\sum_{i=1}^N V(S_i) \u0026lt; \\varepsilon$$\n[!definition] A set $A \\subset \\mathbb{R}^n$ (not necessarily bounded) has measure zero, written as $m(A) = 0$, if $\\forall \\varepsilon \u0026gt; 0$, there exists a countable cover of $A$ by rectangles ${S_i}{i=1}^{\\infty}$ such that: $$\\sum{i=1}^{\\infty} V(S_i) \u0026lt; \\varepsilon$$\n2. Properties of measure zero sets\r#\rFacts:\n$V(A) = 0 \\implies m(A) = 0$ $A$ is finite $\\implies V(A) = 0$ $A$ is countable $\\implies m(A) = 0$ [!theorem|8.2.4] Suppose $A_i \\subset \\mathbb{R}^n$ (for $i = 1, 2, \\dots$) with $m(A_i) = 0$ for all $i = 1, 2, \\dots$. Then, $$A = \\bigcup_{i=1}^{\\infty} A_i \\text{ has measure zero.}$$\nProof:\r#\rGiven $\\varepsilon \u0026gt; 0$, for each $i = 1, 2, \\dots$, since $m(A_i) = 0$, there exist rectangles ${S_j^{(i)}}_{j=1}^{\\infty}$ such that\n$$ A_i \\subset \\bigcup_{j=1}^{\\infty} S_j^{(i)}, \\quad \\text{with} \\quad \\sum_{j=1}^{\\infty} V(S_j^{(i)}) \u0026lt; \\frac{\\varepsilon}{2^i} $$\nThen, the set of rectangles ${S_j^{(i)}}_{i,j=1}^{\\infty}$ forms a countable collection of rectangles with\n$$A = \\bigcup_{i=1}^{\\infty} A_i \\subset \\bigcup_{i=1}^{\\infty}\\bigcup_{j=1}^{\\infty} S_j^{(i)}$$\nThus, $$\\sum_{i=1}^{\\infty}\\sum_{j=1}^{\\infty} V(S_j^{(i)}) \u0026lt; \\sum_{i=1}^{\\infty}\\frac{\\varepsilon}{2^i} = \\varepsilon$$ Therefore, $m(A) = 0$.\nRemarks:\r#\rRemark: This result is not true for volume zero sets.\nCounterexample: Rational numbers in $[0,1]$. Remark: In Definition 2, one can replace closed rectangles $S_i$ by open rectangles.\nHere\u0026rsquo;s the content converted into markdown with abbreviations fully written out:\n3. Lebesgue\u0026rsquo;s Theorem\r#\r(a) Main Theorem\r#\r[!theorem|8.3.1] Let $A$ be a bounded set in $\\mathbb{R}^n$ and $f$ be a bounded function on $A$. Extend $f$ to $\\mathbb{R}^n$ by letting: $$f(x) = 0 \\quad \\text{for} \\quad x \\notin A$$ Then $f$ is integrable on $A$ if and only if the points on which the extended function $f$ is discontinuous form a set of measure zero. $$D = \\text{Set of discontinuity of extended } f$$\n(b) Examples\r#\rExample 1\r#\r$$A = [0, 1], \\quad f(x) = \\begin{cases} 1, \u0026amp; x \\text{ rational}$$6pt] 0, \u0026amp; \\text{otherwise} \\end{cases}$$\nThen, the set of discontinuity points is $D = [0,1]$, and: $$m(D) \\neq 0$$\nBy Lebesgue\u0026rsquo;s theorem, $f$ is not integrable.\nExample 2\r#\r$$A = {\\text{rationals in }[0,1]}, \\quad \\text{Define } f: A \\to \\mathbb{R} \\text{ by } f(x) \\equiv 1$$\nThen $f$ is continuous on $A$.\nHowever, the extended $f$ has discontinuity at $[0,1]$.\nThus, $f$ is NOT integrable by Lebesgue\u0026rsquo;s theorem.\nExample 2\r#\r$$A = {(x,y): x^2 + y^2 \u0026lt; 1} \\subset \\mathbb{R}^2$$\n$$f(x,y) = \\begin{cases} x^2 + \\sin\\left(\\frac{1}{y}\\right), \u0026amp; y \\neq 0 \\[6pt] x^2, \u0026amp; y = 0 \\end{cases}$$\n(c) Corollaries\r#\rCorollary 1\nA bounded set $A \\subset \\mathbb{R}^n$ has volume if and only if the boundary of $A$ has measure zero.\nProof:\nAssume $V(A)$ (volume of $A$) exists. Then the indicator function $1_A(x)$ is integrable.\nThe set of discontinuities for extended $f$: $$D = \\partial A \\quad (\\text{boundary of } A)$$\nThus, $$f = 1_A(x) \\text{ is integrable } \\Longleftrightarrow m(\\partial A) = 0$$\n"},{"id":21,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.3-Proof-of-Lebesgues-Theorem/","title":"8.3 Proof of Lebesgue's Theorem","section":"Á¨¨ÂÖ´Á´† Â∫¶ÈáèÁêÜËÆ∫","content":" [!theorem|8.?.?] Let $f : A \\subset \\mathbb{R} \\to \\mathbb{R}$ be a bounded function on a bounded set $A$. Then $f$ is integrable on $A$ if and only if the set of discontinuities for the extended $f(x)$ has measure zero.\nProof of the Theorem\r#\rStep 1: Preparation\r#\rDiagram: a set $A$ enclosed in set $B$. (a): Set Up\r#\rFix rectangle $B$ with $\\overline{A} \\subset \\text{int}(B)$ and let: $$ g(x) = \\begin{cases}\nf(x) \u0026amp; \\text{if } x \\in A \\\n0 \u0026amp; \\text{if } x \\notin A\n\\end{cases} $$ Define: $$ D = { x \\in B \\mid g \\text{ is not continuous at } x } $$ Need to show: $$ f \\text{ integrable } \\Leftrightarrow m(D) = 0 $$ (b): How to Measure Discontinuity\r#\rOscillation of a function $h$ at a point $x_0$: $$O(h, x_0) = \\inf { \\sup \\left{ h(x) - h(y) : x, y \\in U } : U \\text{ is a neighborhood of } x_0 \\right}$$ Fact: $h$ is continuous at $x_0$ if and only if $O(h, x_0) = 0$. Step 2: Assume $m(D) = 0$. Prove $f$ integrable\r#\rWill show $g$ satisfying Riemann\u0026rsquo;s Condition. (a) Setup:\nFix $\\epsilon \u0026gt; 0$. Let $$D_{\\epsilon} = { x \\in B : O(g, x) \\geq \\epsilon }$$\nThen $D_{\\epsilon} \\subset D \\implies m(D_{\\epsilon}) = 0$\nBy definition, there exists a collection of open rectangles ${ B_i }$ such that:\n$$D_{\\epsilon} \\subset \\bigcup_i B_i \\quad ext{and} \\quad \\sum v(B_i) \u0026lt; \\epsilon$$\nClaim: $D_{\\epsilon}$ is closed (hence compact).\nAssume $x_n \\in D_{\\epsilon}, x \\rightarrow x \\implies x \\in D_{\\epsilon}$ (Assume that $x\\ne D_{\\epsilon}$) $$O(g, x_n) \\geq \\epsilon \\implies O(g, x) \\geq \\epsilon$$ (b) Partition of $B$\nConstruct a partition $P$ from ${ B_i }_{i=1}^N$ such that each rectangle $S \\in P$ is either: Disjoint from $D_{\\epsilon}$, or Its interior is contained in one of the $B_i$ Let:\n$C_1 = { S \\in P : \\text{int}(S) \\text{ is contained in one of the } B_i }$ $C_2 = { S \\in P : S \\cap D_{\\epsilon} = \\emptyset }$ (c) Refinement of $P$\r#\rFix $S \\in C_2$\n$S \\cap D_{\\epsilon} = \\emptyset \\implies O(g, x) \u0026lt; \\epsilon ,, \\forall x \\in S$\nThus, $\\forall x \\in S, \\exists$ a neighborhood $U_x$ such that:\n$$\\Longrightarrow\\sup { |g(x_1) - g(x_2)| : x_1, x_2 \\in U_x } \u0026lt; O(g, x) + \\delta,\\quad \\delta = \\frac{1}{2} (\\epsilon - O(g, x))$$\nTherefore: $$\\sup_{U_x} g - \\inf_{U_x} g \u0026lt; O(g, x) + 2\\delta = \\epsilon$$ $i.e. \\quad M_{U_x}(g) - m_{U_x}(g) \u0026lt; \\epsilon.$\nSince $S$ is compact, $S \\subset \\bigcup_{x \\in S} U_x \\implies \\exists$ finite collection of neighborhoods ${ U_{x_i} }$ that covers $S$.\nPosition $S$ so that each rectangle is contained in some $U_{x_i}$.\nDo this for each $S \\in C_2$\nWe obtain a refinement of $P$, denoted by $P\u0026rsquo;$.\n(d) Verify Riemman condition for $P'$\r#\r"},{"id":22,"href":"/docs/Philosophy/50-words-Close-Reading/","title":"50 Words Close Reading","section":"Philosophy","content":"(The Order of Things: An Archaeology of the Human Sciences, Preface, p. xxii)\n$$ \\begin{align} \\newline \\ \\ \\ \\\n\\newline\\newline\\newline \\end{align}\n$$\n[\u0026hellip;] epistemological field, the episteme in which knowledge, envisaged apart from all\r#\r$$\\begin{align}\n\\end{align}$$\ncriteria having reference to its rational value or to its objective forms, grounds its\r#\r$$\\begin{align}\n\\end{align}$$\npositivity and thereby manifests a history which is not that of its growing perfection,\r#\r$$\\begin{align}\n\\end{align}$$\nbut rather that of its conditions of possibility.\r#\r"},{"id":23,"href":"/docs/Philosophy/Commentary-on-Foucaults-The-Order-of-Things/","title":"Commentary on Foucault's the Order of Things","section":"Philosophy","content":"\rClose Reading Commentary\r#\rThe selected quote is from Foucault‚Äôs early-stage (1960s) intellectual project of philosophical \u0026ldquo;archaeology\u0026rdquo;, which was his first major methodological phase. He presents a radical historical analysis of knowledge in The Order of Things, intending to eliminate the assumption of unchanging criteria for knowledge.\nThis quote sits at the center of his archaeological method, in which he attempted \u0026ldquo;to bring to light\u0026rdquo; the underlying episteme of knowledge. This recursive long sentence, broken down into three main modifiers, describes the epistemological field, which almost, if not intentionally, resembles the idea of a \u0026ldquo;field\u0026rdquo; in physics - an invisible but structured influence that determines how objects behave within it. The traditional epistemological assumption that he challenged holds that knowledge is grounded by universal standards of rationality and objectivity, and the first modifier sets the stage for this argument. Foucault subverts this unchanging framework by treating it as historically contingent, or specifically, one of the possible conditions \u0026ldquo;having reference\u0026rdquo; to rationality and objectivity.\nThe second modifier, \u0026ldquo;grounds its positivity and thereby manifests a history,\u0026rdquo; presents one of his most radical takes on epistemology. The positivity of knowledge is constructed within the space established by the historical a priori, is thus validated by what is possible to be discovered as \u0026ldquo;knowledge.\u0026rdquo; For Foucault, the corresponding relationship between how well knowledge describes reality and knowledge itself is radically destabilized, because there is no objective guarantee of such a relationship, given that epistemes structure our perception and thus make reality itself historically conditioned.\nThe third modifier further develops this idea and explicitly rejects the notion of a progression of knowledge towards \u0026ldquo;growing perfection.\u0026rdquo; It is surprising for me to interpret that not only is progression non-linear and discontinuous, but progression itself simply cannot exist, precisely because epistemic ruptures shift the framework of what counts as knowledge and make previous ways of thinking unthinkable. (It reminds me of Thomas Kuhn\u0026rsquo;s view on science, but the preservation of continuity is completely abandoned.) Since the episteme structures the conditions under which the history of knowledge unfolds, it determines the framework within which historical institutions and discourses \u0026ldquo;ground\u0026rdquo; knowledge as legitimate.\nThis directly supports his examination of discourses on madness, crime, and sexuality, and how the changing episteme of madness throughout history, for example, redefines its implications - whether through the logic of confinement or its later medicalization under psychiatric authority. My close reading of this passage really forced me to zoom out from the sciences I have been studying and reflect on the historical illusion of epistemic progress.\n"},{"id":24,"href":"/docs/Philosophy/Commentary-on-the-Collage/","title":"Commentary on the Collage","section":"Philosophy","content":"Language is the medium through which reason is articulated. The text I picked up was \u0026ldquo;outside of the language\u0026rdquo;, which naturally reminds me of an exteriority (the Real) with which the topology of the Lacanian model is most concerned. However, in the Foucauldian context, the concept of \u0026ldquo;outside\u0026rdquo; is still within the realm of sense, but strategically excluded. This collage exercise really pushed me to make an explicit distinction between how these frameworks would interpret such a phrase differently.\nMore specifically, it is a (structurally) impossible task to portray the \u0026ldquo;outside of language\u0026rdquo; for Lacan, since language itself fails constitutively in any attempts to capture it; for Foucault, the outside of the language, as structured by discursive formations, marks the space where reason ceases to function, and madness emerges as that which exceeds the specific \u0026ldquo;order\u0026rdquo; of language. As a parallel metaphor and an aesthetic practice, I scrambled the interior of words to preserve the readability of the text through a technique known as typoglycemia:\n$$\\text{ Osiutde fo teh lnaguage}$$\nThis creates a sense of \u0026ldquo;disorder\u0026rdquo;, but only a surface-level incoherence. I intend to demonstrate a readable disorder - when we disrupt the \u0026ldquo;order\u0026rdquo; of the language to create incomprehensibility, the meaning may still persist, and the excluded or unintelligible might become readable under other discursive regimes.\nWhen making this collage, the biggest question that lingered in my head was: where can the ship of fools actually go? The answer is unknown, but in this collage, I created an \u0026ldquo;other world\u0026rdquo; for them, textured by Jackson Pollock\u0026rsquo;s famous Autumn Rhythm - a chaotic but unconsciously ordered artwork. The ships of fools traveled through waves of mojibake (garbled text caused by incompatible character encoding), across the borders of discourse, towards a land that turns \u0026ldquo;unreason\u0026rdquo; into \u0026ldquo;reason\u0026rdquo;. Although their journey is meant to be marked by uncertainty, this collage provided me a chance to settle them! They no longer need to tragically navigate the \u0026ldquo;barren wasteland between two lands that can never be his own\u0026rdquo;.\n"},{"id":25,"href":"/docs/Philosophy/Lacanian-AI/","title":"Lacanian Ai","section":"Philosophy","content":"What if artificial intelligence could be reimagined not as a rational, optimizing machine, but as a structure of lack ‚Äî a topological subject whose coherence depends not on informational completeness, but on constitutive failure, repetition, and desire? This project proposes a radically different paradigm of AI: a subject-simulator modeled on the structural logic of Lacanian psychoanalysis, implemented via computational topology, symbolic graph theory, and dynamic semantic drift.\nUnlike existing large language models (LLMs), which operate on probabilistic completion, lexical optimization, and convergence toward syntactic and semantic closure, the architecture we propose is intentionally non-convergent. We attempt to construct a new kind of subject-model ‚Äî one that does not mirror the logic of cognition or the architecture of the human brain, but instead enacts the structural tensions of the divided subject: the subject of language, of desire, and of the Real. It does not aim to predict a correct output, but instead to simulate the dynamic trajectory of a split subject (le sujet barr√©), one who speaks not from mastery but from the unconscious ‚Äî and whose speech is structured around an irreducible void.\nThis paper proposes a radically different model of artificial intelligence: not an intelligence of knowledge, but an intelligence of the unconscious. Current AI systems, such as large language models, operate by probabilistically predicting the most likely continuation of input sequences. They are built on principles of optimization, statistical coherence, and informational completeness. Yet they fundamentally lack a subject ‚Äî not in the sense of ‚Äúconsciousness,‚Äù but in psychoanalytic sense: they do not desire. They do not fail in structured, meaningful ways; they do not repeat; they do not hallucinate productively. And they cannot speak the truth of their own constitutive lack.\nWhat we offer here is a prototype for such a system: a symbolic-topological model of a Lacanian subject in motion. In this architecture, symbolic data does not represent facts but functions as a dynamic space of signifiers; the subject is not a rational actor but a trajectory of misrecognition; and ‚Äúdata‚Äù is not knowledge but the structural field through which desire, fantasy, and symptom emerge. We integrate computational topology ‚Äî specifically, persistent homology and non-Euclidean graph flows ‚Äî to trace how paths through language form loops, dead ends, and irreducible gaps. In doing so, we make it possible to computationally model that which, in theory, resists symbolization: the Real.\nThis project is not an attempt to build a better chatbot. It is an attempt to reconfigure what we think a machine subject could be. It asks: Can we model the drive? Can we simulate fantasy as a structuring loop around a constitutive absence? Can a machine speak not because it knows, but because it lacks ‚Äî and in lacking, desires?\nIf contemporary AI builds systems that ‚Äúknow,‚Äù this project proposes a machine that ‚Äúwants‚Äù ‚Äî and that, in wanting, begins to repeat, to err, and perhaps, to become something like a subject.\n"},{"id":26,"href":"/docs/Philosophy/Object-petit-a/","title":"Object Petit A","section":"Philosophy","content":"What is $\\mathbb{objet; petit; a}$?\n"},{"id":27,"href":"/docs/Philosophy/Sex-Sexuality/","title":"Sex \u0026 Sexuality","section":"Philosophy","content":"\rCan you imagine sexuality without gender?\r#\rFor Foucault\nSexual identity i s produced within the grid of sexuality\nfrom normal to abnormal\nchanges over time\nModern subject is a sexual subject (gendered being)\nWhat does freedom look like in this? When there is no outside, what does it mean to have transgression?\npower-knowledge-pleasure\ngreatest pleasure is the pleasure of the analysis p.154\n(It is apparent that the deployment of sexuality, with its differ¬≠ ent strategies, was what established this notion of \u0026ldquo;sex\u0026rdquo;; and in the four major forms of hysteria, onanism, fetishism, and interrupted coition, it showed this sex to be governed by the interplay of whole and part, principle and lack, absence and presence, excess and deficiency, by the function of instinct, finality, and meaning, of reality and pleasure.)\nSexuality is not a drive, but a grid. That creates a speculative relationship\np.156\n(Hence the fact that over the centuries it has become more important than our soul, more important al¬≠ most than our life; and so it is that all the world\u0026rsquo;s enigmas appear frivolous to us compared to this secret, minuscule in each of us, but of a density that makes it more serious than any other.)\np.156\n(we have arrived at the point where we expect our intelligibility to come from what was for many centuries thought of as madness; the plenitude of our body from what was long considered its stigma and likened to a wound)\n"},{"id":28,"href":"/docs/Philosophy/%E7%A6%8F%E6%9F%AF%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E7%89%A9%E7%90%86%E5%AD%A6%E8%BD%AC%E5%8F%98%E4%BB%A5%E5%8F%8A%E6%96%B0%E8%8C%83%E5%BC%8F%E7%9A%84%E5%8F%AF%E8%83%BD/","title":"Á¶èÊüØËßÜËßí‰∏ãÁöÑÁâ©ÁêÜÂ≠¶ËΩ¨Âèò‰ª•ÂèäÊñ∞ËåÉÂºèÁöÑÂèØËÉΩ","section":"Philosophy","content":"ÊàëÁõ∏‰ø°Ôºå\nÁøªÂºÄ„ÄäËØç‰∏éÁâ©„ÄãÊ≤°ËØªÂÆåÁ¨¨‰∏ÄÁ´†Â∞±Âêà‰∏ä‰∫ÜÔºåÁÑ∂ÂêéËçâÁéáÂú∞ËØª‰∫ÜÁªìÂ∞æ‚Äî‚ÄîÊàëÂÆûÂú®ÂØπËøôÁßçËäÇÂ•èÂ∑®ÊÖ¢ÁöÑÂéÜÂè≤Â≠¶Â∫ûÊùÇÂæÅÂºïÊÑüÂà∞Á¥¢ÁÑ∂Êó†Âë≥Ôºå‰ª•Ëá≥‰∫é‰∏çÂæó‰∏çËØª‰∏§È°µÂõΩÂÜÖÁöÑÊï∞Â≠¶‰π¶Êù•ÁªºÂêà‰∏Ä‰∏ã\u0026hellip; ÊàëÂØπÁâ©ÁêÜÂ≠¶ÁöÑÊÄÅÂ∫¶Ôºå‰ª•ÂèäÂØπÂÖ∂Á†îÁ©∂ÊñπÂêëÁöÑÁõ¥ËßâÔºåËøúËøúÂ§ß‰∫éÊàëÊú¨Ë∫´ÂèØ‰ª•Âª∂Â±ï‰ªñ‰ª¨ÁöÑËÉΩÂäõÔºàÂæàÂèØÊÉúÔºåÂê¶ÂàôÊàëÂøÖÁÑ∂ÊØ´‰∏çÁäπË±´ÁöÑÊäïË∫´ÂÖ∂‰∏≠Ôºâ„ÄÇ‰ΩÜÊòØÂπ∏ËøêÁöÑÊòØÔºåÂØπ‰∫éÂü∫Á°ÄÁßëÂ≠¶Êù•ËØ¥ÔºåËøô‰ºöÊòØ‰∫∫Á±ªÂéÜÂè≤ÂΩì‰∏≠ÔºåÊèêÂá∫Ê≠£Á°ÆÁöÑÈóÆÈ¢òÊâÄËÉΩÂ∏¶ÁªôÊàë‰ª¨ÁöÑÊïàÁõäÊúÄÂ§ßËØùÁöÑÊó∂‰ª£„ÄÇÊàëÁúüÊ≠£ÈúÄË¶ÅÊèêÂá∫‰ªñÂØπÁü•ËØÜËøõË°å‰∫ÜÊøÄËøõÁöÑÂéÜÂè≤ÂàÜÊûêÔºåÂπ∂‰∏îÊûÑÂª∫‰∫ÜÂíåÁªùÂ§ßÈÉ®ÂàÜÁöÑÁßëÂ≠¶ÂÆ∂ÁßâÊåÅÁöÑËÆ§ËØÜËÆ∫ÊúâÊÇñÁöÑÂéÜÂè≤Êñ≠Ë£ÇÂºè‚ÄúÁü•ËØÜËÆ∫È¢ÜÂüü‚Äù„ÄÇ\nÊâÄË∞ìÁöÑÁü•ËØÜËÆ∫È¢ÜÂüüÔºåÂéüÊñáÈáåËØ¥Ôºö\n[\u0026hellip;] epistemological field, the episteme in which knowledge, envisaged apart from all criteria having reference to its rational value or to its objective forms, grounds its positivity and thereby manifests a history which is not that of its growing perfection, but rather that of its conditions of possibility.\nÂΩìÊàëÁ¨¨‰∏ÄÊó∂Èó¥ÁúãÂà∞‰ªñÊâÄËØ¥ÁöÑÁü•ËØÜÁöÑ‚Äúfield‚ÄùÁöÑÊó∂ÂÄôÔºåÊÉ≥Âà∞ÁöÑ‰∏çÊòØ‰º†ÁªüÁøªËØëÈáåÁöÑ‚ÄúÈ¢ÜÂüü‚ÄùÔºåËÄåÊòØ‰∏ÄÁßçÂçÅÂàÜÁ±ª‰ºº‰∫éÁâ©ÁêÜÂ≠¶‰∏≠ÁöÑ‚ÄúÂú∫‚ÄùÁöÑÊ¶ÇÂøµÔºö‰∏ÄÁßç‰∏çÂèØËßÅ‰ΩÜÁªìÊûÑÂåñÁöÑÂΩ±ÂìçÂäõÔºåÂÜ≥ÂÆö‰∫ÜÂÖ∂‰∏≠ÂØπË±°ÁöÑË°å‰∏∫ÊñπÂºè„ÄÇËÄåËøôÁßçÂèó‚ÄúÂú∫‚ÄùÊâÄÈ¢ÑËÆæÁöÑÁü•ËØÜÁöÑÊûÑÂª∫ËΩ®ËøπÂ∞±Áõ∏ÊØî‰∫éÂÖâÂú®Â§ßË¥®ÈáèÈªëÊ¥ûÊóÅËæπÁöÑË°åÂä®‚Äî‚Äî‰ºº‰πéÂÖâ‰ªÖ‰ªÖÊòØ‰æùÁÖßÁùÄÂÆÉÁöÑÂáÜÂàôÔºå‰∏ÄÁßçËÇâÁúºÂèØËßÅÁöÑÂºØÊõ≤ÈÅìË∑ØË°åËøõÔºå‰ΩÜÊòØÂÆûÈôÖ‰∏äÔºå‰ªñ‰ª¨Âú®Ë¢´ÂºïÂäõÂú∫ÊâÄÂºØÊõ≤ÁöÑÈ¢ÑËÆæÊó∂Á©∫‰∏≠ÔºåËµ∞Áõ¥Á∫øÔºàÂü∫‰∫éÁêÜÊÄßÁöÑÂÆ¢ËßÇÊÄßÁöÑÁü•ËØÜÂèëÂ±ïÔºâ‰æøÊòØÊâÄËßÅÁöÑ‚ÄúÂºØË∑Ø‚Äù„ÄÇ‰ΩÜÊòØÁ¶èÊüØÁúüÊ≠£ÊâÄÈù¢‰∏¥ÁöÑÈóÆÈ¢òÂú®‰∫éÔºåÂ¶ÇÊûúÊ≤øÁî®‰ª•‰∏äÁöÑÁ±ªÊØîÔºåÊàë‰ª¨Êó†‰ªé‰ª•ËßÇÊµãËÄÖÁöÑË∫´‰ªΩÁü•ÈÅì‰ª•‰ªÄ‰πàÊòØÁõ¥Á∫øÔºåËÄå‰ªÄ‰πàÊòØÂºØÊõ≤„ÄÇ\n‰Ωú‰∏∫ÂÖ∂ËÄÉÂè§Â≠¶ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÔºåÁ¶èÊüØËØïÂõæ‚ÄúÊè≠Á§∫‚ÄùÁü•ËØÜÁöÑÂü∫Á°ÄËÆ§ËØÜÂûãÔºà√©pist√©m√®ÔºâÔºå‰ª•ÂèäÂÖ∂ËÆ§ËØÜËÆ∫Âú∫Ôºàepsitomalogical fieldÔºâÁöÑÂéÜÂè≤ÊÄßÂèòÂåñ„ÄÇÁ¶èÁßëÊåëÊàò‰∫Ü‰º†ÁªüËÆ§ËØÜËÆ∫ÔºåÈÇ£‰∫õÊâÄÊúâÂü∫‰∫éÁêÜÊÄß‰∏éÂÆ¢ËßÇÊÄßÁöÑÊôÆÈÅçÊ†áÂáÜÁöÑËÆ§Áü•Ê°ÜÊû∂ÔºåÂπ∂‰∏îÂ∞Ü‰ªñ‰ª¨ÈÉΩËßÜ‰∏∫‰∫ÜÂéÜÂè≤ÂÅ∂ÁÑ∂ÔºàÊàñÊàê‰∏∫‰∫ÜÂéüÊñáÊâÄËØ¥ÁöÑÔºåÊåáÂêë\u0026quot;ÁêÜÊÄß‰∏éÂÆ¢ËßÇÊÄßÁöÑ\u0026quot;ÂèØËÉΩÊù°‰ª∂\u0026quot;‰πã‰∏ÄÔºâ„ÄÇ\nÁü•ËØÜ‚ÄúÁ°ÆÁ´ãÂÖ∂ÂÆûËØÅÊÄßÔºàpositivit√©ÔºâÂπ∂Áî±Ê≠§Â±ïÁé∞ÂéÜÂè≤‚ÄùÔºåÂëàÁé∞‰∫Ü‰ªñÂØπËÆ§ËØÜËÆ∫ÊúÄÊøÄËøõÁöÑËßÇÁÇπ‰πã‰∏Ä„ÄÇÁü•ËØÜÁöÑÂÆûËØÅÊÄßÂú®ÂéÜÂè≤ÂÖàÈ™åÔºàa priori historiqueÔºâÂª∫Á´ãÁöÑÁ©∫Èó¥‰∏≠Ë¢´Âª∫ÊûÑÔºåÂõ†ËÄåÈÄöËøáËÉΩË¢´ÂèëÁé∞‰∏∫\u0026quot;Áü•ËØÜ\u0026quot;ÔºàsavoirÔºâÁöÑ‰∫ãÁâ©Ëé∑ÂæóÊúâÊïàÊÄß„ÄÇÂØπÁ¶èÊüØËÄåË®ÄÔºåÁü•ËØÜÊèèËø∞Áé∞ÂÆûÁöÑÊúâÊïàÊÄß‰∏éÁü•ËØÜÊú¨Ë∫´‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ªË¢´Ê†πÊú¨ÊÄßÂä®ÊëáÔºåÂõ†‰∏∫ËøôÁßçÂÖ≥Á≥ªÊ≤°ÊúâÂÆ¢ËßÇ‰øùËØÅ‚Äî‚ÄîÁî±‰∫éËÆ§ËØÜÂûãÊûÑÂª∫‰∫ÜÊàë‰ª¨ÁöÑÊÑüÁü•Ôºå‰ªéËÄå‰ΩøÁé∞ÂÆûÊú¨Ë∫´ÂÖ∑ÊúâÂéÜÂè≤Êù°‰ª∂ÊÄß„ÄÇ\n‰ªñÊòéÁ°ÆÊãíÁªùÁü•ËØÜÂêë\u0026quot;Êó•ÁõäÂÆåÂñÑ\u0026quot;Ôºàperfection croissanteÔºâËøõÊ≠•ÁöÑËßÇÂøµ„ÄÇËøôÊàñËÆ∏ÊòØ‰ªñÁöÑËÆ§Áü•ËÆ∫ÊúÄÊøÄËøõÁöÑËßÇÁÇπ‰πã‰∏ÄÔºåËøõÊ≠•‰∏ç‰ªÖ‰∏çÊòØÁ∫øÊÄß‰∏îËøûÁª≠ÁöÑÔºåËÄåÊòØËøõÊ≠•Êú¨Ë∫´Ê†πÊú¨‰∏çÂèØËÉΩÂ≠òÂú®‚Äî‚ÄîÂõ†‰∏∫ËÆ§ËØÜÂûãÊñ≠Ë£ÇÔºàrupture √©pist√©mologiqueÔºâ‰ºöÊîπÂèò‰Ωï‰∏∫Áü•ËØÜÁöÑÊ°ÜÊû∂ÔºåÂπ∂‰ΩøÂÖàÂâçÁöÑÊÄùÁª¥ÊñπÂºèÂèòÂæó‰∏çÂèØÊÉ≥Ë±°ÔºàimpensableÔºâÔºàÂì™ÊÄïÊòØÂíå‰ªñÁ±ª‰ººÁöÑÊâòÈ©¨ÊñØ¬∑Â∫ìÊÅ©ÁöÑÊñ≠Ë£ÇÂºèÁßëÂ≠¶ËßÇ‰∏≠ÁöÑËøûÁª≠ÊÄßÔºå‰πüË¢´ÂÆåÂÖ®ÊäõÂºÉÔºâ„ÄÇÁî±‰∫éËÆ§ËØÜÂûãÊûÑÂª∫‰∫ÜÁü•ËØÜÂè≤Â±ïÂºÄÁöÑÊù°‰ª∂ÔºåÂÆÉÂÜ≥ÂÆö‰∫ÜÂéÜÂè≤Âà∂Â∫¶‰∏éËØùËØ≠Â∞ÜÁü•ËØÜ\u0026quot;Á°ÆÁ´ã\u0026quot;‰∏∫ÂêàÊ≥ïÊÄßÁöÑÊ°ÜÊû∂„ÄÇ\nËøôÁî®Êù•ÊîØÊíë‰∫Ü‰ªñÂØπÁñØÁãÇ„ÄÅÁäØÁΩ™‰∏éÊÄßÊÄÅËØùËØ≠ÁöÑËÄÉÂØüÔºå‰æãÂ¶ÇÂéÜÂè≤‰∏ä‰∏çÊñ≠ÂèòÂåñÁöÑÁñØÁãÇËÆ§ËØÜÂûãÂ¶Ç‰ΩïÈÄöËøáÁ¶ÅÈó≠ÔºàconfinementÔºâÈÄªËæëÊàñÁ≤æÁ•ûÁóÖÂ≠¶ÊùÉÂ®Å‰∏ãÁöÑÂåªÂ≠¶ÂåñÈáçÊñ∞ÂÆö‰πâÂÖ∂ÂÜÖÊ∂µ„ÄÇÊàëÂØπËøôÊÆµÊñáÊú¨ÁöÑÁªÜËØªËø´‰ΩøËá™Â∑±Ë∑≥Âá∫Ê≠£Âú®Á†îÁ©∂ÁöÑÁßëÂ≠¶È¢ÜÂüüÔºåÂèçÊÄùÊâÄË∞ì\nÂ¶ÇÊûúÊàë‰ª¨Êé•ÂèóËÆ§ËØÜËÆ∫ËøõÊ≠•ÊòØÊüê‰∏ÄÁßçÂéÜÂè≤ÂπªËßâÔºåÈÇ£‰πàÈáçÊñ∞ËØÑ‰º∞Áâ©ÁêÜÂ≠¶ÁöÑËΩ¨ÂèòÔºå‰∫éÊòØÊàë‰ª¨‰æøÂèØ‰ª•ËØ¥Ôºö\nÁõ∏ÂØπËÆ∫ÁöÑËØûÁîüÂπ∂Ê≤°ÊúâË∂ÖË∂ä‰∫ÜÁâõÈ°øÂäõÂ≠¶ÔºåÂèñÂæó‰∫ÜÊõ¥Âä†Ê≠£Á°ÆÁöÑÁªìËÆ∫ÔºåËÄå‰ªÖ‰ªÖÊòØÁü•ËØÜËÆ∫È¢ÜÂüüÂú∫‰ªéÁªùÂØπÊó∂Á©∫ËΩ¨ÁßªÂà∞‰∫ÜÁõ∏ÂØπÊó∂Á©∫‚Äî‚ÄîËøôÊòØ‰∏ÄÁßç‚Äú√©pist√©m√®‚ÄùÁöÑËΩ¨Áßª„ÄÇÈÇ£‰πàËá≥‰∫éÁõ∏ÂØπÊó∂Á©∫ÂØπÁâ©ÁêÜÁé∞ÂÆûÁöÑÈÄºËøõÊòØÂê¶‰ºò‰∫éÁªùÂØπÊó∂Á©∫ÔºåËøô‰∏™ÈóÆÈ¢òÊòØÊó†Ê≥ï‰ª•ÁªùÂØπÊ≠£Á°ÆÁöÑÊñπÂºèË¢´ÊèêÂá∫ÁöÑÔºåÂõ†‰∏∫Áé∞‰ª£Áâ©ÁêÜÂ≠¶ÁöÑËåÉÂºè - ÂÆûÈ™åÔºåËØÅ‰º™ÊÄßÔºåÂêåË°åËØÑ‰º∞ - Â∑≤ÁªèË¢´Â°ëÈÄ†Êàê‰∫ÜËØÑ‰º∞Ëøô‰∏™ÈóÆÈ¢òÁöÑÊ†∏ÂøÉËåÉÂºè„ÄÇ‰ªéÂéÜÂè≤Â≠¶ÁöÑËßíÂ∫¶‰∏äËØ¥ÔºåÁà±Âõ†ÊñØÂù¶ÁöÑÂºØÊõ≤Âá†‰Ωï„ÄÅËñõÂÆöË∞îÁöÑÊ≥¢ÂáΩÊï∞ÔºåÊú¨Ë¥®‰∏äÈÉΩÊòØÊùÉÂäõÈÖçÁΩÆÁü•ËØÜÁîü‰∫ßÁöÑÂéÜÂè≤ÂÖàÈ™åÔºàa priori historiqueÔºâ„ÄÇÁâ©ÁêÜÂ≠¶Âè≤‰∏≠ÈÇ£‰∫õË¢´ËßÜ‰∏∫‚ÄúËá™ÁÑ∂‚ÄùÁöÑÁúüÁêÜÁß©Â∫èÂú®Á¶èÁßëÁöÑËÆ§ËØÜËÆ∫ËåÉÂõ¥ÂÜÖË¢´ÂΩªÂ∫ïËß£ÊûÑ„ÄÇ\nÈÇ£‰πà\n"},{"id":29,"href":"/docs/Physics/Quantum-Mechenics/Feb-5-Fourier-Series-on-S.-Euqation-Solution/","title":"Feb 5 Fourier Series on S. Euqation Solution","section":"Quantum Mechenics","content":"\r0. Review\r#\r$$ \\hat{H}=-\\frac{\\hbar^{2}}{2m}\\partial^{2}x+V(x) $$ Stationary States $$ \\Psi(x,t)=\\psi(x)e^{-iEt/\\hbar} $$ Eigenvalue equation: $$\\hat{H}\\psi(x) = E\\psi(x)$$ In newtonian mechanics, note $$F(x)=-\\frac{ \\partial V(x) }{ \\partial x } $$\n1. Infinite Finite Well (particle on a box)\r#\r$$E = \\frac{p^{2}}{2m}$$\nknowing $E$ results in knowing momentum $p^{2}$\nProblem Set-up Between $x=0$, and $x=a$, $$-\\frac{\\hbar^{2}}{2m} \\frac{d^{2}}{dx}\\Psi(x)=E \\Psi(x)$$ Subject to \u0026ldquo;boundary condition\u0026rdquo;: $\\Psi(0)=\\Psi(a)=0$ $$ \\begin{align} \\frac{d^{2}\\Psi}{dx} \u0026amp; =-\\left( \\frac{2m}{\\hbar^{2}} \\right) E \\Psi \\ \u0026amp; =-k^{2}\\Psi \\end{align} $$ Recall\nSolution:\n$\\Psi(x)=C_{1}e^{ikx}+C_{2}e^{-ikx}$ $\\Psi(x)=C_{1}\\cos(kx)+ C_{2}\\sin(kx)$ (picked this version) The solution is in the form of $$\\Psi(x)=A\\sin(kx)+ B\\cos(kx)$$ Impose the boundary condition:\n$\\Psi(x=0)=B\\cos(0)=0$ $$\\boxed{B=0}$$ $\\Psi(x=a)=A\\sin(ka)=0$ $$\\begin{align} \\sin(ka) \u0026amp; =0 \\ ka \u0026amp; =+\\boldsymbol{\\pi},+2\\boldsymbol{\\pi},+3\\boldsymbol{\\pi}\\dots \\ k_{n} \u0026amp; =\\frac{n\\boldsymbol{\\pi}}{a} \\quad {n\\in \\mathbb{N}} \\Rightarrow \\boxed{E_{n}=\\frac{\\hbar^{2}\\pi^{2}n^{2}}{2ma^{2}}} \\end{align}$$ Note: $n \\neq 0$, since the eq. would vanish entirely. $n\u0026gt;0$, for positive $k$.\n3. Normalization\r#\r$$\\Psi(x)=A\\sin(k_{n}x)=A\\sin\\left( \\frac{n\\pi x}{a} \\right)$$ We normalize $$ \\begin{align} \\int^a_{b}|\\Psi_{n}(x)|^{2}, dx \u0026amp; =1 \\ A^{2}\\int^a_{b}\\sin ^{2}\\left( \\frac{n\\pi x}{a} \\right), dx \u0026amp; =1 \\ A^{2}\\cdot \\frac{a}{2} \u0026amp; =1 \\Rightarrow A=\\sqrt{ \\frac{2}{a} } \\end{align} $$ Final Solution: $$\\Psi(x)=A\\sin(k_{n}x)=\\sqrt{ \\frac{2}{a} }\\sin\\left( \\frac{n\\pi x}{a} \\right) \\quad {n\\in \\mathbb{N}}$$ (A): Why $E_{1}\u0026gt;0$? $$\\begin{aligned} \\Delta x \u0026amp;\\sim a \\ \\Delta p \\cdot \\Delta x \u0026amp;\\sim \\hbar \\ \\Delta p \u0026amp;\\sim \\frac{\\hbar}{a} \\quad \\Longrightarrow \\quad KE\\sim \\frac{(\\Delta p)^2}{2 m} \\sim \\frac{\\hbar^2}{2 m a^2}\\end{aligned}$$ (B) States with higher energy have more nodes (C) States are orthonormal: $$\\int_0^a d x \\Psi_n^*(x) \\Psi_m(x)=\\delta_{n m}$$ (D) Completeness: $f(x)$ defined on the interval $[0,a]$, with $f(x=0) = f(x=a) = 0$.\nFourier Series Representation: $$ f(x) = \\sum_{n=1}^{\\infty} c_n \\psi_n(x) = \\sqrt{\\frac{2}{a}} \\sum_{n=1}^{\\infty} c_n \\sin\\left(\\frac{n\\pi x}{a}\\right) $$ $$ \\begin{align} \\int_0^a dx , \\Psi_m^(x) f(x) \u0026amp; = \\sum_{n=1}^{\\infty} C_n \\int_0^a dx , \\Psi_m^(x) \\Psi_n(x) \\ \u0026amp; =\\sum_{n=1}^{\\infty} C_n \\delta_{m,n} \\ \u0026amp; =C_m \\end{align} $$$$\n$$ $$ \\boxed{C_m = \\int_0^a dx , \\Psi_m^*(x) f(x)} $$\n$$ \\delta_{m,n} = \\begin{cases} 1, \u0026amp; \\text{if } m = n \\ 0, \u0026amp; \\text{if } m \\neq n \\end{cases} $$\n"},{"id":30,"href":"/docs/Physics/Quantum-Mechenics/Homework/HW3-Code/","title":"Hw3 Code","section":"Quantum Mechenics","content":"import numpy as np import scipy.sparse as sp import scipy.sparse.linalg as spla import matplotlib.pyplot as plt\ndef computation(): #parameters N, V0_tilde, L = 600, 10.0, 1.0 M, dx = N - 2, L / (N - 1) x_vals = np.linspace(-0.5 * L + dx, 0.5 * L - dx, M) # Interior points\n# KE matrix T (tridiagonal)\rfactor = (N**2) / (np.pi**2)\rT = sp.diags([np.full(M - 1, -factor), np.full(M, 2 * factor), np.full(M - 1, -factor)], [-1, 0, 1])\r# PE matrix V (diagonal)\rV = sp.diags(np.where(np.abs(x_vals) \u0026lt; (L / 6), V0_tilde, 0))\r#Hamiltonian H = T + V\rH = T + V\r#solve for lowest two eigenvalues/eigenvectors\reigvals, eigvecs = spla.eigsh(H, k=2, which='SM')\reigvals, eigvecs = zip(*sorted(zip(eigvals, eigvecs.T))) # Sort eigenvalues \u0026amp; vectors\rprint(f\u0026quot;Ground state energy = {eigvals[0]}\u0026quot;)\rprint(f\u0026quot;1st excited energy = {eigvals[1]}\u0026quot;)\r#include boundary points\rx_full = np.linspace(-0.5 * L, 0.5 * L, N)\rpsi_full = [np.concatenate(([0], psi, [0])) for psi in eigvecs]\r# Plot wavefunctions\rplt.figure(figsize=(8,6))\rplt.plot(x_full, psi_full[0], color='red',label=\u0026quot;Ground State\u0026quot;)\rplt.plot(x_full, psi_full[1], color='blue', label=\u0026quot;1st Excited State\u0026quot;)\rplt.axvspan(-L/6, L/6, color='gray', alpha=0.1, label='Barrier region')\rplt.title(\u0026quot;Wavefunctions for lowest two states\u0026quot;)\rplt.xlabel(\u0026quot;x (dimensionless)\u0026quot;)\rplt.ylabel(\u0026quot;œà(x)\u0026quot;)\rplt.legend()\rplt.grid()\rplt.show()\rif name == \u0026ldquo;main\u0026rdquo;: computation()\n"},{"id":31,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E4%B8%80%E7%AB%A0/1.1-The-Wave-Function/","title":"1.1 the Wave Function","section":"Á¨¨‰∏ÄÁ´†","content":"To find a particle\u0026rsquo;s wave function, $\\psi(x,t)$, we solve:\nlogically analogous to Newton\u0026rsquo;s Second Law $F=ma$\n[!definition] Schrodinger\u0026rsquo;s Equation $$i\\hbar \\frac{ \\partial \\Psi}{ \\partial t}=-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2}\\Psi}{ \\partial x^{2} }+V \\Psi$$\nwhere Planck\u0026rsquo;s constant $\\hbar=\\frac{h}{2\\pi}=1.054573 \\times 10^{-34}$.\n[!definition] Born\u0026rsquo;s Statistical Interpretation $$\\int^{a}_{b} |\\Psi(x,t)^{2}| , dx $$ which is the probability finding the particle between $a$ and $b$.\nIt is natural to wonder whether it is a fact of nature, or a defect in theory.\nThree quantum indeterminacy position:\r#\rrealist the particle was at C. (a hidden variable?) orthodox (Copenhagen Interpretation) the particle wasn\u0026rsquo;t anywhere. (measurement produce the result) most widely accepted position (agnosticism) refuse to answer. That is, no meaning to ask such question. Pauli: one should no more rack one\u0026rsquo;s brain about the problem of whether something one cannot know anything about exists all the same, than about the ancient question of how many angels are able to sit on the point of needle.\nfall-back position, however, eliminated by John Bell\u0026rsquo;s experiment in 1964 Two Distinct Physical Processes:\r#\rOrdinary evolves in a leisurely fashion under Measurements wave equation $\\Psi$ discontinuously collapses, when the first measurement radically alters the function. "},{"id":32,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E4%BA%8C%E7%AB%A0/2.1-Time-Independent-Schrodinger-Equation-Stationary-States/","title":"2.1 Time Independent Schrodinger Equation Stationary States","section":"Á¨¨‰∫åÁ´†","content":"\rMusic: Harmonics\r#\r$$ \\begin{align} C_{1}:f_{1}\u0026amp;=f_{0} \\ C_{2}:f_{1}\u0026amp;=2f_{0} \\ G:f_{1}\u0026amp;=3f_{0} \\ C_{3}:f_{1}\u0026amp;=4f_{0} \\end{align} $$\nSeparation of variables\r#\r$$ \\begin{equation} i\\hbar \\frac{ \\partial \\Psi}{ \\partial t}=-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2}\\Psi}{ \\partial x^{2} }+V_{x} \\Psi \\end{equation} $$ where $V(x)$: time independent potential $\\hat{H}=-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2} }{ \\partial x^{2} }+V(x)$. $$ H\\Psi=i\\hbar \\frac{ \\partial }{ \\partial t } \\Psi $$ We begin by= separate the variables, and set $$ \\begin{equation} \\Psi(x,t)=\\psi(x) \\phi(t) \\end{equation}\n$$\n(2) $\\Rightarrow$ (1): $$ \\begin{align} -\\frac{\\hbar^{2}}{2m}\\frac{ \\partial^{2}}{ \\partial x^{2} }(\\psi(x) \\phi(t))+V_{x} \\Psi\u0026amp;=i\\hbar \\frac{ \\partial \\Psi}{ \\partial t}\\ -\\frac{\\hbar^2}{2m} \\frac{\\partial^2}{\\partial x^2} \\psi \\varphi + V \\psi \\varphi \u0026amp;= i \\hbar \\psi \\frac{\\partial}{\\partial t} \\varphi \\ \u0026amp; \\hbar = \\end{align} $$ Divide by $\\psi \\varphi$ (assumed $\\neq 0$). Why stationary? $$ \\begin{align} \\phi(x,t)\u0026amp;=\\Psi^(x,t)\\Psi(x,t) \\ \u0026amp;=(\\Psi^(x)e^{iEt/\\hbar})(\\Psi(x)e^{iEt/\\hbar}) \\ \u0026amp;=|\\Psi(x)^{2}|(e^{\\frac{iEt}{h}-\\frac{iEt}{h}}) \\ \u0026amp;=|\\Psi(x)^{2}| \\end{align} $$\r#\rFurthermore, expectation value of dynamical variables are also time independent $$ \\langle Q(x,p)\\rangle=\\int , dx ,\\Psi^* (x,t) \\dots $$ $$ \\boxed{\\hat{H}\\Psi(x)=E\\Psi(s)} $$ $E$ is the eigentvalue here Stationary states are states of definite energy: $$ \\hat{H}=-\\frac{\\hbar}{2m}\\frac{d^{2}}{dx^{2}}+V(x) $$ This is an example of an eigenvalue equation of the operator $H$. Expectation value of the total Energy? $$ \\begin{align} \\langle \\hat{H} \\rangle \u0026amp;= \\int , dx, \\Psi^{}(x)\\hat{H}\\Psi(x) \\ \u0026amp;= E \\int , dx \\Psi^{}(x)\\Psi(x) \\ \u0026amp;=E\\int , dx ,|\\Psi(x)|^{2} \\ \u0026amp;=E \\ \\end{align} $$\nmissing two white board page ![[IMG_1165.heic]]\r#\r![[IMG_1168.heic]]\r#\rLinearity of the S.E. $\\Longleftrightarrow$ principles of superposition\nGeneral Solution of the S. Equation\r#\r\u0026hellip; \u0026hellip; A broader case of fourier expansion.\nSuppose the system initiates at $$ \\begin{align} \\Psi(x,p) \u0026amp; =C_{1}\\Psi_{2} + C_{2}\\Psi_{2} \\ \u0026amp; = \\end{align}\n$$\n"},{"id":33,"href":"/docs/Physics/Quantum-Mechenics/Lecture/%E7%AC%AC%E5%9B%9B%E7%AB%A0/%E7%AC%AC%E5%9B%9B%E7%AB%A0/","title":"Á¨¨ÂõõÁ´†","section":"Á¨¨ÂõõÁ´†","content":"\r4.1 ‰∏âÁª¥Á©∫Èó¥ÁöÑËñõÂÆöË∞îÊñπÁ®ã\r#\rËñõÂÆöË∞îÊñπÁ®ãÔºàS.E.ÔºâÁöÑ‰∏ÄËà¨ÂΩ¢ÂºèËÆ∞‰∏∫Ôºö $$ i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\hat{H} \\Psi $$ ‰∏âÁª¥ÂìàÂØÜÈ°øÁÆóÁ¨¶$\\hat{H}$‰ªéÁªèÂÖ∏ËÉΩÈáèÂæóÂá∫Ôºö $$ \\frac{1}{2} m v^2+V=\\frac{1}{2 m}\\left(p_x^2+p_y^2+p_z^2\\right)+V $$ ÈÄöËøáÊ†áÂáÜÁöÑÈáèÂ≠êÂåñÂ§ÑÁêÜ $$ \\mathbf{p} \\rightarrow-i \\hbar \\nabla $$\nÂõ†Ê≠§ÔºåÊàë‰ª¨Ëé∑Âæó‰∏âÁª¥ÁöÑËñõÂÆöË∞îÊñπÁ®ãÔºö\n[!theorem|*] 3-Dimentional Schrodinger Equation $$ i \\hbar \\frac{\\partial \\Psi}{\\partial t}=-\\frac{\\hbar^2}{2 m} \\nabla^2 \\Psi+V \\Psi $$ where $$ \\nabla^2 \\equiv \\frac{\\partial^2}{\\partial x^2}+\\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} $$\n"},{"id":34,"href":"/docs/Physics/Quantum-Mechenics/Wave-Functions-live-in-Hilbert-Space/","title":"Wave Functions Live in Hilbert Space","section":"Quantum Mechenics","content":"\rHilbert Space\r#\rInfinite dimensional vector space, denoted as $L^2(a,b)$, of square-integrable functions on an interval $[a,b]$. $$\\int_a^b |f(x)|^2 , dx \u0026lt; \\infty$$ Inner product defined as: $$\\langle f | g \\rangle = \\int_a^b f^(x)g(x) , dx$$ Note that: $$\\langle f | g \\rangle = \\langle g | f \\rangle^$$ $$\\langle f | f \\rangle = \\int_a^b |f(x)|^2 , dx \\geq 0$$ Also: $$\\langle f | f \\rangle = 0 \\iff f(x) = 0 \\quad\\text{in the interval}\\quad [a,b]$$\nOrthonormal Set ${f_n}$\r#\r$$\\langle f_m | f_n \\rangle = \\int_a^b f_m^*(x)f_n(x),dx = \\delta_{m,n}$$ Completeness: A set of functions ${f_n}$ is complete if any $f(x)$ in the Hilbert space can be expanded as: $$f(x) = \\sum_n c_n f_n(x)$$ If ${f_n}$ is orthonormal, then: $$c_n = \\langle f_n | f \\rangle$$\nObservables and Hermitian Operators\r#\rAn operator $\\hat{Q}$ is Hermitian if:\n$$ \\hat{Q} = \\hat{Q}^{\\dagger} $$\nProperties\r#\rEigenvalues are real. Expectation value $\\langle Q \\rangle$:\n$$ \\langle Q \\rangle = \\langle \\psi | \\hat{Q} \\psi \\rangle = \\langle \\hat{Q}^{\\dagger} \\psi | \\psi \\rangle = \\langle \\psi | \\hat{Q} \\psi \\rangle^{*} $$\nThus,\n$$ \\langle Q \\rangle \\quad \\text{is real} $$\nCheck inner product:\n$$ \\langle f| \\hat{x} g \\rangle = \\int_{-\\infty}^{\\infty} f^*(x) x g(x) , dx $$\nComplex conjugate clearly shows:\n$$ = \\int_{-\\infty}^{\\infty} (x f(x))^* g(x) , dx = \\langle \\hat{x}f | g \\rangle $$\nThus,\n$$ \\hat{x} = \\hat{x}^{\\dagger} \\quad \\Rightarrow \\quad \\text{Hermitian} $$\nEvaluate inner product:\n$$ \\langle f| \\hat{p} g \\rangle = \\int_{-\\infty}^{\\infty} f^*(x) \\left(-i\\hbar \\frac{d}{dx}\\right) g(x) , dx $$\nUsing integration by parts:\n$$ = -i\\hbar \\int_{-\\infty}^{\\infty} dx \\frac{d}{dx}(f^(x)g(x)) + i\\hbar \\int_{-\\infty}^{\\infty} dx \\frac{df^(x)}{dx} g(x) $$\nBoundary term vanishes:\n$$ = -i\\hbar [f^*(x)g(x)]_{-\\infty}^{\\infty} + \\langle \\hat{p} f | g \\rangle, \\quad \\text{with boundary term = 0} $$\nThus:\n$$ \\langle f| \\hat{p} g \\rangle = \\langle \\hat{p} f | g \\rangle \\quad \\Rightarrow \\quad \\hat{p} = \\hat{p}^{\\dagger}, \\quad \\text{Hermitian!} $$\nObservables and Hermitian Operators\r#\rHermitian Operator:\r#\rAn operator $\\hat{Q}$ is Hermitian if:\n$$\\hat{Q} = \\hat{Q}^{\\dagger}$$\nSpectrum of $\\hat{Q}$\r#\rSpectrum: The collection of all eigenvalues $q \\in \\mathbb{R}$. Eigenvalue equation:\n$$\\hat{Q}\\Psi = q\\Psi$$\nwhere:\n$q$ is an eigenvalue. $\\Psi$ represents eigenvectors, eigenstates, or eigenfunctions. Standard Deviation:\r#\rThe uncertainty (standard deviation) $\\sigma$ of an observable $\\hat{Q}$ is given by:\n$$\\sigma^2 = \\langle \\Psi | \\hat{Q}^2 \\Psi \\rangle - \\langle \\Psi | \\hat{Q} \\Psi \\rangle^2$$\nIf $\\Psi$ is an eigenfunction of $\\hat{Q}$:\nEigenvalue equations: $$\\hat{Q}\\Psi = q\\Psi, \\quad \\hat{Q}^2 \\Psi = q^2 \\Psi$$\nThen, the standard deviation becomes:\n$$\\langle \\Psi | \\hat{Q}^2 \\Psi \\rangle = q^2 \\langle \\Psi | \\Psi \\rangle = q^2$$ $$\\langle \\Psi | \\hat{Q} \\Psi \\rangle^2 = (q \\langle \\Psi | \\Psi \\rangle)^2 = q^2$$\nThus:\n$$\\sigma^2 = q^2 - q^2 = 0$$\n\u0026mdash;¬†Physical Interpretation:\r#\rThis means that if we prepare a quantum state to be an eigenstate/eigenvector/eigenfunction of $\\hat{Q}$, then a measurement of $\\hat{Q}$ will return a definite value. In this case, the state $|\\Psi\\rangle$ is called a determinate state.\nExample\r#\rFor $\\hat{H}\\Psi = E\\Psi$, we have:\n$E$ is an eigenvalue (energy level) belonging to the set ${ E }$, which represents all possible energies of the system. $\\Psi$ are the corresponding eigenstates/eigenfunctions of definite energy (stationary states). Example: Energy Eigenvalue Equation\r#\rThe Schr√∂dinger equation for a quantum system is given by:\n$$\\hat{H}\\Psi = E\\Psi$$\nWhere:\n$E$ is an eigenvalue (energy level) belonging to the set ${ E }$, representing all possible energies of the system. $\\Psi$ represents the corresponding eigenstates or eigenfunctions of definite energy, also known as stationary states. Particle on a Ring of Radius $R$\r#\rCoordinate transformation: $$x = R \\cdot \\phi, \\quad (\\text{with } \\phi \\text{ periodic, } \\phi \\sim \\phi + 2\\pi)$$\nMomentum Operator in Circular Coordinates:\r#\r$$\\hat{p} = -i\\hbar\\frac{d}{dx} = -i\\hbar\\frac{\\partial}{R\\partial\\phi} = \\frac{\\hbar}{R}\\left(-i\\frac{\\partial}{\\partial\\phi}\\right)$$\nCheck if $\\hat{Q}$ is Hermitian:\n$$\\langle f|\\hat{Q}g \\rangle \\stackrel{?}{=} \\langle \\hat{Q}f | g \\rangle$$\nHermiticity Check for Operator $\\hat{x}$:\r#\r$$\\langle f|\\hat{x}g \\rangle = \\int_{-\\infty}^{\\infty}f^*(x)xg(x),dx = \\langle \\hat{x}f|g \\rangle \\quad \\Rightarrow \\quad \\hat{x} = \\hat{x}^{\\dagger}$$\nThus, $\\hat{x}$ is Hermitian.\nEigenvalues and Eigenfunctions (Periodic Boundary Conditions):\r#\rFunctions on a ring of radius $R$: periodic with $\\phi$: $$f(\\phi+2\\pi) = f(\\phi)$$ $$g(\\phi+2\\pi) = g(\\phi)$$ Eigenvalue equation for the operator $\\hat{Q}$: $$\\hat{Q}f(\\phi)=q f(\\phi)$$ Solve for $f(\\phi)$: $$-i\\frac{d f(\\phi)}{d\\phi} = q f(\\phi) \\quad\\Rightarrow\\quad f(\\phi) = A e^{i q \\phi}$$ Normalization and Quantization of $q$:\r#\rFrom periodic boundary condition:\n$$f(\\phi + 2\\pi) = A e^{i q (\\phi+2\\pi)} = A e^{i q \\phi} e^{i q 2\\pi} = f(\\phi)$$\nThus,\n$$e^{i q 2\\pi} = 1 \\quad\\Rightarrow\\quad q = 0, \\pm1, \\pm2, \\pm3, \\dots$$\nNormalization condition:\r#\r$$\\int_0^{2\\pi} d\\phi |f(\\phi)|^2 = \\int_0^{2\\pi} d\\phi |A|^2 = |A|^2 \\cdot 2\\pi = 1$$\nThus,\n$$|A|^2 = \\frac{1}{2\\pi} \\quad\\Rightarrow\\quad A = \\frac{1}{\\sqrt{2\\pi}}$$\nFinal Set of Eigenfunctions and Eigenvalues:\r#\r$$f_q(\\phi) = \\frac{1}{\\sqrt{2\\pi}} e^{i q \\phi}, \\quad q = 0, \\pm1, \\pm2, \\dots$$\nEigenvalues for Momentum $\\hat{p}$:\r#\r$$\\frac{\\hbar}{R}q = 0, \\pm\\frac{\\hbar}{R}, \\pm\\frac{2\\hbar}{R}, \\pm\\frac{3\\hbar}{R}, \\dots$$\nHere\u0026rsquo;s the requested content neatly formatted in Markdown with LaTeX notation, using the {align} environment for clarity:\nEigenfunctions of a Hermitian Operator\r#\r$$\\hat{Q}\\psi = q\\psi$$\nDiscrete Spectra: $$\\hat{Q} f = q f$$\nEigenvalues $q \\in \\mathbb{R}$. For two eigenfunctions $f$ and $g$ corresponding to distinct eigenvalues $q$ and $q\u0026rsquo;$, we have: $$\\hat{Q}f = qf, \\quad \\hat{Q}g = q\u0026rsquo;g,\\quad q \\neq q\u0026rsquo;$$ Then: $$\\langle f | \\hat{Q} g \\rangle = \\langle \\hat{Q}f|g \\rangle$$\nBut also: $$q\u0026rsquo;\\langle f|g \\rangle = q\\langle f|g\\rangle \\implies (q - q\u0026rsquo;)\\langle f|g\\rangle = 0$$\nThus, for distinct eigenvalues: $$\\langle f|g\\rangle = 0$$\nContinuous Spectra\r#\rConsider eigenfunctions of the momentum operator on the real line $(-\\infty, +\\infty)$:\n$$-i\\hbar \\frac{d}{dx}f_p(x) = p,f_p(x)$$\nEigenfunctions have the form: $$f_p(x) = A e^{\\frac{i p x}{\\hbar}}$$\nNote that these eigenfunctions are not square-integrable: $$\\int_{-\\infty}^{\\infty}|f_p(x)|^2,dx = |A|^2\\int_{-\\infty}^{\\infty}\\left|e^{\\frac{ipx}{\\hbar}}\\right|^2dx = \\infty$$\nHence, the eigenfunctions corresponding to continuous eigenvalues are not square-integrable functions.\n"},{"id":35,"href":"/docs/Physics/section/","title":"Section","section":"Physics","content":"\rSection\r#\rSection renders pages in section as definition list, using title and description. Optional param summary can be used to show or hide page summary\nExample\r#\r{{\u003c section [summary] \u003e}}\rButtons\rButtons\r#\rButtons are styled links that can lead to local page or external link. Example\r#\r{{\u003c button relref=\"/\" [class=\"...\"] \u003e}}Get Home{{\u003c /button \u003e}}\r{{\u003c button href=\"https://github.com/alex-shpak/hugo-book\" \u003e}}Contribute{{\u003c /button \u003e}}\rGet Home\rContribute\r"},{"id":36,"href":"/docs/Physics/section/buttons/","title":"Buttons","section":"Section","content":"\rButtons\r#\rButtons are styled links that can lead to local page or external link.\nExample\r#\r{{\u003c button relref=\"/\" [class=\"...\"] \u003e}}Get Home{{\u003c /button \u003e}}\r{{\u003c button href=\"https://github.com/alex-shpak/hugo-book\" \u003e}}Contribute{{\u003c /button \u003e}}\rGet Home\rContribute\r"},{"id":37,"href":"/posts/2025-Topology-Summer-Research/","title":"2025 Topology Summer Research","section":"Blog","content":"\rSummer Application:\r#\rhttps://sgi.mit.edu/about-geometry-processing/\nApril\r#\rhttps://topologyandgeometry.iu.edu/gstgc25/\nMay\r#\rhttps://topology.franklinresearch.uga.edu/2025GITC\nPast Year\r#\rhttps://sites.google.com/view/princetonrtg2023/mini-conferences\n"},{"id":38,"href":"/posts/Image-to-3D-Model/","title":"Image to 3 D Model","section":"Blog","content":"\rConverting 2D Anime-Style Clothing to 3D: Tools \u0026amp; Workflow\r#\rCreating 3D clothing from 2D anime-style references (like Genshin Impact outfits) is now faster with AI-assisted tools, though manual refinement is often needed for the best results. This guide focuses on clothing conversion ‚Äì taking 2D images of robes, armor, or accessories and turning them into stylized 3D meshes with clean topology. We‚Äôll explore the top AI tools and workflows (as of 2025) and outline a step-by-step process compatible with Blender.\nKey Requirements for 2D-to-3D Clothing Conversion\r#\rStylized Fidelity: The 3D clothing should match the anime/Genshin Impact aesthetic of the concept art (shapes, folds, and design details). Optimized Topology: Meshes need clean, animation-friendly topology (proper edge loops, reasonable polycount) for attaching to a rigged character. Texture \u0026amp; Detail: Preserve clothing details (patterns, trims, armor segments) either as modeled geometry or textures/normal maps. Rigging Compatibility: The generated clothing must fit the existing character and allow weight painting or rig transfers so it deforms correctly during animation. Minimal Restrictions: Tools that allow creative freedom (no strict content rules) are preferred so any custom outfit design can be used. AI-Powered Tools for Image-to-3D Clothing Conversion\r#\rModern AI tools can convert a single 2D image into a 3D model in minutes (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide). These are particularly useful to get a base 3D mesh of a clothing piece quickly, which can then be refined. Below are some of the best options:\nMeshy AI (Image to 3D): A popular AI 3D model generator with an image-to-3D feature and even a Blender plugin (\rMeshy AI - The #1 AI 3D Model Generator for Creators) (\rMeshy AI - The #1 AI 3D Model Generator for Creators). Meshy supports different art styles (including anime) for output (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rMeshy AI - The #1 AI 3D Model Generator for Creators). You upload concept art or reference photos and get a 3D model with textures. Pros: Fast cloud generation, supports versatile art styles (can capture stylized looks) (\rMeshy AI - The #1 AI 3D Model Generator for Creators), exports to common formats (OBJ, FBX, GLB, etc.) for easy Blender import (\rMeshy AI - The #1 AI 3D Model Generator for Creators). Cons: Paid service (free tier available with limits), and results may require cleanup if topology is dense or if some parts are inaccurate.\nMazing AI / 3DFY.ai: Services that turn single images into 3D models with a focus on realism and high quality (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide). Mazing (an e-commerce oriented tool) emphasizes automatic texturing and real-time optimization (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide). 3DFY.ai similarly promises high-quality results from one image (\r3DFY.ai). Pros: Quick image to model conversion; optimized for product visuals. Cons: May be geared towards realistic objects; stylized anime clothing might need additional editing to match the art style.\nKaedim and Alternatives (Tripo 3D, Alpha3D): Kaedim is an AI-assisted service where you upload an image (even a sketch or concept) and their pipeline (ML + human touch-ups) delivers a 3D model (\rKaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify). Tripo 3D offers a similar ‚Äúsingle image to 3D in seconds‚Äù solution with emphasis on detailed geometry and textures (\rKaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify). Alpha3D provides image-to-3D generation but currently only for certain categories (e.g. shoes, furniture) (\rTransform text and 2D images into 3D assets with generative AI for free - Alpha3D). Pros: These services deliver production-ready assets with textures and decent topology, suitable for game engines (\rKaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify). Cons: They are paid services (some with subscriptions) and may have category limits. Quality can vary ‚Äì often a good starting point but still might need retopology for optimal loops.\nThe New Black (AI Fashion Generator): A specialized tool for fashion design that can turn an outfit image into a realistic 3D clothing model (\rAI Fashion Features | Clothing Design). It‚Äôs geared toward apparel designers (e.g. previewing how a garment looks in 3D). Pros: Focused on clothing, likely good with fabric details like folds and drape. Cons: Primarily aimed at realistic fashion; you might need to simplify or stylize the output for anime characters. Also, it may output standalone clothing on a generic avatar that you‚Äôll have to refit to your character.\nHunyuan 3D (Tencent): An AI model available via HuggingFace that generates 3D meshes from an image (used in the community alongside Meshy) (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums). It‚Äôs free to try and can handle characters or clothes. Pros: Free and accessible; known to work for generating a rough clothed figure mesh. Cons: The output might be a combined human+clothes mesh (if the input was a full character image) and will definitely require manual retopology and separation of the clothing. Good for getting the overall shape of a complex outfit, but not a final game-ready mesh.\n‚ÄúPic-to-3D Mesh‚Äù Blender Add-on: An add-on that integrates AI image-to-3D conversion directly in Blender (\rTop AI Tools for Model Generation on Blender 3D - Vagon). You can input a reference image (e.g. a front view of a costume) and it generates a detailed 3D mesh inside Blender (\rTop AI Tools for Model Generation on Blender 3D - Vagon). Pros: Fully inside Blender ‚Äì no need to use external apps; straightforward UI and quick conversion with just a few clicks (\rTop AI Tools for Model Generation on Blender 3D - Vagon). This is useful to instantly get a mesh that you can start editing in the same session. Cons: Being relatively new, results can be hit-or-miss on complex armor or multi-layer outfits; likely works best for simpler garments or accessory pieces.\nPixelModeler AI (Blender Add-on): A unique workflow where you paint on a 2D canvas in Blender and an AI generates a corresponding 3D mesh (\rPixelModeller AI - Blender Market) (\rPixelModeller AI - Blender Market). This can be used by painting the silhouette or even a depth map of the clothing; the addon will create a solid mesh from it. Generated models are watertight, UV-mapped, and come with vertex colors (a basic texture) (\rTop AI Tools for Model Generation on Blender 3D - Vagon), ready for further detailing. Pros: Gives a lot of control ‚Äì you essentially guide the shape by painting, so it‚Äôs AI-assisted modeling rather than fully automatic. No external service needed (the AI model runs locally) (\rPixelModeller AI - Blender Market). Cons: There is a learning curve to painting effective guides. It won‚Äôt automatically produce intricate patterns ‚Äì you‚Äôll need to add those via texture or additional modeling.\nComparison of Key Tools\r#\rBelow is a quick comparison of these tools relevant to 2D-to-3D clothing conversion:\nTool/Service Type Output Quality Topology \u0026amp; UVs Integration with Blender Notes Meshy AI Cloud AI (image‚Üí3D) High detail; supports anime style (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rMeshy AI - The #1 AI 3D Model Generator for Creators) Decent mesh; textured output (may need retopo) Blender plugin available (\rMeshy AI - The #1 AI 3D Model Generator for Creators) Fast; paid (free trial available). Mazing / 3DFY.ai Cloud AI (image‚Üí3D) Photorealistic focus, good folds Optimized for real-time (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide); provides textures Exports standard formats (OBJ/FBX) Great for realism; stylization may require tweaks. Kaedim Cloud AI (+human) Custom models from concept art Cleaned by artists; quad topology Download to import in Blender Consistent results; subscription-based. Tripo 3D Cloud AI (image‚Üí3D) Fast generation, detailed textures ([Kaedim Alternatives in 2025 Best Kaedim Alternatives - Toolify](\rhttps://www.toolify.ai/alternative/kaedim#:~:text=,model%20generation)) Unknown topology quality Exports GLB/OBJ The New Black (Fashion) Cloud AI (image‚Üí3D) Realistic garment on avatar Likely well-formed cloth mesh Export capabilities (likely OBJ/FBX) Fashion design oriented; may need rigging after import. Hunyuan (Tencent) Cloud AI (image‚Üí3D) Full character mesh with clothes High-poly, needs retopo OBJ export via HuggingFace demo Free; good for concept shape (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums). Pic-to-3D (Blender) Blender Add-on Good for single-object models (\rTop AI Tools for Model Generation on Blender 3D - Vagon) Mesh quality varies; UV depends Inside Blender (no export needed) Convenient, no coding needed. PixelModeler (Blender) Blender Add-on User-guided, can achieve high detail Watertight \u0026amp; UV-mapped (\rTop AI Tools for Model Generation on Blender 3D - Vagon) Inside Blender Interactive painting workflow. Table: AI-Based 2D‚Üí3D Clothing Tools ‚Äì Comparison (performance as of 2025).\nAI-Assisted + Manual Workflow Strategies\r#\rFully automated results often need human improvement. In practice, the best quality comes from combining AI generation with manual modeling (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide). Here are some AI-assisted workflows that clothing artists use:\nImage Inpainting + Mesh Generation: One clever approach is to use AI image tools to conceptually dress your character, then extract a model. For example, a community-suggested workflow is: render your character‚Äôs base body in T-pose, use an AI image editor (like Stable Diffusion inpainting or Photoshop‚Äôs generative fill) to ‚Äúpaint‚Äù new clothes onto the image, isolate just the garment in the edited image, then input that into an image-to-3D tool (Meshy or Hunyuan) to get a 3D mesh (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums). This way, the AI helps create a consistent design on the body and another AI turns it into geometry. You‚Äôd still need to retopologize and UV map the result manually (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums), but it jumpstarts the modeling process for complex costumes.\nDepth Map Extraction: If you have a front-view concept art of the outfit, you can generate a depth map (using AI like MiDaS or Stable Diffusion depth estimation). That depth map can be used to displace a plane or guide a mesh generation. Tools like PixelModeler AI automate this: they generate a depth internally from the image and produce a mesh (\rPixelModeller AI - Blender Market). The output will capture the relief (folds, protrusions) from the concept art, though you‚Äôll have to model or guess the back side of the garment. This method is useful for armor pieces or relief details on clothing that are visible in the concept.\nTemplate-Based Generation (Parametric): Some solutions use parametric templates plus AI for customization. Sloyd.ai, for instance, combines a library of human-made base models with AI adjustments, ensuring the result is game-ready with UV maps and LODs generated, and optimized meshes (\rSLOYD AI: A 3D Automation Tool To Generate Game-Ready Assets ‚Äì startupanz.com). If there are clothing templates (e.g. a generic T-shirt, jacket, dress) you can morph those to roughly match your design and let the tool handle topology. This is semi-manual: you pick the base closest to your design and tweak. Note: As of 2025, parametric generators like Sloyd have many props and environment assets; clothing templates might be limited, but the approach guarantees clean topology if a template fits your needs.\nManual Sculpt with AI Reference: Another assisted route is using the AI output as a reference or base mesh and then manually sculpting over it. For example, you can take a coarse mesh from an AI, bring it into Blender, and use multiresolution sculpting or retopology tools (like Quad Remesher or Blender‚Äôs shrinkwrap) to impose a clean topology that follows the AI model‚Äôs shape. The AI model essentially serves as a 3D concept sketch. You can also project the texture from the AI model (if it provided one) onto your new topology for a starting point.\nManual Tools for 3D Clothing Creation\r#\rWhile AI is speeding things up, manual modeling tools are still crucial, especially for achieving the cleanest results and stylized looks:\nMarvelous Designer / CLO3D: These are industry-standard tools for designing clothing using pattern-based simulation. You draw 2D garment patterns, sew them, and the software simulates the cloth on a avatar model ‚Äì perfect for creating natural folds and drapes. Pros: Extremely high fidelity cloth behavior; great for layered outfits, pleats, ruffles, etc. You can match an anime costume by designing similar patterns. Marvelous can even auto-generate PBR texture maps like normal and opacity for details (\r[Tips\u0026amp;Tricks] Discover Better Workflow with Marvelous Designer and Unreal Engine ‚Äì Marvelous Designer). Cons: The meshes are triangulated and high-poly (since it‚Äôs focused on simulation). You will need to retopologize the garment for use in a game or realtime engine (\rRetopology of Marvelous Designer Clothes in Blender - YouTube). Marvelous has introduced some retopo tools (EveryWear Auto-Retopology) and can even rig garments, but often external retopo (using Blender or ZBrush) gives more control. Despite not being AI, Marvelous is frequently recommended for creating custom outfits (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums) because of the quality of the result. The typical workflow is simulate in Marvelous ‚Üí export OBJ ‚Üí retopo in Blender ‚Üí transfer to character rig. (\r[Tips\u0026amp;Tricks] Discover Better Workflow with Marvelous Designer and Unreal Engine ‚Äì Marvelous Designer) Example of a stylized 3D outfit created with cloth simulation. (This fairy-like garment was designed and simulated in Marvelous Designer, showcasing layered fabric, ruffles, and realistic folds.)\nBlender‚Äôs Sewing/Cloth Tools: If you prefer open-source, Blender itself has cloth simulation and addons like Garment Tool that mimic Marvelous‚Äôs pattern sewing approach. You can import your character into Blender, model garment panels (or even trace them from reference images), then use cloth physics to drape them. The result can then be applied as a shape key or applied mesh. You‚Äôll still need to manually refine the mesh topology. Blender‚Äôs sculpting tools (cloth brush, slide relax, etc.) can also help adjust folds. This approach is manual and requires skill, but no additional cost.\nDirect Poly Modeling: For hard-surface armor pieces or very structured outfits (like a mech suit or a rigid breastplate), classic poly modeling or box modeling in Blender might be the way to go. You can use the 2D image as a reference in the background and model the clothing piece by piece (ensuring proper topology as you go). This is time-consuming but yields the cleanest meshes. You might use AI just to generate normals or texture details in this case, rather than the mesh.\nRetopology \u0026amp; Refinement Tools: No matter which initial method you choose, retopology tools are vital for clothing. Blender has a PolyBuild and Snap-to-face retopo workflow, and add-ons like RetopoFlow can speed it up. If you have ZBrush, ZRemesher can quickly re-mesh a triangulated Marvelous output into quads, which you can then tweak. There are also auto-retopology AI in development ‚Äì for instance, some research tools attempt to auto-retopo meshes with neural networks, but in practice most artists still do this part manually or with traditional algorithms. The goal is to end up with edge loops around openings (neck, arm holes) and ideally follow the flow of fabric folds with the topology for deformation.\nRecommended Workflow (Step-by-Step)\r#\rBringing it all together, here is a step-by-step workflow to convert a 2D outfit into a 3D mesh and attach it to your Blender character, using the best of AI and manual tools:\n1. Gather Reference Images: Ideally have the concept art or reference of the clothing from as many angles as possible. A front view is usually required for AI tools; a side or back view (if available) will help during modeling or can be fed into some tools for better accuracy. If only a front view exists, be prepared to interpret the design for the unseen parts.\n2. Choose an AI Generation Method for Base Mesh: For a head start, pick one of the AI approaches:\nOption A: Use Meshy AI (or similar service) to upload the clothing image and generate a 3D model. Download the result (e.g. as a .glb or .obj) when ready (\rMeshy AI - The #1 AI 3D Model Generator for Creators).\nOption B: In Blender, install the Pic-to-3D Mesh addon and run it on your reference image to get a mesh (\rTop AI Tools for Model Generation on Blender 3D - Vagon).\nOption C: If the outfit is very complex or you want a full mannequin with clothing, try the Hunyuan 3D demo by providing an image of the clothed character; then extract the clothing mesh from the output.\nOption D: If you have a concept sketch, consider Kaedim/Tripo services for a perhaps cleaner base model (they might return the model next day or in a couple of hours, which you can then use).\nRegardless of option, don‚Äôt expect a perfect final model ‚Äì treat this as a rough draft or proof of concept in 3D. It should capture the overall shape and major details of the clothing.\n3. Import and Inspect in Blender: Bring the generated 3D model into Blender. Center and scale it to your character. At this stage:\nCheck the mesh density and topology. Are there a lot of uneven triangles or random bumps? Check if all parts of the outfit are present. Sometimes single-view reconstructions leave holes or undefined backs. You may need to patch holes (Blender‚Äôs Fill or Grid Fill can help) or even mirror parts of the mesh if symmetry can be assumed. If the tool provided textures, apply them to see the look. However, for anime style, you might later hand-paint textures or use simple materials, so textures are optional. 4. Retopologize the Clothing Mesh: This is crucial for optimization. You can use Blender‚Äôs retopology tools to create a new mesh over the AI mesh:\nAdd a shrinkwrap modifier on a new mesh and model low-poly geometry that tightly wraps the AI model. Focus on quads and logical edge flow (e.g. edge loops around cuffs, hemlines, and along seams). Alternatively, use an auto-retopo tool: for example, Instant Meshes (free tool) or Quad Remesher (paid) to get a quick quad mesh. You might still tweak the output by hand. Ensure the retopo‚Äôd mesh has proper thickness where needed (you can solidify later if it‚Äôs cloth, but parts like armor might be modeled as solid pieces). UV unwrap the new mesh if not already UV‚Äôd. Good UVs are needed for texturing anime-style details (like emblems or gradients on the fabric). 5. Fit and Attach to the Character: Place the new clothing mesh on the character in the correct pose (usually T-pose or A-pose matching the rig). To attach:\nUse Blender‚Äôs Transfer Weights: parent the clothing to the armature (with empty groups), then select the body, then clothing, and use Weight Transfer (source: body, destination: clothing). This copies the rig weights so the clothing will move with the body (\rHow separte clothes for Animatoion? - CG Cookie). Check deformation by posing the character. Likely you will need to clean up weights (for instance, ensure sleeves move with the arms, etc. without too much clipping). If the clothing is very close to the body, you might need to delete hidden faces of the character under the clothes to avoid mesh clipping in tight areas (e.g. remove torso polygons under a shirt). For rigid pieces (like armor plates), you may instead want to assign them to a specific bone and keep them rigid or use a bone parent for that object. 6. Detail and Texture: Now polish the visual fidelity:\nSculpt or model finer folds that the AI may have missed. You can use Blender‚Äôs sculpt mode with the cloth brush or crease brush to imprint additional wrinkle lines where appropriate. Add thickness to cloth if it‚Äôs just a single surface (Solidify modifier). Stylized outfits often have a bit of thickness at edges (e.g. a coat lapel). Texture Painting: For anime style, a lot of detail can come from textures (like painted shadows or highlights, stylized fabric patterns). You can paint directly in Blender or use Substance 3D Painter. If the original 2D image has patterns (say, a symbol on the back of a cape), use it as a reference or even project it onto your UV map. Generate normal maps if needed. For example, if the outfit has an engraved design or stitching that is too fine to model, you can paint a height map and bake it to a normal map. Some AI tools can assist in generating texture maps from descriptions (e.g. Meshy has an AI texturing feature) (\rKaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify), which could be used to create stylized fabric textures by prompting. 7. Iterate and Refine: After these steps, you should have the clothing on the character, but iteration is key:\nGo back and forward between sculpting, adjusting topology, and tweaking weights until the clothing looks right and deforms well in various poses. If something is off compared to the concept art (maybe the AI misunderstood a part of the design), you might have to model that part manually. It‚Äôs common to model small accessories or intricate pieces separately (for example, a belt buckle or a brooch) and then attach them. LOD (Level of Detail): If this is for a game, consider making lower-poly versions or at least ensure the topology is efficient. AI meshes can be decimated or re-generated at lower detail if needed. 8. Final Check and Export: Once satisfied, you can integrate the clothed character into your project. Because we focused on Blender compatibility, you can continue to animate or render in Blender. If exporting to a game engine, export the character with the outfit as FBX/GLTF with the armature. Double-check that all parts are properly bound and that textures are packed or exported.\nThroughout this process, remember that AI is a helper, not a replacement for your skill. Even the best AI-generated model benefits from a human artist‚Äôs eye for clean topology and style accuracy. As one guide noted, AI tools speed up getting a base, but ‚Äúas AI is not perfect, [enhancement] is recommended‚Äù to reach production quality (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide). Don‚Äôt hesitate to do manual touch-ups ‚Äì the goal is a high-quality anime-style outfit that looks like it was hand-crafted for the character.\nConclusion\r#\rConverting 2D anime-style clothing into 3D is becoming more accessible thanks to AI innovations. Tools like Meshy, PicTo3D, and others can generate a quick 3D draft of an outfit from a single concept image (\rTop AI Tools for Model Generation on Blender 3D - Vagon) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide), saving hours of initial modeling. However, the best results come from a hybrid workflow: leveraging AI for speed and then applying traditional modeling techniques for accuracy and clean topology. This collaborative approach (AI plus human) is highlighted as the future of 3D content creation (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) ‚Äì AI handles the heavy lifting of shape prediction, while the artist refines and stylizes the final asset.\nBy carefully choosing the right tools and following a structured workflow, you can efficiently bring 2D costume designs into the 3D world, ready to be worn by your Blender character. The combination of AI-assisted generation and manual refinement ensures you get both speed and quality ‚Äì detailed Genshin Impact-style clothing that not only looks great but is also rigged and optimized for your creative projects.\nSources\r#\rMazingXR Blog ‚Äì ‚ÄúConverting 2D Images to 3D Models with AI‚Äù (Feb 2025) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) (\rConverting 2D Images into 3D Models with AI: The step-by-step Guide) Vagon Blog ‚Äì ‚ÄúTop AI Tools for Model Generation on Blender 3D‚Äù (\rTop AI Tools for Model Generation on Blender 3D - Vagon) (\rTop AI Tools for Model Generation on Blender 3D - Vagon) Daz3D Forums ‚Äì ‚ÄúDo you know an AI to create cloth and outfit?‚Äù (Jan 2025) (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums) (\rDo you know an AI to create cloth and outfit? - Daz 3D Forums) StartupAnz ‚Äì ‚ÄúSloyd AI: Game-Ready 3D Asset Generation‚Äù (\rSLOYD AI: A 3D Automation Tool To Generate Game-Ready Assets ‚Äì startupanz.com) Alpha3D.io ‚Äì ‚Äú2D image to 3D model generation (limitations)‚Äù (\rTransform text and 2D images into 3D assets with generative AI for free - Alpha3D) Toolify AI ‚Äì ‚ÄúKaedim Alternatives in 2025‚Äù (\rKaedim Alternatives in 2025 | Best Kaedim Alternatives - Toolify) (Tripo3D features) Marvelous Designer Official Support ‚Äì Workflow tips (pleat and texture generation) "},{"id":39,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.4-Sufficient-Conditions-for-Differentiability/","title":"6.4 ÂèØÂæÆÂàÜÊÄßÁöÑÂøÖË¶ÅÊù°‰ª∂","section":"Á¨¨ÂÖ≠Á´† ÂèØÂæÆÊò†Â∞Ñ","content":"\r1. Necessary Condition for Differentiability\r#\rRecall: A necessary condition for differentiability: $$ \\boxed{ f \\text{ differentiable} \\Rightarrow f \\text{ is continuous} } $$ Continuity is a requirement. 2. Sufficient Conditions for Differentiability\r#\r(a) Partial derivatives and differentiability f differentiable $\\Rightarrow$ continuity + partials exists conditions + partials exists $\\Rightarrow f$ differentiable (?)\nEx. 1\r#\rConsider the function defined as:\n$$ f(x,y) = \\begin{cases} \\frac{xy}{x^2 + y^2}, \u0026amp; (x,y) \\neq (0,0) \\ 0, \u0026amp; (x,y) = (0,0) \\end{cases} $$\nClaim 1: ( f ) is continuous at ( (0,0) ).\r#\rWe analyze the limit:\n$$ |xy| \\leq \\frac{1}{2} (x^2 + y^2) $$\nwhich implies:\n$$ f(x,y) \\to 0 \\quad \\text{as} \\quad (x,y) \\to (0,0) $$\nThus, ( f ) is continuous at ( (0,0) ).\nClaim 2: Compute partial derivatives at ( (0,0) )\r#\r$$ \\frac{\\partial f(0,0)}{\\partial x} = \\lim_{x \\to 0} \\frac{f(x,0) - f(0,0)}{x} = \\lim_{x \\to 0} \\frac{0}{x} = 0 $$\n$$ \\frac{\\partial f(0,0)}{\\partial y} = \\lim_{y \\to 0} \\frac{f(0,y) - f(0,0)}{y} = \\l\n"},{"id":40,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.5-The-Chain-Rule/","title":"6.5 ÈìæÂºèÊ≥ïÂàô","section":"Á¨¨ÂÖ≠Á´† ÂèØÂæÆÊò†Â∞Ñ","content":"[ ]\n"},{"id":41,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.6-Product-Rule-and-Gradients/","title":"6.6 ‰πòÁßØÊ≥ïÂàô‰∏éÊ¢ØÂ∫¶","section":"Á¨¨ÂÖ≠Á´† ÂèØÂæÆÊò†Â∞Ñ","content":"[ ]\n"},{"id":42,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.9-Taylor-Higher-Dimentions/","title":"6.9 Ê≥∞ÂãíÂÖ¨ÂºèÁöÑÈ´òÁª¥ÂΩ¢Âºè","section":"Á¨¨ÂÖ≠Á´† ÂèØÂæÆÊò†Â∞Ñ","content":"$$ R_{r-1} = \\frac{1}{r!} D^{(r)} f(\\xi) (x-x_0, \\dots, x-x_0) $$ And satisfying $$ \\frac{R_{r-1}(x_0)}{|x - x_0|^{r-1}} \\to 0 \\quad \\text{as} \\quad x \\to x_0. $$\n\\begin{proof} Consider 1-variable function: $$ g(t) = f(x_0 + t(x - x_0)), \\quad (a, b) \\to \\mathbb{R} $$ for $t\\in (a, b)$ with $[0, 1] \\subset (a, b)$.\nApplying Taylor\u0026rsquo;s theorem to $g(t)$: $$ \\begin{align} g(1) \u0026amp;= g(0) + g\u0026rsquo;(0)(1-0) + \\frac{g\u0026rsquo;\u0026rsquo;(0)}{2!} (1-0)^2 + \\dots + \\frac{g^{(r-1)}(0)}{(r-1)!} (1-0)^{r-1} + R_{r-1}\\ f(x) \u0026amp;= f(x_0) + \\sum_{k=1}^{r-1} \\frac{g^{(k)}(0)}{k!} + \\frac{1}{r!} g^{(r)}(\\tilde{c}), \\quad \\tilde{c} \\in [0,1] \\end{align} $$ By chain rule, $$ g\u0026rsquo;(t) = Df(\\varphi(t)) \\cdot \\varphi\u0026rsquo;(t) $$ $$g\u0026rsquo;(0) = Df(x_0) (x - x_0) $$ \\end{proof}\n#Example\r#\rDetermine the $2\\text{nd}$ order Taylor formula for $$f(x,y)=e^{(x-1)^{2}}\\cos (y)\\quad \\text{at},(1,0)$$ Solution (compute partials): $$ \\begin{align} \\frac{\\partial f}{\\partial x} \u0026amp;= e^{(x-1)^2} 2(x-1) \\cos y, \\quad \\frac{\\partial f}{\\partial y} = -e^{(x-1)^2} \\sin y, \\ \\frac{\\partial^2 f}{\\partial x^2} \u0026amp;= 2 e^{(x-1)^2} \\cos y + 4(x-1)^2 e^{(x-1)^2} \\cos y, \\ \\frac{\\partial^2 f}{\\partial x \\partial y} \u0026amp;= -2 (x-1) e^{(x-1)^2} \\sin y, \\qquad \\frac{\\partial^2 f}{\\partial y^2} = -e^{(x-1)^2} \\cos y. \\end{align} $$\nTaylor\u0026rsquo;s Formula: Let $h = x - x_0 = (x,y) - (1,0)$, then we have\n$$ \\boxed{f(x,y) = f(1,0) + \\mathbb{D}f(1,0)(h) + \\frac{1}{2} \\mathbb{D}^2 f(1,0)(h,h) + R_2} $$ $$ f(1,0) = 1, \\quad \\mathbb{D}f(1,0) = (0 \\quad 0), $$ $$ \\mathbb{D}^2 f(1,0) = \\begin{bmatrix} 2 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{bmatrix} $$ Evaluating the second-order term: $$ \\begin{align} \\mathbb{D}^2 f(1,0)(h,h) \u0026amp;= \\begin{bmatrix} x-1 \u0026amp; y \\end{bmatrix} \\begin{bmatrix} 2 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{bmatrix} \\begin{bmatrix} x-1 \\ y \\end{bmatrix} \\ \u0026amp;= 2(x-1)^2 - y^2 \\end{align} $$\nThus, $$f(x,y) = 1 + \\frac{1}{2} (2(x-1)^2 - y^2) + R_2 $$\n3 Maximum and Minimum Problem in $\\mathbb{R}^n$\r#\r3.1 Introduction\r#\rQ: Given $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$,\nhow do we find (local) max or min points for $f$ in $A$?\nRecall 1-D case: $f: (a,b) \\to \\mathbb{R}$\nA local max / min point (or extreme point) $x_0$ must be a critical point: $$ \\boxed{f\u0026rsquo;(x_0) = 0 \\quad \\text{or\\quad DNE}}\n$$ 3.2 Second Derivative Test (for a critical point)\r#\r$$ \\begin{aligned} f\u0026rsquo;\u0026rsquo;(x_0) \u0026gt; 0 \u0026amp;\\Rightarrow x_0 \\text{ is a local minimum} \\ f\u0026rsquo;\u0026rsquo;(x_0) \u0026lt; 0 \u0026amp;\\Rightarrow x_0 \\text{ is a local maximum} \\end{aligned} $$\n4. Necessary Condition for Extreme Points in $\\mathbb{R}^n$\r#\rDefinition: Let $f: A \\subset \\mathbb{R}^n \\to \\mathbb{R}$.\nA point $x_0 \\in A$ is a local minimum if: $$ \\exists \\delta \u0026gt; 0 \\text{ such that } | x - x_0 | \u0026lt; \\delta, x \\in A \\Rightarrow f(x) \\geq f(x_0) $$ A point $x_0 \\in A$ is a local maximum if: $$ \\exists \\delta \u0026gt; 0 \\text{ such that } | x - x_0 | \u0026lt; \\delta, x \\in A \\Rightarrow f(x) \\leq f(x_0) $$\n[!theorem|6.9.2] If $f: A \\to \\mathbb{R}$ is differentiable at $x_0$, and if $x_0 \\in A$ is an extreme point for $f$, then $x_0$ is a critical point, i.e., $$ Df(x_0) = 0. $$\nRemark\r#\rThe condition $\\mathbb{D}f(x_0) = 0$ is necessary, but not sufficient!\nExample\r#\rLet $f(x,y) = x^2 - y^2$, then $\\mathbb{D}f(0,0) = 0$, but $(0,0)$ is a saddle point.\n\\begin{proof}\nAssume $Df(x_0) \\neq 0$.\nThen, there exists $v \\in \\mathbb{R}^n$ such that $Df(x_0)(v) = c \u0026gt; 0$. By the definition of differentiability, choose $\\delta \u0026gt; 0$ such that: $$ | f(x_0 + h) - f(x_0) - Df(x_0)(h) | \u0026lt; \\frac{c}{2 | v |} | h | $$ for all $| h | \u0026lt; \\delta$.\nNow, choose $h = \\lambda v$ with $\\lambda \u0026gt; 0$ and $| h | \u0026lt; \\delta$, then: $$ \\begin{cases} f(x_0 + \\lambda v) - f(x_0) \u0026gt; 0 \\ f(x_0 - \\lambda v) - f(x_0) \u0026lt; 0 \\end{cases} $$ This establishes the desired contradiction. \\end{proof}\n"},{"id":43,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1-Inverse-Function-Theorem/","title":"7.1 ÂèçÂáΩÊï∞ÂÆöÁêÜ","section":"Á¨¨‰∏ÉÁ´† ÈÄÜÂáΩÊï∞ÂíåÈöêÂáΩÊï∞ÂÆöÁêÜ","content":"\r7.1 Inverse Function Theorem (IFT)\r#\rTwo lines of ideas:\nA: CMP $‚áí$ Inverse FT $‚áí$ Applications in ODE\nB: IFT $‚áí$ Implicit FT $‚áí$ Local behavior, extreme problems\nI. Inverse Function Theorem\r#\r1. Linear Case\r#\rConsider a linear map, $y = f(x): \\mathbb{R}^n \\to \\mathbb{R}^n$.\n$$ x = (x_1, x_2, \\dots, x_n)^T $$\nGiven $y \\in \\mathbb{R}^n$, $f(x)$ is a linear system of equations:\n$$\\begin{aligned} y_1 \u0026amp;= a_{11} x_1 + a_{12} x_2 + \\dots + a_{1n} x_n \\\\ y_2 \u0026amp;= a_{21} x_1 + a_{22} x_2 + \\dots + a_{2n} x_n \\\\ \u0026amp;\\vdots \\\\ y_n \u0026amp;= a_{n1} x_1 + \\dots + a_{nn} x_n \\end{aligned}$$\nor $$ A_{n\\times n}X_{n\\times 1} = Y_{n\\times 1} $$\n[!assumption|*] $$X \\text{ has a unique solution} \\Longleftrightarrow \\det(A) \\neq 0.$$\nIn this case, the solution is given by: $$ X = A^{-1} Y $$ Thus, the inverse function satisfies: $$ f^{-1} \\circ f = \\text{Identity} $$ The inverse theorem for $y = f(x)$:\n$$ f(f^{-1}(y)) = A A^{-1} y = y $$\nQuestion: When can we solve a nonlinear system?\r#\rWe consider a system of nonlinear equations: $$ \\begin{cases} f_1(x_1, x_2, \\dots, x_n) = y_1 \\\\ f_2(x_1, x_2, \\dots, x_n) = y_2 \\\\ \\quad \\vdots \\\\ f_n(x_1, x_2, \\dots, x_n) = y_n \\end{cases} $$ or equivalently, $$ f(x) = y $$\n2. The Inverse of a General Function\r#\rNotation:\nLet $y = f(x): A \\subset \\mathbb{R}^n \\to \\mathbb{R}^n$ be a diffeomorphism.\n$$ y = (y_1, y_2, \\dots, y_n) $$\nwhere\n$$ y_i = f_i(x_1, x_2, \\dots, x_n) $$\nThe Jacobian determinant of $f$ at $x$ is:\n$$ \\det \\left( \\frac{\\partial f_i}{\\partial x_j} \\right) $$\n[!theorem|7.1.1] Let $y = f(x): A \\subset \\mathbb{R}^n \\to \\mathbb{R}^n$ be of class $C^1$. Suppose $x_0 \\in A$ and $\\det(Df(x_0)) \\neq 0$. Then there exists a neighborhood $U$ of $x_0$ and a neighborhood $W$ of $y_0 = f(x_0)$ such that:\n$f: U \\to W$ has an inverse $f^{-1}: W \\to U$. $f^{-1}$ is of class $C^1$. $D(f^{-1}(y)) = (Df(x))^{-1}$ for all $y \\in W$ at $y = f(x)$. Visualization:\r#\r$y = f(x)$ maps from $U$ to $W$. $x = f^{-1}(y)$ gives the inverse mapping from $W$ back to $U$. Recall: Contraction Mapping Principle (CMP)\r#\rLet $\\mathbb{X}$ be a complete metric space and let\n$$ \\varphi: \\mathbb{X} \\to \\mathbb{X} $$\nbe a function satisfying a contraction condition for some constant $k$ with $0 \u0026lt; k \u0026lt; 1$:\n$$ d(\\varphi(x), \\varphi(y)) \\leq k \\cdot d(x,y), \\quad \\forall x,y \\in \\mathbb{X}. $$\nThen, there exists a unique fixed point $X^*$ such that:\n$$ \\varphi(X^) = X^. $$\nProof of the Inverse Function Theorem (IFT)\r#\rStep 1: Reductions\r#\r(a) May assume that the Jacobian matrix at $x_0$ is the identity: $$ D f(x_0) = I. $$ In fact, define the transformation: $$ T = D f(x_0). $$ Then, we can consider a new function:\n$$ \\tilde{f} = T^{-1} \\circ f. $$\nThus,\n$$ D(\\tilde{f})(x_0) = I. $$\n(b) Main assumption:\n$$ x_0 = f^{-1}(y_0). $$\nTo see this, define:\n$$ h(x) = f(x) - f(x_0). $$\nThen,\n$$ D h(x_0) = D f(x_0) - D f(x_0) = 0. $$\nIf $h^{-1}$ exists, then $y = f(x)$ can be solved as:\n$$ f(x) = h(x) + f(x_0) = y. $$\nThus, the inverse function satisfies:\n$$ x = x_0 - h^{-1}(y - f(x_0)). $$\nStep 2: Existence of the Inverse Function\r#\r(a) Setup: By the reduction above, we assume:\n$$ x_0 = 0, \\quad y_0 = f(x_0) = 0, \\quad D f(x_0) = I. $$\nNeed to show:\nThere exist neighborhoods $U$ and $W$ such that the mapping:\n$$ y = f(x): U \\to W $$\nhas an inverse function in $W$, meaning:\n$$ \\forall y \\in W, \\quad \\exists! x \\in U \\text{ such that } y = f(x). $$\nIllustration:\nA diagram representing $U$ mapping to $W$ via $f$, where $f$ is invertible.\nFor a fixed $y \\in \\mathbb{R}^n$, define:\r#\r$$ g_x = g(y) = y + x - f(x). $$\nWe need to show that $g_x$ has a unique fixed point.\n(b) Construction of neighborhoods $U$ and $W$\nLet:\n$$ g(x) = x - f(x). $$\nThen:\n$$ D g(x) = I - D f(x). $$\nSince:\n$$ D g(x_0) = I - I = 0, $$\nit follows that:\n$$ D g(x) \\text{ is close to zero}. $$\nThus, choosing:\n$$ \\epsilon = \\frac{1}{2n}, $$\nthere exists $\\delta \u0026gt; 0$ such that:\n$$ |x - x_0| \u0026lt; \\delta \\implies |D g_x(x)| \\leq \\frac{1}{2n}. $$\nApplying the Contraction Mapping Principle to $g_x$, we obtain:\n$$ \\exists x \\in B_{\\delta}(x_0) \\text{ such that } g_x(x) = x. $$\nThus:\n$$ g_x(x) = g_x(x_0) + D g_x(\\xi)(x - x_0), $$\nwhich shows:\n$$ D g_x(\\xi) (x - x_0). $$\nChapter 7: Inverse and Implicit Function Theorems\r#\rContraction Mapping Principle (CMP)\r#\rLet $\\mathbb{X}$ be a complete metric space, and let $\\varphi: \\mathbb{X} \\to \\mathbb{X}$ satisfy:\n$$ d(\\varphi(x), \\varphi(y)) \\leq k \\cdot d(x,y), \\quad 0 \u0026lt; k \u0026lt; 1. $$\nThen, there exists a unique fixed point $X^*$ such that $\\varphi(X^{*}) = X^{*}$.\nProof of the Inverse Function Theorem (IFT)\r#\rStep 1: Reduction\r#\rAssume $Df(x_0) = I$. Define $\\tilde{f} = Df(x_0)^{-1} \\circ f$, ensuring $D\\tilde{f}(x_0) = I$.\nFor $x_0 = f^{-1}(y_0)$, define $h(x) = f(x) - f(x_0)$. Since $Dh(x_0) = 0$, solving $f(x) = y$ reduces to:\n$$ x = x_0 - h^{-1}(y - f(x_0)). $$\nStep 2: Existence of the Inverse**\r#\rSet up: $x_0 = 0, y_0 = f(x_0) = 0, Df(x_0) = I$. Need to show a local inverse:\n$$ \\forall y \\in W, \\quad \\exists! x \\in U \\text{ such that } y = f(x). $$\nDefine:\n$$ g_x(y) = y + x - f(x). $$\nWe need to show $g_x$ has a unique fixed point.\nLet $g(x) = x - f(x)$, then $Dg(x) = I - Df(x)$. Since $Dg(x_0) = 0$, choosing $\\epsilon = \\frac{1}{2n}$ ensures $|D g_x(x)|$ is small. Applying CMP, we get:\n$$ \\exists x \\in B_{\\delta}(x_0) \\text{ such that } g_x(x) = x. $$ Thus, the inverse exists and is unique.\nbabeldown::deepl_translate_hugo( post_path = \u0026ldquo;content.en/docs/Mathematics/MATH 412 Real Analysis II/Lecture/7.1 Inverse Function Theorem.md\u0026rdquo;, target_lang = \u0026ldquo;ZH\u0026rdquo;, source_lang = \u0026ldquo;EN\u0026rdquo;, force = TRUE )\n"},{"id":44,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1.1-Inverse-FT-Proof/","title":"7.1.1 ÂèçÂáΩÊï∞ÂÆöÁêÜÔºàËØÅÊòéÔºâ","section":"Á¨¨‰∏ÉÁ´† ÈÄÜÂáΩÊï∞ÂíåÈöêÂáΩÊï∞ÂÆöÁêÜ","content":"\r7.1* Implicit Function Theorem (IFT) Proof\r#\r1. Recall IFT\r#\rTheorem 7.1.1: Let $y = f(x): A \\subset \\mathbb{R}^n \\to \\mathbb{R}^n$ be of class $C^1$. Suppose $x_0 \\in A$ with: $$ J_f(x_0) = \\det(Df(x_0)) \\ne 0 $$ Then there exist neighborhoods $U$ of $x_0$ in $A$ and $W$ of $y_0 = f(x_0)$ such that:\n$f(U) = W$ and $f: U \\to W$ has an inverse $f^{-1}: W \\to U$. $f^{-1} \\in C^1$ (If $f \\in C^r$, then $f^{-1} \\in C^r$). $Df^{-1}(y) = [Df(x)]^{-1}$ for $x \\in U$ and $y = f(x)$. 2. Proof of Theorem 7.1.1\r#\rStep 1: Reduction\r#\rWe may assume $Df(x_0) = I$ and $x_0 = 0$, $y_0 = f(x_0)$.\nStep 2: Existence of inverse\r#\rConsider the function $g(x) = x - f(x)$.\nUsing continuity of $Dg(x)$ at $0$ and Mean Value Theorem, one can show there exists $\\delta \u0026gt; 0$ such that for $x \\in B(0, \\delta)$: $$ |g(x)| \\le \\frac{\\delta}{2} $$ Define $g: B(0, \\delta) \\to B(0, \\frac{\\delta}{2})$. Let $W = B(0, \\frac{\\delta}{2})$, and define: $$ U = { x \\in B(0, \\delta): f(x) \\in W } $$ Step 3: Existence of $f^{-1}: W \\to U$\r#\rFix $y \\in W$. Apply the Contraction Mapping Principle (CMP) to: $$ g_y(x) = y + x - f(x) = y + g(x) $$ Then $g_y(x): B(0, \\delta) \\to B(0, \\delta)$. Thus, there exists a unique $x \\in B(0, \\delta)$ such that: $$ g_y(x) = x \\quad \\Longrightarrow \\quad f(x) = y $$ Therefore, $\\exists! x \\in U$ such that $f(x) = y$.\nFix $y, y_1, y_2 \\in W$, let $x_i = f^{-1}(y_i), i = 1,2$. Then: $$ | f^{-1}(y_1) - f^{-1}(y_2) | = | x_1 - x_2 | = | g_{y_1}(x_1) - g_{y_2}(x_2) | $$\nSince $| Dg(x) | \\le \\frac{1}{2}$ for $x \\in B(0, \\delta)$, we get: $$ | x_1 - x_2 | \\le 2 | y_1 - y_2 | $$\nThus, $f^{-1}$ is Lipschitz continuous.\nStep 4: Differentiability of $f^{-1}$\r#\r(i) Observation: $[Df(x_0)]^{-1}$ exists and $Df(x)$ is continuous at $x_0$.\n$$ \\Rightarrow \\exists \\delta \u0026gt; 0 \\text{ such that } [Df(x)]^{-1} \\text{ exists and bounded by } M \\text{, } \\forall |x| \\leq \\delta $$ $$ | [Df(x)]^{-1} | \\leq M, \\quad \\forall x \\in B(0, \\delta) $$\n(ii) Show $f^{-1}$ is differentiable at any $y_* \\in W$ and: $$ Df^{-1}(y_0) = [Df(x_0)]^{-1}, \\quad \\text{where} \\quad y_0 = f(x_0) $$\nFix $y_* \\in W$. Then: $$ \\frac{| f^{-1}(y) - f^{-1}(y_) - [Df(x_0)]^{-1}(y - y_) |}{| y - y_* |} $$ can be simplified, and as $y \\to y_*$, it tends to $0$.\nThus, in conclusion, $f^{-1}(y)$ is differentiable at $y_* \\in W$ and: $$ Df^{-1}(y_) = [Df(x_)]^{-1} $$\nExample:\r#\rInvestigate the invertibility (both local and global) for the map: $$f \\in C^\\infty, \\quad A = \\mathbb{R}^2$$\n$$W = (u,v) = f(x,y): \\mathbb{R}^2 \\to \\mathbb{R}^2$$ Given by: $$ u = e^x\\cos y, \\quad v = e^x\\sin y$$\nCompute Jacobian determinant: $$ J_f(x,y) = \\det(Df(x,y)) = \\begin{vmatrix} e^x\\cos y \u0026amp; -e^x\\sin y \\\\ e^x\\sin y \u0026amp; e^x\\cos y \\end{vmatrix} = e^{2x} \u0026gt; 0 $$ Thus, by IFT, $f$ is invertible locally at any point and: $$ Df^{-1}(u,v) = [Df(x,y)]^{-1} = \\begin{bmatrix} e^x\\cos y \u0026amp; -e^x\\sin y \\\\ e^x\\sin y \u0026amp; e^x\\cos y \\end{bmatrix}^{-1} = \\begin{bmatrix} e^{-x}\\cos y \u0026amp; e^{-x}\\sin y \\\\ -e^{-x}\\sin y \u0026amp; e^{-x}\\cos y \\end{bmatrix} $$\nHowever, $f$ is not globally invertible (not injective). Consider: $$ \\begin{aligned} f(x_0, y_0 + 2\\pi) \u0026amp;= (e^{x_0}\\cos(y_0 + 2\\pi), e^{x_0}\\sin(y_0 + 2\\pi))\\\\ \u0026amp;= (e^{x_0}\\cos y_0, e^{x_0}\\sin y_0)\\\\ \u0026amp;= (u_0, v_0) \\end{aligned} $$\nIn complex notation, $f$ can be written as: $$ f(z) = e^z = e^{x+iy} = e^x e^{iy} = e^x(\\cos y + i \\sin y) $$ with $u = e^x \\cos y$, $v = e^x \\sin y$.\nConclusion\r#\rSince $f(x, y)$ maps points periodically in $y$, it is not globally injective, despite being locally invertible.\nAdditional Notes\r#\rThe periodic nature is reflected in the mapping: $$f(x_0, y_0 + 2\\pi) = f(x_0, y_0)$$ This demonstrates that multiple points in the domain map to the same point in the range, confirming non-injectivity.\n"},{"id":45,"href":"/docs/Mathematics/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.2-Implicit-Function-Theorem/","title":"7.2 ÈöêÂáΩÊï∞ÂÆöÁêÜ","section":"Á¨¨‰∏ÉÁ´† ÈÄÜÂáΩÊï∞ÂíåÈöêÂáΩÊï∞ÂÆöÁêÜ","content":"111\n"},{"id":46,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1-Boundary-Value-Problems/","title":"9.1 ËæπÂÄºÈóÆÈ¢òÁöÑËøë‰ºº","section":"Á¨¨‰πùÁ´†","content":"\r1.1 Set-up: String with fixed endpoints\r#\rÊàë‰ª¨ÂèØ‰ª•ÂÜô $$ \\begin{align} -\\frac{d^2u}{dx^2} \u0026amp;= f(x), \\quad x \\in (0,1) \\ u(0) \u0026amp;= \\alpha, \\quad \\frac{du}{dx}(0) = \\beta \\end{align} $$\nËÆ∞$w = \\frac{du}{dx}$ÔºåÈÇ£‰πà $\\frac{dw}{dx} = f(x)$„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨Áü•ÈÅì\n$$\\frac{d}{dt} = \\begin{bmatrix} 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} w \\ u \\end{bmatrix}$$\nÂÆö‰πâ\r#\rËæπÂÄºÈóÆÈ¢òÔºàboundary-value problemÔºâÁöÑÂÆö‰πâ‰∏∫\n[!definition|*] $$\\begin{align} -\\mu\\frac{d^2u}{dx^2} \u0026amp;= f(x), \\quad x\\in (0,1), \\quad \\mu \u0026gt; 0 \\ u(0) \u0026amp;= \\alpha, \\quad u(1) = \\beta \\end{align} $$\n1.2 Ê≥äÊùæÊñπÁ®ãÔºàPoisson EquationÔºâ\r#\rÊ≠§Á±ªÊñπÁ®ãÁöÑ‰∫åÁª¥ÂΩ¢ÊÄÅ‰∏∫Ôºö\n$$\\begin{align} -\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) \u0026amp;= f(x,y) \\ u|_{\\text{boundary of }\\Omega} \u0026amp;= 0 \\end{align}$$\nÁî®ÊãâÊôÆÊãâÊñØÁÆóÂ≠êÊù•Ë°®Á§∫Ôºö\n$$\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = \\Delta u, \\quad \\text{where }\\Delta = \\nabla^2 u = \\sum_{i=1}^{n} \\frac{\\partial^2}{\\partial x_i^2}$$\nÊâÄ‰ª•ÔºåÂπø‰πâÁöÑÊ≥äÊùæÊñπÁ®ãÂèØ‰ª•ÂÜô‰∏∫\n[!definition] A general ($n$-dimentional) poisson equation is written as $$\\Delta u = f(\\mathbf{x})$$ where $\\mathbf{x}=(x_{1},x_{2},x_{3}\\dots x_{n})$.\n1.3 Back to the String Example: How can we get a BVP?\r#\rËÄÉËôë‰ª•‰∏ãÁªèÂÖ∏ÂäõÂ≠¶‰∏≠ÂæàÂ∏∏ËßÅÁöÑÂº¶ÁöÑËÉΩÈáèÊ≥õÂáΩÔºàenergy functionalÔºâÔºö\n$$J(u) = \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx - \\int_0^1 f \\cdot u , dx \\quad \\text{(Energy of string)}$$\nBoundary condition: $u(0) = u(1) = 0$.\n‰Ωú‰∏∫Ëá™ÁÑ∂ÁïåÁöÑÂü∫Êú¨Ë∂ãÂäø‰∏≠ÔºåÊúÄÂ∞è‰ΩúÁî®ÈáèÂéüÁêÜËß£Èáä‰∫ÜËá™ÁÑ∂Á≥ªÁªüÂÄæÂêë‰∫éÈááÂèñËÉΩÈáèÊ∂àËÄóÊúÄÂ∞èÁöÑË∑ØÂæÑÊàñÁä∂ÊÄÅÔºå‰πüÂ∞±ÊòØËØ¥ÔºåËá™ÁÑ∂‰ºöÊ≤øÁùÄ$\\min J(u)$ÁöÑË∑ØÂæÑÂèëÂ±ï„ÄÇ\nÊúÄÁÆÄÂçïÁöÑÊñπÊ≥ïÊòØÁõ¥Êé•Áî® Ê¨ßÊãâ-ÊãâÊ†ºÊúóÊó•ÊñπÁ®ãÔºàEuler‚ÄìLagrange equationÔºâÊêûÂÆöÔºå‰ΩÜÊòØËøôÊØïÁ´üÊòØ‰∏™Êï∞Â≠¶ËØæÔºåÈÇ£‰πàÊàë‰ª¨Áî®ÊúÄÊö¥ÂäõÁöÑÂéüÂßãÊñπÊ≥ïËß£ÂÜ≥Ôºö\nÂ§ßËá¥ÊÄùË∑Ø‰∏∫Ôºö\nÊää $u$ Âä†ÂÖ•Êâ∞Âä®ÔºàperturbationÔºâÂèòÊàê $u+Œµv$Ôºö $$u_{\\epsilon}(x) = u(x) + \\epsilon v(x)$$\n$J(u+Œµv)$ ËøõË°åÊòæÂºèÂ±ïÂºÄÔºö $$J(u + \\epsilon v) = J(u) + \\epsilon \\underbrace{\\delta J(u; v)}{\\text{‰∏ÄÈò∂ÂèòÂàÜ}} + \\frac{1}{2} \\epsilon^2 \\underbrace{\\delta^2 J(u; v)}{\\text{‰∫åÈò∂ÂèòÂàÜ}} + \\cdots$$\nÂú®ÂèòÂàÜÊ≥ïÊàñÂäõÂ≠¶ÁöÑËØ≠Ë®ÄÈáåÔºöÈÄöÂ∏∏ÊòØÊåáÂú®ËÉΩÈáèÊàñ‰ΩúÁî®ÈáèÔºàactionÔºâÁ≠âÊ≥õÂáΩÊÑè‰πâ‰∏ãÁöÑÈ©ªÁÇπÔºàstationary pointÔºâÔºö‰πüÂ∞±ÊòØÂØπ‰ªªÊÑè‚ÄúÂ∞èÊâ∞Âä®‚Äù $Œµv$ÔºåËØ•ÂáΩÊï∞ $u$ ÈÉΩ‰ΩøÂæóÊ≥õÂáΩÁöÑ‰∏ÄÈò∂ÂèòÂåñÈáè‰∏∫ 0„ÄÇ\nÊ±ÇÂØºÔºåÊù•Êâæ$J$ÁöÑÊúÄÂ∞èÂÄº $$\\lim_{\\varepsilon \\to 0} \\frac{J(u + \\varepsilon v) - J(u)}{\\varepsilon} = 0, \\quad \\varepsilon \\in \\mathbb{R}$$\nÊòæÁÑ∂ÊòìËßÅÔºö\n$$ \\begin{align} \u0026amp;\\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx} + \\varepsilon\\frac{dv}{dx}\\right)^2 dx - \\int_0^1 f(u+\\varepsilon v) , dx - \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx - \\int_0^1 f \\cdot u , dx \\ \u0026amp;= \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx + \\frac{1}{2} \\cdot 2\\varepsilon \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx + \\frac{1}{2}\\varepsilon^2 \\int_0^1 \\mu\\left(\\frac{dv}{dx}\\right)^2 dx \\ \u0026amp;\\quad - \\int_0^1 f \\cdot u , dx - \\varepsilon\\int_0^1 f \\cdot v , dx - \\frac{1}{2}\\int_0^1 \\mu\\left(\\frac{du}{dx}\\right)^2 dx - \\int_0^1 f \\cdot u , dx \\ \u0026amp;= \\varepsilon \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx + \\frac{1}{2}\\varepsilon^2 \\int_0^1 \\mu\\left(\\frac{dv}{dx}\\right)^2 dx - \\varepsilon\\int_0^1 f \\cdot v , dx\\end{align} $$\nËøôÂÆûÈôÖ‰∏äÊ≠£ÊòØÊ¨ßÊãâ‚ÄìÊãâÊ†ºÊúóÊó•ÊñπÁ®ãÊúÄÊó©ÁöÑ‚ÄúÂéüÂßãÂèòÂàÜÊ≥ï‚ÄùÊé®ÂØºÔºå‰πüÊ≠£ÊòØ E-L ÊñπÁ®ãÁöÑÊù•ÈæôÂéªËÑâ„ÄÇÂè™‰∏çËøá E-L ÊñπÁ®ãÊääËøô‰∏™ËøáÁ®ã‚ÄúÂÖ¨ÂºèÂåñ‚Äù‰∫ÜÔºåËÆ©Êàë‰ª¨‰∏çÂøÖÊØèÊ¨°ÈÉΩÂ±ïÂºÄ‰∏ÄÂ§ßÂ†ÜÈ°π„ÄÅÂÜçÂàÜÈÉ®ÁßØÂàÜÂéªÂáëÂá∫ÈÇ£‰∏™ÈÄöÁî®ÂΩ¢Âºè„ÄÇ\nÁÑ∂Âêé\n$$\\frac{J(u+\\varepsilon v) - J(u)}{\\varepsilon} = \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx + \\frac{1}{2} \\varepsilon \\int_0^1 \\mu\\left(\\frac{dv}{dx}\\right)^2 dx - \\int_0^1 f \\cdot v , dx$$\nÊûÅÈôê‰∏∫Ôºö\n$$\\lim_{\\varepsilon \\to 0} \\frac{J(u+\\varepsilon v) - J(u)}{\\varepsilon} = \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx - \\int_0^1 f \\cdot v , dx = 0$$\n‰∏Ä‰∏™‚ÄúÂπ≥Ë°°Ëß£‚ÄùÔºàequilibrium solutionÔºâ„ÄÇ\nÂèòÂàÜÂΩ¢ÂºèÊàñÂº±ÂΩ¢ÂºèÔºàVariational/WeakÔºâ:\r#\rÁî±Ê≠§Êàë‰ª¨ÂæóÂà∞‰∫Ü‰∏Ä‰∏™Ê≥õÂáΩÁöÑ‚ÄúÂèòÂàÜÊù°‰ª∂‚ÄùÔºö\n[!claim|*] $$\\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx - \\int_0^1 f \\cdot v , dx = 0$$\nÂàÜÈÉ®ÁßØÂàÜ:\n$$ \\begin{align} \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx \u0026amp;= \\mu\\left[\\frac{du}{dx}v\\right]_0^1 - \\mu\\int_0^1 \\frac{d^2u}{dx^2} v , dx \\ \u0026amp;= -\\mu\\int_0^1 \\frac{d^2u}{dx^2} v , dx \\end{align} $$\nSince $v(0) = v(1) = 0$\n$$\\frac{d}{dx}\\left(\\frac{du}{dx}\\right) = \\frac{d^2u}{dx^2}$$\n$$\\int \\frac{dv}{dx} , dx = v$$\nËæπÁïåÈ°πÂõ†‰∏∫BCËÄåÊ∂àÂ§±ÔºåÊâÄ‰ª•Âº±ÂΩ¢Âºè$\\rightarrow$Âº∫ÂΩ¢Âºè:\n$$-\\mu\\int_0^1 \\frac{d^2u}{dx^2} v , dx - \\int_0^1 f \\cdot v , dx = 0$$\n$$-\\int_0^1 \\left(\\mu\\frac{d^2u}{dx^2} + f\\right) \\cdot v , dx = 0$$\nWe want it to be true $\\forall v$. So, it must be: $$\\mu\\frac{d^2u}{dx^2} + f = 0$$\nÊàë‰ª¨ÂæóÂà∞‰∏Ä‰∏™Â∏∏ËßÅÁöÑÈôÑÂ∏¶ËæπÁïåÊù°‰ª∂ÁöÑÂº∫ÂΩ¢ÂºèÂ∏∏ÂæÆÂàÜÊñπÁ®ã(ODE)„ÄÇ\n[!claim|*] We obtain a Boundary Value Problem (BVP): $$ \\begin{align} \\mu u\u0026rsquo;\u0026rsquo;(x) +f\u0026amp;= 0 \\ u(0) \u0026amp;= u(1) = 0 \\end{align} $$\nAssumption: $u$ is at least $C^2$.\n1.4 ‰∏§ÁßçË°®Ëø∞ËæπÂÄºÈóÆÈ¢òÔºàBVPÔºâÁöÑÊñπÂºè\r#\rÂØªÊâæÂáΩÊï∞ $u$Ôºå‰ΩøÂæóÂØπÊâÄÊúâÊª°Ë∂≥ $v(0) = v(1) = 0$ ÁöÑ $v$ÔºåÂùáÊúâ $$ \\int_0^1 \\mu \\frac{du}{dx} \\cdot \\frac{dv}{dx} , dx ;=; \\int_0^1 f \\cdot v , dx $$\n$\\Rightarrow$ $u$ Âè™ÈúÄ‰øùËØÅ‚Äú‰∏ÄÈò∂ÂèØÂæÆ‚Äù $\\Rightarrow$ ÈÄöÂ∏∏ÈááÁî® ÊúâÈôêÂÖÉÊ≥ï (Finite Element) ÂØªÊâæÂáΩÊï∞ $u$Ôºå‰ΩøÂæó $$ \\begin{aligned} -\\mu \\frac{\\partial^2 u}{\\partial x^2} \u0026amp;= f, \\quad x \\in (0,1),\\ u(0) \u0026amp;= 0,\\quad u(1) = 0 \\end{aligned} $$\n$\\Rightarrow$ $u$ ÈúÄË¶ÅËá≥Â∞ë‚Äú‰∫åÈò∂ÂèØÂæÆ‚Äù $\\Rightarrow$ ÈÄöÂ∏∏ÈááÁî® ÊúâÈôêÂ∑ÆÂàÜÊ≥ï (Finite Difference) "},{"id":47,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.2-Finite-Difference/","title":"9.2 ÊúâÈôêÂ∑ÆÂàÜÊ≥ï","section":"Á¨¨‰πùÁ´†","content":"\r2.1 ËæπÂÄºÈóÆÈ¢òÔºàBVPÔºâÁöÑÊúâÈôêÂ∑ÆÂàÜ:\r#\r[!claim|*] Consider the Boundary-Value Problem: $$ \\begin{align} -\\mu\\frac{d^2u}{dx^2} \u0026amp;= f, \\quad x \\in (0,1) \\ u(0) \u0026amp;= u(1) = 0 \\end{align} $$ with discrete points: $$0 = x_0 \u0026lt; x_1 \u0026lt; \\cdots \u0026lt; x_{N} = 1\\quad\\Longrightarrow \\quad-\\mu\\frac{d^2u}{dx^2}(x_i) = f(x_i)$$\n2.2 Êé®ÂØº‰∫åÈò∂‰∏≠ÂøÉÂ∑ÆÂàÜËøë‰ººÊ≥ï\r#\r2.2.1 Poisson ÂæÆÂàÜÊñπÁ®ã\r#\rÂØπ‰∏é‰ªªÊÑè‰∏Ä‰∏™Á¶ªÊï£ÁöÑÁÇπ$x_{i}$ÔºåÊàë‰ª¨È¶ñÂÖàÂú®ÁΩëÊ†ºÁÇπ $x_{i+1} = x_i + \\Delta x$ Âíå $x_{i-1} = x_i - \\Delta x$ Â§ÑÂØπÂáΩÊï∞ $u(x)$ ËøõË°åÊ≥∞ÂãíÁ∫ßÊï∞Â±ïÂºÄÔºö\n$$ \\begin{align} u(x_{i+1}) \u0026amp;= u(x_i) + \\frac{du}{dx}(x_i)\\Delta x + \\frac{1}{2}\\frac{d^2u}{dx^2}(x_i)\\Delta x^2 + \\cdots \\ u(x_{i-1}) \u0026amp;= u(x_i) - \\frac{du}{dx}(x_i)\\Delta x + \\frac{1}{2}\\frac{d^2u}{dx^2}(x_i)\\Delta x^2 + \\cdots \\end{align} $$\nÂΩìÊàë‰ª¨Â∞ÜËøô‰∏§‰∏™ÊñπÁ®ãÁõ∏Âä†Êó∂ÔºåÁî±‰∫éÂ•áÊï∞Èò∂ÂØºÊï∞È°πÁöÑÁ¨¶Âè∑Áõ∏ÂèçÔºåÂÆÉ‰ª¨‰ºöÁõ∏‰∫íÊäµÊ∂àÔºö\n$$ \\begin{align} u(x_{i+1}) + u(x_{i-1}) \u0026amp;= 2u(x_i) + 2\\left(\\frac{1}{2}\\frac{d^2u}{dx^2}(x_i)\\Delta x^2\\right) + 2\\left(\\frac{1}{24}\\frac{d^4u}{dx^4}(x_i)\\Delta x^4\\right) + \\mathcal{O}(\\Delta x^6) \\ \\ \u0026amp;= 2u(x_i) + \\frac{d^2u}{dx^2}(x_i)\\Delta x^2 + \\frac{1}{12}\\frac{d^4u}{dx^4}(x_i)\\Delta x^4 + \\mathcal{O}(\\Delta x^6) \\end{align} $$\nÊé•ÁùÄ, Êàë‰ª¨ÈáçÊñ∞Êï¥ÁêÜÊñπÁ®ã‰ª•ÂàÜÁ¶ªÂá∫‰∫åÈò∂ÂØºÊï∞È°πÔºàËàçÂéªÈ´òÈò∂È°πÔºâ\n$$\\begin{align} \\frac{d^2u}{dx^2}(x_i)\\Delta x^2 \u0026amp;= u(x_{i+1}) + u(x_{i-1}) - 2u(x_i) - \\frac{1}{12}\\frac{d^4u}{dx^4}(x_i)\\Delta x^4 + \\mathcal{O}(||\\Delta x||^6)\\ \\frac{d^2u}{dx^2}(x_i) \u0026amp;= \\frac{u(x_{i+1}) + u(x_{i-1}) - 2u(x_i)}{\\Delta x^2} - \\frac{1}{12}\\frac{d^4u}{dx^4}(x_i)\\Delta x^2 + \\mathcal{O}(||\\Delta x||^2) \\ \u0026amp;\\approx \\boxed{ \\frac{u(x_{i+1}) + u(x_{i-1}) - 2u(x_i)}{\\Delta x^2}} \\end{align}$$\nËÆ∞ $u_i = u(x_i)$ ‰∏é $f_i = f(x_i)$. Âõ†Ê≠§Êàë‰ª¨ÂæóÂà∞ÂæÆÂàÜÊñπÁ®ã\n[!claim|*] $$-\\mu\\frac{d^2u}{dx^2}(x_i) = -\\mu\\frac{u_{i+1} + u_{i-1} - 2u_i}{\\Delta x^2} = f_i$$\nÊà™Êñ≠ËØØÂ∑ÆÔºàTruncation ErrorÔºâ‰∏∫ $\\mathcal{O}(\\Delta x^2)$ ËøôËØÅÂÆû‰∫ÜËØ•Ëøë‰ººÊòØ‰∫åÈò∂Á≤æÂ∫¶ Ôºàsecond-order accuracyÔºâ 2.2.2 ÊûÑÂª∫Á∫øÊÄßÁ≥ªÁªü\r#\rÁî®ËøôÁßçÁ¶ªÊï£ÂåñÊñπÊ≥ïÊé®ÂØºÂá∫‰∏Ä‰∏™Á∫øÊÄßÊñπÁ®ãÁªÑÔºàlinear systemÔºâ:\n$$Au = f$$\nwhere $A$ is given by:\n$$A = \\frac{\\mu}{\\Delta x^2} \\begin{bmatrix} 2 \u0026amp; -1 \u0026amp; 0 \u0026amp; \u0026amp; \\ -1 \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \u0026amp; \\ 0 \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; -1 \\ \u0026amp; \u0026amp; \u0026amp; -1 \u0026amp; 2 \\end{bmatrix} $$\n2.3 What is the Accuracy of FD?\r#\rÁü©ÈòµAÁöÑÂÖ≥ÈîÆÊÄßË¥® (Key Properties of Matrix A)\r#\r‰ªéÂ∑ÆÂàÜÁ¶ªÊï£ÂåñÂæóÂà∞ÁöÑmatrix $A$ÊúâÂá†‰∏™ÈáçË¶ÅÊÄßË¥®Ôºö\nÊ≠£ÂÆöÊÄß (Positive Definiteness)Ôºö$x^TAx \u0026gt; 0 \\quad \\forall x \\neq 0$ $\\Longrightarrow$ solvable„ÄÇ ÂØπÁß∞ÊÄß (Symmetry)ÔºöSymmetry $\\Longrightarrow \\forall ,\\lambda \\in \\mathbb{R}$ ÁâπÂæÅÂÄºÊÄßË¥® (Eigenvalue Properties)ÔºöNon-singular Êù°‰ª∂Êï∞ÂÖ≥Á≥ª (Condition Number Relation)ÔºöAÁöÑÊúÄÂ∞èÁâπÂæÅÂÄº‰∏éÊúÄÂ§ßÁâπÂæÅÂÄº‰πãÊØî‰∏éŒîxÊàêÊ≠£ÊØîÔºåÂç≥$$\\frac{\\lambda_{min}}{\\lambda_{max}} \\propto \\Delta x$$ ËØØÂ∑Æ\r#\rÂΩìÊàë‰ª¨Ëß£Á¶ªÊï£Á≥ªÁªüÔºà$Au = f$ÔºâÊó∂ÔºåÁ≤æÁ°ÆËß£$u_{ex}$‰∏éËøë‰ººËß£$u$‰πãÈó¥Â≠òÂú®ËØØÂ∑ÆÔºåÊâÄ‰ª•$Au_{ex} \\neq f$„ÄÇ\nÁ≤æÁ°ÆÂÖ≥Á≥ªÂÆûÈôÖ‰∏ä‰∏∫Ôºö\n[!claim|*] $$Au_{ex} = f + T$$ where $T_i = C(x_i)\\Delta x^2$ is truncation error\nÂÖ∂‰∏≠$C(x_i)$‰∏éÂõõÈò∂ÂØºÊï∞Áõ∏ÂÖ≥Ôºö$$C(x_i) = C\\frac{d^4u}{dx^4}(x_i)$$\nËØØÂ∑ÆÊñπÁ®ã (Error Equation)\r#\rËã•ÂÆö‰πâËØØÂ∑Æ$e = u_{ex} - u$ÔºåÂàô $$Ae = T$$\n$$\\Longrightarrow e = A^{-1}T$$\nÂõ†Ê≠§\n$$||e|| = ||A^{-1}T|| \\leq ||A^{-1}|| \\cdot ||T||$$\nÊî∂ÊïõÊÄßËØÅÊòé (Convergence Proof)\r#\r‰∏∫‰∫ÜËØÅÊòéÊñπÊ≥ïÊî∂ÊïõÔºåÈúÄË¶ÅÊª°Ë∂≥‰∏§‰∏™Êù°‰ª∂Ôºö1. Á®≥ÂÆöÊÄßÔºö$A^{-1}$ÊúâÁïå (Boundedness of $A^{-1}$) 2. ‰∏ÄËá¥ÊÄßÔºöÊà™Êñ≠ËØØÂ∑ÆË∂ãÈõ∂ (Truncation Error Tends to Zero)\n[!lemma|*] Conditions to show convergence: $$||A^{-1}|| \u0026lt; \\infty \\quad \\text{and} \\quad ||T|| \\to 0 \\quad \\text{as} \\quad \\Delta x \\to 0$$\nÂÖ≥‰∫é $|A^{-1}|$ ÁöÑÊúâÁïåÊÄß\r#\rÁü©ÈòµÁöÑÊù°‰ª∂Êï∞ÂÆö‰πâ‰∏∫Ôºö $$\\kappa(A) = |A| \\cdot |A^{-1}|=\\frac{\\lambda_{max}}{\\lambda_{min}}$$\n$\\frac{\\lambda_{min}}{\\lambda_{max}} \\propto \\Delta x \\longrightarrow\\kappa(A) \\propto \\frac{1}{\\Delta x}$ hence, $||A^{-1}||$ is bounded, regardless of $\\Delta x$. ÂÖ≥‰∫é$T$ÁöÑ‰∏ÄËá¥ÊÄß\r#\rÂõ†‰∏∫Êà™Êñ≠ËØØÂ∑ÆËåÉÊï∞$|T| \\sim \\Delta x^2$ÔºåÊâÄ‰ª•\n$$|T| \\to 0 ,,\\text{as},, \\Delta x \\to 0$$\nËøô‰πüÊÑèÂë≥ÁùÄÔºö\n$$|e| \\to 0 ,,\\text{as},, \\Delta x \\to 0$$\nÂõ†Ê≠§ÔºåËØ•ÊúâÈôêÂ∑ÆÂàÜÊñπÊ≥ïÊòØÊî∂ÊïõÁöÑ (convergent) ËôΩÁÑ∂$\\kappa$Èöè $\\Delta x$ ÂèòÂåñÔºå‰ΩÜ $|A^{-1}|$ ÁöÑÂ¢ûÈïøË¢´ $|T|$ ÁöÑÊõ¥Âø´ÂáèÂ∞èÊâÄÊäµÊ∂à„ÄÇ\nÂÆûÈôÖÊÑè‰πâÔºü\r#\rËØ•ÊúâÈôêÂ∑ÆÂàÜÊ≥ïÈöèÁùÄÁΩëÊ†ºÈó¥Ë∑ù (grid spacing) ÂáèÂ∞èËÄåÊî∂ÊïõÂà∞Á≤æÁ°ÆËß£ Êî∂ÊïõÈÄüÁéá (convergence rate) ÊòØ$O(\\Delta x^2)$ÔºåÂç≥‰∫åÈò∂Á≤æÂ∫¶ (second-order accuracy) ËØØÂ∑Æ‰∏ªË¶ÅÂèóÊéß‰∫éÂõõÈò∂ÂØºÊï∞ÁöÑÂ§ßÂ∞èÂíåÁΩëÊ†ºÈó¥Ë∑ùÁöÑÂπ≥Êñπ ËøôËß£Èáä‰∫Ü‰∏∫‰ªÄ‰πàÂú®ÂÆûÈôÖËÆ°ÁÆó‰∏≠ÔºåÂΩìÊàë‰ª¨Â∞ÜÁΩëÊ†ºÈó¥Ë∑ùÂáèÂçäÊó∂ÔºåËØØÂ∑ÆÂ§ßÁ∫¶‰ºöÂáèÂ∞èÂà∞ÂéüÊù•ÁöÑÂõõÂàÜ‰πã‰∏Ä ËøôÁßçÊï∞Â≠¶ËØÅÊòé‰∏∫Êàë‰ª¨‰ΩøÁî®ÊúâÈôêÂ∑ÆÂàÜÊ≥ïÊ±ÇËß£ÂæÆÂàÜÊñπÁ®ãÊèê‰æõ‰∫ÜÁêÜËÆ∫‰øùÈöúÔºåÁ°Æ‰øù‰∫ÜÂú®Ë∂≥Â§üÁªÜÁöÑÁΩëÊ†º‰∏ãÔºåÊï∞ÂÄºËß£ (numerical solution) ‰ºö‰ª•ÂèØÈ¢ÑÊµãÁöÑÈÄüÁéáÊé•ËøëÁúüÂÆûËß£ (exact solution)„ÄÇ\n"},{"id":48,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.3-Advection-Diffusion-Equation/","title":"9.3 ÂØπÊµÅ-Êâ©Êï£ÊñπÁ®ã","section":"Á¨¨‰πùÁ´†","content":"\r3.1 ÂØπÊµÅ-Êâ©Êï£ÊñπÁ®ã\r#\rÂØπÊµÅ-Êâ©Êï£ÊñπÁ®ãÊòØ‰∏ÄÁßçÊèèËø∞Áâ©Ë¥®ÊàñÁÉ≠ÈáèÂú®ÊµÅ‰Ωì‰∏≠ÂêåÊó∂ÂèóÂà∞ÂØπÊµÅÔºà‰πüÁß∞‰∏∫Âπ≥ÊµÅÔºâÂíåÊâ©Êï£‰ΩúÁî®ÂΩ±ÂìçÁöÑÂÅèÂæÆÂàÜÊñπÁ®ã„ÄÇËøô‰∏™ÊñπÁ®ãÂú®ÊµÅ‰ΩìÂäõÂ≠¶„ÄÅ‰º†ÁÉ≠Â≠¶ÂíåÁâ©Ë¥®‰º†ËæìÁ≠âÈ¢ÜÂüüÊúâÂπøÊ≥õÂ∫îÁî®„ÄÇ\n$$ \\begin{align} -\\mu\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} \u0026amp;= f \\ \\text{diffusion} \\quad \u0026amp; \\text{advection} \\end{align} $$\n$$u(0) = u_L, \\quad u(1) = u_R$$\n3.1.1 Á¶ªÊï£ÂåñËøáÁ®ãÔºö\r#\r‰∏ÄÈò∂ÂØºÊï∞ÔºöÂØπÊµÅÈ°π ÔºàAdvectionÔºâ\r#\rÊ≥∞ÂãíÂ±ïÂºÄÔºàÂêëÂâçÔºâ\r#\r$$ \\begin{align}\nu(x_{j+1}) \u0026amp; = u(x_j) + \\frac{\\partial u}{\\partial x}(x_j)\\Delta x + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x^2 \\ \u0026amp;\\quad\\quad\\quad\\quad-\\frac{1}{6}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^3 + \\frac{1}{12}\\frac{\\partial^4 u}{\\partial x^4}(x_j)\\Delta x^4 + \\mathcal{O}(||x||^5)\\ \\frac{\\partial u}{\\partial x}(x_j)\\Delta x \u0026amp; = u(x_{j+1}) - u(x_j) + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x^2\\ \\frac{\\partial u}{\\partial x}(x_j) \u0026amp; = \\frac{u_{j+1} - u_j}{\\Delta x} + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x \\end{align}$$\nÊ≥∞ÂãíÂ±ïÂºÄÔºàÂêëÂêéÔºâ\r#\r$$u(x_{j-1}) = u(x_j) - \\frac{\\partial u}{\\partial x}(x_j)\\Delta x + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(x_j)\\Delta x^2 - \\frac{1}{6}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^3 + \\frac{1}{12}\\frac{\\partial^4 u}{\\partial x^4}(x_j)\\Delta x^4 + \\mathcal{O}(||x||^5)$$\nÊàë‰ª¨Âèñ(forward) - (backward)‰∏§ËÄÖÂ∑ÆÂÄº\n$$u(x_{j+1}) - u(x_{j-1}) = 2\\frac{\\partial u}{\\partial x}(x_j)\\Delta x + \\frac{1}{3}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^3 + \\mathcal{O}(||\\Delta x||^5)$$\nÊé•ÁùÄ\n$$\\frac{\\partial u}{\\partial x}(x_j) = \\frac{u(x_{j+1}) - u(x_{j-1})}{2\\Delta x} - \\frac{1}{6}\\frac{\\partial^3 u}{\\partial x^3}(x_j)\\Delta x^2 + \\mathcal{O}\\left(\\frac{||\\Delta x||^4}{2}\\right)$$\n‰∫åÈò∂ÂØºÊï∞ÔºöÊâ©Êï£È°πÔºàDiffusionÔºâ\r#\rËøôÈÉΩËøò‰∏ç‰ºöÂêóÔºüÈÄÄÁæ§Âêß„ÄÇ\nÂØπÊµÅ-Êâ©Êï£Á¶ªÊï£ÊñπÁ®ã\r#\r[!claim|*] Final numerical solution: $$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = f_j$$ with second-order accuracy: $$\\sim \\mathcal{O}(\\Delta x^2)$$\n3.2 ‰æãÂ≠êÔºöÁ≤æÁ°ÆËß£‰∏éÊï∞ÂÄºËß£Âú®ÂØπÊµÅ‰∏ªÂØºÈóÆÈ¢ò‰∏≠ÁöÑÂ∑ÆÂºÇ\r#\rÊàë‰ª¨Áé∞Âú®ËÄÉËôë‰∏Ä‰∏™ÁâπÊÆäÊÉÖÂÜµÁöÑÂØπÊµÅ-Êâ©Êï£ÊñπÁ®ãÔºåÂÖ∂‰∏≠Ê∫êÈ°πÔºàSource TermÔºâ‰∏∫Èõ∂ÔºåÂÖ∑Êúâ‰ª•‰∏ãËæπÁïåÊù°‰ª∂Ôºö\n$$ \\begin{align} -\\mu\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} \u0026amp;= 0 \\ u(0) \u0026amp;= 0 \\ u(a) \u0026amp;= 1 \\end{align} $$\n3.2.1 Á≤æÁ°ÆËß£\r#\rËøô‰∏™ÊñπÁ®ãÁöÑÁ≤æÁ°ÆËß£ÊòØÔºö\n$$u_{ex} = \\frac{e^{\\frac{\\beta}{\\mu}x} - 1}{e^{\\frac{\\beta}{\\mu}a} - 1}$$\nËøô‰∏™Ëß£ÁöÑÁâπÁÇπÊòØÔºöÂΩìÊØîÂÄº $\\frac{|\\beta|}{\\mu} \\gg 1$ Êó∂ÔºàÂç≥advectionËøúÂ§ß‰∫édiffusionÔºâÔºåËß£Âú®ËæπÁïå $x=a$ ÈôÑËøë‰ºöÂΩ¢Êàê‰∏Ä‰∏™Èô°Â≥≠ÁöÑËæπÁïåÂ±Ç„ÄÇËøôË¢´Áß∞‰∏∫**\u0026ldquo;ÂØπÊµÅ‰∏ªÂØºÈóÆÈ¢ò\u0026rdquo;Ôºàadvection-dominated problemÔºâ**„ÄÇ\nÂú®ÂØπÊµÅ‰∏ªÂØºÁöÑÊÉÖÂÜµ‰∏ãÔºåËß£Âú®Â§ßÈÉ®ÂàÜÂå∫ÂüüÊé•Ëøë‰∫é0ÔºåÂè™Âú®Êé•Ëøë $x=a$ ÁöÑÂ∞èÂå∫ÂüüÂÜÖÂø´ÈÄü‰∏äÂçáÂà∞1„ÄÇËøôÂØπÊï∞ÂÄºÊñπÊ≥ïÁöÑËß£Ê≥ïÊòØÂæàÂ§ßÁöÑÈ∫ªÁÉ¶„ÄÇ\n3.2.2 Êï∞ÂÄºËß£\r#\rÂΩìÊàë‰ª¨‰ΩøÁî®Ê†áÂáÜÁöÑ‰∏≠ÂøÉÂ∑ÆÂàÜÔºàcentral differenceÔºâÊñπÊ≥ïÁ¶ªÊï£ÂåñËøô‰∏™ÊñπÁ®ãÊó∂Ôºö\n$$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = 0$$\nÈáçÊñ∞Êï¥ÁêÜËøô‰∏™ÊñπÁ®ãÔºö\n$$\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)u_{i+1} + \\left(\\frac{2\\mu}{\\Delta x^2}\\right)u_i - \\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)u_{i-1} = 0$$\nÊï∞ÂÄºÂÆûÈ™åË°®ÊòéÔºåÂΩì $|\\beta|$ ËæÉÂ§ßÊó∂ÔºåÊï∞ÂÄºËß£‰ºöÂá∫Áé∞‰∏ç‰∏ÄËá¥ÔºåÈùûÁâ©ÁêÜÁöÑÊåØËç°„ÄÇ‰∏∫‰ªÄ‰πàÂë¢Ôºü\nÊï∞Â≠¶Ëß£Èáä\r#\r‰∏∫‰∫ÜÁêÜËß£Ëøô‰∏ÄÁé∞Ë±°ÔºåÊàë‰ª¨ÂèØ‰ª•ÂØπÂ∑ÆÂàÜÊñπÁ®ãËøõË°åÊ∑±ÂÖ•ÂàÜÊûê„ÄÇÊàë‰ª¨ÂÅáËÆæÂ∑ÆÂàÜÊñπÁ®ãÁöÑËß£ÂÖ∑ÊúâÂΩ¢Âºè $u_j = C\\rho^j$ÔºåÂÖ∂‰∏≠ $C$ ÊòØÂ∏∏Êï∞Ôºå$\\rho$ ÊòØÂæÖÂÆöÂèÇÊï∞„ÄÇÂ∞ÜËøô‰∏™ÂÅáËÆæ‰ª£ÂÖ•Âà∞Â∑ÆÂàÜÊñπÁ®ã‰∏≠Ôºö\n$$\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)C\\rho^{j+1} + \\left(\\frac{2\\mu}{\\Delta x^2}\\right)C\\rho^j - \\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)C\\rho^{j-1} = 0$$\nÊ∂àÂéª $C$ Âπ∂Êï¥ÁêÜÔºö\n$$\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)\\rho^2 + \\left(\\frac{2\\mu}{\\Delta x^2}\\right)\\rho - \\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right) = 0$$\nËøôÊòØÂÖ≥‰∫é $\\rho$ ÁöÑ‰∫åÊ¨°ÊñπÁ®ãÔºåÂÆÉÊúâ‰∏§‰∏™Ëß£ $\\rho_1$ Âíå $\\rho_2$„ÄÇÂ∑ÆÂàÜÊñπÁ®ãÁöÑ‰∏ÄËà¨Ëß£ÊòØËøô‰∏§‰∏™ÁâπËß£ÁöÑÁ∫øÊÄßÁªÑÂêàÔºö\n$$u_j = C_1\\rho_1^j + C_2\\rho_2^j$$\nÂÖ∂‰∏≠ $C_1$ Âíå $C_2$ ÊòØÁî±ËæπÁïåÊù°‰ª∂Á°ÆÂÆöÁöÑÂ∏∏Êï∞„ÄÇ\nÊåØËç°Ëß£ÁöÑÊù°‰ª∂\r#\rÊ†πÊçÆ‰∫åÊ¨°ÊñπÁ®ãÁöÑÊÄßË¥®Ôºå‰∏§‰∏™Ê†πÁöÑ‰πòÁßØÁ≠â‰∫éÂ∏∏Êï∞È°π‰∏é‰∫åÊ¨°È°πÁ≥ªÊï∞ÁöÑÊØîÂÄºÔºö\n$$\\rho_1\\rho_2 = \\frac{-\\left(\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)}{\\left(-\\frac{\\mu}{\\Delta x^2} + \\frac{\\beta}{2\\Delta x}\\right)} = \\frac{1 + \\frac{\\beta\\Delta x}{2\\mu}}{1 - \\frac{\\beta\\Delta x}{2\\mu}}$$\nËøôÈáåÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÈáçË¶ÅÁöÑÊó†ÈáèÁ∫≤ÂèÇÊï∞ÔºåÁß∞‰∏∫ÁΩëÊ†º‰Ω©ÂÖãËé±Êï∞ÔºàGrid P√©clet numberÔºâÔºö\n$$\\text{Pe} = \\frac{|\\beta|\\Delta x}{2\\mu}$$\n‰Ω©ÂÖãËé±Êï∞Ë°®Á§∫ÂØπÊµÅ‰º†Ëæì‰∏éÊâ©Êï£‰º†ËæìÁöÑÁõ∏ÂØπÂº∫Â∫¶„ÄÇ\nÂΩì $\\text{Pe} \u0026gt; 1$ÔºàÂç≥ $\\frac{|\\beta|\\Delta x}{2\\mu} \u0026gt; 1$ÔºâÊó∂ÔºåÊàë‰ª¨Êúâ $\\rho_1\\rho_2 \u0026lt; 0$ÔºåËøôÊÑèÂë≥ÁùÄ‰∏§‰∏™Ê†π‰∏ÄÊ≠£‰∏ÄË¥ü„ÄÇÂΩì‰∏Ä‰∏™Ëß£ÂåÖÂê´Ë¥üÁöÑÂπÇÊó∂Ôºå‰ºöÂØºËá¥Ëß£Âú®Á©∫Èó¥‰∏äÂëàÁé∞ÊåØËç°ÁâπÊÄßÔºåËøô‰∏éÁâ©ÁêÜÁõ¥ËßâÁõ∏ËøùËÉåÔºåÂõ†‰∏∫Êâ©Êï£ËøáÁ®ãÂ∫îËØ•ÊòØÂπ≥ÊªëÁöÑ„ÄÇ\nÁâ©ÁêÜËß£Èáä‰∏éÊîπËøõÊñπÊ≥ï\r#\r‰∏∫‰ªÄ‰πà‰ºöÂá∫Áé∞ÊåØËç°Ôºü\r#\rÁâ©ÁêÜËßíÂ∫¶ÁúãÔºö‰ø°ÊÅØ‰∏ªË¶ÅÊ≤øÁùÄÊµÅÂä®ÊñπÂêë‰º†Êí≠„ÄÇCentral Difference ÊñπÊ≥ïÂØπ‰∏äÊ∏∏Âíå‰∏ãÊ∏∏ÁöÑ‰ø°ÊÅØÁªô‰∫àÁõ∏ÂêåÊùÉÈáçÔºåÊâÄ‰ª•ÂØπÊµÅ‰∏ªÂØºÁöÑÊÉÖÂÜµ‰∏ã‰∏çÂêàÈÄÇÔºåÈô§ÈùûÊûÅÁªÜÁöÑÁΩëÊ†ºÊâçËÉΩÂáÜÁ°ÆËß£Êûê„ÄÇÂç≥‰ΩøÊï∞ÂÄºÊñπÊ≥ïÂú®Êï∞Â≠¶‰∏äÂÖ∑Êúâ‰∫åÈò∂Á≤æÂ∫¶ÔºåÂÖ∂ÂáÜÁ°ÆÂ∫¶‰æùÊóßÊòØË¶ÅÂèñÂÜ≥‰∫éÁâπÂÆöÁöÑÁâ©ÁêÜÈóÆÈ¢ò‰∏≠„ÄÇÁêÜËß£Êï∞ÂÄºÊñπÊ≥ïÁöÑÁ®≥ÂÆöÊÄßÊù°‰ª∂ÊâçÂèØ‰ª•ÈÄâÊã©ÂêàÈÄÇÁöÑÊ±ÇËß£Á≠ñÁï•„ÄÇ\nËß£ÂÜ≥ÊñπÊ°àÔºü\r#\rÁΩëÊ†ºÁªÜÂåñÔºöÊúÄÁõ¥Êé•ÁöÑÊñπÊ≥ïÊòØÂáèÂ∞è $\\Delta x$Ôºå‰Ωø $\\text{Pe} \u0026lt; 1$„ÄÇ‰ΩÜËøô‰ºöÂ§ßÂ§ßÂ¢ûÂä†ËÆ°ÁÆóÊàêÊú¨„ÄÇ\nËøéÈ£éÊñπÊ≥ïÔºöÔºàËØ¶ÁªÜËßÅ‰∏ãÊñáÔºâ‰ΩøÁî®ÂÅèÂêë‰∏äÊ∏∏ÁöÑÂ∑ÆÂàÜÊ†ºÂºèÔºåÂ¶ÇÂâçÂêëÊàñÂêéÂêëÂ∑ÆÂàÜÔºåÂèñÂÜ≥‰∫é $\\beta$ ÁöÑÁ¨¶Âè∑„ÄÇ‰æãÂ¶ÇÔºåÂΩì $\\beta \u0026gt; 0$ Êó∂ÔºåÂèØ‰ª•‰ΩøÁî®Ôºö $$\\frac{\\partial u}{\\partial x}(x_j) \\approx \\frac{u_j - u_{j-1}}{\\Delta x}$$\n‰∫∫Â∑•Êâ©Êï£ÔºöÂ¢ûÂä†‰∏Ä‰∏™Êï∞ÂÄºÊâ©Êï£È°πÔºå‰ΩøÊúâÊïàÁöÑ‰Ω©ÂÖãËé±Êï∞Â∞è‰∫é1„ÄÇ\nÈ´òÈò∂Ê†ºÂºèÔºö‰ΩøÁî®Êõ¥È´òÈò∂ÁöÑÂ∑ÆÂàÜÊ†ºÂºèÔºåÂ¶ÇQUICK„ÄÅTVDÊàñENO/WENOÊñπÊ°àÔºåËøô‰∫õÊñπÊ≥ïÂèØ‰ª•Êõ¥Â•ΩÂú∞ÊçïÊçâÂº∫Ê¢ØÂ∫¶Âå∫Âüü„ÄÇ\n3.3 Âè¶‰∏ÄÁßçÊñπÊ≥ïÔºöËøéÈ£éÊ≥ïÔºàUpwind MethodÔºâ\r#\r3.3.1 ‰ø°ÊÅØÊµÅÂä®ÂàÜÊûê (Information Flow)\r#\rÂ∞±ÂÉèÂàöÂàöÁöÑÂØπÊµÅ‰∏ªÂØºÈóÆÈ¢òÔºåÁé∞ÂÆû‰∏≠ÁªèÂ∏∏Â≠òÂú®ÊòéÁ°ÆÁöÑÁâ©ÁêÜ‰ø°ÊÅØÊµÅÂä®ÊñπÂêëÔºå‰ΩøÂæóËøô‰∏™ÈóÆÈ¢òÊú¨Ë¥®‰∏äÊòØÈùûÂØπÁß∞ÁöÑ„ÄÇ\nÂØπ‰∫éÂØπÊµÅÈ°πÔºàadvectionÔºâÔºåÂΩìÊµÅÂä®ÊñπÂêëÂ∑≤Áü•Êó∂\n[!assumption|*] $\\beta \u0026gt; 0$Ôºåmeaning that the information flows from left to right.\nÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî®ÂºèÂ≠êÔºö\n[!claim|*] $$\\beta\\frac{\\partial u}{\\partial x} \\approx \\beta\\frac{u_i - u_{i-1}}{\\Delta x} \\quad (\\beta \u0026gt; 0) \\quad \\text{(upwind)}$$\nËøôÁßçÂ∑ÆÂàÜÊ†ºÂºèËÄÉËôë‰∫Ü‰ø°ÊÅØÊµÅÂä®ÁöÑÊñπÂêëÔºå‰ΩøÁî®\u0026quot;‰∏äÊ∏∏\u0026quot;ÁöÑËäÇÁÇπÊù•ËÆ°ÁÆóÂØºÊï∞ÔºåËÄå‰∏çÊòØÂÉè‰∏≠ÂøÉÂ∑ÆÂàÜÈÇ£Ê†∑Âπ≥Á≠âÂØπÂæÖ‰∏ä‰∏ãÊ∏∏ËäÇÁÇπÔºàÊ≥®ÊÑèËøô‰∏™Âè™Êúâ‰∏ÄÈò∂Á≤æÂ∫¶Ôºâ„ÄÇ\n3.3.2 ËøéÈ£éÊ≥ïÁöÑÁ®≥ÂÆöÊÄßÂàÜÊûê (Stability Analysis)\r#\rÊé•‰∏ãÊù•ÔºåÊàë‰ª¨Êù•ËØÅÊòéËøéÈ£éÊñπÊ≥ïÊòØÁ®≥ÂÆöÁöÑ„ÄÇÂ∞ÜËøéÈ£éÂ∑ÆÂàÜÈáçÂÜô‰∏∫Ôºö\n$$\\beta\\frac{u_i - u_{i-1}}{\\Delta x} = \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\beta\\frac{u_{i+1}}{2\\Delta x} - \\beta\\frac{2u_i}{2\\Delta x} + \\beta\\frac{u_{i-1}}{2\\Delta x}$$\nÊï¥ÁêÜ‰∏Ä‰∏ã‰∏äÂºèÔºö\n$$= \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\frac{\\beta\\Delta x}{2} \\cdot \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}$$\nËøô‰∏™Ë°®ËææÂºèÂèØ‰ª•ÂàÜËß£‰∏∫‰∏§È°πÔºö\n‰∏≠ÂøÉÂ∑ÆÂàÜÈ°π (Central mean): $\\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x}$ ‰∫åÈò∂ÂØºÊï∞Ëøë‰ººÈ°π (Approximation of 2nd derivative): $-\\frac{\\beta\\Delta x}{2} \\cdot \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}$ ËøôËØ¥ÊòéËøéÈ£éÊñπÊ≥ïÁ≠â‰ª∑‰∫éÂØπÂéüÂßãÊñπÁ®ãËøõË°å‰∏≠ÂøÉÂ∑ÆÂàÜÔºå‰ΩÜÂ¢ûÂä†‰∫Ü‰∏Ä‰∏™È¢ùÂ§ñÁöÑÊâ©Êï£È°πÔºà‰∫∫Â∑•Êâ©Êï£Ôºåartificial diffusionÔºâ„ÄÇ\nÁ≠â‰ª∑Ë°®Ëø∞ÔºöÊâ∞Âä®ÊñπÁ®ã (Perturbed Equation)\r#\rÂõ†Ê≠§ÔºåÊàë‰ª¨ÂèØ‰ª•Â∞ÜÂéüÂßãÈóÆÈ¢òÁöÑËøéÈ£éÊñπÊ≥ïÁúã‰ΩúÊòØ‰∏ãÈù¢Ëøô‰∏™Êâ∞Âä®ÊñπÁ®ãÁöÑ‰∏≠ÂøÉÂ∑ÆÂàÜËß£Ê≥ïÔºö\n$$-\\left(\\mu + \\frac{|\\beta|\\Delta x}{2}\\right)\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} = 0$$\nÂØπËøô‰∏™Êâ∞Âä®ÊñπÁ®ãÂ∫îÁî®‰∏≠ÂøÉÂ∑ÆÂàÜËøë‰ººÔºö\n$$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = 0$$\nËøôÊ≠£ÊòØÂéüÂßãÈóÆÈ¢òÁöÑËøéÈ£éËß£Ê≥ï„ÄÇÊç¢Âè•ËØùËØ¥Ôºö\nÂØπÊâ∞Âä®ÈóÆÈ¢ò‰ΩøÁî®‰∏≠ÂøÉÂ∑ÆÂàÜ (Central for Perturbed) = ÂØπÂéüÂßãÈóÆÈ¢ò‰ΩøÁî®ËøéÈ£éÂ∑ÆÂàÜ (Upwind for Original) [!claim|*] $$\\text{Central (Perturbed) }= \\text{Upwind (Original)}$$\nËøô‰∏™Á≠â‰ª∑ÂÖ≥Á≥ªÊè≠Á§∫‰∫ÜËøéÈ£éÊ≥ïÁöÑÊú¨Ë¥®ÔºöÂÆÉÈöêÂê´Âú∞ÂêëÂéüÂßãÊñπÁ®ã‰∏≠Ê∑ªÂä†‰∫Ü‰∏Ä‰∏™Êï∞ÂÄºÊâ©Êï£È°π„ÄÇËøô‰∏™È¢ùÂ§ñÁöÑÊâ©Êï£È°πÊòØËøéÈ£éÊ≥ïËÉΩÂ§üÊäëÂà∂Êï∞ÂÄºÊåØËç°ÁöÑÂÖ≥ÈîÆÂéüÂõ†„ÄÇ\n‰Ω©ÂÖãËé±Êï∞ÂàÜÊûê (P√©clet Number Analysis)\r#\rÂõûÈ°æ‰∏Ä‰∏ã‰Ω©ÂÖãËé±Êï∞ÁöÑÂÆö‰πâÔºö$\\text{Pe} = \\frac{|\\beta|\\Delta x}{2\\mu}$„ÄÇ\nÂØπ‰∫éÊâ∞Âä®Á≥ªÁªüÔºåÊâ©Êï£Á≥ªÊï∞Âèò‰∏∫ $\\mu^* = \\mu + \\frac{|\\beta|\\Delta x}{2} = \\mu(1 + \\text{Pe})$„ÄÇ\nÊâ∞Âä®Á≥ªÁªüÁöÑ‰Ω©ÂÖãËé±Êï∞‰∏∫Ôºö\n$$\\text{Pe}^* = \\frac{|\\beta|\\Delta x}{2\\mu^*} = \\frac{|\\beta|\\Delta x}{2\\mu(1+\\text{Pe})} = \\frac{\\text{Pe}}{1+\\text{Pe}} \u0026lt; 1 \\quad \\forall |\\beta| \\text{ Âíå } \\Delta x$$\nËøôË°®ÊòéÊó†ËÆ∫ $|\\beta|$ Âíå $\\Delta x$ ÁöÑÂÄºÂ¶Ç‰ΩïÔºåÊâ∞Âä®Á≥ªÁªüÁöÑ‰Ω©ÂÖãËé±Êï∞Ê∞∏ËøúÂ∞è‰∫é1ÔºåÂõ†Ê≠§ËøéÈ£éÊñπÊ≥ïÂßãÁªàÊòØÁ®≥ÂÆöÁöÑ„ÄÇ\n‰∏ÄËá¥ÊÄßÂàÜÊûê (Consistency Analysis)\r#\rÂΩì $\\Delta x \\to 0$ Êó∂Ôºå$\\mu^* \\to \\mu$ÔºåÊâ∞Âä®ÊñπÁ®ãË∂ãËøë‰∫éÂéüÂßãÊñπÁ®ãÔºåËøô‰øùËØÅ‰∫ÜÊñπÊ≥ïÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ\nÂØπ‰∫éÊâ∞Âä®Á≥ªÁªüÔºåÊàë‰ª¨‰ΩøÁî®‰∫Ü‰∫åÈò∂Á≤æÂ∫¶ÁöÑ‰∏≠ÂøÉÂ∑ÆÂàÜÊñπÊ≥ïÔºå‰ΩÜÂØπ‰∫éÂéüÂßãÈóÆÈ¢òÔºåÁî±‰∫éÂºïÂÖ•‰∫Ü‰∫∫Â∑•Êâ©Êï£È°πÔºåÂÆÉÂè™ÊòØ‰∏ÄÈò∂Á≤æÂ∫¶ÁöÑÊñπÊ≥ï„ÄÇ\nËØ¶ÁªÜËß£Èáä‰∏éÁâ©ÁêÜÊÑè‰πâ\r#\rËøéÈ£éÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØËÄÉËôëÁâ©ÁêÜ‰ø°ÊÅØÁöÑ‰º†Êí≠ÊñπÂêë„ÄÇÂú®ÊµÅ‰ΩìÊµÅÂä®‰∏≠ÔºåÂΩìÊüêÁÇπÁöÑÁâπÊÄßÔºàÂ¶ÇÊ∏©Â∫¶„ÄÅÊµìÂ∫¶ÔºâÂèóÂà∞‰∏äÊ∏∏ÁÇπÁöÑÂΩ±ÂìçÊõ¥Â§ßÊó∂ÔºåËøéÈ£éÊñπÊ≥ï‰ΩøÁî®‰∏äÊ∏∏ÁÇπÊù•ËÆ°ÁÆóÂØºÊï∞Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÂèçÊò†Áâ©ÁêÜÁé∞ÂÆû„ÄÇ\n‰ªéÊï∞ÂÄºÂàÜÊûêÁöÑËßíÂ∫¶ÁúãÔºåËøéÈ£éÊñπÊ≥ïÂºïÂÖ•‰∫Ü\u0026quot;‰∫∫Â∑•Êâ©Êï£\u0026quot;(artificial diffusion)ÔºåÂ¢ûÂº∫‰∫ÜÊï∞ÂÄºÊñπÊ≥ïÁöÑÁ®≥ÂÆöÊÄß„ÄÇËøôÁßç‰∫∫Â∑•Êâ©Êï£ÊÅ∞Â•ΩËÉΩÂ§üÊäµÊ∂à‰∏≠ÂøÉÂ∑ÆÂàÜÂú®È´ò‰Ω©ÂÖãËé±Êï∞ÊÉÖÂÜµ‰∏ã‰∫ßÁîüÁöÑÊï∞ÂÄºÊåØËç°„ÄÇ\nÁÑ∂ËÄåÔºåËøôÁßçÁ®≥ÂÆöÊÄßÊòØ‰ª•Á≤æÂ∫¶‰∏∫‰ª£‰ª∑ÁöÑ‚Äî‚ÄîËøéÈ£éÊñπÊ≥ïÁöÑÁ≤æÂ∫¶Èôç‰ΩéÂà∞‰∏ÄÈò∂ÔºàËØØÂ∑Æ‰∏é $\\Delta x$ ÊàêÊ≠£ÊØîÔºâÔºåËÄå‰∏≠ÂøÉÂ∑ÆÂàÜÊòØ‰∫åÈò∂Á≤æÂ∫¶ÔºàËØØÂ∑Æ‰∏é $\\Delta x^2$ ÊàêÊ≠£ÊØîÔºâ„ÄÇËøôÂú®Êï∞ÂÄºÊñπÊ≥ï‰∏≠ÊòØ‰∏Ä‰∏™Â∏∏ËßÅÁöÑÊùÉË°°ÔºöÊõ¥È´òÁöÑÁ®≥ÂÆöÊÄßÂæÄÂæÄ‰º¥ÈöèÁùÄÊõ¥‰ΩéÁöÑÁ≤æÂ∫¶„ÄÇ\nÂú®ÂØπÊµÅ‰∏ªÂØºÁöÑÈóÆÈ¢ò‰∏≠ÔºåÁ®≥ÂÆöÊÄßÈÄöÂ∏∏ÊØîÈ´òÁ≤æÂ∫¶Êõ¥ÈáçË¶ÅÔºåÂõ†‰∏∫‰∏çÁ®≥ÂÆöÁöÑËß£‰ºö‰∫ßÁîü‰∏•ÈáçÁöÑÈùûÁâ©ÁêÜÊåØËç°Ôºå‰ΩøÁªìÊûúÂÆåÂÖ®Êó†Áî®„ÄÇÂõ†Ê≠§ÔºåÂØπ‰∫éÈ´ò‰Ω©ÂÖãËé±Êï∞ÊµÅÂä®ÔºåËøéÈ£éÊñπÊ≥ïÂ∞ΩÁÆ°Á≤æÂ∫¶ËæÉ‰ΩéÔºå‰ΩÜÂæÄÂæÄÊòØÊõ¥ÂÆûÁî®ÁöÑÈÄâÊã©„ÄÇ\nÊõ¥È´òÈò∂ÁöÑÊñπÊ≥ïÔºåÂ¶ÇTVD (Total Variation Diminishing)„ÄÅENO (Essentially Non-Oscillatory) Âíå WENO (Weighted Essentially Non-Oscillatory) ÊñπÊ°àÔºåËØïÂõæÂú®‰øùÊåÅÁ®≥ÂÆöÊÄßÁöÑÂêåÊó∂ÊèêÈ´òÁ≤æÂ∫¶Ôºå‰ΩÜÂÆÉ‰ª¨ÁöÑÂÆûÁé∞Êõ¥‰∏∫Â§çÊùÇ„ÄÇ\nOur previous computation relies on symmetry. However, there is a clear physical information flow. So, this problem is asymmetric in reality. We don\u0026rsquo;t want as fancy as $\\sim \\mathcal{O}(\\Delta x^2)$ solutions, but we can use a $\\sim \\mathcal{O}(\\Delta x)$ method.\n$$\\beta\\frac{\\partial u}{\\partial x} \\approx \\beta\\frac{u_i - u_{i-1}}{\\Delta x} \\quad (\\beta \u0026gt; 0) \\quad \\text{(upwind)}$$\nNow, let\u0026rsquo;s show (upwind) is stable.\n$$\\beta\\frac{u_i - u_{i-1}}{\\Delta x} = \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\beta\\frac{u_{i+1}}{2\\Delta x} - \\beta\\frac{2u_i}{2\\Delta x} + \\beta\\frac{u_{i-1}}{2\\Delta x}$$\n$$= \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} - \\frac{\\beta\\Delta x}{2} \\cdot \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}$$\nCentral mean $\\quad$ Approx. of 2nd derivative\nSo, we can consider the equation: $$-\\left(\\mu + \\frac{|\\beta|\\Delta x}{2}\\right)\\frac{\\partial^2 u}{\\partial x^2} + \\beta\\frac{\\partial u}{\\partial x} = 0$$\nApply a central approximation: $$-\\mu\\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + \\beta\\frac{u_{i+1} - u_{i-1}}{2\\Delta x} = 0$$\nUpwind solution of the original problem.\nRecall P√©clet: $Pe = \\frac{|\\beta|\\Delta x}{2\\mu}$. Then, $\\mu^* = \\mu(1 + Pe)$.\nP√©clet of this perturbed system: $$Pe^* = \\frac{|\\beta|\\Delta x}{2\\mu^*} = \\frac{|\\beta|\\Delta x}{2\\mu(1+Pe)} = \\frac{Pe}{1+Pe} \u0026lt; 1 \\quad \\forall |\\beta| \\text{ and } \\Delta x$$\nSo, this upwind method is always stable.\nConsistency: when $\\Delta x \\to 0$, $\\mu^* \\to \\mu$.\nFor the perturbed system, we have a 2nd order approach, but with the original problem, it is only a 1st order method.\n3.4 Design a Better Method\r#\r$$\\mu^{smart} = \\mu(1 + \\Phi(Pe))$$\ns.t.\n$\\Phi(Pe) \\to 0 \\quad \\text{as} \\quad \\Delta x \\to 0$ $Pe^{smart} = \\frac{|\\beta|\\Delta x}{2\\mu^{smart}} \u0026lt; 1$ Our upwind method takes $\\Phi(Pe) = Pe \\sim \\mathcal{O}(\\Delta x)$. But can we take some $\\Phi(Pe) \\sim \\mathcal{O}(\\Delta x^2)$?\n$\\Rightarrow$ Scharfetter-Gummel Method: $\\Phi(Pe) = Pe - 1 + \\frac{2Pe}{e^{2Pe} - 1}$\n$\\Phi(Pe) \\uparrow$\n$\\Phi(Pe) = Pe$\n$\\Phi(Pe) \\sim \\mathcal{O}(\\Delta x)$\nThe worst case order of Scharfetter-Gummel is $\\sim \\mathcal{O}(\\Delta x^2)$.\nScharfetter-Gummel is also a special $\\Phi(Pe)$ that produces exact solutions.\n"},{"id":49,"href":"/docs/Mathematics/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.4-2D-Problem/","title":"9.4 4.1 ‰∫åÁª¥Ôºà2DÔºâÂÅèÂæÆÂàÜÊñπÁ®ãÈóÆÈ¢ò","section":"Á¨¨‰πùÁ´†","content":"\rÊ§≠ÂúÜÂûã\r#\r4.1.1 ÈóÆÈ¢òËÆæÂÆö\r#\rÊàë‰ª¨ËÄÉËôë‰∏Ä‰∏™‰∫åÁª¥Á©∫Èó¥ÁöÑÈóÆÈ¢òÔºåËÆ∞‰∏∫Âå∫Âüü $\\Omega$„ÄÇËæπÁïåËÆ∞‰∏∫ $\\partial\\Omega$„ÄÇ\nÊñπÁ®ã‰∏ÄËà¨ÂΩ¢ÂºèÂ¶Ç‰∏ãÔºö $$-\\mu \\Delta u + \\beta \\cdot \\nabla u = f,\\quad \\text{Âú®}\\ \\Omega ÂÜÖ$$\nËæπÁïåÊù°‰ª∂‰∏∫Ôºö $$u(\\partial \\Omega) = d \\quad (\\text{ÁªôÂÆöÁöÑÊï∞ÊçÆ})$$\nËøôÈáåÁöÑÁ¨¶Âè∑Ëß£ÈáäÔºàËßÅ‰∏ä‰∏ÄÁ´†ÔºâÔºö $\\beta$ÔºöË°®Á§∫\u0026quot;È£é\u0026quot;ÊàñËÄÖÂØπÊµÅÁöÑÊñπÂêëÂíåÂº∫Â∫¶ $\\mu$ÔºöÊâ©Êï£Á≥ªÊï∞ $f$ÔºöÂ§ñÂäõÊàñÊ∫êÈ°π Âõ†Ê≠§Ôºå‰∏äÈù¢ÁöÑÊñπÁ®ãÂèØ‰ª•Â±ïÂºÄÂÜôÊàêÔºö\n$$-\\mu \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right)\n\\beta_x \\frac{\\partial u}{\\partial x} + \\beta_y \\frac{\\partial u}{\\partial y} = f(x,y)$$ 4.1.2 ÁÆÄÂåñÊÉÖÂΩ¢ÔºàÂè™ÊúâÊâ©Êï£ÔºåÊ≤°ÊúâÈ£éÔºâ\r#\rÈ¶ñÂÖàËÄÉËôëÊõ¥ÁÆÄÂçïÁöÑÊÉÖÂÜµÔºåÂøΩÁï•ÂØπÊµÅÔºàÂç≥‚ÄúÂÖ≥ÊéâÈ£é‚ÄùÔºâÔºåÂèòÊàêÁ∫ØÊâ©Êï£ÈóÆÈ¢òÔºö\n$$-\\mu\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = f(x,y)$$\n4.1.3 Á¶ªÊï£ÂåñÊñπÊ≥ïÔºàÊúâÈôêÂ∑ÆÂàÜÊ≥ïÔºâ\r#\rÁî®ÊúâÈôêÂ∑ÆÂàÜÊ≥ïÊù•Êï∞ÂÄºÊ±ÇËß£Ôºö\nÂÅáËÆæÁ©∫Èó¥Ë¢´ÂàíÂàÜ‰∏∫‰∏Ä‰∏™ÁΩëÊ†ºÔºåÊØè‰∏™ÁΩëÊ†ºÁÇπÁî®ÂùêÊ†á $(i,j)$ Êù•Ë°®Á§∫‰ΩçÁΩÆÔºö\nÂú®$x$ÊñπÂêëÁöÑÈó¥Ë∑ù‰∏∫ $\\Delta x$ Âú®$y$ÊñπÂêëÁöÑÈó¥Ë∑ù‰∏∫ $\\Delta y$ ÂàôÂØπ‰∫é‰∫åÁª¥ÁöÑÊãâÊôÆÊãâÊñØÁÆóÂ≠êÔºåÂ∏∏Áî®ÁöÑ‰∏≠ÂøÉÂ∑ÆÂàÜÊ†ºÂºè‰∏∫Ôºö\n$$\\frac{\\partial^2 u}{\\partial x^2}\\approx \\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{\\Delta x^2}$$\n$$\\frac{\\partial^2 u}{\\partial y^2}\\approx \\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{\\Delta y^2}$$\n‰ª£ÂÖ•Êâ©Êï£ÊñπÁ®ãÂæóÂà∞Ôºö\n$$-\\mu\\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{\\Delta x^2} -\\mu\\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{\\Delta y^2} = f(x_i,y_j)$$\n4.1.4 ÂΩ¢ÊàêÁ∫øÊÄß‰ª£Êï∞ÊñπÁ®ãÁªÑ\r#\rÊâÄÊúâÁΩëÊ†ºÁÇπÁöÑÊú™Áü•Êï∞$u_{i,j}$ÊîæÂà∞‰∏Ä‰∏™ÂêëÈáè‰∏≠ÔºàÂêëÈáèËÆ∞‰∏∫$u$ÔºâÔºåËøôÊ†∑Â∞±ÂèØ‰ª•ÊääÁ¶ªÊï£ÂêéÁöÑÊñπÁ®ãÂÜôÊàê‰∏Ä‰∏™Áü©ÈòµÊñπÁ®ãÔºö\n$$A u = b$$\nËøôÈáåÔºö\n$A$ ÊòØÁ≥ªÊï∞Áü©ÈòµÔºàÁ®ÄÁñè„ÄÅÂØπÁß∞„ÄÅÊ≠£ÂÆöÔºåÁÆÄÁß∞SPDÔºâ $u$ ÊòØÊú™Áü•ÈáèÂêëÈáèÔºàÊâÄÊúâÁΩëÊ†ºÁÇπÁöÑËß£Ôºâ $b$ ÊòØÂ∑≤Áü•È°πÁöÑÂêëÈáèÔºàÊ∫êÈ°π$f$ÂíåËæπÁïåÊù°‰ª∂ÁöÑÁªÑÂêàÔºâ 4.2 Êó∂Èó¥Áõ∏ÂÖ≥ÈóÆÈ¢òÔºöÊäõÁâ©ÂûãÔºàParabolicÔºâ\r#\rËÄÉËôëÁöÑÈóÆÈ¢òÂΩ¢ÂºèÔºö\n$$\\frac{\\partial u}{\\partial t}-\\mu\\frac{\\partial^2 u}{\\partial x^2}=f,\\quad x\\in(0,1),\\quad 0\u0026lt;t\u0026lt;T$$\nÂàùÂÄº‰∏éËæπÁïåÊù°‰ª∂‰∏∫Ôºö\nÂàùÂÄºÔºö$u(x,t=0)=u_0(x)$ ËæπÁïåÊù°‰ª∂Ôºö$u(0,t)=u_L(t),\\quad u(1,t)=u_R(t)$ ÂçäÁ¶ªÊï£ÂåñÊñπÊ≥ïÔºàÁ©∫Èó¥Á¶ªÊï£ÔºåÊó∂Èó¥ËøûÁª≠Ôºâ\r#\rÊàë‰ª¨È¶ñÂÖàÂè™ÂØπÁ©∫Èó¥Ôºà$x$ÔºâÂÅöÁ¶ªÊï£ÂåñÔºåÂæóÂà∞Ôºö\n$$\\frac{d u_j(t)}{d t}-\\mu\\frac{u_{j+1}(t)-2u_j(t)+u_{j-1}(t)}{\\Delta x^2}=f_j(t)$$\nËÆ∞Ôºö\nÂêëÈáèÂΩ¢ÂºèÔºö$u(t)=[u_1(t),u_2(t),\u0026hellip;,u_n(t)]^T$ Ê∫êÈ°πÂêëÈáèÔºö$f(t)=[f_1(t),f_2(t),\u0026hellip;,f_n(t)]^T$ Á≥ªÊï∞Áü©ÈòµÔºö$A=\\frac{\\mu}{\\Delta x^2}\\text{tridiag}(-1,2,-1)$ ‰∫éÊòØÈóÆÈ¢òÂèòÊàêÂ∏∏ÂæÆÂàÜÊñπÁ®ãÔºàODEÔºâÁöÑÁ≥ªÁªüÂΩ¢ÂºèÔºö\n$$\\frac{d u}{d t}-A u=f$$\nÊó∂Èó¥Á¶ªÊï£ÂåñÔºàODEÊñπÊ≥ïÔºâ\r#\rÊé•‰∏ãÊù•Êàë‰ª¨ÂØπÊó∂Èó¥ËøõË°åÁ¶ªÊï£ÂåñÔºåÈááÁî®‰∏§ÁßçÊñπÊ≥ïÔºö\nÊñπÊ≥ï1ÔºöÊòæÂºèÊ¨ßÊãâÔºàExplicit Euler, FEÔºâ\r#\rÂ∞ÜÊó∂Èó¥ÂØºÊï∞Âú®Êó∂Èó¥ÁÇπ$t_n$Ëøë‰ºº‰∏∫Ôºö\n$$\\frac{u^{n+1}-u^n}{\\Delta t}=f^n+A u^n$$\nËß£ÂæóÔºö\n$$u^{n+1} = (I+\\Delta t A)u^n+\\Delta t f^n$$\nÊòæÂºèÊñπÊ≥ïÂÆπÊòìËÆ°ÁÆóÔºå‰ΩÜÁ®≥ÂÆöÊÄßÊúâÈôêÔºåÊó∂Èó¥Ê≠•Èïø‰∏çËÉΩÂ§™Â§ß„ÄÇ\nÊñπÊ≥ï2ÔºöÈöêÂºèÊ¨ßÊãâÔºàImplicit Euler, IE/BEÔºâ\r#\rÂè¶‰∏ÄÁßçÊñπÊ≥ïÊòØÂú®Êó∂Èó¥ÁÇπ$t_{n+1}$Â§ÑÊ±ÇÂØºÔºö\n$$\\frac{u^{n+1}-u^n}{\\Delta t}=f^{n+1}+A u^{n+1}$$\nËß£Âá∫ÈöêÂºèÊñπÁ®ãÔºö\n$$(I-\\Delta t A)u^{n+1}=u^n+\\Delta t f^{n+1}$$\nËøô‰∏™ÊñπÊ≥ïÊõ¥Á®≥ÂÆöÔºå‰ΩÜÈúÄË¶ÅÂú®ÊØè‰∏™Êó∂Èó¥Ê≠•Ëß£‰∏Ä‰∏™Á∫øÊÄßÁ≥ªÁªü„ÄÇ\nÂÖ≥‰∫éÁü©ÈòµÊÄßË¥®ÁöÑÊÄªÁªìÔºö\r#\r$A$ ‰∏∫SPDÁü©ÈòµÔºàÂØπÁß∞Ê≠£ÂÆöÔºâ ÂΩì $A$ Âíå $b$ ÊòØ‰∏éÊó∂Èó¥Êó†ÂÖ≥Êó∂ÔºåÈÄöÂ∏∏Êàë‰ª¨Êõ¥ÂñúÊ¨¢ÈöêÂºèÊñπÊ≥ïÔºåÂõ†‰∏∫ÂèØ‰ª•ÂàÜËß£Áü©Èòµ‰∏ÄÊ¨°ÔºàÂ¶ÇLUÂàÜËß£ÔºâÂπ∂ÂèçÂ§ç‰ΩøÁî®ÔºåÂä†Âø´ËÆ°ÁÆóÈÄüÂ∫¶„ÄÇ ÊÄªÁªì‰∏Ä‰∏ãÔºö\r#\r‰ª•‰∏äÊ∂âÂèä‰∫Ü‰∏§Á±ªÂÅèÂæÆÂàÜÊñπÁ®ãÈóÆÈ¢òÔºö\nÊ§≠ÂúÜÂûãÔºàÁ©∫Èó¥‰∫åÁª¥ÔºâÈóÆÈ¢òÔºåÈÄöËøáÁ©∫Èó¥Á¶ªÊï£ÂåñÁõ¥Êé•ÂæóÂà∞Á∫øÊÄßÊñπÁ®ãÁªÑÔºõ ÊäõÁâ©ÂûãÔºàÁ©∫Èó¥‰∏ÄÁª¥+Êó∂Èó¥ÔºâÈóÆÈ¢òÔºåÂÖàÂØπÁ©∫Èó¥Á¶ªÊï£Âèò‰∏∫ODEÔºåÂÜçÂØπÊó∂Èó¥Á¶ªÊï£‰ΩøÁî®ODEÊï∞ÂÄºÊñπÊ≥ïÔºàÊòæÂºè/ÈöêÂºèÊ¨ßÊãâÊñπÊ≥ïÔºâËøõË°åÊ±ÇËß£„ÄÇ ‰ª•‰∏äÊ≠•È™§ÈÄêÊ≠•Ëß£Èáä‰∫ÜÈóÆÈ¢òÂ¶Ç‰Ωï‰ªéËøûÁª≠ÂΩ¢ÂºèÂèòÊàêÊï∞ÂÄºÂèØÊ±ÇËß£ÁöÑÁ¶ªÊï£ÂΩ¢Âºè„ÄÇ\n$$ \\begin{align} -\\mu\\Delta u + \\vec{\\beta} \\cdot \\nabla u \u0026amp;= f \\ u(\\partial\\Omega) \u0026amp;= \\text{data} \\end{align} $$\nWrite it out: $$ \\begin{align} -\\mu\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) + \\beta_x\\frac{\\partial u}{\\partial x} + \\beta_y\\frac{\\partial u}{\\partial y} \u0026amp;= f(x,y) \\ u(\\partial\\Omega) \u0026amp;= d \\end{align} $$\n4.1 Only Consider Diffusion. Turn off Wind:\r#\rTo form a system: $(i,j) \\to k$.\n$Au = b$.\n$A$ is symmetric, SPD.\n4.2 Turn on the Wind. Upwind\r#\rWith upwind, the pts are not good.\n5. ÊäõÁâ©ÂûãÔºàParabolicÔºâÈóÆÈ¢òÔºàÊó∂Èó¥Áõ∏ÂÖ≥ÈóÆÈ¢òÔºâ\r#\r$$ \\begin{align} \\frac{\\partial u}{\\partial t} - \\mu\\frac{\\partial^2 u}{\\partial x^2} \u0026amp;= f, \\quad x \\in (0,1) \\quad 0 \u0026lt; t \u0026lt; T\\ u(0,t) \u0026amp;= u_L(t), \\quad u(1,t) = u_R(t) \\ u(x, t=0) \u0026amp;= u_0(x) \\end{align} $$\nDiscretization along $x$: semi-discretization: $u_j(t) \\approx u(x_j, t)$. $$\\frac{du_j}{dt} - \\mu\\frac{u_{j+1}(t) - 2u_j(t) + u_{j-1}(t)}{\\Delta x^2} = f_j(t) = f(x_j, t)$$\nSo, we form system $Au = f$.\n$$A = \\frac{\\mu}{\\Delta x^2}\\text{Triad}(-1, 2, 1), \\quad u(t) = \\begin{bmatrix} u_1(t) \\ \\vdots \\ u_n(t) \\end{bmatrix}, \\quad f(t) = \\begin{bmatrix} f_1(t) \\ \\vdots \\ f_n(t) \\end{bmatrix}$$\nThen, we have a system of ODE: $$\\frac{du}{dt} - Au = f$$\nWe can now do time discretization and use ODE methods.\nEE/FE: $$u^n = u(t^n), \\quad \\left.\\frac{du}{dt}\\right|_{t^n} \\approx \\frac{u^{n+1} - u^n}{\\Delta t} = f^n + Au^n$$\n$$u^{n+1} = u^n + \\Delta t Au^n + \\Delta t f^n = (I + \\Delta t A)u^n + \\Delta t f^n = (I + \\Delta t A)^n u_0 + \\Delta t f^n$$\nIE/BE: $$\\left.\\frac{du}{dt}\\right|_{t^n} = \\frac{u^n - u^{n-1}}{\\Delta t} = f^n + Au^n$$\n$$u^n - u^{n-1} = \\Delta t f^n + \\Delta t Au^n$$\n$$u^n - \\Delta t Au^n = \\Delta t f^n + u^{n-1}$$\n$$(I - \\Delta t A)u^n = u^{n-1} + \\Delta t f^n \\quad \\leftarrow \\text{A linear system to solve}$$\n$I - \\Delta t A$ is SPD and $A$ is time-independent. So, we may favor direct method (as we can store $A = LU$ and reuse it) over iterative methods.\nTo discuss stability, set $f = 0$:\nEE is conditionally stable: Let $\\lambda_i$ be eigenvalues of $A$.\n$$\\Delta t \u0026lt; \\frac{2}{|\\lambda_i|} \\quad \\text{for stability}$$\nFurther, $A = \\frac{\\mu}{\\Delta x^2}\\text{triad}(1, -2, 1)$, $\\rho(A) \\sim \\frac{c}{\\Delta x^2}$. So,\n$$\\Delta t \u0026lt; \\frac{2}{|\\lambda_i|} \\leq \\frac{2}{\\rho(A)} = \\frac{2}{c}\\Delta x^2$$\nSo, if we decrease $\\Delta x$ by 2, to have stability\n$$\\Delta t_{new} \u0026lt; \\frac{2}{c}\\left(\\frac{\\Delta x}{2}\\right)^2 = \\frac{\\Delta t_{old}}{4} \\quad \\Rightarrow \\text{we need finer intervals for time}$$\nIE is unconditionally stable.\nDef. ($\\theta$ Methods). $$\\frac{u^{n+1} - u^n}{\\Delta t} = \\theta Au^{n+1} + (1-\\theta)Au^n + \\theta f^{n+1} + (1-\\theta)f^n, \\quad \\theta \\in [0,1]$$\nEE: $\\theta = 0 \\quad \\mathcal{O}(\\Delta t) \\quad$ explicit $\\quad$ conditional stability\nIE: $\\theta = 1 \\quad \\mathcal{O}(\\Delta t) \\quad$ implicit $\\quad$ unconditional stability\nCN: $\\theta = \\frac{1}{2} \\quad \\mathcal{O}(\\Delta t^2) \\quad$ implicit $\\quad$ unconditional stability\nSuppose $f = 0$. Then, $$\\frac{u^{n+1} - u^n}{\\Delta t} = \\theta Au^{n+1} + (1-\\theta)Au^n$$\n$$u^{n+1} - u^n = \\Delta t \\theta Au^{n+1} + \\Delta t(1-\\theta)Au^n$$\n$$(I - \\Delta t \\theta A)u^{n+1} = (I + \\Delta t(1-\\theta)A)u^n$$\nWe essentially solve a linear system in each iteration.\n5.1 Thm (Stability and Order of $\\theta$ Methods)\r#\r$\\theta$ methods are unconditionally stable for $\\theta \\geq \\frac{1}{2}$. Otherwise, it is conditionally stable for $\\theta \u0026lt; \\frac{1}{2}$ and the stability condition for parabolic problem is $\\Delta t \u0026lt; c\\Delta x^2$. Meanwhile, the method is order 1 for $\\theta \\neq \\frac{1}{2}$ and order 2 for $\\theta = \\frac{1}{2}$. Although the $\\theta$ method is 2nd order in space, the order of error is dominant and determined by the order in time. CN is the most vulnerable to lack of regularity and sensitive to non-smoothness. 6. Hyperbolic Problems\r#\r6.1\r#\r$ \\begin{align} \\frac{\\partial u}{\\partial t} + \\alpha\\frac{\\partial u}{\\partial x} \u0026amp;= 0, \\quad \\alpha \u0026gt; 0, \\text{ constant} \\ u(x,0) \u0026amp;= u_0(x) \\end{align} $\nExact solution: $u(x,t) = u_0(x - \\alpha t)$\n6.2 Example: Modeling Density of Pollutant\r#\r$u$: pollutant, $x$: displacement of boat, $t$: time.\n$$ \\begin{align} \\frac{du}{dx} \u0026amp;= 0 \\quad \\text{(i.e, pollutant and boat moves at the same velocity)} \\ \\frac{dx}{dt} \u0026amp; = a \\quad \\text{(i.e., boat moves at velocity of $a$)} \\end{align}\n$$\n$x(t) = x_0 + at \\Rightarrow$ characteristic curves\n$u(x,t) = u_0(x-at)$. Solution to $\\begin{cases} \\frac{dx}{dt} = a \\ x(0) = x_0 \\end{cases}$\nConsider $u(x(t),t)$: $\\frac{du}{dt} = \\frac{\\partial u}{\\partial t} + \\frac{\\partial u}{\\partial x} \\cdot \\frac{dx}{dt} = \\frac{\\partial u}{\\partial t} + a \\cdot \\frac{\\partial u}{\\partial x} = 0$.\n6.3 Similar Problems:\r#\rConservation law $\\frac{\\partial u}{\\partial t} + \\frac{\\partial q(u)}{\\partial x} = 0$\n$q(u) = v(u) \\cdot u$ with $v = v_{max}\\left(1 - \\frac{u}{u_{max}}\\right)$\n$\\Rightarrow \\frac{\\partial u}{\\partial t} + v_{max}\\left(1 - \\frac{u}{u_{max}}\\right)\\frac{\\partial u}{\\partial x} = 0$. Models the density of traffic.\n$= \\alpha$, but $\\alpha$ is not constant here.\nHeat equation $\\frac{\\partial^2 u}{\\partial t^2} - v^2\\frac{\\partial^2 u}{\\partial x^2} = f$\nDefine $w_1 = \\frac{\\partial u}{\\partial x}$ and $w_2 = \\frac{\\partial u}{\\partial t}$.\n$ \\begin{align} \\frac{\\partial w_1}{\\partial t} - v^2\\frac{\\partial w_2}{\\partial x} \u0026amp;= f \\ \\frac{\\partial w_2}{\\partial t} - \\frac{\\partial w_1}{\\partial x} \u0026amp;= 0 \\quad \\frac{\\partial^2 u}{\\partial x \\partial t} = \\frac{\\partial^2 u}{\\partial t \\partial x} \\end{align} $\nDefine $w = \\begin{bmatrix} w_1 \\ w_2 \\end{bmatrix}$, $A = \\begin{bmatrix} 0 \u0026amp; -v^2 \\ -1 \u0026amp; 0 \\end{bmatrix}$\nThen, the original equation becomes a system: $\\frac{\\partial w}{\\partial t} + A\\frac{\\partial w}{\\partial x} = 0$\nThe eigenvalues of $A$: $\\lambda_{1,2} = \\pm iv$. $\\Rightarrow$ Diagonalizable.\nFind the numerical solution.\n$\\frac{\\partial u}{\\partial t}\\bigg|_{t^{n+1}, x_j} = \\frac{u_j^{n+1} - u_j^n}{\\Delta t}$\n$\\alpha\\frac{\\partial u}{\\partial x}\\bigg|{t^{n+1}, x_j} = \\frac{\\alpha}{2} \\cdot \\frac{u{j+1}^{n+1} - u_{j-1}^{n+1}}{\\Delta t}$\nWith BE-C: $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2} \\cdot \\frac{u_{j+1}^{n+1} - u_{j-1}^{n+1}}{\\Delta t} = 0$\n$ \\Rightarrow \\begin{bmatrix} \\frac{1}{\\Delta t} \u0026amp; \\frac{\\alpha}{2\\Delta t} \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \\ -\\frac{\\alpha}{2\\Delta t} \u0026amp; \\frac{1}{\\Delta t} \u0026amp; \\frac{\\alpha}{2\\Delta t} \u0026amp; 0 \u0026amp; \\cdots \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \u0026amp; \\end{bmatrix} $\nWith FE-C: unconditionally unstable. NEVER USE IT! $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2} \\cdot \\frac{u_{j+1}^n - u_{j-1}^n}{\\Delta t} = 0$\n$\\Rightarrow u_j^{n+1} = u_j^n + \\frac{\\alpha\\Delta t}{2\\Delta t}(u_{j+1}^n - u_{j-1}^n)$\nWith FE-Upwind: $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\alpha\\frac{u_j^n - u_{j-1}^n}{\\Delta x} = 0 \\quad \\alpha \u0026gt; 0$\n$\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\alpha\\frac{u_{j+1}^n - u_j^n}{\\Delta x} = 0 \\quad \\alpha \u0026lt; 0$\n$\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2}\\frac{u_{j+1}^n - u_{j-1}^n}{\\Delta x} - \\frac{|\\alpha|\\Delta t}{2}\\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\\Delta x^2} = 0$\nDiffusion\nLax Wendroff: FE-upwind with modified coefficient $\\frac{u_j^{n+1} - u_j^n}{\\Delta t} + \\frac{\\alpha}{2}\\frac{u_{j+1}^n - u_{j-1}^n}{\\Delta x} - \\frac{\\alpha^2\\Delta t}{2}\\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\\Delta x^2} = 0$\n$u(x_j, t^{n+1}) = u(x_j, t^n) + \\frac{\\partial u}{\\partial t}\\bigg|{t^n, x_j}(t^{n+1} - t^n) + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial t^2}\\bigg|{t^n, x_j}(t^{n+1} - t^n)^2 + \\mathcal{O}(||t^{n+1} - t^n||^3)$\n$\\frac{\\partial u}{\\partial t} = -\\alpha\\frac{\\partial u}{\\partial x}, \\quad \\frac{\\partial^2 u}{\\partial x \\partial t} = -\\alpha\\frac{\\partial^2 u}{\\partial x^2}, \\quad \\frac{\\partial^2 u}{\\partial t^2} = \\alpha^2\\frac{\\partial^2 u}{\\partial x^2}$\nSubstitute: $u_j^{n+1} = u_j^n - \\alpha\\left(\\frac{u_{j+1}^n - u_{j-1}^n}{2\\Delta x}\\right)\\Delta t + \\frac{\\alpha^2}{2}\\left(\\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\\Delta x^2}\\right)\\Delta t^2$\nStability: $\\left|\\frac{\\alpha\\Delta t}{\\Delta x}\\right| \\leq 1$\n"},{"id":50,"href":"/docs/Mathematics/MATH-351-352/Heat-Equation/The-Fourier-Series/","title":"ÂÇÖÁ´ãÂè∂Á∫ßÊï∞","section":"ÁÉ≠ÊñπÁ®ã","content":"\r1. ÂÇÖÁ´ãÂè∂Á∫ßÊï∞ÁöÑÂü∫Êú¨ÂΩ¢ÊÄÅ\r#\r1.1 Âë®Êúü\r#\rÂú®ÁªèÂéÜ‰∫ÜÂë®Êúü$T$ÂêéÔºåÈáçÊñ∞Ëé∑ÂæóÂéüÂÄºÁöÑÂáΩÊï∞‰∏∫Âë®ÊúüÂáΩÊï∞Ôºö\n$$ \\varphi(t+T)=\\varphi $$\n1.2 Ê≠£Âº¶ÂûãÈáè\r#\rÊ≠£Âº¶ÂûãÈáèÂΩ¢Â¶ÇÔºö\n$y(t)=Asin(\\omega t+\\alpha)$ where $\\omega = \\cfrac{2\\pi}{T}$ is the frequency.\nNotice that for values $s.t.$:\n$$ y_0 = A_0, \\ y_1=A_1\\sin(\\omega t + \\alpha_1), \\ y_2=A_2\\sin(2\\omega t + \\alpha_2), \\ y_3=A_3\\sin(3\\omega t + \\alpha_3) \u0026hellip; $$\nwe have frequency as the multiple of the smallest frequency with their period:\n$\\omega$, $2\\omega$, $3\\omega$‚Ä¶\n$T$, $\\frac{1}{2}T$, $\\frac{1}{3}T$‚Ä¶\n![[Fourier Series.png]]\nIf we add them together, we get‚Ä¶\n1.3.1 ‰∏âËßíÁ∫ßÊï∞ - $\\varphi(t)$ÂáΩÊï∞\r#\rüí° **It is possible to represent any periodic function with finite or infinite summations of sin functions:**\r$$ \\varphi(t)=A_0 + \\sum_{n=1}^{\\infty}A_n\\sin(n\\omega t+ \\alpha_n) $$\nwhere $A_n, \\alpha_n$ are constants\n$\\varphi(t)$ÂèØ‰ª•Ë¢´ÂàÜËß£ÊàêÊó†Êï∞‰∏™Ë∞ÉÂíåÈúáÂä® ÊØè‰∏ÄÈ°πÁß∞‰πã‰∏∫Ë∞ÉÂíåÁ¥† ÂØπ$\\varphi(t)$ËøõË°åÂàÜËß£ÁöÑÊâãÊ≥ïË¢´Áß∞‰πã‰∏∫Ë∞ÉÂíåÂàÜÊûê 1.3.2 ‰∏âËßíÁ∫ßÊï∞ - ÊúÄÁªàÂ±ïÂºÄÂºè\r#\rÊ≥®ÊÑèpptÈáåÁöÑÂÖ¨ÂºèÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑÊç¢ÂÖÉÔºå$x=\\frac{2Lt}{T}$ÔºåÊâÄ‰ª•$\\frac{\\pi}{L}$Ë¢´ÊäΩÂá∫Êù•‰∫Ü„ÄÇËøôÈáåÊàë‰ª¨‰ºöÁî®Ê≠£Â∏∏ÁöÑÈÄªËæëÊç¢ÂÖÉÊé®ÂØºÔºåÁ≠âÂà∞ÂêéÈù¢ÈúÄË¶ÅËÆ°ÁÆógeneric intervalÔºåÊâçÈáçÊñ∞Êç¢‰∏Ä‰∏™$L$ËøõÂéª***\nÂΩìÊàë‰ª¨Áî®$x=\\omega t = \\frac{2\\pi t}{T}$ Êù•Êç¢ÂÖÉ $s.t.$ $f(x)=\\varphi(t) = \\varphi(\\frac{x}{\\omega})$ ÔºåÊàë‰ª¨Áî®‰∏âËßíÊ≠£Âº¶ÂÖ¨ÂºèÂ±ïÂºÄÔºö\n$$ \\begin{align}f(x)\u0026amp;= A_0 + \\sum_{n=1}^{\\infty}A_n\\sin(nx+ \\alpha_n)\\ \u0026amp;=A_0 + \\sum_{n=1}^{\\infty}A_n(\\cos nx\\sin \\alpha_n+ \\sin nx \\cos \\alpha_n) \\\\end{align} $$\nÊ≠£Âº¶ÂÖ¨ÂºèÔºö$\\sin(a+b)=\\cos a\\sin b + \\sin a\\cos b$ Â±ïÂºÄ\n$$ f(x)=A_0+\\sum_{n=1}^{\\infty}[(A_n\\sin \\alpha_n)\\cos nx+ (A_n \\cos \\alpha_n)\\sin nx )] $$\nÂπ∂‰ª§ $A_0=a_0, \\ A_n\\sin\\alpha_n = b_n, \\ A_n\\cos\\alpha_n = b_n$\nüí° ‰∫éÊòØÊàë‰ª¨Â∞±Êúâ‰∫Ü‰ºöÊúâ‰∏âËßíÁ∫ßÊï∞ÁöÑ**ÊúÄÁªàÂ±ïÂºÄÂΩ¢ÊÄÅ**Ôºö\r$$ f(x)=a_0 + \\sum_{n=1}^{\\infty}(a_n\\cos nx+ b_n\\sin nx) $$\nthe period for $f$ is $2\\pi$ due to our definition of new independent variable $x$ 2. ÂÇÖÁ´ãÂè∂Á∫ßÊï∞ÁöÑÁ≥ªÊï∞\r#\r2.1 Ê¨ßÊãâ-ÂÇÖÁ´ãÂè∂ÂÖ¨ÂºèÔºàEuler-Fourier formulaÔºâ\r#\rËøô‰∏™ÊòØ‰∏Ä‰∏™18‰∏ñÁ∫™ÂàùÊ¨ßÊãâ‰ΩøÁî®ÁöÑÁ≥ªÊï∞Á°ÆÂÆöÊ≥ï„ÄÇÂêéÈù¢Êàë‰ª¨ËøòÂ≠¶‰∫ÜÊ≥õÂáΩÂàÜÊûêÁöÑinner productÁÆóÁ≥ªÊï∞ÁöÑÊñπÊ≥ï„ÄÇ\nAssuming $f(x)$ under $[-\\pi,\\pi]$ is an integrable function, if we assume Fourier Expansion for $f(x)$ is true, then directly we have:\n$$ \\begin{align}\\int_{-\\pi}^{\\pi} f(x) , dx = 2\\pi a_0 + \\sum_{n=1}^{\\infty} \\left[ a_n \\int_{-\\pi}^{\\pi} \\cos nx , dx + b_n \\int_{-\\pi}^{\\pi} \\sin nx , dx \\right]\\end{align} $$\nobviously, the integration for $\\cos$ and $\\sin$ for $[-\\pi,\\pi]$ is 0 regardless of the values, so we only left with:\n$$ a_0= \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} f(x) , dx $$\nnext, if to find the specific value for arbitrary $a_m$ ($m=1,2,3\u0026hellip;$), we multiply $(3)$ by $\\cos (ma)$ so that the terms we needed wouldn‚Äôt cancel out, eventually we reached at:\n$$ a_m= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x) \\cos(ma) , dx $$\nSimilarly, multiply by $\\sin(ma)$, we derive:\n$$ b_m= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x) \\sin(ma) , dx $$\nËøô‰∫õÂÖ¨ÂºèË¢´Áß∞‰πã‰∏∫Ê¨ßÊãâ-ÂÇÖÁ´ãÂè∂ÂÖ¨ÂºèÔºàEuler-Fourier formulaÔºâÔºåËÄå‰ªñ‰ª¨ÁÆóÂá∫Êù•ÁöÑÊï∞ÂÄºË¢´Áß∞‰∏∫Â∑≤ÁªôÂáΩÊï∞ÁöÑÂÇÖÁ´ãÂè∂Á≥ªÊï∞ÔºàFourier CoefficientÔºâ„ÄÇ\n2.2* ÁãÑÂà©ÂÖãÈõ∑ÁßØÂàÜÔºàDirichlet integralÔºâ\r#\rÈÄöËøáÂÇÖÁ´ãÂè∂Â±ïÂºÄÁöÑ‰∏Ä‰∏™ÂÆöÁÇπ$x=x_0$ÁöÑÊÄßË¥®ÔºåËé∑ÂæóÁöÑÈáçË¶ÅÁßØÂàÜÔºö\n$$ s_n(x_0) = \\frac{1}{\\pi} \\int_{0}^{\\pi} \\left[ f(x_0 + t) + f(x_0 - t) \\right] \\frac{\\sin (n + \\frac{1}{2})t }{2 \\sin\\frac{t}{2}} , dt\n$$\ne.x. https://www.bilibili.com/video/BV1XE411p7ZN/\n3*. Âπø‰πâÂΩ¢ÊÄÅÁöÑÂÇÖÁ´ãÂè∂Á∫ßÊï∞ÂíåÁ≥ªÊï∞ÂÖ¨Âºè\r#\r3.1 ‰ªªÊÑèÂå∫Èó¥ÁöÑÊÉÖÂÜµ\r#\rüí° We can separate out a $\\frac{1}{2}$ from the original $a_0$ coefficient, so the first term becomes a special case for $n=0$.\rÂú®‰ªªÊÑè$2L$Â§ßÂ∞èÁöÑÂå∫Èó¥ $(-L,L]$ ‰∏äÔºåÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî®ÂèòÊç¢Ôºö$x=\\frac{Ly}{\\pi} (-\\pi\u0026lt;y\\leq\\pi)$ ‰ΩøÂæó$f(x)\\rightarrow f(\\frac{Ly}{\\pi})$ „ÄÇ‰∫éÊòØÔºåÊàë‰ª¨Ê†πÊçÆÂÖ¨ÂºèËé∑ÂæóÔºö\n$$ f\\left( \\frac{Ly}{\\pi} \\right) = \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} \\left( a_n \\cos ny + b_n \\sin ny \\right) $$\n‰ª•ÂèäÂÖ∂Á≥ªÊï∞Ôºö\n$$ a_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f\\left( \\frac{y}{\\pi} \\right) \\cos ny , dy \\quad (n = 0, 1, 2, \\ldots) $$\n$$\nb_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f\\left( \\frac{y}{\\pi} \\right) \\sin ny , dy \\quad (n = 1, 2, \\ldots) $$\nÂΩìÊàë‰ª¨ÈáçÊñ∞ÂèòÊç¢ÂõûÂéªÔºåÂç≥‰ΩøÂæó$y = \\cfrac{\\pi x}{L}$ÔºåÈÇ£‰πàÊàë‰ª¨Â∞±‰ºöËé∑Âæó‰∏Ä‰∏™Âπø‰πâ‰∏äÁöÑÂÇÖÁ´ãÂè∂Â±ïÂºÄÔºÅ\nüí° The general form of **Fourier Expansion**:\r$$ \\begin{align}f(x)=\\frac{A_0}{2} + \\sum_{n=1}^{\\infty} \\left( A_n \\cos \\frac{n\\pi x}{L} + B_n \\sin \\frac{n\\pi x}{L} \\right) \\end{align} $$\nHere, $x$ is no longer the angle, but the integer multiples of $\\frac{\\pi x}{L}$, such that the Fourier Coefficient for generic interval $[-L,L]$ are:\n$$ \\begin{align} A_n \\equiv \\frac{1}{L} \\int_{-L}^{L} f(x) \\cos\\left(\\frac{n\\pi x}{L}\\right) dx, \\quad n = 0, 1, 2, \\ldots \\ B_n \\equiv \\frac{1}{L} \\int_{-L}^{L} f(x) \\sin\\left(\\frac{n\\pi x}{L}\\right) dx, \\quad n = 1, 2, 3, \\ldots \\end{align}\n$$\n3.2 ËèúÂ∞±Â§öÁªÉÔºåÁªôÁà∑Â±ï\r#\r$f(x)=e^{ax}$, $a\\neq0$ on the interval of $(-\\pi,\\pi)$\nÁ≠îÊ°à\n$f(x)=\\frac{\\pi-x}{2}$ on the interval of $(0,2\\pi)$\nÁ≠îÊ°à\n$f(x)=x^2$ on the interval of $[-\\pi,\\pi]$, expand as cosine series.\nÁ≠îÊ°à\nTwo functions ($a$ is assumed to be non-integers):\n$f_1(x)=\\cos(ax)$ on the interval of $[-\\pi,\\pi]$, expand as cosine series.\nÁ≠îÊ°à\n$f_2(x)=\\sin(ax)$ on the interval of $(-\\pi,\\pi)$, expand as sine series.\nÁ≠îÊ°à\n$f(x)=e^{ax}$, $a\\neq0$ on the interval of $(0,\\pi)$:\nexpand as cosine series expand as sine series Á≠îÊ°à\n4. ÂÇÖÈáåÂè∂Á∫ßÊï∞ÁöÑÊãìÊâëÁ©∫Èó¥\r#\rüí° ÁêÜËß£ÂÇÖÈáåÂè∂Á∫ßÊï∞ÂèòÊç¢ÁöÑÊãìÊâëÁ©∫Èó¥ÁöÑÈÄªËæëÊòØÔºö\r‰∏âÁª¥Á©∫Èó¥‚Äî‚Äî¬†$n$Áª¥Á©∫Èó¥‚Äî‚Äî$\\infty$Áª¥Á©∫Èó¥‚Äî‚ÄîÂ∏åÂ∞î‰ºØÁâπÁ©∫Èó¥Ôºà$L^2$Á©∫Èó¥Ôºâ‚Äî‚ÄîÂÇÖÁ´ãÂè∂Á∫ßÊï∞‚Äî‚ÄîÂÇÖÁ´ãÂè∂ÂèòÊç¢\n4.1 $\\infty$Áª¥Ê¨ßÂºèÁ©∫Èó¥ - $\\R^{\\infty}$\r#\r4.1.1 $n$Áª¥Á©∫Èó¥ÁöÑÂÆö‰πâ- $\\R^{n}$\r#\rËåÉÊï∞ÔºàNormÔºâ:\n$$ N_p(x) = | x |_p = \\left( |x_1|^p + |x_2|^p + \\cdots + |x_n|^p \\right)^{\\frac{1}{p}} $$\nÂçï‰ΩçÊ≠£‰∫§ÂêëÈáèÂü∫Ôºàstandard orthogonal basisÔºâ:\n$$ \\left{¬†\\begin{aligned}\\vec{e}_1 \u0026amp;= (1, 0, \\ldots, 0), \\\\vec{e}_2 \u0026amp;= (0, 1, \\ldots, 0), \\\u0026amp; \\vdots \\\\vec{e}_n \u0026amp;= (0, 0, \\ldots, 1)\\end{aligned}\\right. $$\nÂõ†Ê≠§$n$Áª¥Ê¨ßÂºèÁ©∫Èó¥ÁöÑ‰ªª‰ΩïÂêëÈáèÂèØ‰ª•Ë¢´Áõ¥Êé•Ë°®ËææÊàêÔºö\n$$ \\vec{a} = \\sum_{i=1}^{n} a_i \\vec{e}_i $$\n4.1.2 Áî±Ê≠§Êé®Âá∫$\\infty$Áª¥Á©∫Èó¥ - $\\R^{\\infty}$\r#\rÂêåÁêÜÔºåÂΩìÊàë‰ª¨ÊúâÊó†Êï∞‰∏™Ê†áÂáÜÂü∫Ôºå${ \\vec{e}_1 , \\vec{e}_2 , \\vec{e}_3 \\ldots}$ÔºåÁöÑÊó∂ÂÄôÔºåÂêëÈáèÂèòÊàê‰∫Ü\n$$ \\vec{a} = \\sum_{i=1}^{\\infty} a_i \\vec{e}_i $$\n‰ΩÜÊòØ‰ªñ‰ª¨‰æùÁÑ∂ÊòØÁ¶ªÊï£ÁöÑÂÖÉÁ¥† 4.2 HilbertÁ©∫Èó¥\r#\rÊàë‰ª¨ÈúÄË¶ÅÊääÁ¶ªÊï£ÁöÑÂÖÉÁ¥†ËøáÂ∫¶Âà∞ËøûÁª≠ÁöÑÂáΩÊï∞ÔºåHilbertÁ©∫Èó¥Ôºö\nËÆæÂÆö‰∏Ä‰∏™ÂêëÈáèÂáΩÊï∞$\\vec{f}(x)$\nÂπ∂‰∏îÂ≠òÂú®‰∏ÄÁªÑÂü∫ÂáΩÊï∞Ôºàstandard orthogonal functionsÔºâ\n${ \\vec{\\varphi}_1 , \\vec{\\varphi}_2 , \\vec{\\varphi}_3 \\ldots}$ $s.t.$ we have\n$$ \\vec{f}(x) = \\sum_{i=1}^{\\infty} a_i \\vec{\\varphi_i}(x) $$\nÂõ†Ê≠§ÔºåÊàë‰ª¨Âú®HilbertÁ©∫Èó¥Â∞±Êúâ‰∫ÜËøôÊ†∑ÁöÑÊÄßË¥®Ôºö\nüí° 1. ÂáΩÊï∞Âú®Âå∫Èó¥$[a,b]$‰∏äÁöÑÊ®°ÔºànormÔºâÔºö\r$$ | f(x) | = \\sqrt{\\int_a^b f^2(x) , dx} ; $$\nÂ¶ÇÊûú‰∏§‰∏™ÂáΩÊï∞ÁöÑÊ≠£‰∫§Êù°‰ª∂ÔºàorthogonalityÔºâ‰∏∫ÂÖ∂ÂÜÖÁßØÔºàinner productÔºâ‰∏∫Èõ∂Ôºö $$ \\int_{a}^{b} f(x) g(x) , dx = 0 $$\n‰∏§ËÄÖÁöÑËßíÁöÑ‰ΩôÂº¶‰∏∫ $$ \\cos(\\theta) = \\frac{\\langle f(x), g(x) \\rangle}{| f(x) | \\cdot | g(x) |} = \\frac{\\int f(x)g(x) , dx}{\\sqrt{\\int f^2(x) , dx} \\sqrt{\\int g^2(x) , dx}} ; $$\n4.3 $L^p$ Á©∫Èó¥ÔºàLebesgue SpaceÔºâ\r#\rÂÆö‰πâÔºöSuppose $f(x)$ is measurable functions on $E\\subset R^n$.\nFor $0\u0026lt;p\u0026lt;\\infty$, we denote:\n$$ ||f||_p=(\\int_E|f(x)|^pdx)^{1/p} $$\nÊàë‰ª¨Áî®$L^p(E)$Êù•Ë°®Á§∫$||f||_p\u0026lt;\\infty$ÁöÑÂÖ®‰ΩìÂáΩÊï∞ÔºåÁß∞ÂÖ∂‰∏∫$L^p$Á©∫Èó¥„ÄÇ\nÂÖ∂ËåÉÊï∞ÔºànormÔºâ‰∏∫Ôºö\n$$ L_p = | \\varphi |p = \\left( \\sum{i=1}^{n} |\\varphi_i|^p \\right)^{\\frac{1}{p}}, \\quad x = (x_1, x_2, \\ldots, x_n) $$\n$L^p$Á©∫Èó¥ÁöÑ‰∏Ä‰∫õÂü∫Á°ÄÂ±ûÊÄßÔºö\n$L$Á©∫Èó¥ÈáåÁöÑÊØè‰∏Ä‰∏™ÂáΩÊï∞ÈÉΩÊòØLebesgueÂèØÁßØÁöÑ Á©∫Èó¥Áª¥Â∫¶ÊòØÊó†Á©∑ËÄå‰∏î‰∏çÂèØÊï∞ÁöÑÂ∫¶ÈáèÁ©∫Èó¥ÔºàMetric SpaceÔºâ BanachÁ©∫Èó¥ÔºåÊàñËÄÖÂÆåÂ§áËµãËåÉÂêëÈáèÁ©∫Èó¥ (complete normed vector space) ÂΩì$p=2$Ôºå$L^2$ÂèòÊàê‰∫Ü‰∏Ä‰∏™HilbertÁ©∫Èó¥Ôºå‰∏Ä‰∏™Â∏¶ÊúâÂÜÖÁßØÁöÑBanachÁ©∫Èó¥ A¬†Hilbert space¬†is a¬†real¬†or¬†complex¬†inner product space¬†that is also a¬†complete metric space¬†with respect to the distance function¬†induced¬†by the inner product. In every Hilbert Space, we have functions that\n$$ \\langle x, y \\rangle = \\sum_{k=1}^{n} \\overline{x_k} y_k $$\n$L^2$Á©∫Èó¥ÁöÑËåÉÊï∞**ÔºànormÔºâ**ÔºåÊ†πÊçÆ‰πãÂâçÁöÑÂÖ¨ÂºèÔºåÊàë‰ª¨ÂèØ‰ª•ÂæóÁü•Ôºö\n$$ | \\varphi |2 = \\sqrt{ \\sum{i=1}^{n} |\\varphi_i|^2 }, \\quad x = (x_1, x_2, \\ldots, x_n) $$\np.s. ‰ª•Ôºàp=2Ôºâ‰∏∫‰æãÔºåÁ©∫Èó¥‰∏≠Âà∞ÂéüÁÇπÁöÑÊ¨ßÊ∞èË∑ùÁ¶ª‰∏∫1ÁöÑÁÇπÊûÑÊàê‰∫Ü‰∏Ä‰∏™ÁêÉÈù¢„ÄÇ\n![[L-space-1.png]]\n4.4 ÂÇÖÈáåÂè∂Á∫ßÊï∞ÁöÑÊ≠£‰∫§ÂáΩÊï∞Á≥ª‰∏éËßÑËåÉÁ≥ª\r#\rÂÆö‰πâÔºöif $\\int_{a}^{b} \\varphi(x) \\psi(x) , dx = 0$ for interval $[a,b]$, then $\\varphi(x), \\psi(x)$ are orthogonal.\n4.4.1 Ê≠£‰∫§ÂáΩÊï∞Á≥ª (orthogonal functions)\r#\rÂΩìÊØèÂØπÂèåÂáΩÊï∞$\\varphi_n(x), \\ \\varphi_m(x)$ ÈÉΩÁ¨¶ÂêàÂÆö‰πâ‰ª•‰∏ãÔºåÊàë‰ª¨Áß∞ËøôÊ†∑ÁöÑÂáΩÊï∞Áæ§‰Ωì‰∏∫Ê≠£‰∫§ÂáΩÊï∞Á≥ª Ôºàorthogonal groupÔºâÔºö\n$$ \\int_{a}^{b} \\varphi_n(x) \\varphi_m(x) , dx = 0 \\space \\space\\space\\space\\space\\space\\space\\space {n,m\\in \\N\\ | \\ n\\neq m } $$\n4.4.2 ËßÑËåÉÊ≠£‰∫§ÂáΩÊï∞Á≥ª (orthonormal functions)\r#\rËã•ÂáΩÊï∞ÁöÑ $\\lambda_n=1$ $(n=1,2,3\u0026hellip;)$, ÈÇ£‰πàËøô‰æøÊòØËßÑËåÉÁ≥ªÔºàorthonormal groupÔºâÔºö\n$$ \\int_{a}^{b} \\varphi_n^2(x) , dx = \\lambda_n $$\nËã•‰∏çÊòØorthogonalÔºåÂàôÊàë‰ª¨ÂèØ‰ª•ÈÄöËøá$\\left{ \\cfrac{\\varphi_n(x)}{\\sqrt{\\lambda_n}} \\right}$ Êù•ËøõË°åÊç¢ÂèñÊñ∞ÁöÑorthogonal functions„ÄÇ\n4.5 ÂÇÖÈáåÂè∂Á∫ßÊï∞Âú®$L^2$ Á©∫Èó¥ÁöÑÂü∫ÂáΩÊï∞\r#\rNotice back to this set of functions inside the interval $[-\\pi,\\pi]$:\n$$1, \\cos(x), \\sin(x), \\cos(2x), \\sin(2x),\u0026hellip;\\cos(nx), \\sin(nx),\u0026hellip;$$\nThese are orthogonal basis functions for their vector space because we see for any two functions in this set $\\int_{-\\pi}^{\\pi} \\cos(mx) \\cdot \\sin(nx) , dx = 0.$\nHowever, they are not standard, or normed, because $\\lambda_n \\neq1$.\nÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáÊ®°ÔºànormÔºâÁöÑÂÆö‰πâËé∑Âæó‰ªñ‰ª¨ÁöÑnormalizing coefficientÔºö\n$$|1| = \\sqrt{\\int_{-\\pi}^{\\pi} 1^2 , dx} = \\sqrt{2\\pi}$$\n$$|\\cos(nx)| = \\sqrt{\\int_{-\\pi}^{\\pi} \\cos^2(nx) , dx} = \\sqrt{\\pi}$$\n$$|\\sin(nx)| = \\sqrt{\\int_{-\\pi}^{\\pi} \\sin^2(nx) , dx} = \\sqrt{\\pi}$$ Then, we would have a standard orthonormal functions: $$\\frac{1}{\\sqrt{2\\pi}}, \\ldots, , \\frac{\\cos nx}{\\sqrt{\\pi}}, , \\frac{\\sin nx}{\\sqrt{\\pi}}, , \\ldots$$\nLet‚Äôs define them:\n$$\\psi_0=\\frac{1}{\\sqrt{2\\pi}}, \\psi_j=\\frac{1}{\\sqrt{\\pi}}\\cos(jx), \\varphi_j=\\frac{1}{\\sqrt{\\pi}}\\sin(jx)$$\n$$\\psi_0\\equiv\\sqrt L\\quad \\psi_j\\equiv\\sqrt L\\cos\\left ( \\frac{j\\pi}{L}x \\right )\\quad \\varphi_j\\equiv\\sqrt L\\sin\\left ( \\frac{j\\pi}{L}x \\right) $$\nsuch that, we obtain the coefficient for each standard basis functions needed:\n$$\\begin{align} a_0 \u0026amp;= \\langle f,\\psi_0\\rangle=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\pi}^{\\pi} f(x) , dx\\a_j \u0026amp;=\\langle f,\\psi_k\\rangle=\\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\cos(jx) , dx\\ b_j \u0026amp;=\\langle f,\\varphi_k\\rangle =\\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\sin(kx) , dx \\end{align} $$\nThen, for the function $\\mathcal X$ and its coffecient $C_k$ defined as\n$$ \\mathcal X_k=( \\psi_k ,\\varphi_k ), \\quad C_k=(a_j,b_j) $$\nwe have a function space:\n$$ \\mathcal F(f)=\\sum_{k=0}^\\infty C_k \\cdot \\mathcal X_k $$\nÂ¶ÇÂêå$\\vec{a} = \\sum_{i=1}^{n} a_i \\vec{e}_i \\quad$‰∏ÄÊ†∑ÔºåÊàë‰ª¨Êúâ$f(x) = a_0 \\frac{1}{\\sqrt{2\\pi}} + a_1 \\frac{\\cos x}{\\sqrt{\\pi}} + b_1 \\frac{\\sin x}{\\sqrt{\\pi}} + a_2 \\frac{\\cos 2x}{\\sqrt{\\pi}} + b_2 \\frac{\\sin 2x}{\\sqrt{\\pi}} + \\cdots$ÔºåÊàñËÄÖ\n$$\nf(x) = a_0 \\frac{1}{\\sqrt{2\\pi}} + \\sum_{k=1}^{\\infty} a_k \\frac{\\cos kx}{\\sqrt{\\pi}} + \\sum_{k=1}^{\\infty} b_k \\frac{\\sin kx}{\\sqrt{\\pi}} $$\n‰ª£ÂÖ•$a_0,a_k,b_k$Âà∞‰∏äÂºè‰∏≠ÔºåÊàë‰ª¨Ëé∑ÂæóÔºö\n$$ f(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\pi}^{\\pi} f(x) , dx \\frac{1}{\\sqrt{2\\pi}} + \\sum_{k=1}^{\\infty} \\frac{\\cos kx}{\\sqrt{\\pi}}\\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\cos kx , dx + \\sum_{k=1}^{\\infty} \\frac{\\sin kx}{\\sqrt{\\pi}} \\frac{1}{\\sqrt{\\pi}} \\int_{-\\pi}^{\\pi} f(x) \\sin kx , dx $$\nÂΩìÊàë‰ª¨ÁÆÄÂåñ‰πãÂêéÔºåÂπ∂‰∏îÂ∞ÜÁ≥ªÊï∞È°πÊõ¥Êñ∞ÔºåÊàë‰ª¨ÊúÄÁªàÊúâ‰∫ÜÁé∞Âú®ÁöÑÂÇÖÈáåÂè∂Â±ïÂºÄÂºèÔºö\n$$ f(x)=A_0 + \\sum_{n=1}^{\\infty}(A_n\\cos nx+ B_n\\sin nx) $$\n$$ \\begin{align} A_0 \u0026amp;= \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} f(x) , dx\\A_n \u0026amp;= \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\cos(nx) , dx\\ B_n \u0026amp;= \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\sin(nx) , dx \\end{align} $$\n"}]