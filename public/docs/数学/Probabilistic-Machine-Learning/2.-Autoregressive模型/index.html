<!doctype html><html lang=en dir=ltr><head><script>(function(){const e=localStorage.getItem("theme");e&&document.documentElement.setAttribute("data-theme",e)})()</script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="An AR (autoregressive) generative model represents the joint probability of a sequence by chaining conditionals left-to-right (the chain rule
$$P\left(w_{1: T}\right)=\prod_{t=1}^T P\left(w_t \mid w_{<t}\right)$$

  1. Probability Distributions (recap)
  #


Discrete: $X ∈ {1,…,K}, ; P(X=i)=p_i, ; \sum_i p_i=1.$
Continuous: $\int p(x) dx = 1, ; p(x) ≥ 0.$
Multivariate: $p(\mathbf{x}) = p(x_1,…,x_d).$


  Autoregressive Models
  #


Scope. This note mirrors the theoretical portions of Lecture 2: Autoregressive Models and keeps the same notation as the slides/notebook. We use:"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://erisehe.github.io/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/2.-Autoregressive%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="学习笔记"><meta property="og:title" content="学习笔记"><meta property="og:description" content="An AR (autoregressive) generative model represents the joint probability of a sequence by chaining conditionals left-to-right (the chain rule $$P\left(w_{1: T}\right)=\prod_{t=1}^T P\left(w_t \mid w_{<t}\right)$$
1. Probability Distributions (recap) # Discrete: $X ∈ {1,…,K}, ; P(X=i)=p_i, ; \sum_i p_i=1.$ Continuous: $\int p(x) dx = 1, ; p(x) ≥ 0.$ Multivariate: $p(\mathbf{x}) = p(x_1,…,x_d).$ Autoregressive Models # Scope. This note mirrors the theoretical portions of Lecture 2: Autoregressive Models and keeps the same notation as the slides/notebook. We use:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-09-03T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-03T00:00:00+00:00"><title>2. Autoregressive模型 | 学习笔记</title>
<link rel=icon href=/notebook/topo.png><link rel=manifest href=/notebook/manifest.json><link rel=canonical href=https://erisehe.github.io/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/2.-Autoregressive%E6%A8%A1%E5%9E%8B/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/computer-modern-font@1.0.1/index.min.css><link rel=stylesheet href=/notebook/book.min.d4fcf68ad3a73edc9e9b9a7d234d919af3b78be64e2fa759b4532b4e02091d2d.css integrity="sha256-1Pz2itOnPtyem5p9I02RmvO3i+ZOL6dZtFMrTgIJHS0=" crossorigin=anonymous><script defer src=/notebook/fuse.min.js></script><script defer src=/notebook/en.search.min.7f2da718abc7a38664f7a08a0e49cca385aa15ddfc4923c0d01cc0091a631e5c.js integrity="sha256-fy2nGKvHo4Zk96CKDknMo4WqFd38SSPA0BzACRpjHlw=" crossorigin=anonymous></script><script defer src=/notebook/sw.min.6d6d6cd5980f9f82f4b196d40b720c7ccec378cd7fb657b73ecdcb888b1dfac7.js integrity="sha256-bW1s1ZgPn4L0sZbUC3IMfM7DeM1/tle3Ps3LiIsd+sc=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class=container><div class=book-layout><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center justify-center" href=https://erisehe.github.io/notebook/><span>学习笔记</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><nav class=subject-menu><div class=subject-tabs><button class="subject-tab active" data-subject=数学>数学</button>
<button class=subject-tab data-subject=物理>物理</button></div><div class=subject-content><div class=subject-panel data-subject=数学><ul><li class=book-section-flat><span>点集拓扑学基础</span><ul><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/%E6%8B%93%E6%89%91%E5%AD%A6/T-n%E5%88%86%E7%A6%BB%E5%85%AC%E7%90%86/>T N分离公理</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-2626c0c7c40e5ba289bbf96779ed7be9 class=toggle>
<label for=section-2626c0c7c40e5ba289bbf96779ed7be9 class="flex justify-between"><a role=button>微分几何</a></label><ul><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/1.-Overview-of-Differential-Geometry/>1. Overview of Differential Geometry</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/2.-Parametrized-Curves-and-Surfaces/>2. Parametrized Curves and Surfaces</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/Lecture-2-ACTUAL/>Lecture 2 ( Actual)</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/Tangent-Vectors-on-a-Surface/>Tangent Vectors on a Surface</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-6e5f2db787427bb8bccbb3c3eafa6cb6 class=toggle checked>
<label for=section-6e5f2db787427bb8bccbb3c3eafa6cb6 class="flex justify-between"><a role=button>概率机器学习</a></label><ul><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/2.-Autoregressive%E6%A8%A1%E5%9E%8B/ class=active>2. Autoregressive模型</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/3.-Autoregressive%E6%A8%A1%E5%9E%8B-2/>3. Autoregressive模型 2</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-e9a29b9c49921437d6f50841fa0b7a2c class=toggle>
<label for=section-e9a29b9c49921437d6f50841fa0b7a2c class="flex justify-between"><a role=button>实分析 II</a></label><ul><li><input type=checkbox id=section-e63b59f3907b6b5c0ca40a1419c50fbe class=toggle>
<label for=section-e63b59f3907b6b5c0ca40a1419c50fbe class="flex justify-between"><a role=button>第六章 可微映射</a></label><ul><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.4-Sufficient-Conditions-for-Differentiability/>6.4 可微分性的必要条件</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.6-Product-Rule-and-Gradients/>6.6 乘积法则与梯度</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.9-Taylor-Higher-Dimentions/>6.9 泰勒公式的高维形式</a></li></ul></li><li><input type=checkbox id=section-516b62a4eda22b0c44d32f044080ba6a class=toggle>
<label for=section-516b62a4eda22b0c44d32f044080ba6a class="flex justify-between"><a role=button>第七章 逆函数和隐函数定理</a></label><ul><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1-Inverse-Function-Theorem/>7.1 反函数定理</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1.1-Inverse-FT-Proof/>7.1.1 反函数定理（证明）</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.2-Implicit-Function-Theorem/>7.2 隐函数定理</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.6-Morse-Lemma/>7.6 莫尔斯引理</a></li></ul></li><li><input type=checkbox id=section-7be78291145a12f96f25aeffb3b16554 class=toggle>
<label for=section-7be78291145a12f96f25aeffb3b16554 class="flex justify-between"><a role=button>第八章 度量理论</a></label><ul><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.1-%E5%BA%A6%E9%87%8F%E7%90%86%E8%AE%BA/>8.1 度量理论</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.2-Criterion-for-Integrability/>8.2 Criterion for Integrability</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.3-Proof-of-Lebesgues-Theorem/>8.3 Proof of Lebesgue's Theorem</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.5-Improper-Integral/>8.5 不定积分</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.6-Lebesgue-Convergence-Theorem/>8.6 勒贝格收敛定理</a></li></ul></li><li><input type=checkbox id=section-56e664b694094194eea41293621edc8e class=toggle>
<label for=section-56e664b694094194eea41293621edc8e class="flex justify-between"><a role=button>第九章</a></label><ul><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1-Computing-Integrals/>积分计算</a></li></ul></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%8D%81%E7%AB%A0/10.1-Fourier-Analysis/>10.1 Fourier Analysis</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-358a11d2cd962ab8fdb9c9ba7faf532a class=toggle>
<label for=section-358a11d2cd962ab8fdb9c9ba7faf532a class="flex justify-between"><a role=button>偏微分方程</a></label><ul><li><input type=checkbox id=section-7097c8722aa477fcc3c6ccc5730510d5 class=toggle>
<label for=section-7097c8722aa477fcc3c6ccc5730510d5 class="flex justify-between"><a role=button>数值方法</a></label><ul><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/01-Mid-Point-Method/>01 Mid Point Method</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/02-Three-Point-Backward-Differentiation-Formula/>02 Three Point Backward Differentiation Formula</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/03-Numerical-Solution-of-Ordinary-Differential-Equations/>常微分方程的数值解</a></li><li><input type=checkbox id=section-039c2604ef8d15d0cdd62911e85f20cb class=toggle>
<label for=section-039c2604ef8d15d0cdd62911e85f20cb class="flex justify-between"><a role=button>第四章 有限元分析</a></label><ul></ul></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E5%8D%81%E7%AB%A0/Finite-Element-Method/>Finite Element Method</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E5%8D%81%E7%AB%A0/Quiz-4/>Quiz 4</a></li><li><input type=checkbox id=section-24cf0d34881526d98bfa6ff3655d07cb class=toggle>
<label for=section-24cf0d34881526d98bfa6ff3655d07cb class="flex justify-between"><a role=button>第九章</a></label><ul><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1-Boundary-Value-Problems/>9.1 边值问题的近似</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.2-Finite-Difference/>9.2 有限差分法</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.3-Advection-Diffusion-Equation/>9.3 对流-扩散方程</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.4-2D-Problem/>9.4 4.1 二维（2D）偏微分方程问题</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/Exercise/>习题</a></li></ul></li></ul></li><li><input type=checkbox id=section-2e9d41360137a1ae6194db40069654e3 class=toggle>
<label for=section-2e9d41360137a1ae6194db40069654e3 class="flex justify-between"><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/Heat-Equation/>热方程</a></label><ul><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/Heat-Equation/Heat-Equation-Solution/>Heat Equation Solution</a></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/Heat-Equation/The-Fourier-Series/>傅立叶级数</a></li></ul></li><li><input type=checkbox id=section-7f42ebdf414be775241761c5b3262618 class=toggle>
<label for=section-7f42ebdf414be775241761c5b3262618 class="flex justify-between"><a role=button>波方程</a></label><ul></ul></li><li><input type=checkbox id=section-3cdad47fb9156c74a9aac21d7840334d class=toggle>
<label for=section-3cdad47fb9156c74a9aac21d7840334d class="flex justify-between"><a role=button>拉普拉斯方程</a></label><ul></ul></li></ul></li><li><a href=/notebook/docs/%E6%95%B0%E5%AD%A6/PS/>Ps</a></li></ul></div><div class="subject-panel hidden" data-subject=物理><ul></ul></div></div></nav><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".subject-tab"),s=document.querySelectorAll(".subject-panel");function t(t){e.forEach(e=>e.classList.remove("active")),s.forEach(e=>e.classList.add("hidden"));const n=document.querySelector(`.subject-tab[data-subject="${t}"]`);n&&n.classList.add("active");const o=document.querySelector(`.subject-panel[data-subject="${t}"]`);o&&o.classList.remove("hidden"),localStorage.setItem("activeSubjectTab",t)}const n=localStorage.getItem("activeSubjectTab");n&&t(n),e.forEach(e=>{e.addEventListener("click",function(){const n=e.getAttribute("data-subject");t(n)})})})</script><div class=after-menu-offset><ul><li><a href=/notebook/posts/>Blog</a></li><li><a href=https://github.com/EriseHe/notebook target=_blank rel=noopener>Github</a></li><li><a href=https://themes.gohugo.io/themes/hugo-book/ target=_blank rel=noopener>Hugo Themes</a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/notebook/svg/menu.svg class=book-icon alt=Menu></label><h3>2. Autoregressive模型</h3><label for=toc-control><img src=/notebook/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#1-probability-distributions-recap>1. Probability Distributions (recap)</a></li></ul></li><li><a href=#autoregressive-models>Autoregressive Models</a><ul><li><a href=#1-generative-modeling-two-distinct-tasks>1. Generative Modeling: Two Distinct Tasks</a></li><li><a href=#2-probability-recap-notation--constraints>2. Probability Recap (Notation & Constraints)</a><ul><li><a href=#21-conditional-probability>2.1 Conditional Probability</a></li><li><a href=#22-joint-distribution>2.2 Joint distribution</a></li></ul></li><li><a href=#3-autoregressive-ar-factorization-via-the-chain-rule>3. Autoregressive (AR) Factorization via the Chain Rule</a></li><li><a href=#4-training--density-estimation-maximum-likelihood>4. Training = Density Estimation (Maximum Likelihood)</a><ul><li><a href=#41-entropy>4.1 Entropy</a></li><li><a href=#42-cross-entropy>4.2 Cross-Entropy</a></li><li><a href=#43-kullbackleibler-divergence>4.3 Kullback–Leibler divergence</a></li><li><a href=#44-perplexity>4.4 Perplexity</a></li></ul></li><li><a href=#5-inference--sampling--generation-left-to-right>5. Inference = Sampling / Generation (Left-to-Right)</a><ul><li><a href=#51-decoding-strategies>5.1 Decoding Strategies</a></li></ul></li><li><a href=#6-why-ar-models-are-attractive>6. Why AR Models are Attractive</a><ul><li><a href=#61-ar-vs-other-generative-families>6.1 AR vs. Other Generative Families</a></li></ul></li><li><a href=#9-minimal-derivations-collected>9. Minimal Derivations (Collected)</a><ul><li><a href=#91-chain-rule-for-sequences>9.1 Chain Rule for Sequences</a></li><li><a href=#92-mle--cross-entropy-minimization>9.2 MLE = Cross-Entropy Minimization</a></li><li><a href=#93-perplexity-and-bits-per-token>9.3 Perplexity and Bits-per-Token</a></li></ul></li></ul></li></ul></nav></aside></header><article class=book-article><h1 style="text-align:center;font-size:1.8em;font-family:computer modern,cmu serif,serif"></h1><div class="markdown posts"><p>An AR (autoregressive) generative model represents the <strong>joint probability of a sequence</strong> by chaining conditionals left-to-right (the chain rule
$$P\left(w_{1: T}\right)=\prod_{t=1}^T P\left(w_t \mid w_{&lt;t}\right)$$</p><h2 id=1-probability-distributions-recap>1. Probability Distributions (recap)
<a class=anchor href=#1-probability-distributions-recap>#</a></h2><ul><li>Discrete: $X ∈ {1,…,K}, ; P(X=i)=p_i, ; \sum_i p_i=1.$</li><li>Continuous: $\int p(x) dx = 1, ; p(x) ≥ 0.$</li><li>Multivariate: $p(\mathbf{x}) = p(x_1,…,x_d).$</li></ul><h1 id=autoregressive-models>Autoregressive Models
<a class=anchor href=#autoregressive-models>#</a></h1><blockquote><p><strong>Scope.</strong> This note mirrors the theoretical portions of <em>Lecture 2: Autoregressive Models</em> and keeps the <strong>same notation</strong> as the slides/notebook. We use:</p><ul><li>Generic data: $x \in \mathcal{X}$, model $p_\theta(x)$.</li><li>Sequences for language: $Y=(w_1,\dots,w_T)$ with tokens $w_t \in \mathcal{V}$.</li><li>Context prefix: $w_{&lt;t} := (w_1,\dots,w_{t-1})$.</li><li>When needed, a neural representation $h_t=f_\theta(\cdot)$ and a softmax decoder.</li></ul></blockquote><hr><h2 id=1-generative-modeling-two-distinct-tasks>1. Generative Modeling: Two Distinct Tasks
<a class=anchor href=#1-generative-modeling-two-distinct-tasks>#</a></h2><p>A <strong>generative model</strong> is a probability distribution $p_\theta$ meant to approximate the true (unknown) data distribution $p_{\text{data}}$.</p><ul><li><p><strong>(1) Density Estimation.</strong>
<strong>Input:</strong> a concrete point $x$.
<strong>Output:</strong> the model’s likelihood (or density/mass) $p_\theta(x)$.
<strong>Use cases:</strong> evaluation, model selection, anomaly detection, compression.</p></li><li><p><strong>(2) Sampling / Generation.</strong>
<strong>Input:</strong> nothing (optionally, a prompt or random seed).
<strong>Output:</strong> a new sample $x&rsquo; \sim p_\theta$ that <em>looks like</em> data.
<strong>Use cases:</strong> simulation, text/image/audio generation.</p></li></ul><blockquote><p><strong>Key distinction.</strong> These are <strong>different computational problems</strong>. Density estimation needs a normalized $p_\theta(\cdot)$ you can evaluate; sampling needs a way to <em>draw</em> from $p_\theta$.</p></blockquote><p><strong>Explicit</strong> density models (e.g., autoregressive models) support <strong>both</strong> tasks. <strong>Implicit</strong> models (e.g., GANs) emphasize <strong>sampling</strong> but typically do not provide tractable $p_\theta(x)$.</p><h2 id=2-probability-recap-notation--constraints>2. Probability Recap (Notation & Constraints)
<a class=anchor href=#2-probability-recap-notation--constraints>#</a></h2><ul><li><strong>Discrete:</strong> $X\in{1,\dots,K}$, $P(X=i)=p_i$, $\sum_i p_i=1$.</li><li><strong>Continuous:</strong> $p(x)\ge 0$, $\int p(x),dx=1$.</li><li><strong>Multivariate:</strong> $p(\mathbf{x})=p(x_1,\dots,x_d)$.</li></ul><p>High-dimensional joint distributions (e.g., images with $10^5$–$10^6$ pixels) are combinatorially huge, motivating structured factorizations.</p><h3 id=21-conditional-probability>2.1 Conditional Probability
<a class=anchor href=#21-conditional-probability>#</a></h3><p>The definition of <strong>conditional probability</strong> for events is
$$
\boxed{P(A \mid B)=\frac{P(A \cap B)}{P(B)}}
$$
and equivalently, the &ldquo;product rule&rdquo;:
$$P(A \cap B)=P(A \mid B) P(B)=P(B \mid A) P(A)$$</p><h3 id=22-joint-distribution>2.2 Joint distribution
<a class=anchor href=#22-joint-distribution>#</a></h3><p><strong>Joint distribution</strong> can be written as the product of conditionals, that is:
$$
\begin{aligned}
p(x_1, x_2, \dots, x_n)
&= p(x_1), p(x_2, \dots, x_n \mid x_1) \[6pt]
&= p(x_1), p(x_2 \mid x_1), p(x_3, \dots, x_n \mid x_1, x_2) \[6pt]
&= p(x_1), p(x_2 \mid x_1), p(x_3 \mid x_1, x_2), p(x_4, \dots, x_n \mid x_1, x_2, x_3) \[6pt]
&;;\vdots \[6pt]
&= \prod_{i=1}^n p(x_i \mid x_1, \dots, x_{i-1}).
\end{aligned}
$$</p><h2 id=3-autoregressive-ar-factorization-via-the-chain-rule>3. Autoregressive (AR) Factorization via the Chain Rule
<a class=anchor href=#3-autoregressive-ar-factorization-via-the-chain-rule>#</a></h2><p>For a token sequence $Y=(w_1,\dots,w_T)$, the <strong>chain rule</strong> gives
$$
P(Y) ;=; P(w_1,\dots,w_T)
;=; \prod_{t=1}^{T} P!\big(w_t \mid w_{&lt;t}\big)
$$
where $w_{&lt;t}:=(w_{1,\dots}w_{t-1})$. In other words, for any $T \geq 2$,
$$
\boxed{\begin{aligned}
p!\left(w_{1:T}\right)
&= p!\left(w_T \mid w_{&lt;T}\right), p!\left(w_{&lt;T}\right) \
&= p!\left(w_T \mid w_{&lt;T}\right), p!\left(w_{T-1} \mid w_{&lt;T-1}\right), p!\left(w_{&lt;T-1}\right) \
&;;\vdots \
&= \prod_{t=1}^T p!\left(w_t \mid w_{&lt;t}\right)
\end{aligned}}
$$
An <strong>autoregressive (AR) model</strong> chooses a <em>direction/order</em> (here: left-to-right) and <strong>parametrizes each conditional</strong>:
$$
p_\theta!\big(w_t \mid w_{&lt;t}\big) ;=; \operatorname{Softmax}!\big(Wh_{t-1}+b\big),
\quad h_{t-1} ;=; f_\theta(e_1,\dots,e_{t-1}),
$$
where $e_t$ are token embeddings. Because each factor is a <strong>proper distribution</strong>, the product is <strong>normalized</strong>—giving an <strong>exact</strong> likelihood.</p><blockquote><p><strong>Order matters.</strong> AR models require a fixed order; for text we use left→right, but any total order (e.g., pixels in a raster scan) is valid.</p></blockquote><h2 id=4-training--density-estimation-maximum-likelihood>4. Training = Density Estimation (Maximum Likelihood)
<a class=anchor href=#4-training--density-estimation-maximum-likelihood>#</a></h2><h3 id=41-entropy>4.1 Entropy
<a class=anchor href=#41-entropy>#</a></h3><p>The entropy of a probability distribution can be interpreted as a <strong>measure of uncertainty</strong>, or lack of predictability, associated with a random variable drawn from a given distribution. We can also use entropy to define the information content of a data source.</p><p>Definition (<em>theoretical</em>) for population <strong>entropy (discrete)</strong> of a discrete distribution $p$ is:
$$
\boxed{\begin{aligned}
H(p) &\triangleq-\sum_x p(x) \log p(x)\&=-\mathbb{E}_X\left[\log p(X)\right]
\end{aligned}}
$$</p><h3 id=42-cross-entropy>4.2 Cross-Entropy
<a class=anchor href=#42-cross-entropy>#</a></h3><p>Definition (<em>theoretical</em>) for population <strong>cross-entropy</strong> between data $p$ and model $q$ is:​
$$\boxed{H(p, q) \triangleq-\sum_x p(x) \log q(x)}$$
And the <em>empirical estimator</em> (what we compute on data $\left{w_1, \ldots, w_N\right}$ ) for AR model becomes
$$
\boxed{H(\hat{p},q)=-\frac{1}{N} \sum_{t=1}^N \log q\left(w_t \mid w_{&lt;t}\right)}
$$</p><ul><li>$N$: number of tokens in the test sequence</li><li>$w_t$: the $t$-th token</li><li>$w_{&lt;t}$: all tokens before position $t$</li><li>$q(w_t \mid w_{&lt;t})$: the probability assigned by the model</li></ul><p>Given a dataset $\mathcal D={Y^{(n)}}<em>{n=1}^N$, <strong>maximum likelihood estimation (MLE)</strong> maximizes
$$
\mathcal{L}(\theta)
= \sum</em>{n=1}^N \log q\big(Y^{(n)}\big)
= \sum_{n=1}^N \sum_{t=1}^{T^{(n)}} \log q\big(w^{(n)}<em>t \mid w^{(n)}</em>{&lt;t}\big).
$$
Equivalently, we <strong>minimize next-token cross-entropy</strong>:
$$
\underbrace{-\frac{1}{N}\sum_{n,t} \log q\big(w^{(n)}<em>t \mid w^{(n)}</em>{&lt;t}\big)}_{\text{empirical NLL}}
\quad\Longleftrightarrow\quad
\text{CrossEntropy}(p, q).
$$</p><h3 id=43-kullbackleibler-divergence>4.3 Kullback–Leibler divergence
<a class=anchor href=#43-kullbackleibler-divergence>#</a></h3><p>For a true next-token data distribution $p$ and model prediction $q$, the definition for <strong>KL divergence</strong> is:
$$
\boxed{D_{\mathrm{KL}}\left(p | q\right)=\sum_x p(x) \log \frac{p(x)}{q(x)} \geq 0}
$$</p><ul><li>$H(p)$ : <em>entropy</em> of the true distribution (constant w.r.t. the model)</li><li>$D_{\mathrm{KL}}(p | q)$ : <em>penalty</em> for how far $q$ is from $p$</li></ul><p>and $D_{KL} =0 \Longleftrightarrow p=q \text { (almost everywhere) }$. The <strong>identity</strong> linking all three is:
$$
\boxed{H(p,q) ;=; H(p) + D_{\mathrm{KL}}(p!\parallel q)}
$$
Since $H(p)$ is independent of $q$, minimizing cross-entropy (and perplexity) is equivalent to minimizing $D_{\mathrm{KL}}(p\parallel q)$.</p><h3 id=44-perplexity>4.4 Perplexity
<a class=anchor href=#44-perplexity>#</a></h3><p><strong>Perplexity (PP)</strong> of a model is the <em>exponential</em> of the cross-entropy:
$$
\boxed{\text{PP}(q) = \exp!\Big(H(p,q)\Big)
= \exp!\left(-\frac{1}{N}\sum_{t=1}^N \log q(w_t \mid w_{&lt;t})\right)}
$$</p><ul><li>Perplexity can be understood as the <strong>effective average branching factor</strong> of the model.</li><li>A <strong>lower perplexity</strong> means the model is less “perplexed” → better predictions.</li><li>A <strong>higher perplexity</strong> means the model is more “confused” → worse predictions.</li></ul><p>Examples:</p><ul><li>Perfect model → $\text{PP} = 1$</li><li>Uniform distribution over $V$ tokens → $\text{PP} = V$</li></ul><h2 id=5-inference--sampling--generation-left-to-right>5. Inference = Sampling / Generation (Left-to-Right)
<a class=anchor href=#5-inference--sampling--generation-left-to-right>#</a></h2><p>To <strong>generate</strong> text:</p><ol><li>Start from a beginning-of-sequence token $\langle\mathrm{bos}\rangle$ (or a prompt).</li><li>For $t=1,2,\dots$: draw $w_t \sim p_\theta(\cdot \mid w_{&lt;t})$.</li><li>Stop at $\langle\mathrm{eos}\rangle$ or a length limit.</li></ol><h3 id=51-decoding-strategies>5.1 Decoding Strategies
<a class=anchor href=#51-decoding-strategies>#</a></h3><ul><li><strong>Greedy (argmax):</strong> $w_t=\arg\max_v p_\theta(v\mid w_{&lt;t})$. Deterministic; can be bland.</li><li><strong>Random sampling:</strong> sample from $p_\theta(\cdot\mid\cdot)$ directly.</li><li><strong>Temperature:</strong> rescale logits/probabilities by $1/T$ before sampling; $T&lt;1$ sharper, $T>1$ more diverse.</li><li><strong>Top-$k$:</strong> restrict to the $k$ most probable tokens, renormalize, then sample.</li><li><strong>Nucleus (top-$p$):</strong> choose the smallest set $S$ such that $\sum_{v\in S} p_\theta(v\mid\cdot)\ge p$; sample within $S$.</li><li><strong>Beam search:</strong> approximate $\arg\max_Y p_\theta(Y)$ by expanding the $B$ best partial hypotheses.</li></ul><hr><h2 id=6-why-ar-models-are-attractive>6. Why AR Models are Attractive
<a class=anchor href=#6-why-ar-models-are-attractive>#</a></h2><ul><li><strong>Exact likelihood.</strong> Each conditional is normalized → tractable training and evaluation (perplexity, NLL).</li><li><strong>Simple sampling.</strong> Left-to-right draws match the factorization.</li><li><strong>Flexible function class.</strong> Conditionals $p_\theta(w_t\mid w_{&lt;t})$ can be parameterized by RNNs, LSTMs/GRUs, Transformers, etc.</li></ul><p><strong>Trade-off:</strong> Sampling is <strong>sequential</strong>—inherently less parallel than feed-forward decoders (but see practical accelerations such as speculative decoding; beyond this lecture’s scope).</p><hr><h3 id=61-ar-vs-other-generative-families>6.1 AR vs. Other Generative Families
<a class=anchor href=#61-ar-vs-other-generative-families>#</a></h3><ul><li><strong>AR (explicit):</strong> exact likelihood; slow sampling.</li><li><strong>Normalizing flows (explicit):</strong> exact likelihood; fast sampling; invertibility constraints.</li><li><strong>VAEs (explicit latent):</strong> tractable <strong>lower bounds</strong> on likelihood; amortized inference.</li><li><strong>GANs (implicit):</strong> high-fidelity samples; no tractable likelihood (density estimation not available).</li></ul><h2 id=9-minimal-derivations-collected>9. Minimal Derivations (Collected)
<a class=anchor href=#9-minimal-derivations-collected>#</a></h2><h3 id=91-chain-rule-for-sequences>9.1 Chain Rule for Sequences
<a class=anchor href=#91-chain-rule-for-sequences>#</a></h3><p>$$
\begin{aligned}
P(w_1,\dots,w_T)
&= P(w_1),P(w_2\mid w_1)\cdots P(w_T\mid w_{&lt;T}) \
&= \prod_{t=1}^T P(w_t\mid w_{&lt;t}).
\end{aligned}
$$</p><h3 id=92-mle--cross-entropy-minimization>9.2 MLE = Cross-Entropy Minimization
<a class=anchor href=#92-mle--cross-entropy-minimization>#</a></h3><p>Let $\hat p$ be the empirical distribution from $\mathcal D$. Then</p><p>$$
\arg\max_\theta ;\mathbb{E}<em>{Y\sim \hat p}\big[\log p</em>\theta(Y)\big]
= \arg\min_\theta ; D_{\mathrm{KL}}(\hat p\parallel p_\theta)
= \arg\min_\theta ; \mathbb{E}<em>{\hat p}\big[-\log p</em>\theta(Y)\big].
$$</p><h3 id=93-perplexity-and-bits-per-token>9.3 Perplexity and Bits-per-Token
<a class=anchor href=#93-perplexity-and-bits-per-token>#</a></h3><p>$$
\mathrm{PP} ;=; \exp!\Big(\tfrac{\mathrm{NLL}}{T}\Big),
\qquad
\mathrm{BPT} ;=; \tfrac{\mathrm{NLL}}{T\ln 2}
\quad\Rightarrow\quad
\mathrm{PP} ;=; 2^{\mathrm{BPT}}.
$$</p></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/EriseHe/notebook/edit/main//content.en/docs/%e6%95%b0%e5%ad%a6/Probabilistic%20Machine%20Learning/2.%20Autoregressive%e6%a8%a1%e5%9e%8b.md target=_blank rel=noopener><img src=/notebook/svg/edit.svg class=book-icon alt>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside id=book-toc class="book-toc hidden"><div class=book-toc-content><div class=button-prev></div><div class=toc-entries><nav id=TableOfContents><ul><li><ul><li><a href=#1-probability-distributions-recap>1. Probability Distributions (recap)</a></li></ul></li><li><a href=#autoregressive-models>Autoregressive Models</a><ul><li><a href=#1-generative-modeling-two-distinct-tasks>1. Generative Modeling: Two Distinct Tasks</a></li><li><a href=#2-probability-recap-notation--constraints>2. Probability Recap (Notation & Constraints)</a><ul><li><a href=#21-conditional-probability>2.1 Conditional Probability</a></li><li><a href=#22-joint-distribution>2.2 Joint distribution</a></li></ul></li><li><a href=#3-autoregressive-ar-factorization-via-the-chain-rule>3. Autoregressive (AR) Factorization via the Chain Rule</a></li><li><a href=#4-training--density-estimation-maximum-likelihood>4. Training = Density Estimation (Maximum Likelihood)</a><ul><li><a href=#41-entropy>4.1 Entropy</a></li><li><a href=#42-cross-entropy>4.2 Cross-Entropy</a></li><li><a href=#43-kullbackleibler-divergence>4.3 Kullback–Leibler divergence</a></li><li><a href=#44-perplexity>4.4 Perplexity</a></li></ul></li><li><a href=#5-inference--sampling--generation-left-to-right>5. Inference = Sampling / Generation (Left-to-Right)</a><ul><li><a href=#51-decoding-strategies>5.1 Decoding Strategies</a></li></ul></li><li><a href=#6-why-ar-models-are-attractive>6. Why AR Models are Attractive</a><ul><li><a href=#61-ar-vs-other-generative-families>6.1 AR vs. Other Generative Families</a></li></ul></li><li><a href=#9-minimal-derivations-collected>9. Minimal Derivations (Collected)</a><ul><li><a href=#91-chain-rule-for-sequences>9.1 Chain Rule for Sequences</a></li><li><a href=#92-mle--cross-entropy-minimization>9.2 MLE = Cross-Entropy Minimization</a></li><li><a href=#93-perplexity-and-bits-per-token>9.3 Perplexity and Bits-per-Token</a></li></ul></li></ul></li></ul></nav></div><div class=button-next></div></div></aside></div><div class=toolbar-trigger></div><aside class=toolbar><button id=theme-toggle class=theme-toggle-btn>
<img src=/notebook/blackhole2.png alt="Switch Theme">
</button>
<button id=toc-toggle class=toc-toggle-btn>
<img src=/notebook/toc-icon.svg alt="Toggle TOC"></button><div class=font-size-control><button id=font-toggle class=font-size-btn>
A</button><div class=font-size-buttons><button id=font-increase class=font-control-btn>+</button>
<button id=font-reset class=font-control-btn>A</button>
<button id=font-decrease class=font-control-btn>−</button></div></div></aside><script>document.addEventListener("DOMContentLoaded",function(){var e=document.getElementById("book-toc"),t=!0;t?e.classList.remove("hidden"):e.classList.add("hidden"),document.getElementById("toc-toggle").addEventListener("click",function(){e.classList.toggle("hidden")})})</script><script src=/notebook/font-size.min.js></script><script src=/notebook/toolbar.min.js></script><script src=/notebook/toc-animation.min.js></script><script src=/notebook/menu-autohide.min.js></script></main><script>document.addEventListener("DOMContentLoaded",()=>{const e=document.getElementById("theme-toggle");if(!e)return;e.onclick=()=>{const s=e.getBoundingClientRect(),o=s.left+s.width/2,i=s.top+s.height/2,l=document.documentElement.getAttribute("data-theme"),r=l||"auto";let n;r==="auto"?n=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":n=r;const a=n==="dark"?"light":"dark";document.documentElement.setAttribute("data-theme",a),localStorage.setItem("theme",a);const c=n==="dark"?"#0b031c":"white",t=document.createElement("div");t.style.position="fixed",t.style.top="0",t.style.left="0",t.style.width="100vw",t.style.height="100vh",t.style.zIndex="9999",t.style.pointerEvents="none",t.style.setProperty("--cx",`${o}px`),t.style.setProperty("--cy",`${i}px`),t.style.setProperty("--r","0px"),t.style.backgroundImage=`radial-gradient(circle at var(--cx) var(--cy), transparent 0%, transparent var(--r), ${c} calc(var(--r) + 1px))`,document.body.appendChild(t);const u=window.innerWidth,h=window.innerHeight,m=Math.hypot(Math.max(o,u-o),Math.max(i,h-i)),f=600,p=performance.now();function d(e){const o=e-p,s=Math.min(o/f,1),n=s*m;if(t.style.setProperty("--r",`${n}px`),t.style.backgroundImage=`radial-gradient(circle at var(--cx) var(--cy), transparent 0%, transparent ${n}px, ${c} ${n+1}px)`,s<1)requestAnimationFrame(d);else{t.remove();const e=localStorage.getItem("theme")||"light";document.documentElement.setAttribute("data-theme",e)}}requestAnimationFrame(d)}})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,packages:{"[+]":["ams","noerrors","color"]}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"},svg:{fontCache:"global"},startup:{ready:function(){const e=MathJax.startup.findTeX;MathJax.startup.findTeX=function(t){const n=e.call(this,t);for(const e of n)if(e.display){let t=e.math;t=t.replace(/\\begin\{aligned\}([\s\S]*?)\\end\{aligned\}/g,function(e,t){return"\\begin{aligned}"+t.replace(/\\/g,"\\\\")+"\\end{aligned}"}),e.math=t}return n},MathJax.startup.defaultReady()}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js id=MathJax-script async></script><script>window.__tikzjax_injected||(window.__tikzjax_injected=!0,fetch("/content.en/docs/.obsidian/plugins/obsidian-tikzjax/main.js").then(e=>e.text()).then(e=>{const t=e.match(/var tikzjax_default = `([\s\S]*?)`;?/);if(t){const n=t[1],e=document.createElement("script");e.id="tikzjax",e.type="text/javascript",e.innerHTML=n,document.head.appendChild(e)}else{const e=document.createElement("script");e.src="https://tikzjax.com/v1/tikzjax.js",e.defer=!0,document.head.appendChild(e)}}).catch(()=>{const e=document.createElement("script");e.src="https://tikzjax.com/v1/tikzjax.js",e.defer=!0,document.head.appendChild(e)}))</script><script>window.processTheoremBlocks=function(){document.querySelectorAll("blockquote").forEach(function(e){const t=e.querySelector("p");if(!t)return;const n=t.textContent||"";for(const s of["definition","proposition","lemma","theorem","assumption","claim"]){const i=new RegExp(`^\\s*\\[!${s}\\|(\\*|[\\w\\.\\-]+)\\]\\s*`,"i"),o=n.match(i);if(o){console.log(`Found ${s} with label: ${o[1]}`);const m=o[1]!=="*",j=m?o[1]:"",a=n.replace(i,"");let d=a.length;const c=[],u=e=>{e>0&&c.push(e)};u(a.search(/[.;:—–(]/));const g=a.search(/\s(is|are)\s/i);u(g);const b=a.search(/\s{2,}/);if(u(b),c.length&&(d=Math.min(...c)),!c.length&&a.length>48){const t=a.slice(0,48),e=t.lastIndexOf(" ");d=e>0?e:48}const h=a.slice(0,d).trim();if(e.classList.contains("math-theorem"))break;const v=Array.from(e.childNodes);e.innerHTML="",e.classList.add("math-theorem",s);const r=document.createElement("div");r.className="theorem-header";const p=s.charAt(0).toUpperCase()+s.slice(1),f=document.createElement("span");if(f.textContent=m?`${p} ${j}`:p,r.appendChild(f),h){const e=document.createElement("span");e.className="theorem-subtitle",e.textContent=` (${h})`,r.appendChild(e)}e.appendChild(r);const l=document.createElement("div");l.className="theorem-content",e.appendChild(l),v.forEach(function(e){let n=e.cloneNode(!0);if(e===t&&n.nodeType===1){const e=n.textContent||"",t=e.replace(i,"").slice(h.length).trimStart();n.textContent=t}l.appendChild(n)});break}}})},document.addEventListener("DOMContentLoaded",function(){try{window.processTheoremBlocks()}catch(e){console.error(e)}})</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".copy-button");e.forEach(e=>{const t=e.innerHTML;e.addEventListener("click",function(){const n=this.parentElement.querySelector("pre code");if(!n)return;navigator.clipboard.writeText(n.innerText).then(()=>{e.textContent="Copied",setTimeout(()=>{e.innerHTML=t},1500)}).catch(n=>{console.error("Failed to copy code:",n),e.textContent="Error",setTimeout(()=>{e.innerHTML=t},1500)})})})})</script><script defer src=/notebook/partial-load.min.229dc70291b4c666ebc20a65d92bc3027b36401a9d280a928581e8aaf77d7b22.js integrity="sha256-Ip3HApG0xmbrwgpl2SvDAns2QBqdKAqShYHoqvd9eyI=" crossorigin=anonymous></script><script defer src=/notebook/menu-recursive-close.min.893276da918db977c98771d5746eab05ce5761bc6fa1ca400efe1db9be2ac1f3.js integrity="sha256-iTJ22pGNuXfJh3HVdG6rBc5XYbxvocpADv4dub4qwfM=" crossorigin=anonymous></script></body></html>