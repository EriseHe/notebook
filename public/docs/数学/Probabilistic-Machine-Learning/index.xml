<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>概率机器学习 on 学习笔记</title><link>https://erisehe.github.io/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/</link><description>Recent content in 概率机器学习 on 学习笔记</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 08 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://erisehe.github.io/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://erisehe.github.io/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/3.-Autoregressive%E6%A8%A1%E5%9E%8B-2/</link><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate><guid>https://erisehe.github.io/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/3.-Autoregressive%E6%A8%A1%E5%9E%8B-2/</guid><description>&lt;p>An AR (autoregressive) generative model represents the &lt;strong>joint probability of a sequence&lt;/strong> by chaining conditionals left-to-right (the chain rule&lt;br>
$$P\left(w_{1: T}\right)=\prod_{t=1}^T P\left(w_t \mid w_{&amp;lt;t}\right)$$&lt;/p>
&lt;h2 id="1-decoding--inference-in-language-models">
 1. Decoding / Inference in Language Models
 &lt;a class="anchor" href="#1-decoding--inference-in-language-models">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>local higher probability do not always produce the global solution. We always&lt;/li>
&lt;li>we care about the joint probability&lt;/li>
&lt;/ul>
&lt;h3 id="1-1-argmax-greedy-decoding">
 1. 1. Argmax (Greedy Decoding)
 &lt;a class="anchor" href="#1-1-argmax-greedy-decoding">#&lt;/a>
&lt;/h3>
&lt;p>Choose the most likely token:&lt;br>
$$&lt;br>
w_t = \arg\max_{w} p_\theta(w \mid w_{&amp;lt;t})&lt;br>
$$&lt;/p></description></item><item><title/><link>https://erisehe.github.io/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/2.-Autoregressive%E6%A8%A1%E5%9E%8B/</link><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate><guid>https://erisehe.github.io/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/2.-Autoregressive%E6%A8%A1%E5%9E%8B/</guid><description>&lt;p>An AR (autoregressive) generative model represents the &lt;strong>joint probability of a sequence&lt;/strong> by chaining conditionals left-to-right (the chain rule&lt;br>
$$P\left(w_{1: T}\right)=\prod_{t=1}^T P\left(w_t \mid w_{&amp;lt;t}\right)$$&lt;/p>
&lt;h2 id="1-probability-distributions-recap">
 1. Probability Distributions (recap)
 &lt;a class="anchor" href="#1-probability-distributions-recap">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>Discrete: $X ∈ {1,…,K}, ; P(X=i)=p_i, ; \sum_i p_i=1.$&lt;/li>
&lt;li>Continuous: $\int p(x) dx = 1, ; p(x) ≥ 0.$&lt;/li>
&lt;li>Multivariate: $p(\mathbf{x}) = p(x_1,…,x_d).$&lt;/li>
&lt;/ul>
&lt;h1 id="autoregressive-models">
 Autoregressive Models
 &lt;a class="anchor" href="#autoregressive-models">#&lt;/a>
&lt;/h1>
&lt;blockquote>
&lt;p>&lt;strong>Scope.&lt;/strong> This note mirrors the theoretical portions of &lt;em>Lecture 2: Autoregressive Models&lt;/em> and keeps the &lt;strong>same notation&lt;/strong> as the slides/notebook. We use:&lt;/p></description></item><item><title/><link>https://erisehe.github.io/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/3.1-Matrix-Calculus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://erisehe.github.io/notebook/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/3.1-Matrix-Calculus/</guid><description/></item></channel></rss>