<!doctype html><html lang=en dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1314&amp;path=livereload" data-no-instant defer></script><script>(function(){const e=localStorage.getItem("theme");e&&document.documentElement.setAttribute("data-theme",e)})()</script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Introduction
  #

The subject of this Chapter is the numerical approximation of the Cauchy problem:
$$(1) \quad \frac{dy}{dt} = f(t,y) \quad \text{in } t > 0 \quad (I.C.)$$
with
$$y(0) = y_0 \text{ given}.$$
or, more in general, a system:
$$(2) \quad \frac{d\mathbf{y}}{dt} = \mathbf{f}(t,\mathbf{y}) \quad \text{in } t > 0$$
with
$$\mathbf{y}(0) = \mathbf{y}_0.$$
Let&rsquo;s recall some basic results on the function $f$. It is said to be Lipschitz continuous in an interval $I$ if there exists a positive $L > 0$ s.t."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1314/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/03-Numerical-Solution-of-Ordinary-Differential-Equations/"><meta property="og:site_name" content="学习笔记"><meta property="og:title" content="常微分方程的数值解"><meta property="og:description" content="Introduction # The subject of this Chapter is the numerical approximation of the Cauchy problem:
$$(1) \quad \frac{dy}{dt} = f(t,y) \quad \text{in } t > 0 \quad (I.C.)$$
with $$y(0) = y_0 \text{ given}.$$
or, more in general, a system:
$$(2) \quad \frac{d\mathbf{y}}{dt} = \mathbf{f}(t,\mathbf{y}) \quad \text{in } t > 0$$
with $$\mathbf{y}(0) = \mathbf{y}_0.$$
Let’s recall some basic results on the function $f$. It is said to be Lipschitz continuous in an interval $I$ if there exists a positive $L > 0$ s.t."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>常微分方程的数值解 | 学习笔记</title>
<link rel=icon href=/topo.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1314/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/03-Numerical-Solution-of-Ordinary-Differential-Equations/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/computer-modern-font@1.0.1/index.min.css><link rel=stylesheet href=/book.min.b3dea21ef7167d137d5c3d91df2df1ae008874baa457a6eb3fa1217af8da3509.css integrity="sha256-s96iHvcWfRN9XD2R3y3xrgCIdLqkV6brP6EhevjaNQk=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.02146d62883b84f93e0524565edbcf1e9c978da904f495247ceef863dda858c9.js integrity="sha256-AhRtYog7hPk+BSRWXtvPHpyXjakE9JUkfO74Y92oWMk=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class=container><div class=book-layout><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center justify-center" href=http://localhost:1314/><span>学习笔记</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><nav class=subject-menu><div class=subject-tabs><button class="subject-tab active" data-subject=数学>数学</button>
<button class=subject-tab data-subject=物理>物理</button></div><div class=subject-content><div class=subject-panel data-subject=数学><ul><li class=book-section-flat><span>点集拓扑学基础</span><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/%E6%8B%93%E6%89%91%E5%AD%A6/T-n%E5%88%86%E7%A6%BB%E5%85%AC%E7%90%86/>T N分离公理</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-2626c0c7c40e5ba289bbf96779ed7be9 class=toggle>
<label for=section-2626c0c7c40e5ba289bbf96779ed7be9 class="flex justify-between"><a role=button>微分几何</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/1.-Overview-of-Differential-Geometry/>1. Overview of Differential Geometry</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/2.-Lecture-2-ACTUAL/>2. Lecture 2 ( Actual)</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/2.-Parametrized-Curves-and-Surfaces/>2. Parametrized Curves and Surfaces</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/Tangent-Vectors-on-a-Surface/>Tangent Vectors on a Surface</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-6e5f2db787427bb8bccbb3c3eafa6cb6 class=toggle>
<label for=section-6e5f2db787427bb8bccbb3c3eafa6cb6 class="flex justify-between"><a role=button>概率机器学习</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/2.-Autoregressive%E6%A8%A1%E5%9E%8B/>2. Autoregressive模型</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/3.-Autoregressive%E6%A8%A1%E5%9E%8B-2/>3. Autoregressive模型 2</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-e9a29b9c49921437d6f50841fa0b7a2c class=toggle>
<label for=section-e9a29b9c49921437d6f50841fa0b7a2c class="flex justify-between"><a role=button>实分析 II</a></label><ul><li><input type=checkbox id=section-e63b59f3907b6b5c0ca40a1419c50fbe class=toggle>
<label for=section-e63b59f3907b6b5c0ca40a1419c50fbe class="flex justify-between"><a role=button>第六章 可微映射</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.4-Sufficient-Conditions-for-Differentiability/>6.4 可微分性的必要条件</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.6-Product-Rule-and-Gradients/>6.6 乘积法则与梯度</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.9-Taylor-Higher-Dimentions/>6.9 泰勒公式的高维形式</a></li></ul></li><li><input type=checkbox id=section-516b62a4eda22b0c44d32f044080ba6a class=toggle>
<label for=section-516b62a4eda22b0c44d32f044080ba6a class="flex justify-between"><a role=button>第七章 逆函数和隐函数定理</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1-Inverse-Function-Theorem/>7.1 反函数定理</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1.1-Inverse-FT-Proof/>7.1.1 反函数定理（证明）</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.2-Implicit-Function-Theorem/>7.2 隐函数定理</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.6-Morse-Lemma/>7.6 莫尔斯引理</a></li></ul></li><li><input type=checkbox id=section-7be78291145a12f96f25aeffb3b16554 class=toggle>
<label for=section-7be78291145a12f96f25aeffb3b16554 class="flex justify-between"><a role=button>第八章 度量理论</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.1-%E5%BA%A6%E9%87%8F%E7%90%86%E8%AE%BA/>8.1 度量理论</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.2-Criterion-for-Integrability/>8.2 Criterion for Integrability</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.3-Proof-of-Lebesgues-Theorem/>8.3 Proof of Lebesgue's Theorem</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.5-Improper-Integral/>8.5 不定积分</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.6-Lebesgue-Convergence-Theorem/>8.6 勒贝格收敛定理</a></li></ul></li><li><input type=checkbox id=section-56e664b694094194eea41293621edc8e class=toggle>
<label for=section-56e664b694094194eea41293621edc8e class="flex justify-between"><a role=button>第九章</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1-Computing-Integrals/>积分计算</a></li></ul></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%8D%81%E7%AB%A0/10.1-Fourier-Analysis/>10.1 Fourier Analysis</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-358a11d2cd962ab8fdb9c9ba7faf532a class=toggle checked>
<label for=section-358a11d2cd962ab8fdb9c9ba7faf532a class="flex justify-between"><a role=button>偏微分方程</a></label><ul><li><input type=checkbox id=section-7097c8722aa477fcc3c6ccc5730510d5 class=toggle checked>
<label for=section-7097c8722aa477fcc3c6ccc5730510d5 class="flex justify-between"><a role=button>数值方法</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/01-Mid-Point-Method/>01 Mid Point Method</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/02-Three-Point-Backward-Differentiation-Formula/>02 Three Point Backward Differentiation Formula</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/03-Numerical-Solution-of-Ordinary-Differential-Equations/ class=active>常微分方程的数值解</a></li><li><input type=checkbox id=section-039c2604ef8d15d0cdd62911e85f20cb class=toggle>
<label for=section-039c2604ef8d15d0cdd62911e85f20cb class="flex justify-between"><a role=button>第四章 有限元分析</a></label><ul></ul></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E5%8D%81%E7%AB%A0/Finite-Element-Method/>Finite Element Method</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E5%8D%81%E7%AB%A0/Quiz-4/>Quiz 4</a></li><li><input type=checkbox id=section-24cf0d34881526d98bfa6ff3655d07cb class=toggle>
<label for=section-24cf0d34881526d98bfa6ff3655d07cb class="flex justify-between"><a role=button>第九章</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1-Boundary-Value-Problems/>9.1 边值问题的近似</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.2-Finite-Difference/>9.2 有限差分法</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.3-Advection-Diffusion-Equation/>9.3 对流-扩散方程</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.4-2D-Problem/>9.4 4.1 二维（2D）偏微分方程问题</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/Exercise/>习题</a></li></ul></li></ul></li><li><input type=checkbox id=section-2e9d41360137a1ae6194db40069654e3 class=toggle>
<label for=section-2e9d41360137a1ae6194db40069654e3 class="flex justify-between"><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/Heat-Equation/>热方程</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/Heat-Equation/Heat-Equation-Solution/>Heat Equation Solution</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/Heat-Equation/The-Fourier-Series/>傅立叶级数</a></li></ul></li><li><input type=checkbox id=section-7f42ebdf414be775241761c5b3262618 class=toggle>
<label for=section-7f42ebdf414be775241761c5b3262618 class="flex justify-between"><a role=button>波方程</a></label><ul></ul></li><li><input type=checkbox id=section-3cdad47fb9156c74a9aac21d7840334d class=toggle>
<label for=section-3cdad47fb9156c74a9aac21d7840334d class="flex justify-between"><a role=button>拉普拉斯方程</a></label><ul></ul></li></ul></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/PS/>Ps</a></li></ul></div><div class="subject-panel hidden" data-subject=物理></div></div></nav><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".subject-tab"),s=document.querySelectorAll(".subject-panel");function t(t){e.forEach(e=>e.classList.remove("active")),s.forEach(e=>e.classList.add("hidden"));const n=document.querySelector(`.subject-tab[data-subject="${t}"]`);n&&n.classList.add("active");const o=document.querySelector(`.subject-panel[data-subject="${t}"]`);o&&o.classList.remove("hidden"),localStorage.setItem("activeSubjectTab",t)}const n=localStorage.getItem("activeSubjectTab");n&&t(n),e.forEach(e=>{e.addEventListener("click",function(){const n=e.getAttribute("data-subject");t(n)})})})</script><div class=after-menu-offset><ul><li><a href=/posts/>Blog</a></li><li><a href=https://github.com/EriseHe/notebook target=_blank rel=noopener>Github</a></li><li><a href=https://themes.gohugo.io/themes/hugo-book/ target=_blank rel=noopener>Hugo Themes</a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>常微分方程的数值解</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#local-theorem>Local Theorem</a></li><li><a href=#global-theorem>Global Theorem</a></li></ul></li><li><a href=#stability-definitions>Stability Definitions</a></li><li><a href=#remark>Remark</a></li><li><a href=#some-simple-examples>Some Simple Examples</a></li><li><a href=#analysis-of-forward-euler>Analysis of Forward Euler</a></li><li><a href=#convergence-of-forward-euler>Convergence of Forward Euler</a></li></ul></li><li><a href=#other-one-step-methods>Other One-Step Methods</a><ul><li><a href=#crank-nicolson>Crank-Nicolson</a></li><li><a href=#heun>Heun</a></li><li><a href=#the-concept-of-zero-stability>The Concept of Zero-Stability</a></li><li><a href=#the-concept-of-absolute-stability>The Concept of Absolute Stability</a><ul><li><a href=#remark-1>Remark</a></li></ul></li><li><a href=#definition>DEFINITION</a></li><li><a href=#another-example-heun>Another Example: Heun</a></li><li><a href=#remark-we-found-that-the-explicit-methods-are-conditionally-stable-while-the-implicit-ones-are-unconditionally-stable>REMARK: We found that the explicit methods are conditionally stable, while the implicit ones are unconditionally stable.</a></li></ul></li><li><a href=#multistep-methods>Multistep Methods</a><ul><li><a href=#example>Example:</a></li><li><a href=#bdf-backward-difference-formulas>BDF (Backward Difference Formulas)</a></li><li><a href=#adams>Adams</a></li><li><a href=#a-rapid-recall-of-difference-equations-theory>A Rapid Recall of Difference Equations Theory</a></li><li><a href=#zero-stability-definition>Zero-Stability Definition</a></li><li><a href=#analysis-of-multistep-methods>Analysis of Multistep Methods</a></li><li><a href=#a-clarification-on-stability-concepts>A Clarification on Stability Concepts</a></li><li><a href=#example-extreme>Example (Extreme)</a></li><li><a href=#predictor-corrector-methods>Predictor-Corrector Methods</a></li><li><a href=#examples>Examples:</a></li><li><a href=#runge-kutta-methods>Runge-Kutta Methods</a></li><li><a href=#derivation-of-an-explicit-rk-method>Derivation of an Explicit RK Method</a></li><li><a href=#analysis-of-rk>Analysis of RK</a></li><li><a href=#absolute-stability>ABSOLUTE STABILITY</a></li><li><a href=#why-are-rk-so-popular>Why are RK so popular?</a></li><li><a href=#a-final-note-on-stiff-problems>A Final Note on Stiff problems</a></li><li><a href=#exercise-on-lmm>EXERCISE on LMM</a></li></ul></li></ul></nav></aside></header><article class=book-article><h1 style="text-align:center;font-family:computer modern,cmu serif,serif">常微分方程的数值解</h1><div class="markdown posts" x-data><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><p data-raw='The subject of this Chapter is the numerical approximation of the Cauchy problem:'>The subject of this Chapter is the numerical approximation of the Cauchy problem:</p><p data-raw='$$(1) \quad \frac{dy}{dt} = f(t,y) \quad \text{in } t > 0 \quad (I.C.)$$'>$$(1) \quad \frac{dy}{dt} = f(t,y) \quad \text{in } t > 0 \quad (I.C.)$$</p><p data-raw='with
$$y(0) = y_0 \text{ given}.$$'>with
$$y(0) = y_0 \text{ given}.$$</p><p data-raw='or, more in general, a system:'>or, more in general, a system:</p><p data-raw='$$(2) \quad \frac{d\mathbf{y}}{dt} = \mathbf{f}(t,\mathbf{y}) \quad \text{in } t > 0$$'>$$(2) \quad \frac{d\mathbf{y}}{dt} = \mathbf{f}(t,\mathbf{y}) \quad \text{in } t > 0$$</p><p data-raw='with
$$\mathbf{y}(0) = \mathbf{y}_0.$$'>with
$$\mathbf{y}(0) = \mathbf{y}_0.$$</p><p data-raw='Let&rsquo;s recall some basic results on the function $f$. It is said to be Lipschitz continuous in an interval $I$ if there exists a positive $L > 0$ s.t.'>Let&rsquo;s recall some basic results on the function $f$. It is said to be Lipschitz continuous in an interval $I$ if there exists a positive $L > 0$ s.t.</p><p data-raw='$$|f(t,y_1) - f(t,y_2)| < L |y_1 - y_2|.$$'>$$|f(t,y_1) - f(t,y_2)| &lt; L |y_1 - y_2|.$$</p><p data-raw='Clearly, if $f$ is differentiable with continuity in $I$, it is also Lipschitz continuous with $L \leq \max_I \left|\frac{\partial f}{\partial y}\right|$.'>Clearly, if $f$ is differentiable with continuity in $I$, it is also Lipschitz continuous with $L \leq \max_I \left|\frac{\partial f}{\partial y}\right|$.</p><h3 id=local-theorem>Local Theorem
<a class=anchor href=#local-theorem>#</a></h3><p data-raw='If $f$ is Lipschitz continuous in a range $t \in I_1$ and $y \in I \subseteq \mathbb{R}$, then $\exists$ an interval $\hat{I} \subseteq I$ where the solution to $(1)$ exists and is unique.'>If $f$ is Lipschitz continuous in a range $t \in I_1$ and $y \in I \subseteq \mathbb{R}$, then $\exists$ an interval $\hat{I} \subseteq I$ where the solution to $(1)$ exists and is unique.</p><h3 id=global-theorem>Global Theorem
<a class=anchor href=#global-theorem>#</a></h3><p data-raw='If the $f$ is Lipschitz continuous $\forall t \in I$ and $y \in \mathbb{R}$, then the solution $\exists$ uniquely in $I$.'>If the $f$ is Lipschitz continuous $\forall t \in I$ and $y \in \mathbb{R}$, then the solution $\exists$ uniquely in $I$.</p><h2 id=stability-definitions>Stability Definitions
<a class=anchor href=#stability-definitions>#</a></h2><p data-raw='From the practical point of view, it is important to consider also the perturbed case:'>From the practical point of view, it is important to consider also the perturbed case:</p><p data-raw='$$(1_\epsilon): \quad \frac{dy_\epsilon}{dt} = f(t, y_\epsilon) + \delta(t) \quad t \geq 0$$'>$$(1_\epsilon): \quad \frac{dy_\epsilon}{dt} = f(t, y_\epsilon) + \delta(t) \quad t \geq 0$$</p><p data-raw='$$y^\epsilon(0) = y_0 + \epsilon$$'>$$y^\epsilon(0) = y_0 + \epsilon$$</p><p data-raw='with $|\delta(t)| \leq \epsilon \quad \forall t \geq 0$'>with $|\delta(t)| \leq \epsilon \quad \forall t \geq 0$</p><p data-raw='If there exists a finite constant $C$ such that'>If there exists a finite constant $C$ such that</p><p data-raw='$$|y - y_\epsilon| < C\epsilon \quad (*)$$'>$$|y - y_\epsilon| &lt; C\epsilon \quad (*)$$</p><p data-raw='then we say that the solution is Lyapunov stable.'>then we say that the solution is Lyapunov stable.</p><p data-raw='In general, $C$ may depend on $t$, so that $(*)$ may hold on finite intervals (and not over the entire $[0,\infty)$ axis).'>In general, $C$ may depend on $t$, so that $(*)$ may hold on finite intervals (and not over the entire $[0,\infty)$ axis).</p><p data-raw='To have a stronger concept, we advocate the ASYMPTOTIC STABILITY:'>To have a stronger concept, we advocate the ASYMPTOTIC STABILITY:</p><p data-raw='$$\lim_{t \to \infty} |y(t) - y_\epsilon(t)| = 0.$$'>$$\lim_{t \to \infty} |y(t) - y_\epsilon(t)| = 0.$$</p><p data-raw='From the engineering point of view, this is a central concept, as the theory of control (Automatical Control, Feedback control) relies on this definition.'>From the engineering point of view, this is a central concept, as the theory of control (Automatical Control, Feedback control) relies on this definition.</p><h2 id=remark>Remark
<a class=anchor href=#remark>#</a></h2><p data-raw='The Cauchy problem has a formal (quite useless) solution:'>The Cauchy problem has a formal (quite useless) solution:</p><p data-raw='$$y(t) = y_0 + \int_0^t f(\tau, y(\tau))d\tau$$'>$$y(t) = y_0 + \int_0^t f(\tau, y(\tau))d\tau$$</p><p data-raw='connecting the solution in $t$ with its past.'>connecting the solution in $t$ with its past.</p><p data-raw='This is basically an alternative formulation of the problem and, in fact, will be used to formulate possible numerical approximation alternative to the ones based on $(1)$.'>This is basically an alternative formulation of the problem and, in fact, will be used to formulate possible numerical approximation alternative to the ones based on $(1)$.</p><h2 id=some-simple-examples>Some Simple Examples
<a class=anchor href=#some-simple-examples>#</a></h2><p data-raw='To begin with, we can split the time interval in subintervals, for instance with a uniform reticulation with step $\Delta t$.'>To begin with, we can split the time interval in subintervals, for instance with a uniform reticulation with step $\Delta t$.</p><p data-raw='![Time discretization with points at 0, Δt, 2Δt, 3Δt&mldr;]'>![Time discretization with points at 0, Δt, 2Δt, 3Δt&mldr;]</p><p data-raw='Then, we can use the formula:'>Then, we can use the formula:</p><p data-raw='$$\frac{dy}{dt}(t_i) \simeq \frac{y(t_{i+1}) - y(t_i)}{\Delta t}$$'>$$\frac{dy}{dt}(t_i) \simeq \frac{y(t_{i+1}) - y(t_i)}{\Delta t}$$</p><p data-raw='that we know is accurate with an error scaling with $\Delta t$. In this way, we have:'>that we know is accurate with an error scaling with $\Delta t$. In this way, we have:</p><p data-raw='[From now on, the approximation of the solution $y(t)$ in $t_i$ will be denoted with $u_i$]'>[From now on, the approximation of the solution $y(t)$ in $t_i$ will be denoted with $u_i$]</p><p data-raw='$$\frac{u_{i+1} - u_i}{\Delta t} = f(t_i, u_i)$$'>$$\frac{u_{i+1} - u_i}{\Delta t} = f(t_i, u_i)$$</p><p data-raw='In practice, starting at $t = 0$:
$$\begin{align}
u_1 &= u_0 + \Delta t , f(t_0, u_0) \quad \rightsquigarrow \quad (u_0 = y_0) \
u_2 &= u_1 + \Delta t , f(t_1, u_1)
\end{align}$$'>In practice, starting at $t = 0$:
$$\begin{align}
u_1 &= u_0 + \Delta t , f(t_0, u_0) \quad \rightsquigarrow \quad (u_0 = y_0) \
u_2 &= u_1 + \Delta t , f(t_1, u_1)
\end{align}$$</p><p data-raw='We can easily compute the approximation $u_i$ of $y(t_i)$.'>We can easily compute the approximation $u_i$ of $y(t_i)$.</p><p data-raw='On the other hand, we could do:'>On the other hand, we could do:</p><p data-raw='$$\frac{u_i - u_{i-1}}{\Delta t} = f(t_i, u_i)$$'>$$\frac{u_i - u_{i-1}}{\Delta t} = f(t_i, u_i)$$</p><p data-raw='leading to:'>leading to:</p><p data-raw='$$u_1 = u_0 + \Delta t , f(t_1, u_1) \quad (u_0 = y_0)$$'>$$u_1 = u_0 + \Delta t , f(t_1, u_1) \quad (u_0 = y_0)$$</p><p data-raw='This is not as easy as before: it&rsquo;s a NONLINEAR (ALGEBRAIC) EQUATION; we know how to solve it (first Chapter), but it is a computational additional cost. Then, similarly, we have:'>This is not as easy as before: it&rsquo;s a NONLINEAR (ALGEBRAIC) EQUATION; we know how to solve it (first Chapter), but it is a computational additional cost. Then, similarly, we have:</p><p data-raw='$$u_2 = u_1 + \Delta t , f(t_2, u_2)$$'>$$u_2 = u_1 + \Delta t , f(t_2, u_2)$$</p><p data-raw='We have also another option:'>We have also another option:</p><p data-raw='$$\frac{dy}{dt}(t_i) \simeq \frac{u_{i+1} - u_{i-1}}{2\Delta t}$$'>$$\frac{dy}{dt}(t_i) \simeq \frac{u_{i+1} - u_{i-1}}{2\Delta t}$$</p><p data-raw='In this case, the error scales with $O(\Delta t^2)$. So, in practice we have:'>In this case, the error scales with $O(\Delta t^2)$. So, in practice we have:</p><p data-raw='$$u_{i+1} = 2\Delta t , f(t_i, u_i) + u_{i-1}$$'>$$u_{i+1} = 2\Delta t , f(t_i, u_i) + u_{i-1}$$</p><p data-raw='or specifically: $u_2 = 2\Delta t , f(t_1, u_1) + u_0 \quad (u_0 = y_0)$'>or specifically: $u_2 = 2\Delta t , f(t_1, u_1) + u_0 \quad (u_0 = y_0)$</p><p data-raw='I need to know $u_1$, not just $u_0$, then we can use the method.'>I need to know $u_1$, not just $u_0$, then we can use the method.</p><p data-raw='With these three examples, we have already found many possible types of methods:'>With these three examples, we have already found many possible types of methods:</p><p data-raw='<strong>Implicit vs Explicit</strong>'><strong>Implicit vs Explicit</strong></p><ul><li><strong>Implicit</strong>: Solve a non-linear equation</li><li><strong>Explicit</strong>: No need of solving equations</li></ul><p data-raw='<strong>One Step vs Multistep</strong>'><strong>One Step vs Multistep</strong></p><ul><li><strong>One Step</strong>: $u_{i+1} = g(\Delta t, u_i)$</li><li><strong>Multistep</strong>: $u_{i+1} = g(\Delta t, u_i, u_{i-1}, u_{i-2}&mldr;)$</li></ul><p data-raw='At the top of this, we have the problem of the accuracy and - even most importantly- of the CONVERGENCE.'>At the top of this, we have the problem of the accuracy and - even most importantly- of the CONVERGENCE.</p><p data-raw='In fact, the basic requirement we need is that the method is convergent:'>In fact, the basic requirement we need is that the method is convergent:</p><p data-raw='$$\lim_{\Delta t \to 0} |y(t_i) - u_i| = 0$$'>$$\lim_{\Delta t \to 0} |y(t_i) - u_i| = 0$$</p><p data-raw='Then, if we find that $|y(t_i) - u_i| \sim O(\Delta t^p)$ then the accuracy or the order of the method is $p$.'>Then, if we find that $|y(t_i) - u_i| \sim O(\Delta t^p)$ then the accuracy or the order of the method is $p$.</p><p data-raw='Nature (implicit/explicit), Number of steps and most important CONVERGENCE and accuracy (order) are the categories for analyzing and ranking different methods.'>Nature (implicit/explicit), Number of steps and most important CONVERGENCE and accuracy (order) are the categories for analyzing and ranking different methods.</p><p data-raw='Before we embark ourselves in a general analysis, however, let&rsquo;s focus on a specific case, where important concepts will be highlighted.'>Before we embark ourselves in a general analysis, however, let&rsquo;s focus on a specific case, where important concepts will be highlighted.</p><h2 id=analysis-of-forward-euler>Analysis of Forward Euler
<a class=anchor href=#analysis-of-forward-euler>#</a></h2><p data-raw='The method:'>The method:</p><p data-raw='$$\frac{u_{i+1} - u_i}{\Delta t} = f(t_i, u_i)$$'>$$\frac{u_{i+1} - u_i}{\Delta t} = f(t_i, u_i)$$</p><p data-raw='is called Forward Euler.'>is called Forward Euler.</p><p data-raw='Let&rsquo;s consider it in detail.'>Let&rsquo;s consider it in detail.</p><p data-raw='To start with, let&rsquo;s introduce the distinction of &ldquo;consistency&rdquo; and truncation error.'>To start with, let&rsquo;s introduce the distinction of &ldquo;consistency&rdquo; and truncation error.</p><p data-raw='If we have the exact solution $y_{ex}(t)$ it is easily realized that'>If we have the exact solution $y_{ex}(t)$ it is easily realized that</p><p data-raw='$$\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\Delta t} \neq f(t_i, y_{ex}(t_i))$$'>$$\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\Delta t} \neq f(t_i, y_{ex}(t_i))$$</p><p data-raw='For instance,'>For instance,</p><p data-raw='$$\frac{dy}{dt} = \lambda y \quad y(0) = 1 \implies y_{ex} = e^{\lambda t}$$'>$$\frac{dy}{dt} = \lambda y \quad y(0) = 1 \implies y_{ex} = e^{\lambda t}$$</p><p data-raw=then>then</p><p data-raw='$$\frac{e^{\lambda(t_i+\Delta t)} - e^{\lambda t_i}}{\Delta t} = e^{\lambda t_i} \frac{e^{\lambda \Delta t} - 1}{\Delta t} \neq \lambda e^{\lambda t_i} \quad (\lambda y(t_i))$$'>$$\frac{e^{\lambda(t_i+\Delta t)} - e^{\lambda t_i}}{\Delta t} = e^{\lambda t_i} \frac{e^{\lambda \Delta t} - 1}{\Delta t} \neq \lambda e^{\lambda t_i} \quad (\lambda y(t_i))$$</p><p data-raw='We can be more specific:'>We can be more specific:</p><p data-raw='$$y_{ex}(t_{i+1}) = y_{ex}(t_i) + \frac{dy_{ex}}{dt}(t_i)\Delta t + \frac{1}{2}\frac{d^2y_{ex}}{dt^2}(t_i)\Delta t^2 + &mldr;$$'>$$y_{ex}(t_{i+1}) = y_{ex}(t_i) + \frac{dy_{ex}}{dt}(t_i)\Delta t + \frac{1}{2}\frac{d^2y_{ex}}{dt^2}(t_i)\Delta t^2 + &mldr;$$</p><p data-raw=Now:>Now:</p><p data-raw='$$\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\Delta t} = \frac{dy_{ex}(t_i)}{dt} + \frac{1}{2}\frac{d^2y_{ex}(t_i)}{dt^2}\Delta t + &mldr;$$'>$$\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\Delta t} = \frac{dy_{ex}(t_i)}{dt} + \frac{1}{2}\frac{d^2y_{ex}(t_i)}{dt^2}\Delta t + &mldr;$$</p><p data-raw='This gives us:'>This gives us:</p><p data-raw='$$\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\Delta t} = f(t_i, y_{ex}(t_i)) + \left[\frac{\Delta t}{2}\frac{d^2y_{ex}}{dt^2}\right]$$'>$$\frac{y_{ex}(t_{i+1}) - y_{ex}(t_i)}{\Delta t} = f(t_i, y_{ex}(t_i)) + \left[\frac{\Delta t}{2}\frac{d^2y_{ex}}{dt^2}\right]$$</p><p data-raw='Forward Euler $\quad \quad \quad$ Local Truncation Error (LTE)'>Forward Euler $\quad \quad \quad$ Local Truncation Error (LTE)</p><p data-raw='In some sense, the truncation error is the residual we get when we pretend the exact solution to solve the numerical scheme.'>In some sense, the truncation error is the residual we get when we pretend the exact solution to solve the numerical scheme.</p><p data-raw='Now, to investigate how the error of Forward Euler works, let&rsquo;s consider the following picture:'>Now, to investigate how the error of Forward Euler works, let&rsquo;s consider the following picture:</p><p data-raw='![Error propagation diagram showing exact solution trajectory and numerical approximation]'>![Error propagation diagram showing exact solution trajectory and numerical approximation]</p><p data-raw='From the picture it is evident that the error:'>From the picture it is evident that the error:</p><p data-raw='$$e_{i+1} = y_{ex}(t_{i+1}) - u_{i+1}$$'>$$e_{i+1} = y_{ex}(t_{i+1}) - u_{i+1}$$</p><p data-raw='is the result of two contributions:'>is the result of two contributions:</p><p data-raw='$$e_{i+1} = \underbrace{y_{ex}(t_{i+1}) - u^<em><em>{i+1}}</em>{\text{generated at the local step}} + \underbrace{u^</em><em>{i+1} - u</em>{i+1}}_{\text{propagated from previous steps}}$$'>$$e_{i+1} = \underbrace{y_{ex}(t_{i+1}) - u^<em><em>{i+1}}</em>{\text{generated at the local step}} + \underbrace{u^</em><em>{i+1} - u</em>{i+1}}_{\text{propagated from previous steps}}$$</p><p data-raw='From the previous definition:'>From the previous definition:</p><p data-raw='$$y_{ex}(t_{i+1}) = y_{ex}(t_i) + \Delta t , f(t_i, y_{ex}(t_i)) + \Delta t , \tau_{i+1}$$'>$$y_{ex}(t_{i+1}) = y_{ex}(t_i) + \Delta t , f(t_i, y_{ex}(t_i)) + \Delta t , \tau_{i+1}$$</p><p data-raw='where $\tau_{i+1} = \frac{\Delta t}{2}\frac{d^2y_{ex}(t_i)}{dt^2}$ is the local TE.'>where $\tau_{i+1} = \frac{\Delta t}{2}\frac{d^2y_{ex}(t_i)}{dt^2}$ is the local TE.</p><p data-raw='$$u^*<em>{i+1} = y</em>{ex}(t_i) + \Delta t , f(t_i, y_{ex}(t_i))$$'>$$u^*<em>{i+1} = y</em>{ex}(t_i) + \Delta t , f(t_i, y_{ex}(t_i))$$</p><p data-raw='$$y_{ex}(t_{i+1}) - u^*_{i+1} = \Delta t , \tau_i$$'>$$y_{ex}(t_{i+1}) - u^*_{i+1} = \Delta t , \tau_i$$</p><p data-raw='Now, let&rsquo;s focus on the other part.'>Now, let&rsquo;s focus on the other part.</p><p data-raw='This second component $u^*<em>{i+1} - u</em>{i+1}$ is inherited from the previous errors.'>This second component $u^*<em>{i+1} - u</em>{i+1}$ is inherited from the previous errors.</p><p data-raw='$$u^*<em>{i+1} = y</em>{ex}(t_i) + \Delta t , f(t_i, y_{ex}(t_i))$$
$$u_{i+1} = u_i + \Delta t , f(t_i, u_i)$$'>$$u^*<em>{i+1} = y</em>{ex}(t_i) + \Delta t , f(t_i, y_{ex}(t_i))$$
$$u_{i+1} = u_i + \Delta t , f(t_i, u_i)$$</p><p data-raw='Using the Lipschitz assumption $|f(t,y) - f(t,u)| \leq L |y-u|$ we have:'>Using the Lipschitz assumption $|f(t,y) - f(t,u)| \leq L |y-u|$ we have:</p><p data-raw='$$|u^*<em>{i+1} - u</em>{i+1}| \leq |e_i| + \Delta t , L |e_i| = (1 + \Delta t , L)|e_i|$$'>$$|u^*<em>{i+1} - u</em>{i+1}| \leq |e_i| + \Delta t , L |e_i| = (1 + \Delta t , L)|e_i|$$</p><p data-raw='where $e_i = y_{ex}(t_i) - u_i$'>where $e_i = y_{ex}(t_i) - u_i$</p><p data-raw='All together, we have:'>All together, we have:</p><p data-raw='$$|e_{i+1}| \leq \Delta t |\tau_i| + (1 + \Delta t , L)|e_i|$$'>$$|e_{i+1}| \leq \Delta t |\tau_i| + (1 + \Delta t , L)|e_i|$$</p><p data-raw='Now, take $|\tau^*| = \max_i |\tau_i| = \text{GLOBAL TRUNCATION ERROR}$.'>Now, take $|\tau^*| = \max_i |\tau_i| = \text{GLOBAL TRUNCATION ERROR}$.</p><p data-raw=Then:>Then:</p><p data-raw='$$|e_{i+1}| \leq \underbrace{\Delta t |\tau^*|}<em>{\text{local}} + \underbrace{(1 + \Delta t , L)}</em>{\text{propagated}}|e_i|$$'>$$|e_{i+1}| \leq \underbrace{\Delta t |\tau^*|}<em>{\text{local}} + \underbrace{(1 + \Delta t , L)}</em>{\text{propagated}}|e_i|$$</p><p data-raw='Assume that $e_0 = 0$ (no errors on the initial conditions). Then we have:'>Assume that $e_0 = 0$ (no errors on the initial conditions). Then we have:</p><p data-raw='$$|e_1| \leq \Delta t |\tau^<em>|$$
$$|e_2| \leq \Delta t |\tau^</em>| + (1 + \Delta t , L)|e_1| \leq \Delta t |\tau^<em>|(1 + (1+\Delta t , L))$$
$$|e_3| \leq \Delta t |\tau^</em>| + (1+\Delta t , L)|e_2| \leq \Delta t |\tau^*|(1 + (1+\Delta t , L) + (1+\Delta t , L)^2)$$'>$$|e_1| \leq \Delta t |\tau^<em>|$$
$$|e_2| \leq \Delta t |\tau^</em>| + (1 + \Delta t , L)|e_1| \leq \Delta t |\tau^<em>|(1 + (1+\Delta t , L))$$
$$|e_3| \leq \Delta t |\tau^</em>| + (1+\Delta t , L)|e_2| \leq \Delta t |\tau^*|(1 + (1+\Delta t , L) + (1+\Delta t , L)^2)$$</p><p data-raw='We infer:'>We infer:</p><p data-raw='$$|e_K| \leq \Delta t |\tau^<em>| \sum_{j=0}^{K-1}(1+\Delta t , L)^j = \Delta t |\tau^</em>|\frac{(1+\Delta t , L)^K - 1}{1 - 1 - \Delta t , L} = \frac{|\tau^*|}{L}((1+\Delta t , L)^K - 1)$$'>$$|e_K| \leq \Delta t |\tau^<em>| \sum_{j=0}^{K-1}(1+\Delta t , L)^j = \Delta t |\tau^</em>|\frac{(1+\Delta t , L)^K - 1}{1 - 1 - \Delta t , L} = \frac{|\tau^*|}{L}((1+\Delta t , L)^K - 1)$$</p><p data-raw='Notice that:'>Notice that:</p><p data-raw='$$(1 + x)^K \leq \exp(xK)$$'>$$(1 + x)^K \leq \exp(xK)$$</p><p data-raw=So:>So:</p><p data-raw='$$|e_K| \leq \frac{|\tau^<em>|}{L}(\exp(LKh) - 1) = \frac{|\tau^</em>|}{L}(\exp(Lt_K) - 1)$$'>$$|e_K| \leq \frac{|\tau^<em>|}{L}(\exp(LKh) - 1) = \frac{|\tau^</em>|}{L}(\exp(Lt_K) - 1)$$</p><p data-raw='where $K\Delta t = t_K$'>where $K\Delta t = t_K$</p><p data-raw='Now, if we want to have a bound on the error in the interval $[0,T]$, we have:'>Now, if we want to have a bound on the error in the interval $[0,T]$, we have:</p><p data-raw='$$|e| \leq \frac{|\tau^*|}{L}(\exp(LT) - 1)$$'>$$|e| \leq \frac{|\tau^*|}{L}(\exp(LT) - 1)$$</p><p data-raw='We have proved the following Theorem:'>We have proved the following Theorem:</p><h2 id=convergence-of-forward-euler>Convergence of Forward Euler
<a class=anchor href=#convergence-of-forward-euler>#</a></h2><p data-raw='If the initial data are exact, and $f$ is Lipschitz continuous with respect to the $y$-argument, with a solution $\in C^2(0,T)$, then FE converges.'>If the initial data are exact, and $f$ is Lipschitz continuous with respect to the $y$-argument, with a solution $\in C^2(0,T)$, then FE converges.</p><p data-raw='In fact:'>In fact:</p><p data-raw='$$|\tau^*| = \frac{1}{2}\Delta t \max_i |y^{&rsquo;&rsquo;}| \xrightarrow{\Delta t \to 0} 0$$'>$$|\tau^*| = \frac{1}{2}\Delta t \max_i |y^{&rsquo;&rsquo;}| \xrightarrow{\Delta t \to 0} 0$$</p><p data-raw='and $|e| \leq |\tau^*|\frac{\exp(LT) - 1}{L} \xrightarrow{\Delta t \to 0} 0$ is bounded.'>and $|e| \leq |\tau^*|\frac{\exp(LT) - 1}{L} \xrightarrow{\Delta t \to 0} 0$ is bounded.</p><p data-raw='<strong>FE is convergent with order 1.</strong>'><strong>FE is convergent with order 1.</strong></p><p data-raw='Beyond the Theorem per se (we will generalize it to other methods), it is important to notice that the convergence is the consequences of two peculiarities:'>Beyond the Theorem per se (we will generalize it to other methods), it is important to notice that the convergence is the consequences of two peculiarities:</p><p data-raw='<strong>(1)</strong> $\tau^* \xrightarrow{\Delta t \to 0} 0$ the LTE/GTE vanishes as $\Delta t \to 0$, so locally the error is under control.'><strong>(1)</strong> $\tau^* \xrightarrow{\Delta t \to 0} 0$ the LTE/GTE vanishes as $\Delta t \to 0$, so locally the error is under control.</p><p data-raw='This property is called <strong>CONSISTENCY</strong>.'>This property is called <strong>CONSISTENCY</strong>.</p><p data-raw='<strong>(2)</strong> The factor $\frac{\exp(LT) - 1}{L}$ is independent of $\Delta t$ (or, in general, doesn&rsquo;t blow up for $\Delta t \to 0$). This is related to the way the error propagates so it is a &ldquo;global&rdquo; property through the constant $L$.'><strong>(2)</strong> The factor $\frac{\exp(LT) - 1}{L}$ is independent of $\Delta t$ (or, in general, doesn&rsquo;t blow up for $\Delta t \to 0$). This is related to the way the error propagates so it is a &ldquo;global&rdquo; property through the constant $L$.</p><p data-raw='The control of the error in time is called <strong>STABILITY</strong>.'>The control of the error in time is called <strong>STABILITY</strong>.</p><p data-raw='In some sense, we can say that:'>In some sense, we can say that:</p><p data-raw='<strong>CONVERGENCE = CONSISTENCY + STABILITY</strong>'><strong>CONVERGENCE = CONSISTENCY + STABILITY</strong></p><p data-raw='In spite of the intuitive arguments we used here, we will see that this is a good representation of general results (Lax-Richtmyer equivalence Theorem) holding for method using linear combinations of the function $f$ evaluated in the points $(t_i, u_i)$.'>In spite of the intuitive arguments we used here, we will see that this is a good representation of general results (Lax-Richtmyer equivalence Theorem) holding for method using linear combinations of the function $f$ evaluated in the points $(t_i, u_i)$.</p><h1 id=other-one-step-methods>Other One-Step Methods
<a class=anchor href=#other-one-step-methods>#</a></h1><p data-raw='So far, we have two one-step methods (forward and backward Euler). Let&rsquo;s see other two. It is instructive to see how they are devised.'>So far, we have two one-step methods (forward and backward Euler). Let&rsquo;s see other two. It is instructive to see how they are devised.</p><h2 id=crank-nicolson>Crank-Nicolson
<a class=anchor href=#crank-nicolson>#</a></h2><p data-raw='From $y(t) = y_0 + \int_0^t f(\tau, y(\tau))d\tau$ we can organize the following method:'>From $y(t) = y_0 + \int_0^t f(\tau, y(\tau))d\tau$ we can organize the following method:</p><p data-raw='![Timeline showing points $t_0, t_1, t_2, t_3, t_4, t_5, t_6, t_7$]'>![Timeline showing points $t_0, t_1, t_2, t_3, t_4, t_5, t_6, t_7$]</p><p data-raw='Localize: $y(t_{n+1}) = y(t_n) + \int_{t_n}^{t_{n+1}} f(\tau, y(\tau)) d\tau$'>Localize: $y(t_{n+1}) = y(t_n) + \int_{t_n}^{t_{n+1}} f(\tau, y(\tau)) d\tau$</p><p data-raw='Discretize: approximate the integral with the trapezoidal rule:'>Discretize: approximate the integral with the trapezoidal rule:</p><p data-raw='$$u_{n+1} = u_n + \frac{f(t_{n+1}, u_{n+1}) + f(t_n, u_n)}{2} \Delta t$$'>$$u_{n+1} = u_n + \frac{f(t_{n+1}, u_{n+1}) + f(t_n, u_n)}{2} \Delta t$$</p><p data-raw='This is a Second Order (EXERCISE) one-step implicit method.'>This is a Second Order (EXERCISE) one-step implicit method.</p><h2 id=heun>Heun
<a class=anchor href=#heun>#</a></h2><p data-raw='Let&rsquo;s start from Crank-Nicolson and make it &ldquo;explicit&rdquo;.'>Let&rsquo;s start from Crank-Nicolson and make it &ldquo;explicit&rdquo;.</p><p data-raw='On the right hand side of CN we set:'>On the right hand side of CN we set:</p><p data-raw='$$u_{n+1} \simeq u_n + \Delta t , f(t_n, u_n) \quad \text{(Explicit Euler)}$$'>$$u_{n+1} \simeq u_n + \Delta t , f(t_n, u_n) \quad \text{(Explicit Euler)}$$</p><p data-raw='We obtain the scheme:'>We obtain the scheme:</p><p data-raw='$$u_{n+1} = u_n + \Delta t \frac{f(t_{n+1}, u_n+\Delta t f(t_n, u_n)) + f(t_n, u_n)}{2}$$'>$$u_{n+1} = u_n + \Delta t \frac{f(t_{n+1}, u_n+\Delta t f(t_n, u_n)) + f(t_n, u_n)}{2}$$</p><p data-raw='This is called Heun. It is One-step, still 2nd order (EXERCISE).'>This is called Heun. It is One-step, still 2nd order (EXERCISE).</p><h2 id=the-concept-of-zero-stability>The Concept of Zero-Stability
<a class=anchor href=#the-concept-of-zero-stability>#</a></h2><p data-raw='Let&rsquo;s start applying the concept to one-step methods in the form:'>Let&rsquo;s start applying the concept to one-step methods in the form:</p><p data-raw='$$u_{n+1} = u_n + \Delta t , \Phi(u_n, t_n, f_n; \Delta t)$$'>$$u_{n+1} = u_n + \Delta t , \Phi(u_n, t_n, f_n; \Delta t)$$</p><p data-raw='Let&rsquo;s consider the perturbed scheme:'>Let&rsquo;s consider the perturbed scheme:</p><p data-raw='$$\begin{cases}
w_{n+1} = w_n + \Delta t (\Phi(z_n, t_n, f(t_n, z_n); \Delta t) + \delta_n) \
w_0 = y_0 + \delta_0 & \text{with } |\delta_i| \leq \varepsilon
\end{cases}$$'>$$\begin{cases}
w_{n+1} = w_n + \Delta t (\Phi(z_n, t_n, f(t_n, z_n); \Delta t) + \delta_n) \
w_0 = y_0 + \delta_0 & \text{with } |\delta_i| \leq \varepsilon
\end{cases}$$</p><p data-raw='We say that the method is zero-stable if for $\Delta t < \Delta t_0$, there exists a constant $C > 0$ such that'>We say that the method is zero-stable if for $\Delta t &lt; \Delta t_0$, there exists a constant $C > 0$ such that</p><p data-raw='$$|u_n - z_n| \leq C\varepsilon$$'>$$|u_n - z_n| \leq C\varepsilon$$</p><p data-raw='for $\varepsilon > 0$ sufficiently small.
($C$ and $\Delta t_0$ depend on problem data, $T_{fin}$, $f$).'>for $\varepsilon > 0$ sufficiently small.
($C$ and $\Delta t_0$ depend on problem data, $T_{fin}$, $f$).</p><p data-raw='It is possible to prove the following theorem:'>It is possible to prove the following theorem:</p><p data-raw='<strong>Theorem:</strong> If $\Phi$ is Lipschitz continuous with respect to the dependence on $u_n$ ($\exists \Delta > 0$: $|\Phi(u_n) - \Phi(z_n)| \leq \Delta |u_n - z_n|$), then the One-step method is zero-stable.'><strong>Theorem:</strong> If $\Phi$ is Lipschitz continuous with respect to the dependence on $u_n$ ($\exists \Delta > 0$: $|\Phi(u_n) - \Phi(z_n)| \leq \Delta |u_n - z_n|$), then the One-step method is zero-stable.</p><p data-raw='Lipschitz continuity in this case is enough for zero-stability (and it is generally a consequence of Lipschitz continuity of $f$).'>Lipschitz continuity in this case is enough for zero-stability (and it is generally a consequence of Lipschitz continuity of $f$).</p><p data-raw='Then, we have another theorem. It generalizes the theorem for the explicit Euler.'>Then, we have another theorem. It generalizes the theorem for the explicit Euler.</p><p data-raw='<strong>Theorem:</strong> If $\Phi$ is like in the previous theorem, then:'><strong>Theorem:</strong> If $\Phi$ is like in the previous theorem, then:</p><p data-raw='$$|y(t_n) - u_n| \leq \left(|y_0 - u_0| + t_n , \tau(\Delta t)\right) e^{L t_n}.$$'>$$|y(t_n) - u_n| \leq \left(|y_0 - u_0| + t_n , \tau(\Delta t)\right) e^{L t_n}.$$</p><p data-raw='Therefore, if:'>Therefore, if:</p><ul><li>$\tau(\Delta t) \to 0$ with $\Delta t$</li><li>$y_0 - u_0 \to 0$ with $\Delta t$</li></ul><p data-raw='the method is convergent'>the method is convergent</p><p data-raw='(and the order is $\Delta t^p$, with $p = \min(p_1, p_2)$ where:'>(and the order is $\Delta t^p$, with $p = \min(p_1, p_2)$ where:</p><ul><li>$\tau(\Delta t) \sim O(\Delta t^{p_1})$</li><li>$y_0 - u_0 \sim O(\Delta t^{p_2})$ (generally $p = p_1$).</li></ul><p data-raw='In other terms:'>In other terms:</p><p data-raw='For one-step method (not true for multi-step):'>For one-step method (not true for multi-step):</p><p data-raw='$$\Phi \text{ Lipschitz continuous } \Rightarrow \text{ Method zero-stable} + \text{Consistency} \Rightarrow \text{CONVERGENCE}$$'>$$\Phi \text{ Lipschitz continuous } \Rightarrow \text{ Method zero-stable} + \text{Consistency} \Rightarrow \text{CONVERGENCE}$$</p><p data-raw='$\Rightarrow$ If $\Phi$ is Lipschitz continuous, consistency $\Rightarrow$ convergence.'>$\Rightarrow$ If $\Phi$ is Lipschitz continuous, consistency $\Rightarrow$ convergence.</p><h2 id=the-concept-of-absolute-stability>The Concept of Absolute Stability
<a class=anchor href=#the-concept-of-absolute-stability>#</a></h2><p data-raw='The zero-stability is a property of the numerical scheme, required by the presence of roundoff errors and other possible perturbations.'>The zero-stability is a property of the numerical scheme, required by the presence of roundoff errors and other possible perturbations.</p><p data-raw='However, there is another (maybe more engineering) concept of stability related to the nature of the problem to solve.'>However, there is another (maybe more engineering) concept of stability related to the nature of the problem to solve.</p><p data-raw='In simple words, we want that, if a problem is asymptotically stable, the numerical approximation is asymptotically stable too.'>In simple words, we want that, if a problem is asymptotically stable, the numerical approximation is asymptotically stable too.</p><p data-raw='Is this happening? Let&rsquo;s consider the prototype of asymptotically stable problem (Model Problem).'>Is this happening? Let&rsquo;s consider the prototype of asymptotically stable problem (Model Problem).</p><p data-raw='Let&rsquo;s consider the Cauchy problem:'>Let&rsquo;s consider the Cauchy problem:</p><p data-raw='$$\begin{cases}
\frac{dy}{dt} = \lambda y & t > 0 \
y(0) = y_0
\end{cases}$$'>$$\begin{cases}
\frac{dy}{dt} = \lambda y & t > 0 \
y(0) = y_0
\end{cases}$$</p><p data-raw='We know that the solution is asymptotically stable for $\lambda < 0$ $(y_{ex} = y_0 e^{\lambda t} \xrightarrow{t \to \infty} 0 \text{ for } \lambda < 0)$'>We know that the solution is asymptotically stable for $\lambda &lt; 0$ $(y_{ex} = y_0 e^{\lambda t} \xrightarrow{t \to \infty} 0 \text{ for } \lambda &lt; 0)$</p><p data-raw='In fact: $\frac{dz}{dt} = \lambda z$ with $z(0) = y_0 + \varepsilon \Rightarrow z - y = \varepsilon e^{\lambda t} \xrightarrow{t\to\infty} 0$ for $\lambda < 0$.'>In fact: $\frac{dz}{dt} = \lambda z$ with $z(0) = y_0 + \varepsilon \Rightarrow z - y = \varepsilon e^{\lambda t} \xrightarrow{t\to\infty} 0$ for $\lambda &lt; 0$.</p><h3 id=remark-1>Remark
<a class=anchor href=#remark-1>#</a></h3><p data-raw='For a system: $\begin{cases} \frac{d\mathbf{y}}{dt} = A\mathbf{y} & \mathbf{y} \in \mathbb{R}^n \ \mathbf{y}(0) = \mathbf{y}_0 & A \in \mathbb{R}^{n \times n} \end{cases}$'>For a system: $\begin{cases} \frac{d\mathbf{y}}{dt} = A\mathbf{y} & \mathbf{y} \in \mathbb{R}^n \ \mathbf{y}(0) = \mathbf{y}_0 & A \in \mathbb{R}^{n \times n} \end{cases}$</p><p data-raw='the asymptotic stability is guaranteed if THE REAL PART OF ALL THE EIGENVALUES is negative.'>the asymptotic stability is guaranteed if THE REAL PART OF ALL THE EIGENVALUES is negative.</p><p data-raw='To be general, from now on we will consider $\lambda \in \mathbb{C}$ also for the scalar case. In particular, the left-plane $\text{Real}(\lambda) < 0$ is the region of the complex plane where the original problem is asymptotically stable.'>To be general, from now on we will consider $\lambda \in \mathbb{C}$ also for the scalar case. In particular, the left-plane $\text{Real}(\lambda) &lt; 0$ is the region of the complex plane where the original problem is asymptotically stable.</p><h4 id=question-is-the-solution-of-the-model-problem-with-explicit-euler-asymptotically-vanishing-as-the-exact-solution>Question: is the solution of the model problem with Explicit Euler asymptotically vanishing as the exact solution?
<a class=anchor href=#question-is-the-solution-of-the-model-problem-with-explicit-euler-asymptotically-vanishing-as-the-exact-solution>#</a></h4><p data-raw='Notice that the question is not related to the behavior of the solution for $\Delta t \to 0$, but for $\Delta t$ given and $t \to +\infty$.'>Notice that the question is not related to the behavior of the solution for $\Delta t \to 0$, but for $\Delta t$ given and $t \to +\infty$.</p><p data-raw='Let&rsquo;s see:
$$\frac{dy}{dt} = \lambda y \quad \text{EE}: \frac{u_{i+1} - u_i}{\Delta t} = \lambda u_i$$'>Let&rsquo;s see:
$$\frac{dy}{dt} = \lambda y \quad \text{EE}: \frac{u_{i+1} - u_i}{\Delta t} = \lambda u_i$$</p><p data-raw='$$u_{i+1} = (1 + \lambda \Delta t) u_i \quad (\text{Re}(\lambda) < 0)$$'>$$u_{i+1} = (1 + \lambda \Delta t) u_i \quad (\text{Re}(\lambda) &lt; 0)$$</p><p data-raw='$$|u_{i+1}| = |1 + \lambda \Delta t| |u_i| \Rightarrow |u_{i+1}| = |1 + \lambda \Delta t|^{i+1} |u_0|$$'>$$|u_{i+1}| = |1 + \lambda \Delta t| |u_i| \Rightarrow |u_{i+1}| = |1 + \lambda \Delta t|^{i+1} |u_0|$$</p><p data-raw='The solution asymptotically vanishes if $|1 + \lambda \Delta t| < 1$'>The solution asymptotically vanishes if $|1 + \lambda \Delta t| &lt; 1$</p><p data-raw='Intuitively, if $\lambda$ is Real and negative:'>Intuitively, if $\lambda$ is Real and negative:</p><p data-raw='$$|1 + \lambda \Delta t| < 1$$
$$\Downarrow$$
$$-1 < 1 + \lambda \Delta t < 1$$
$$\Downarrow$$
$$-2 < \lambda \Delta t < 0$$
$$\Downarrow$$
$$\Delta t < \frac{2}{|\lambda|}$$'>$$|1 + \lambda \Delta t| &lt; 1$$
$$\Downarrow$$
$$-1 &lt; 1 + \lambda \Delta t &lt; 1$$
$$\Downarrow$$
$$-2 &lt; \lambda \Delta t &lt; 0$$
$$\Downarrow$$
$$\Delta t &lt; \frac{2}{|\lambda|}$$</p><p data-raw='So, if $\Delta t > \frac{2}{|\lambda|}$ the numerical solution is not stable.'>So, if $\Delta t > \frac{2}{|\lambda|}$ the numerical solution is not stable.</p><p data-raw='For $\lambda$ complex, we can draw the region of the plane $\lambda \Delta t$ where the solution is stable.'>For $\lambda$ complex, we can draw the region of the plane $\lambda \Delta t$ where the solution is stable.</p><p data-raw='![Complex plane diagram showing unit circle with center at (-1,0)]'>![Complex plane diagram showing unit circle with center at (-1,0)]</p><p data-raw='Unit circle with center in $(-1, 0)$'>Unit circle with center in $(-1, 0)$</p><p data-raw='(In magenta the region of stability of the problem).'>(In magenta the region of stability of the problem).</p><p data-raw='So, the method is reproducing the asymptotic behavior of the exact solution ONLY UNDER THE CONDITION THAT:'>So, the method is reproducing the asymptotic behavior of the exact solution ONLY UNDER THE CONDITION THAT:</p><p data-raw='$$\lambda \Delta t \in \text{Unit Circle centered in } (-1, 0).$$'>$$\lambda \Delta t \in \text{Unit Circle centered in } (-1, 0).$$</p><h4 id=what-happens-with-implicit-euler>What happens with Implicit Euler?
<a class=anchor href=#what-happens-with-implicit-euler>#</a></h4><p data-raw='$$\frac{1}{|1 - \lambda \Delta t|} < 1 \quad \forall \Delta t,$$'>$$\frac{1}{|1 - \lambda \Delta t|} &lt; 1 \quad \forall \Delta t,$$</p><p data-raw='so $u^{n+1} \xrightarrow{n \to \infty} 0 \quad \forall \Delta t$'>so $u^{n+1} \xrightarrow{n \to \infty} 0 \quad \forall \Delta t$</p><p data-raw='There is a big difference between the two Eulers: one is stable under some conditions, the other is unconditionally stable.'>There is a big difference between the two Eulers: one is stable under some conditions, the other is unconditionally stable.</p><h4 id=what-about-crank-nicolson>What about Crank-Nicolson?
<a class=anchor href=#what-about-crank-nicolson>#</a></h4><p data-raw='$$u^{n+1} = u^n + \Delta t \frac{\lambda u^{n+1} + \lambda u^n}{2} \Rightarrow u^{n+1} = \frac{1 + \frac{\Delta t}{2}\lambda}{1 - \frac{\Delta t}{2}\lambda} u^n$$'>$$u^{n+1} = u^n + \Delta t \frac{\lambda u^{n+1} + \lambda u^n}{2} \Rightarrow u^{n+1} = \frac{1 + \frac{\Delta t}{2}\lambda}{1 - \frac{\Delta t}{2}\lambda} u^n$$</p><p data-raw='Again, for $\lambda \in \mathbb{C}$ we have:'>Again, for $\lambda \in \mathbb{C}$ we have:</p><p data-raw='$$\left|\frac{1 + \frac{\Delta t}{2}\lambda}{1 - \frac{\Delta t}{2}\lambda}\right| < 1 \quad \forall \Delta t$$'>$$\left|\frac{1 + \frac{\Delta t}{2}\lambda}{1 - \frac{\Delta t}{2}\lambda}\right| &lt; 1 \quad \forall \Delta t$$</p><p data-raw='so also CN is stable with no condition.'>so also CN is stable with no condition.</p><h2 id=definition>DEFINITION
<a class=anchor href=#definition>#</a></h2><p data-raw='A method is said to be ABSOLUTELY STABLE if the solution of the model problem $\frac{dy}{dt} = \lambda y$ vanishes asymptotically when $t \to +\infty$.'>A method is said to be ABSOLUTELY STABLE if the solution of the model problem $\frac{dy}{dt} = \lambda y$ vanishes asymptotically when $t \to +\infty$.</p><p data-raw='We say that the method is UNCONDITIONALLY ABSOLUTELY STABLE if this happens $\forall \Delta t > 0$. On the contrary, it is said to be CONDITIONALLY STABLE if we have limitations on $\Delta t$.'>We say that the method is UNCONDITIONALLY ABSOLUTELY STABLE if this happens $\forall \Delta t > 0$. On the contrary, it is said to be CONDITIONALLY STABLE if we have limitations on $\Delta t$.</p><h2 id=another-example-heun>Another Example: Heun
<a class=anchor href=#another-example-heun>#</a></h2><p data-raw='$$u^{n+1} = u^n + \frac{\Delta t}{2}(\lambda(u^n + \Delta t \lambda u^n) + \lambda u^n) = \left(1 + \Delta t \lambda + \frac{\Delta t^2 \lambda^2}{2}\right) u^n$$'>$$u^{n+1} = u^n + \frac{\Delta t}{2}(\lambda(u^n + \Delta t \lambda u^n) + \lambda u^n) = \left(1 + \Delta t \lambda + \frac{\Delta t^2 \lambda^2}{2}\right) u^n$$</p><p data-raw='Now, consider the curve $\frac{\Delta t^2 \lambda^2}{2} + \Delta t \lambda + 1$ for $\lambda < 0$'>Now, consider the curve $\frac{\Delta t^2 \lambda^2}{2} + \Delta t \lambda + 1$ for $\lambda &lt; 0$</p><p data-raw='We see that we need:'>We see that we need:</p><p data-raw='$$0 < \Delta t < \frac{2}{|\lambda|}$$'>$$0 &lt; \Delta t &lt; \frac{2}{|\lambda|}$$</p><p data-raw='(as for Explicit Euler).'>(as for Explicit Euler).</p><p data-raw='In the complex plane, the region is slightly larger than for Explicit Euler.'>In the complex plane, the region is slightly larger than for Explicit Euler.</p><h2 id=remark-we-found-that-the-explicit-methods-are-conditionally-stable-while-the-implicit-ones-are-unconditionally-stable>REMARK: We found that the explicit methods are conditionally stable, while the implicit ones are unconditionally stable.
<a class=anchor href=#remark-we-found-that-the-explicit-methods-are-conditionally-stable-while-the-implicit-ones-are-unconditionally-stable>#</a></h2><p data-raw='In general, we do not have unconditionally stable explicit methods, but we do have implicit methods that are only conditionally stable.'>In general, we do not have unconditionally stable explicit methods, but we do have implicit methods that are only conditionally stable.</p><p data-raw='The region of absolute stability is the portion of $\mathbb{C}^-$ for $\lambda \Delta t$ to belong to such that the method is absolutely stable. Unconditional absolute stability (a.k.a A-stability) is when this region is the entire left plane $\mathbb{C}^-$.'>The region of absolute stability is the portion of $\mathbb{C}^-$ for $\lambda \Delta t$ to belong to such that the method is absolutely stable. Unconditional absolute stability (a.k.a A-stability) is when this region is the entire left plane $\mathbb{C}^-$.</p><p data-raw='The concept of absolute stability somehow clarifies what are the criteria to select a method for a problem.'>The concept of absolute stability somehow clarifies what are the criteria to select a method for a problem.</p><p data-raw='In fact, let&rsquo;s first consider the general case(s):'>In fact, let&rsquo;s first consider the general case(s):</p><ol><li><p data-raw='$\frac{dy}{dt} = f(t,y) \simeq f(t,y_0) + \frac{\partial f}{\partial t}(t-t_0) + \frac{\partial f}{\partial y}(t,y_0)(y-y_0)$'>$\frac{dy}{dt} = f(t,y) \simeq f(t,y_0) + \frac{\partial f}{\partial t}(t-t_0) + \frac{\partial f}{\partial y}(t,y_0)(y-y_0)$</p><p data-raw='so we can locally take $\lambda \simeq \frac{\partial f}{\partial y}(t,y_0)$'>so we can locally take $\lambda \simeq \frac{\partial f}{\partial y}(t,y_0)$</p></li><li><p data-raw='For a system: $\frac{d\mathbf{y}}{dt} = A \mathbf{y} \Rightarrow \lambda = eig(A)$'>For a system: $\frac{d\mathbf{y}}{dt} = A \mathbf{y} \Rightarrow \lambda = eig(A)$</p></li><li><p data-raw='$\frac{d\mathbf{y}}{dt} = \mathbf{F}(t,\mathbf{y})$ (nonlinear system)'>$\frac{d\mathbf{y}}{dt} = \mathbf{F}(t,\mathbf{y})$ (nonlinear system)</p><p data-raw='$\Rightarrow \lambda = eig\left(\frac{\partial \mathbf{F}}{\partial \mathbf{y}}(t_0, y_0) \right)$ - Jacobian'>$\Rightarrow \lambda = eig\left(\frac{\partial \mathbf{F}}{\partial \mathbf{y}}(t_0, y_0) \right)$ - Jacobian</p></li></ol><p data-raw='Now, for a general problem, we have:'>Now, for a general problem, we have:</p><table><thead><tr><th>Method</th><th>Nature</th><th>Accuracy</th><th>Limitations on $\Delta t$</th></tr></thead><tbody><tr><td>FE</td><td>Explicit</td><td>1</td><td>$\Delta t &lt; \frac{2}{</td></tr><tr><td>BE</td><td>Implicit</td><td>1</td><td>NO</td></tr><tr><td>CN</td><td>Implicit</td><td>2</td><td>NO</td></tr><tr><td>H</td><td>Explicit</td><td>2</td><td>$\Delta t &lt; \frac{2}{</td></tr></tbody></table><p data-raw='Implicit Methods are more computationally expensive.'>Implicit Methods are more computationally expensive.</p><p data-raw='In an extreme synthesis:'>In an extreme synthesis:</p><p data-raw='![Comparison of FE and BE with timeline]'>![Comparison of FE and BE with timeline]</p><p data-raw='With FE each step is low-cost (Explicit) but we may need to do many for the conditional stability.'>With FE each step is low-cost (Explicit) but we may need to do many for the conditional stability.</p><p data-raw='With BE each step is more expensive, but we need to do (generally) fewer steps.'>With BE each step is more expensive, but we need to do (generally) fewer steps.</p><p data-raw='$\Rightarrow$ The optimal choice is largely problem dependent.'>$\Rightarrow$ The optimal choice is largely problem dependent.</p><h1 id=multistep-methods>Multistep Methods
<a class=anchor href=#multistep-methods>#</a></h1><p data-raw='The mid-point method is just an example of multi-step methods:'>The mid-point method is just an example of multi-step methods:</p><p data-raw='$$u_{n+1} = u_{n-1} + 2\Delta t , f(t_n, u_n)$$'>$$u_{n+1} = u_{n-1} + 2\Delta t , f(t_n, u_n)$$</p><p data-raw='In general, a multistep method with $p$ steps take the form:'>In general, a multistep method with $p$ steps take the form:</p><p data-raw='$$u_{n+1} - \sum_0^p a_j u_{n-j} = \Delta t \sum_{j=-1}^p b_j f(t_{n-j}, u_{n-j})$$'>$$u_{n+1} - \sum_0^p a_j u_{n-j} = \Delta t \sum_{j=-1}^p b_j f(t_{n-j}, u_{n-j})$$</p><p data-raw='The method is IMPLICIT when $b_{-1} \neq 0$.'>The method is IMPLICIT when $b_{-1} \neq 0$.</p><h2 id=example>Example:
<a class=anchor href=#example>#</a></h2><p data-raw='$$y(t_{n+1}) = y(t_{n-1}) + \int_{t_{n-1}}^{t_{n+1}} f(\tau, y(\tau)) d\tau$$'>$$y(t_{n+1}) = y(t_{n-1}) + \int_{t_{n-1}}^{t_{n+1}} f(\tau, y(\tau)) d\tau$$</p><p data-raw='$$\Downarrow \text{ SIMPSON}$$'>$$\Downarrow \text{ SIMPSON}$$</p><p data-raw='$$u_{n+1} = u_{n-1} + 2\Delta t \frac{f(t_{n+1}, u_{n+1}) + 4f(t_n, u_n) + f(t_{n-1}, u_{n-1})}{6} =$$'>$$u_{n+1} = u_{n-1} + 2\Delta t \frac{f(t_{n+1}, u_{n+1}) + 4f(t_n, u_n) + f(t_{n-1}, u_{n-1})}{6} =$$</p><p data-raw='$$= u_{n-1} + \frac{\Delta t}{3}(f(t_{n+1}, u_{n+1}) + 4f(t_n, u_n) + f(t_{n-1}, u_{n-1}))$$'>$$= u_{n-1} + \frac{\Delta t}{3}(f(t_{n+1}, u_{n+1}) + 4f(t_n, u_n) + f(t_{n-1}, u_{n-1}))$$</p><p data-raw='$$\mathbf{a} = \begin{bmatrix} 0 \ 1 \end{bmatrix} a_0, a_1 \quad \mathbf{b} = \begin{bmatrix} \frac{1}{3} \ \frac{4}{3} \ \frac{1}{3} \end{bmatrix} b_{-1}, b_0, b_1$$'>$$\mathbf{a} = \begin{bmatrix} 0 \ 1 \end{bmatrix} a_0, a_1 \quad \mathbf{b} = \begin{bmatrix} \frac{1}{3} \ \frac{4}{3} \ \frac{1}{3} \end{bmatrix} b_{-1}, b_0, b_1$$</p><p data-raw='In general, we have two approaches for deriving a Multi-step methods:'>In general, we have two approaches for deriving a Multi-step methods:</p><h2 id=bdf-backward-difference-formulas>BDF (Backward Difference Formulas)
<a class=anchor href=#bdf-backward-difference-formulas>#</a></h2><p data-raw='$$\frac{dy}{dt}(t_{n+1}) = f(t_{n+1}, u_{n+1})$$
$$\downarrow$$
approximate this with a backward finite difference, e.g.,'>$$\frac{dy}{dt}(t_{n+1}) = f(t_{n+1}, u_{n+1})$$
$$\downarrow$$
approximate this with a backward finite difference, e.g.,</p><p data-raw='$$\frac{dy}{dt}(t_{n+1}) \simeq \frac{\frac{3}{2}u_{n+1} - 2u_n + \frac{1}{2}u_{n-1}}{\Delta t}$$'>$$\frac{dy}{dt}(t_{n+1}) \simeq \frac{\frac{3}{2}u_{n+1} - 2u_n + \frac{1}{2}u_{n-1}}{\Delta t}$$</p><p data-raw='$$\Rightarrow u_{n+1} = \frac{4}{3}u_n - \frac{1}{3}u_{n-1} + \Delta t f(t_{n+1}, u_{n+1})$$'>$$\Rightarrow u_{n+1} = \frac{4}{3}u_n - \frac{1}{3}u_{n-1} + \Delta t f(t_{n+1}, u_{n+1})$$</p><p data-raw='They are all in the form:'>They are all in the form:</p><p data-raw='$$\mathbf{b} = \begin{bmatrix} 1 \ 0 \ \vdots \ 0 \end{bmatrix}$$'>$$\mathbf{b} = \begin{bmatrix} 1 \ 0 \ \vdots \ 0 \end{bmatrix}$$</p><h2 id=adams>Adams
<a class=anchor href=#adams>#</a></h2><p data-raw='In this case, we start from:'>In this case, we start from:</p><p data-raw='$$y(t_{n+1}) = y(t_n) + \int_{t_n}^{t_{n+1}} f(\tau, y(\tau)) d\tau$$'>$$y(t_{n+1}) = y(t_n) + \int_{t_n}^{t_{n+1}} f(\tau, y(\tau)) d\tau$$</p><p data-raw='Now, we replace $f$ with an interpolation:'>Now, we replace $f$ with an interpolation:</p><ol><li>We interpolate $f$ over the nodes: $n-p, n-p+1, &mldr; n$ so to have an explicit method (Adams-Bashforth)</li><li>We interpolate $f$ over the nodes: $n-p, n-p+1, &mldr; n, n+1$</li></ol><p data-raw='Adams methods have always:'>Adams methods have always:</p><p data-raw='$$\mathbf{a} = \begin{bmatrix} 1 \ 0 \ \vdots \ 0 \end{bmatrix}$$'>$$\mathbf{a} = \begin{bmatrix} 1 \ 0 \ \vdots \ 0 \end{bmatrix}$$</p><h2 id=a-rapid-recall-of-difference-equations-theory>A Rapid Recall of Difference Equations Theory
<a class=anchor href=#a-rapid-recall-of-difference-equations-theory>#</a></h2><p data-raw='A linear difference equation is an equation in the form:'>A linear difference equation is an equation in the form:</p><p data-raw='$$u_{n+p} + a_{n+p-1}u_{n+p-1} + a_{n+p-2}u_{n+p-2} + \ldots + a_0u_n = \varphi_{n+p}$$'>$$u_{n+p} + a_{n+p-1}u_{n+p-1} + a_{n+p-2}u_{n+p-2} + \ldots + a_0u_n = \varphi_{n+p}$$</p><p data-raw='where the solution is the set ${u_j}$ and initial conditions $u_0, u_1, \ldots u_{p-1}$ are given.'>where the solution is the set ${u_j}$ and initial conditions $u_0, u_1, \ldots u_{p-1}$ are given.</p><p data-raw='The theory on difference equations has several similarities with the theory on differential equations. In particular, the general solution of a linear difference equation is the linear combination of a particular solution of the equation and the general solution of the homogeneous one.'>The theory on difference equations has several similarities with the theory on differential equations. In particular, the general solution of a linear difference equation is the linear combination of a particular solution of the equation and the general solution of the homogeneous one.</p><p data-raw='The general solution of the homogeneous takes the form:'>The general solution of the homogeneous takes the form:</p><p data-raw='$$u_n = \sum_{j=0}^{N}\left(\sum_{s=0}^{m_j-1} V_{js}n^s \right)r_j^n$$'>$$u_n = \sum_{j=0}^{N}\left(\sum_{s=0}^{m_j-1} V_{js}n^s \right)r_j^n$$</p><p data-raw='where $r_j$ are the roots of:'>where $r_j$ are the roots of:</p><p data-raw='$$r^p + a_{p-1}r^{p-1} + a_{p-2}r^{p-2} + \ldots + a_0 = 0$$'>$$r^p + a_{p-1}r^{p-1} + a_{p-2}r^{p-2} + \ldots + a_0 = 0$$</p><p data-raw=and>and</p><ul><li>$N$ is the number of distinct roots</li><li>$m_j$ is the multiplicity of $r_j$</li></ul><p data-raw='We will see a strong connection between this theory and the analysis of the linear multi-step methods.'>We will see a strong connection between this theory and the analysis of the linear multi-step methods.</p><p data-raw='In fact, a LMM reads like:'>In fact, a LMM reads like:</p><p data-raw='$$u_{n+1} - \sum_{j=0}^p a_j u_{n-j} = \Delta t \sum_{j=-1}^p b_j f_{n-j}$$'>$$u_{n+1} - \sum_{j=0}^p a_j u_{n-j} = \Delta t \sum_{j=-1}^p b_j f_{n-j}$$</p><p data-raw='If we consider the model problem, we are lead to the linear difference equation:'>If we consider the model problem, we are lead to the linear difference equation:</p><p data-raw='$$u_{n+1} - \sum_{j=0}^p a_j u_{n-j} - \Delta t \lambda \sum_{j=-1}^p b_j u_{n-j} = 0$$'>$$u_{n+1} - \sum_{j=0}^p a_j u_{n-j} - \Delta t \lambda \sum_{j=-1}^p b_j u_{n-j} = 0$$</p><p data-raw='so we need to solve exactly a linear difference equation.'>so we need to solve exactly a linear difference equation.</p><p data-raw='More precisely, we still define the LTE as the residual of the exact solution in the numerical scheme:'>More precisely, we still define the LTE as the residual of the exact solution in the numerical scheme:</p><p data-raw='$$\Delta t , \tau_{n+1} = y_{ex}(t_{n+1}) - \sum_{j=0}^p a_j y_{ex}(t_{n-j}) - \Delta t \sum_{j=-1}^p b_j \frac{dy_{ex}}{dt}(t_{n-j})$$'>$$\Delta t , \tau_{n+1} = y_{ex}(t_{n+1}) - \sum_{j=0}^p a_j y_{ex}(t_{n-j}) - \Delta t \sum_{j=-1}^p b_j \frac{dy_{ex}}{dt}(t_{n-j})$$</p><p data-raw='The method is CONSISTENT when $\tau_{n+1} \xrightarrow{\Delta t \to 0} 0 \quad \forall n$'>The method is CONSISTENT when $\tau_{n+1} \xrightarrow{\Delta t \to 0} 0 \quad \forall n$</p><h2 id=zero-stability-definition>Zero-Stability Definition
<a class=anchor href=#zero-stability-definition>#</a></h2><p data-raw='The definition of zero-stability is similar to the one for One-step Methods.'>The definition of zero-stability is similar to the one for One-step Methods.</p><p data-raw='Also, we define:'>Also, we define:</p><p data-raw='First characteristic polynomial:'>First characteristic polynomial:</p><p data-raw='$$\rho(z) = z^{p+1} - \sum_{j=0}^p a_j z^{p-j}$$'>$$\rho(z) = z^{p+1} - \sum_{j=0}^p a_j z^{p-j}$$</p><p data-raw='Second characteristic polynomial:'>Second characteristic polynomial:</p><p data-raw='$$\sigma(z) = b_{-1}z^{p+1} - \sum_{j=0}^p b_j z^{p-j}$$'>$$\sigma(z) = b_{-1}z^{p+1} - \sum_{j=0}^p b_j z^{p-j}$$</p><p data-raw='and the polynomial: $\Pi(z) = \rho(z) - \Delta t \lambda \sigma(z)$
(this is the polynomial found for the model problem)'>and the polynomial: $\Pi(z) = \rho(z) - \Delta t \lambda \sigma(z)$
(this is the polynomial found for the model problem)</p><p data-raw='Based on this we define:'>Based on this we define:</p><p data-raw='<strong>Root Condition</strong>: Call $r_i$ the roots of $\rho(z)$. The polynomial (or the associated LMM) fulfills the root condition when:'><strong>Root Condition</strong>: Call $r_i$ the roots of $\rho(z)$. The polynomial (or the associated LMM) fulfills the root condition when:</p><p data-raw='(1) $|r_i| \leq 1 \quad \forall i$
(2) The roots with $|r| = 1$ have multiplicity 1.'>(1) $|r_i| \leq 1 \quad \forall i$
(2) The roots with $|r| = 1$ have multiplicity 1.</p><p data-raw='<strong>Strong Root Condition</strong>:
In addition to the R.C., only $r_0$ is s.t. $|r_0| = 1$, all the other roots $|r_j| < 1$ $(j > 1, \ldots, p)$.'><strong>Strong Root Condition</strong>:
In addition to the R.C., only $r_0$ is s.t. $|r_0| = 1$, all the other roots $|r_j| &lt; 1$ $(j > 1, \ldots, p)$.</p><p data-raw='<strong>Absolute R.C.</strong>: $\exists \Delta t \leq \overline{\Delta t}$ s.t. all the roots $r_j(\Delta t)$ of $\Pi_{\Delta t}(z)$ are s.t. $|r_j(\Delta t)| < 1$, $j = 0, \ldots, p$, $\Delta t \leq \overline{\Delta t}$.'><strong>Absolute R.C.</strong>: $\exists \Delta t \leq \overline{\Delta t}$ s.t. all the roots $r_j(\Delta t)$ of $\Pi_{\Delta t}(z)$ are s.t. $|r_j(\Delta t)| &lt; 1$, $j = 0, \ldots, p$, $\Delta t \leq \overline{\Delta t}$.</p><h2 id=analysis-of-multistep-methods>Analysis of Multistep Methods
<a class=anchor href=#analysis-of-multistep-methods>#</a></h2><p data-raw='We have a sequence of theorems (no proofs):'>We have a sequence of theorems (no proofs):</p><ol><li><p data-raw='A LMM is consistent if and only if:'>A LMM is consistent if and only if:</p><p data-raw='$$\sum_{j=0}^p a_j = 1 \quad -\sum_{j=0}^p j a_j + \sum_{j=-1}^p b_j = 1$$'>$$\sum_{j=0}^p a_j = 1 \quad -\sum_{j=0}^p j a_j + \sum_{j=-1}^p b_j = 1$$</p><p data-raw='Also, the method is at least of order $p$ if the solution is $\in C^{p+1}(I)$ and'>Also, the method is at least of order $p$ if the solution is $\in C^{p+1}(I)$ and</p><p data-raw='$$(*)\ \sum_{j=0}^p (j)^k a_j + k \sum_{j=-1}^p (j)^{k-1} b_j = 1 \quad k = 1, 2, \ldots q$$'>$$(*)\ \sum_{j=0}^p (j)^k a_j + k \sum_{j=-1}^p (j)^{k-1} b_j = 1 \quad k = 1, 2, \ldots q$$</p><p data-raw='<strong>Remark</strong>: The condition $\sum_{j=0}^p a_j = 1$ means that the first characteristic polynomial $\rho(z)$ has at least one root in 1.'><strong>Remark</strong>: The condition $\sum_{j=0}^p a_j = 1$ means that the first characteristic polynomial $\rho(z)$ has at least one root in 1.</p></li><li><p data-raw='A consistent method is zero-stable if and only if it fulfills the root condition.'>A consistent method is zero-stable if and only if it fulfills the root condition.</p></li></ol><p data-raw='With Theorems (1) + (2) we have the convergence and order analysis.'>With Theorems (1) + (2) we have the convergence and order analysis.</p><p data-raw='(1) + (2): If the LMM method falls into (1) and (2) with condition (*) (not true for q+1) and the initial conditions ($u_i \to y_{ex}(t_i)$ $i = 0, \ldots, p$) converge to the exact ones with order $q$, the resulting method is convergent with order $q$.'>(1) + (2): If the LMM method falls into (1) and (2) with condition (*) (not true for q+1) and the initial conditions ($u_i \to y_{ex}(t_i)$ $i = 0, \ldots, p$) converge to the exact ones with order $q$, the resulting method is convergent with order $q$.</p><p data-raw='<strong>Remark (First Dahlquist Barrier)</strong>: There is no zero-stable $p$-LMM with order'><strong>Remark (First Dahlquist Barrier)</strong>: There is no zero-stable $p$-LMM with order</p><ul><li>$> p+1$ for $p$ odd</li><li>$> p+2$ for $p$ even</li></ul><p data-raw='Let&rsquo;s turn now to the Absolute stability.'>Let&rsquo;s turn now to the Absolute stability.</p><ol start=3><li>The absolute root condition is necessary and sufficient for the absolute stability. In fact, if $\overline{\Delta t} = +\infty$, the absolute stability is unconditional.</li></ol><p data-raw='The analysis of a LMM is completed by analyzing the coefficients of the method and the roots of the two polynomials $\rho$ and $\Pi$.'>The analysis of a LMM is completed by analyzing the coefficients of the method and the roots of the two polynomials $\rho$ and $\Pi$.</p><p data-raw='The roots of $\Pi$ can be analyzed numerically (and are tabulated for most of the methods) by Matlab/Python subroutines.'>The roots of $\Pi$ can be analyzed numerically (and are tabulated for most of the methods) by Matlab/Python subroutines.</p><p data-raw='SEE EXAMPLES
(NODEPY library in Python, QSS in Matlab)'>SEE EXAMPLES
(NODEPY library in Python, QSS in Matlab)</p><p data-raw='<strong>Remark (Second Dahlquist Barrier)</strong>: There are no explicit LMM absolutely unconditionally stable. There are no LMM absolutely unconditionally stable with order $q > 2$.'><strong>Remark (Second Dahlquist Barrier)</strong>: There are no explicit LMM absolutely unconditionally stable. There are no LMM absolutely unconditionally stable with order $q > 2$.</p><h2 id=a-clarification-on-stability-concepts>A Clarification on Stability Concepts
<a class=anchor href=#a-clarification-on-stability-concepts>#</a></h2><p data-raw='To clarify the different stability concepts:'>To clarify the different stability concepts:</p><ol><li><p data-raw=Zero-stability:>Zero-stability:</p><p data-raw='$$|u_j| \leq C_{T_{fin}} (|u_0| + \ldots |u_p|)$$'>$$|u_j| \leq C_{T_{fin}} (|u_0| + \ldots |u_p|)$$</p><p data-raw='where the $C$ may depend on $T_{fin}$'>where the $C$ may depend on $T_{fin}$</p></li><li><p data-raw='Absolute stability:'>Absolute stability:</p><p data-raw='$$C_{T_{fin}} \xrightarrow{T_{fin} \to \infty} 0$$'>$$C_{T_{fin}} \xrightarrow{T_{fin} \to \infty} 0$$</p></li><li><p data-raw='$C$ is bounded independently of $T_{fin}$'>$C$ is bounded independently of $T_{fin}$</p><p data-raw='We call this &ldquo;relative stability&rdquo;'>We call this &ldquo;relative stability&rdquo;</p></li></ol><p data-raw='$$\text{for a consistent scheme} \quad \text{R.C.} \Leftarrow \text{Strong R.C.} \Leftarrow\text{A.R.C.}$$
$$\text{CONVERGENCE} \Leftarrow \text{Zero-Stability} \Leftarrow \text{Relative Stability} \Leftarrow \text{Absolute Stability}$$'>$$\text{for a consistent scheme} \quad \text{R.C.} \Leftarrow \text{Strong R.C.} \Leftarrow\text{A.R.C.}$$
$$\text{CONVERGENCE} \Leftarrow \text{Zero-Stability} \Leftarrow \text{Relative Stability} \Leftarrow \text{Absolute Stability}$$</p><h2 id=example-extreme>Example (Extreme)
<a class=anchor href=#example-extreme>#</a></h2><p data-raw=Mid-point:>Mid-point:</p><p data-raw='$$u_{n+1} = u_{n-1} + 2\Delta t , f(t_n, u_n)$$'>$$u_{n+1} = u_{n-1} + 2\Delta t , f(t_n, u_n)$$</p><p data-raw='$$a_0 = 0 \quad a_1 = 1 \quad \Rightarrow \rho(z) = z^2 - 1 = 0 \quad \Rightarrow \rho = \pm 1$$'>$$a_0 = 0 \quad a_1 = 1 \quad \Rightarrow \rho(z) = z^2 - 1 = 0 \quad \Rightarrow \rho = \pm 1$$</p><p data-raw='R.C.: OK'>R.C.: OK</p><p data-raw='$$\Pi_{\Delta t}(z) = z^2 - 2\lambda z - 1$$'>$$\Pi_{\Delta t}(z) = z^2 - 2\lambda z - 1$$</p><p data-raw='The product of the two roots is always -1, if one root is < 1 in magnitude, the other is > 1.'>The product of the two roots is always -1, if one root is &lt; 1 in magnitude, the other is > 1.</p><p data-raw='Unconditionally Absolutely UNSTABLE'>Unconditionally Absolutely UNSTABLE</p><h2 id=predictor-corrector-methods>Predictor-Corrector Methods
<a class=anchor href=#predictor-corrector-methods>#</a></h2><p data-raw='A clear message from the previous examples is that, in general, explicit methods are less absolutely stable, they are never unconditionally stable and have smaller regions of stability.'>A clear message from the previous examples is that, in general, explicit methods are less absolutely stable, they are never unconditionally stable and have smaller regions of stability.</p><p data-raw='However, let&rsquo;s reconsider implicit methods too.'>However, let&rsquo;s reconsider implicit methods too.</p><p data-raw='For instance, let&rsquo;s consider a generic LMM:'>For instance, let&rsquo;s consider a generic LMM:</p><p data-raw='$$u_{n+1} - \sum_{j=0}^p a_j u_{n-j} = b_{-1} \Delta t , f(t_{n+1}, u_{n+1}) + \Delta t \sum_{j=0}^p b_j f(t_{n-j}, u_{n-j})$$'>$$u_{n+1} - \sum_{j=0}^p a_j u_{n-j} = b_{-1} \Delta t , f(t_{n+1}, u_{n+1}) + \Delta t \sum_{j=0}^p b_j f(t_{n-j}, u_{n-j})$$</p><p data-raw='To solve this nonlinear equation, we could use a Newton method but also a fixed-point method and a natural candidate is the following:'>To solve this nonlinear equation, we could use a Newton method but also a fixed-point method and a natural candidate is the following:</p><p data-raw='$$u_{n+1}^{(m)} = \sum_{j=0}^p a_j u_{n-j} + \Delta t \sum_{j=0}^p b_j f_{n-j} + \Delta t b_{-1} f(t_{n+1}, u_{n+1}^{(m-1)})$$'>$$u_{n+1}^{(m)} = \sum_{j=0}^p a_j u_{n-j} + \Delta t \sum_{j=0}^p b_j f_{n-j} + \Delta t b_{-1} f(t_{n+1}, u_{n+1}^{(m-1)})$$</p><p data-raw='Notice that this method converges if:'>Notice that this method converges if:</p><p data-raw='$$\left|\Delta t , b_{-1} , f&rsquo;(t_{n+1}, u_{n+1}) \right| < 1$$'>$$\left|\Delta t , b_{-1} , f&rsquo;(t_{n+1}, u_{n+1}) \right| &lt; 1$$</p><p data-raw='So we have the condition:'>So we have the condition:</p><p data-raw='$$\Delta t < \frac{1}{|b_{-1}||f&rsquo;|}$$'>$$\Delta t &lt; \frac{1}{|b_{-1}||f&rsquo;|}$$</p><p data-raw='Even if the method is unconditionally stable, we may have a condition on $\Delta t$ for the convergence of the fixed-point.'>Even if the method is unconditionally stable, we may have a condition on $\Delta t$ for the convergence of the fixed-point.</p><p data-raw='In any case, a good initial condition is required, because with a good $u^{(0)}$ the number of iterations can be reduced: an explicit method can be used to this.'>In any case, a good initial condition is required, because with a good $u^{(0)}$ the number of iterations can be reduced: an explicit method can be used to this.</p><p data-raw='These considerations lead to the design of a new class of methods, Heun being one of the possible examples.'>These considerations lead to the design of a new class of methods, Heun being one of the possible examples.</p><p data-raw='Predictor [P] is an explicit method of order $q_P$'>Predictor [P] is an explicit method of order $q_P$</p><p data-raw='Corrector [C] is an implicit method of order $q_C$'>Corrector [C] is an implicit method of order $q_C$</p><p data-raw='Predictor Corrector method:'>Predictor Corrector method:</p><p data-raw='P: do one step of P $\Rightarrow u_{n+1}^{(0)}$'>P: do one step of P $\Rightarrow u_{n+1}^{(0)}$</p><p data-raw='E: evaluate $f(t_{n+1}, u_{n+1}^{(0)})$'>E: evaluate $f(t_{n+1}, u_{n+1}^{(0)})$</p><p data-raw='C: compute $m$ fixed-point iterations of C'>C: compute $m$ fixed-point iterations of C</p><p data-raw='Optional: E: compute $f_{n+1} = f(t_{n+1}, u_{n+1}^{(m)})$'>Optional: E: compute $f_{n+1} = f(t_{n+1}, u_{n+1}^{(m)})$</p><p data-raw='These methods go under the name of $\text{PEC}^m$ or $\text{PEC}^m\text{E}$ if the last step is taken.'>These methods go under the name of $\text{PEC}^m$ or $\text{PEC}^m\text{E}$ if the last step is taken.</p><h2 id=examples>Examples:
<a class=anchor href=#examples>#</a></h2><p data-raw='P = Adams-Bashforth of order 2
C = Adams-Moulton of order 3
$m = 1$'>P = Adams-Bashforth of order 2
C = Adams-Moulton of order 3
$m = 1$</p><p data-raw='PEC:
$$\begin{cases}
u_{n+1}^{(0)} = u_n^{(1)} + \frac{\Delta t}{2}(3f_n^{(0)} - f_{n-1}^{(0)}) \
f_{n+1}^{(0)} = f(t_{n+1}, u_{n+1}^{(0)}) \
u_{n+1}^{(1)} = u_n^{(1)} + \frac{\Delta t}{12}(5f_{n+1}^{(0)} + 8f_n^{(0)} - f_{n-1}^{(0)})
\end{cases}$$'>PEC:
$$\begin{cases}
u_{n+1}^{(0)} = u_n^{(1)} + \frac{\Delta t}{2}(3f_n^{(0)} - f_{n-1}^{(0)}) \
f_{n+1}^{(0)} = f(t_{n+1}, u_{n+1}^{(0)}) \
u_{n+1}^{(1)} = u_n^{(1)} + \frac{\Delta t}{12}(5f_{n+1}^{(0)} + 8f_n^{(0)} - f_{n-1}^{(0)})
\end{cases}$$</p><p data-raw='PECE:
$$\begin{cases}
u_{n+1}^{(0)} = u_n^{(1)} + \frac{\Delta t}{2}(3f_n^{(1)} - f_{n-1}^{(1)}) \
f_{n+1}^{(0)} = f(t_{n+1}, u_{n+1}^{(0)}) \
u_{n+1}^{(1)} = u_n^{(1)} + \frac{\Delta t}{12}(5f_{n+1}^{(0)} + 8f_n^{(1)} - f_{n-1}^{(1)}) \
f_{n+1}^{(1)} = f(t_{n+1}, u_{n+1}^{(1)})
\end{cases}$$'>PECE:
$$\begin{cases}
u_{n+1}^{(0)} = u_n^{(1)} + \frac{\Delta t}{2}(3f_n^{(1)} - f_{n-1}^{(1)}) \
f_{n+1}^{(0)} = f(t_{n+1}, u_{n+1}^{(0)}) \
u_{n+1}^{(1)} = u_n^{(1)} + \frac{\Delta t}{12}(5f_{n+1}^{(0)} + 8f_n^{(1)} - f_{n-1}^{(1)}) \
f_{n+1}^{(1)} = f(t_{n+1}, u_{n+1}^{(1)})
\end{cases}$$</p><p data-raw='A natural question is: what is the accuracy of $\text{PEC}^m$ or $\text{PECE}^m$? What is the region of absolute stability?'>A natural question is: what is the accuracy of $\text{PEC}^m$ or $\text{PECE}^m$? What is the region of absolute stability?</p><p data-raw='<strong>Theorem (Accuracy)</strong>: $\text{PECE}$ methods have (under regularity assumptions on the solution) order of accuracy:'><strong>Theorem (Accuracy)</strong>: $\text{PECE}$ methods have (under regularity assumptions on the solution) order of accuracy:</p><p data-raw='$$q_{PC} = min(q_P + m, q_C)$$'>$$q_{PC} = min(q_P + m, q_C)$$</p><p data-raw='For the region of absolute stability, in general:'>For the region of absolute stability, in general:</p><p data-raw='$$\text{Region}(P) \subseteq \text{Region}(\text{PECE}) \subseteq \text{Region}(C)$$'>$$\text{Region}(P) \subseteq \text{Region}(\text{PECE}) \subseteq \text{Region}(C)$$</p><p data-raw='and $\text{Region}(\text{PECE}) \xrightarrow{m \to +\infty} \text{Region}(C)$'>and $\text{Region}(\text{PECE}) \xrightarrow{m \to +\infty} \text{Region}(C)$</p><h2 id=runge-kutta-methods>Runge-Kutta Methods
<a class=anchor href=#runge-kutta-methods>#</a></h2><p data-raw='Heun is also an RK method:'>Heun is also an RK method:</p><p data-raw='$$u_{n+1} = u_n + \Delta t\left(f_n + f(t_{n+1}, u_n + \Delta t f_n)\right)$$'>$$u_{n+1} = u_n + \Delta t\left(f_n + f(t_{n+1}, u_n + \Delta t f_n)\right)$$</p><p data-raw='This is a 2nd order method but the accuracy is not obtained using more steps, but giving up the linearity of the method.'>This is a 2nd order method but the accuracy is not obtained using more steps, but giving up the linearity of the method.</p><p data-raw='In general, RK are in the form:'>In general, RK are in the form:</p><p data-raw='$$u_{n+1} = u_n + \Delta t , \mathbf{K} \cdot \mathbf{b}$$'>$$u_{n+1} = u_n + \Delta t , \mathbf{K} \cdot \mathbf{b}$$</p><p data-raw='where $\mathbf{K}, \mathbf{b} \in \mathbb{R}^s$'>where $\mathbf{K}, \mathbf{b} \in \mathbb{R}^s$</p><p data-raw='and $[\mathbf{K}]_i = f(t_n + c_i \Delta t, u_n + \Delta t[A\mathbf{K}]_i)$'>and $[\mathbf{K}]_i = f(t_n + c_i \Delta t, u_n + \Delta t[A\mathbf{K}]_i)$</p><p data-raw='where $[\mathbf{A}\mathbf{K}]_i$ is the $i^{th}$ entry of the vector $\mathbf{A}\mathbf{K}$'>where $[\mathbf{A}\mathbf{K}]_i$ is the $i^{th}$ entry of the vector $\mathbf{A}\mathbf{K}$</p><p data-raw='The method is characterized by:
$s$ (stages), $\mathbf{b}$, $\mathbf{c}$ and $\mathbf{A} \in \mathbb{R}^{s \times s}$'>The method is characterized by:
$s$ (stages), $\mathbf{b}$, $\mathbf{c}$ and $\mathbf{A} \in \mathbb{R}^{s \times s}$</p><p data-raw='In particular, the three &ldquo;ingredients&rdquo; are generally written as:'>In particular, the three &ldquo;ingredients&rdquo; are generally written as:</p><p data-raw='$$\frac{\mathbf{c} | \mathbf{A}}{\mathbf{b}^T} \quad \text{(Butcher array)}$$'>$$\frac{\mathbf{c} | \mathbf{A}}{\mathbf{b}^T} \quad \text{(Butcher array)}$$</p><p data-raw='It is assumed that $c_i = \sum_{j=1}^s a_{ij} \quad \forall i = 1, \ldots s$'>It is assumed that $c_i = \sum_{j=1}^s a_{ij} \quad \forall i = 1, \ldots s$</p><p data-raw='If $\mathbf{A}$ is such that $a_{ij} = 0 \quad \forall j \geq i$ then:'>If $\mathbf{A}$ is such that $a_{ij} = 0 \quad \forall j \geq i$ then:</p><p data-raw='$$K_i = [\mathbf{K}]<em>i = f(t_n + \Delta t c_i, u_n + \Delta t \sum</em>{j < i} a_{ij} K_j)$$'>$$K_i = [\mathbf{K}]<em>i = f(t_n + \Delta t c_i, u_n + \Delta t \sum</em>{j &lt; i} a_{ij} K_j)$$</p><p data-raw='so the computation of $K_i$ is immediate. We call this case an explicit RK scheme.'>so the computation of $K_i$ is immediate. We call this case an explicit RK scheme.</p><p data-raw='If $a_{ij} = 0 \quad \forall j > i$, then:'>If $a_{ij} = 0 \quad \forall j > i$, then:</p><p data-raw='$$K_i = [\mathbf{K}]<em>i = f(t_n + \Delta t c_i, u_n + \Delta t \sum</em>{j < i} a_{ij} K_j + \Delta t a_{ii} K_i)$$'>$$K_i = [\mathbf{K}]<em>i = f(t_n + \Delta t c_i, u_n + \Delta t \sum</em>{j &lt; i} a_{ij} K_j + \Delta t a_{ii} K_i)$$</p><p data-raw='so we need to solve a non-linear equation to obtain $K_i$. We call this a semi-implicit method.'>so we need to solve a non-linear equation to obtain $K_i$. We call this a semi-implicit method.</p><p data-raw='In general, computing $\mathbf{K}$ requires the solution of a non-linear system (implicit method).'>In general, computing $\mathbf{K}$ requires the solution of a non-linear system (implicit method).</p><p data-raw='Clearly, the computational cost increases in the three cases.'>Clearly, the computational cost increases in the three cases.</p><h2 id=derivation-of-an-explicit-rk-method>Derivation of an Explicit RK Method
<a class=anchor href=#derivation-of-an-explicit-rk-method>#</a></h2><p data-raw='A possible strategy to devise an explicit method is to maximize the order of the LTE with Taylor expansion analysis.'>A possible strategy to devise an explicit method is to maximize the order of the LTE with Taylor expansion analysis.</p><p data-raw='For instance, for $s = 2$:'>For instance, for $s = 2$:</p><p data-raw='$$\begin{pmatrix} 0 & 0 & 0 \ c_2 & a_{21} & 0 \ \hline & b_1 & b_2 \end{pmatrix}$$'>$$\begin{pmatrix} 0 & 0 & 0 \ c_2 & a_{21} & 0 \ \hline & b_1 & b_2 \end{pmatrix}$$</p><p data-raw='We have three parameters, but we set $a_{21} = c_2$.'>We have three parameters, but we set $a_{21} = c_2$.</p><p data-raw='$$u_{n+1} = u_n + \Delta t(b_1 f(t_n, u_n) + b_2 f(t_n + c_2 \Delta t, u_n + \Delta t c_2 f(t_n, u_n)))$$'>$$u_{n+1} = u_n + \Delta t(b_1 f(t_n, u_n) + b_2 f(t_n + c_2 \Delta t, u_n + \Delta t c_2 f(t_n, u_n)))$$</p><p data-raw='$$y_{ex}(t_n) = y_{ex}(t_n) + \Delta t \frac{dy_{ex}}{dt}(t_n) + \frac{\Delta t^2}{2}\frac{d^2y_{ex}}{dt^2}(t_n) + \frac{\Delta t^3}{3!}\frac{d^3y_{ex}}{dt^3}(t_n) + H.O.T.$$'>$$y_{ex}(t_n) = y_{ex}(t_n) + \Delta t \frac{dy_{ex}}{dt}(t_n) + \frac{\Delta t^2}{2}\frac{d^2y_{ex}}{dt^2}(t_n) + \frac{\Delta t^3}{3!}\frac{d^3y_{ex}}{dt^3}(t_n) + H.O.T.$$</p><p data-raw='$$f(t_n + c_2 \Delta t, u_n + \Delta t c_2 f(t_n, u_n)) =$$'>$$f(t_n + c_2 \Delta t, u_n + \Delta t c_2 f(t_n, u_n)) =$$</p><p data-raw='$$= f(t_n, u_n) + \frac{\partial f}{\partial t}(t_n, y_n)c_2 \Delta t + \frac{\partial f}{\partial y}(t_n, y_n)c_2 \Delta t f(t_n, u_n) =$$'>$$= f(t_n, u_n) + \frac{\partial f}{\partial t}(t_n, y_n)c_2 \Delta t + \frac{\partial f}{\partial y}(t_n, y_n)c_2 \Delta t f(t_n, u_n) =$$</p><p data-raw='Notice that:
$$\frac{d^2y}{dt^2} = \frac{df}{dt} = \frac{\partial f}{\partial t} + \frac{\partial f}{dy}\frac{dy}{dt} = \frac{\partial f}{\partial t} + \frac{\partial f}{\partial y}f$$'>Notice that:
$$\frac{d^2y}{dt^2} = \frac{df}{dt} = \frac{\partial f}{\partial t} + \frac{\partial f}{dy}\frac{dy}{dt} = \frac{\partial f}{\partial t} + \frac{\partial f}{\partial y}f$$</p><p data-raw='$$\Rightarrow f(t_n, u_n) + c_2 \Delta t \frac{d^2y}{dt^2}(t_n)$$'>$$\Rightarrow f(t_n, u_n) + c_2 \Delta t \frac{d^2y}{dt^2}(t_n)$$</p><p data-raw='The Taylor expansion applied to the scheme reads:'>The Taylor expansion applied to the scheme reads:</p><p data-raw='$$u_{n+1} = u_n + \Delta t(b_1 + b_2)f + c_2 b_2 \Delta t^2 \frac{d^2y}{dt^2}$$'>$$u_{n+1} = u_n + \Delta t(b_1 + b_2)f + c_2 b_2 \Delta t^2 \frac{d^2y}{dt^2}$$</p><p data-raw='We match therefore the first terms of the Taylor expansion for:'>We match therefore the first terms of the Taylor expansion for:</p><p data-raw='$$\begin{align}
b_1 + b_2 &= 1 \
b_2 c_2 &= \frac{1}{2}
\end{align}$$'>$$\begin{align}
b_1 + b_2 &= 1 \
b_2 c_2 &= \frac{1}{2}
\end{align}$$</p><p data-raw='For $b_2 = \frac{1}{2}$ we obtain the Heun method.'>For $b_2 = \frac{1}{2}$ we obtain the Heun method.</p><p data-raw='The L.T.E is $\Delta t , \tau \sim O(\Delta t^3)$'>The L.T.E is $\Delta t , \tau \sim O(\Delta t^3)$</p><p data-raw='so the method is 2nd order.'>so the method is 2nd order.</p><p data-raw='Implicit methods can be devised from Gaussian quadratures.'>Implicit methods can be devised from Gaussian quadratures.</p><h2 id=analysis-of-rk>Analysis of RK
<a class=anchor href=#analysis-of-rk>#</a></h2><p data-raw='<strong>CONSISTENCY</strong>: As the previous example shows, we need $\sum b_i = 1$. This is necessary and sufficient for the consistency.'><strong>CONSISTENCY</strong>: As the previous example shows, we need $\sum b_i = 1$. This is necessary and sufficient for the consistency.</p><p data-raw='<strong>ZERO-STABILITY</strong>: RK are One-Step methods, if $f$ is Lipschitz-Continuous, they are zero-stable.'><strong>ZERO-STABILITY</strong>: RK are One-Step methods, if $f$ is Lipschitz-Continuous, they are zero-stable.</p><p data-raw='<strong>ORDER</strong>: In general, the order we can obtain depends on the number of stages in a (non-trivial) way:'><strong>ORDER</strong>: In general, the order we can obtain depends on the number of stages in a (non-trivial) way:</p><table><thead><tr><th>Explicit RK:</th><th>ORDER</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th></tr></thead><tbody><tr><td>$s_{min}$</td><td></td><td>1</td><td>2</td><td>3</td><td>4</td><td>6</td><td>7</td><td>9</td><td>11</td></tr></tbody></table><p data-raw='$s_{min}$ = minimum number of stages to obtain the corresponding order.'>$s_{min}$ = minimum number of stages to obtain the corresponding order.</p><h2 id=absolute-stability>ABSOLUTE STABILITY
<a class=anchor href=#absolute-stability>#</a></h2><p data-raw='If we write the method for the model problem, we can write:'>If we write the method for the model problem, we can write:</p><p data-raw='$$u_{n+1} = \mathcal{R}(\Delta t \lambda) u_n$$'>$$u_{n+1} = \mathcal{R}(\Delta t \lambda) u_n$$</p><p data-raw='The region of absolute stability is, in general, the non-trivial region where $|\mathcal{R}(\Delta t \lambda)| < 1$ (in $\mathbb{C}$).'>The region of absolute stability is, in general, the non-trivial region where $|\mathcal{R}(\Delta t \lambda)| &lt; 1$ (in $\mathbb{C}$).</p><h2 id=why-are-rk-so-popular>Why are RK so popular?
<a class=anchor href=#why-are-rk-so-popular>#</a></h2><p data-raw='RK are extremely popular, because they can be high order with &ldquo;only&rdquo; one-step.'>RK are extremely popular, because they can be high order with &ldquo;only&rdquo; one-step.</p><p data-raw='One-step means that:'>One-step means that:</p><ul><li>we do not need high-order approximation of the initial data needed by LMM (the initial condition is enough)</li><li>we can easily perform the time-step ADAPTIVITY (much more difficult with LMM).</li></ul><p data-raw='For the adaptivity, there are smart combinations of RK that obtain the adaptivity (i.e. an error estimate to adapt the step) in an efficient way.'>For the adaptivity, there are smart combinations of RK that obtain the adaptivity (i.e. an error estimate to adapt the step) in an efficient way.</p><p data-raw='One of the most popular is:
RK45 Fehlberg: smart combination of an explicit method of order 4 and an explicit of order 5 to adapt the step.'>One of the most popular is:
RK45 Fehlberg: smart combination of an explicit method of order 4 and an explicit of order 5 to adapt the step.</p><h2 id=a-final-note-on-stiff-problems>A Final Note on Stiff problems
<a class=anchor href=#a-final-note-on-stiff-problems>#</a></h2><p data-raw='Many of the concepts used here can be extended to systems of ODEs:'>Many of the concepts used here can be extended to systems of ODEs:</p><p data-raw='$$\begin{cases}
\frac{d\mathbf{y}}{dt} = \mathbf{F}(t, \mathbf{y}) \
\mathbf{y}(0) = \mathbf{y}_0
\end{cases}$$'>$$\begin{cases}
\frac{d\mathbf{y}}{dt} = \mathbf{F}(t, \mathbf{y}) \
\mathbf{y}(0) = \mathbf{y}_0
\end{cases}$$</p><p data-raw='In the case of a linear ODE system:'>In the case of a linear ODE system:</p><p data-raw='$$\mathbf{F} = A , \mathbf{y} \quad \downarrow \quad \text{matrix}$$'>$$\mathbf{F} = A , \mathbf{y} \quad \downarrow \quad \text{matrix}$$</p><p data-raw='There is, however, an important concept to clarify.'>There is, however, an important concept to clarify.</p><p data-raw='Consider a simple 2×2 problem:'>Consider a simple 2×2 problem:</p><p data-raw='$$\frac{d\mathbf{y}}{dt} = A\mathbf{y}$$'>$$\frac{d\mathbf{y}}{dt} = A\mathbf{y}$$</p><p data-raw='where $A$ has the two eigenvalues: $\begin{cases} \lambda_1 = -10^6 \ \lambda_2 = -1 \end{cases}$'>where $A$ has the two eigenvalues: $\begin{cases} \lambda_1 = -10^6 \ \lambda_2 = -1 \end{cases}$</p><p data-raw='If we use Explicit Euler:'>If we use Explicit Euler:</p><p data-raw='$$\mathbf{y}^{n+1} = \mathbf{y}^n + \Delta t , A \mathbf{y}^n$$'>$$\mathbf{y}^{n+1} = \mathbf{y}^n + \Delta t , A \mathbf{y}^n$$</p><p data-raw='the region of absolute stability is:'>the region of absolute stability is:</p><p data-raw='$$\Delta t \leq \min\left(\frac{2}{10^6}, \frac{2}{1}\right) = 2 \cdot 10^{-6}$$'>$$\Delta t \leq \min\left(\frac{2}{10^6}, \frac{2}{1}\right) = 2 \cdot 10^{-6}$$</p><p data-raw='The solution, on the other hand, is the linear combination of the two functions:'>The solution, on the other hand, is the linear combination of the two functions:</p><p data-raw='$$e^{-10^6 t}, e^{-t}$$'>$$e^{-10^6 t}, e^{-t}$$</p><p data-raw='[Graph showing rapid decay of $e^{-10^6 t}$ vs slower decay of $e^{-t}$]'>[Graph showing rapid decay of $e^{-10^6 t}$ vs slower decay of $e^{-t}$]</p><p data-raw='To capture the fast dynamics ($\lambda = 10^{-6}$), that fades away immediately, we need to take $\Delta t \sim 10^{-6}$!!!'>To capture the fast dynamics ($\lambda = 10^{-6}$), that fades away immediately, we need to take $\Delta t \sim 10^{-6}$!!!</p><p data-raw='An explicit method is certainly not a good choice here.'>An explicit method is certainly not a good choice here.</p><p data-raw='In general, we say that a problem is &ldquo;stiff&rdquo; when it may require very stringent time-step in a non-efficient way. The name &ldquo;stiff&rdquo; originates from the coupling of springs with different stiffness:'>In general, we say that a problem is &ldquo;stiff&rdquo; when it may require very stringent time-step in a non-efficient way. The name &ldquo;stiff&rdquo; originates from the coupling of springs with different stiffness:</p><p data-raw='[Simple diagram of a mass connected to two springs with different spring constants]'>[Simple diagram of a mass connected to two springs with different spring constants]</p><p data-raw='to study the dynamics of the two real balls, one can write an ODE system.'>to study the dynamics of the two real balls, one can write an ODE system.</p><p data-raw='If $K_1 &#171; K_2$ this is a stiff problem.'>If $K_1 &#171; K_2$ this is a stiff problem.</p><h2 id=exercise-on-lmm>EXERCISE on LMM
<a class=anchor href=#exercise-on-lmm>#</a></h2><p data-raw='Consider the following family of methods (LMM):'>Consider the following family of methods (LMM):</p><p data-raw='$$u_{n+1} = \alpha u_n + (1-\alpha)u_{n-1} + \Delta t , \gamma f_{n+1}$$'>$$u_{n+1} = \alpha u_n + (1-\alpha)u_{n-1} + \Delta t , \gamma f_{n+1}$$</p><ol><li>Investigate the convergence properties of the method as function of $\alpha$ and $\gamma$.</li></ol><p data-raw='<strong>Sol</strong>: For $\alpha \neq 1$, the method is 2-step.
For $\gamma \neq 0$, the method is implicit.'><strong>Sol</strong>: For $\alpha \neq 1$, the method is 2-step.
For $\gamma \neq 0$, the method is implicit.</p><p data-raw='It&rsquo;s a LMM with: $\mathbf{a} = \begin{bmatrix} \alpha \ 1-\alpha \ 0 \end{bmatrix} \begin{bmatrix} 0 \ 1 \ 0 \end{bmatrix}$, $\mathbf{b} = \begin{bmatrix} \gamma \ 0 \ 0 \end{bmatrix} \begin{bmatrix} -1 \ 0 \ 1 \end{bmatrix}$'>It&rsquo;s a LMM with: $\mathbf{a} = \begin{bmatrix} \alpha \ 1-\alpha \ 0 \end{bmatrix} \begin{bmatrix} 0 \ 1 \ 0 \end{bmatrix}$, $\mathbf{b} = \begin{bmatrix} \gamma \ 0 \ 0 \end{bmatrix} \begin{bmatrix} -1 \ 0 \ 1 \end{bmatrix}$</p><p data-raw='Consistency:
$\sum a_j = 1$: $\alpha + (1-\alpha) = 1$ ✓OK
$-\sum j a_j + \sum b_j = 1$: $0 \cdot \alpha - 1 \cdot (1-\alpha) + \gamma = 1$
$\Rightarrow \gamma = 2 - \alpha$'>Consistency:
$\sum a_j = 1$: $\alpha + (1-\alpha) = 1$ ✓OK
$-\sum j a_j + \sum b_j = 1$: $0 \cdot \alpha - 1 \cdot (1-\alpha) + \gamma = 1$
$\Rightarrow \gamma = 2 - \alpha$</p><p data-raw='The methods:
$u_{n+1} = \alpha u_n + (1-\alpha)u_{n-1} + \Delta t (2-\alpha)f_{n+1}$
are consistent.'>The methods:
$u_{n+1} = \alpha u_n + (1-\alpha)u_{n-1} + \Delta t (2-\alpha)f_{n+1}$
are consistent.</p><p data-raw='Order: $\sum (j)^2 a_j + 2\sum (-j)^1 b_j = 1$ ?
$\Rightarrow (1-\alpha) + 2(2-\alpha) = 1 \Rightarrow \alpha = \frac{4}{3}, \gamma = \frac{2}{3}$'>Order: $\sum (j)^2 a_j + 2\sum (-j)^1 b_j = 1$ ?
$\Rightarrow (1-\alpha) + 2(2-\alpha) = 1 \Rightarrow \alpha = \frac{4}{3}, \gamma = \frac{2}{3}$</p><ol start=2><li>Investigate the absolute stability of the method with order 2.</li></ol><p data-raw='The method:
$u_{n+1} = \frac{4}{3}u_n - \frac{1}{3}u_{n-1} + \Delta t \frac{2}{3}f_{n+1}$'>The method:
$u_{n+1} = \frac{4}{3}u_n - \frac{1}{3}u_{n-1} + \Delta t \frac{2}{3}f_{n+1}$</p><p data-raw='is of order 2 (it is, in fact, a BDF of order 2).'>is of order 2 (it is, in fact, a BDF of order 2).</p><p data-raw='$\rho(z) = z^2 - \frac{4}{3}z + \frac{1}{3} = 0 \quad r_{1,2} = \frac{\frac{4}{3} \pm \sqrt{\frac{16}{9} - \frac{4}{3}}}{2} = \frac{4 \pm 2}{6} = \frac{2 \pm 1}{3}$'>$\rho(z) = z^2 - \frac{4}{3}z + \frac{1}{3} = 0 \quad r_{1,2} = \frac{\frac{4}{3} \pm \sqrt{\frac{16}{9} - \frac{4}{3}}}{2} = \frac{4 \pm 2}{6} = \frac{2 \pm 1}{3}$</p><p data-raw='R.C. ✓'>R.C. ✓</p><p data-raw='$\Pi_{\Delta t}(z) = z^2 - \frac{4}{3}z + \frac{1}{3} - \Delta t \lambda \frac{2}{3}z^2 = 0$'>$\Pi_{\Delta t}(z) = z^2 - \frac{4}{3}z + \frac{1}{3} - \Delta t \lambda \frac{2}{3}z^2 = 0$</p><p data-raw='Let&rsquo;s consider $\lambda \in \mathbb{R}^-$:'>Let&rsquo;s consider $\lambda \in \mathbb{R}^-$:</p><p data-raw='$(3 - \Delta t \lambda 2)z^2 - 4z + 1 = 0$'>$(3 - \Delta t \lambda 2)z^2 - 4z + 1 = 0$</p><p data-raw='$z^2 - \frac{4z}{3 + 2\Delta t|\lambda|} + \frac{1}{3 + 2\Delta t|\lambda|} = 0$'>$z^2 - \frac{4z}{3 + 2\Delta t|\lambda|} + \frac{1}{3 + 2\Delta t|\lambda|} = 0$</p><p data-raw='$r_1 \cdot r_2 = \frac{1}{3 + 2\Delta t|\lambda|}$'>$r_1 \cdot r_2 = \frac{1}{3 + 2\Delta t|\lambda|}$</p><p data-raw='$r_1 = \frac{2 + \sqrt{1 - 2\Delta t|\lambda|}}{3 + 2\Delta t|\lambda|} \xrightarrow{\Delta t \to 0} 1$'>$r_1 = \frac{2 + \sqrt{1 - 2\Delta t|\lambda|}}{3 + 2\Delta t|\lambda|} \xrightarrow{\Delta t \to 0} 1$</p><p data-raw='$|r_1(\Delta t)| < 1 \quad \forall \Delta t > 0$'>$|r_1(\Delta t)| &lt; 1 \quad \forall \Delta t > 0$</p><p data-raw='$r_2 = \frac{2 - \sqrt{1 - 2\Delta t|\lambda|}}{3 + 2\Delta t|\lambda|} \xrightarrow{\Delta t \to 0} \frac{1}{3}$'>$r_2 = \frac{2 - \sqrt{1 - 2\Delta t|\lambda|}}{3 + 2\Delta t|\lambda|} \xrightarrow{\Delta t \to 0} \frac{1}{3}$</p><p data-raw='$|r_2(\Delta t)| < \frac{1}{3}$'>$|r_2(\Delta t)| &lt; \frac{1}{3}$</p><p data-raw='Method unconditionally stable (Verify with Python/Matlab).'>Method unconditionally stable (Verify with Python/Matlab).</p><ol start=3><li>Solve
$$\begin{cases}
\frac{dy}{dt} = -(1 + t_g(t))y & t \in [0, 1] \
y(0) = 1
\end{cases}$$</li></ol><p data-raw='with the BDF2 method found and verify your results, knowing that the exact solution is $y_{ex} = e^{-x}\cos(x)$.'>with the BDF2 method found and verify your results, knowing that the exact solution is $y_{ex} = e^{-x}\cos(x)$.</p><p data-raw='Using MATLAB, the problem is easily solved with the QSS subroutines:'>Using MATLAB, the problem is easily solved with the QSS subroutines:</p><p data-raw='qssstab.m    (draws the region of absolute stability)
qssmulti.m   (solves with a generic LMM)'>qssstab.m (draws the region of absolute stability)
qssmulti.m (solves with a generic LMM)</p><p data-raw='With Python there are many libraries: <code>SciPy</code>, <code>odeint</code> that uses RK, ode, and allows the selection of BDF methods, but generally with adaptive time step.'>With Python there are many libraries: <code>SciPy</code>, <code>odeint</code> that uses RK, ode, and allows the selection of BDF methods, but generally with adaptive time step.</p><p data-raw='<code>NODEPY</code>  is potentially an excellent library but buggy.'><code>NODEPY</code> is potentially an excellent library but buggy.</p><p data-raw='It&rsquo;s excellent for drawing the stability region (see hmm.py on Canvas)'>It&rsquo;s excellent for drawing the stability region (see hmm.py on Canvas)</p><p data-raw='I have written a simple LMM solver with fixed-point iterations for implicit methods.'>I have written a simple LMM solver with fixed-point iterations for implicit methods.</p><p data-raw='Using my-hmm.py you can verify that our method is 2nd order:'>Using my-hmm.py you can verify that our method is 2nd order:</p><table><thead><tr><th>max error</th><th>$3 \cdot 10^{-4}$</th><th>$8 \cdot 10^{-5}$</th><th>$2 \cdot 10^{-5}$</th></tr></thead><tbody><tr><td>$\Delta t$</td><td>0.05</td><td>0.025</td><td>0.0125</td></tr></tbody></table><p data-raw='[Graph showing exact vs numerical solution]'>[Graph showing exact vs numerical solution]</p><p data-raw='[Stability region diagram showing a circle in the complex plane]
Red = Region of Stability'>[Stability region diagram showing a circle in the complex plane]
Red = Region of Stability</p></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/EriseHe/notebook/edit/main//content.en/docs/%e6%95%b0%e5%ad%a6/MATH%20351%20&amp;amp;%20352/MATH%20352%20PDE%20in%20Action/03%20Numerical%20Solution%20of%20Ordinary%20Differential%20Equations.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside id=book-toc class="book-toc hidden"><div class=book-toc-content><div class=button-prev></div><div class=toc-entries><nav id=TableOfContents><ul><li><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#local-theorem>Local Theorem</a></li><li><a href=#global-theorem>Global Theorem</a></li></ul></li><li><a href=#stability-definitions>Stability Definitions</a></li><li><a href=#remark>Remark</a></li><li><a href=#some-simple-examples>Some Simple Examples</a></li><li><a href=#analysis-of-forward-euler>Analysis of Forward Euler</a></li><li><a href=#convergence-of-forward-euler>Convergence of Forward Euler</a></li></ul></li><li><a href=#other-one-step-methods>Other One-Step Methods</a><ul><li><a href=#crank-nicolson>Crank-Nicolson</a></li><li><a href=#heun>Heun</a></li><li><a href=#the-concept-of-zero-stability>The Concept of Zero-Stability</a></li><li><a href=#the-concept-of-absolute-stability>The Concept of Absolute Stability</a><ul><li><a href=#remark-1>Remark</a></li></ul></li><li><a href=#definition>DEFINITION</a></li><li><a href=#another-example-heun>Another Example: Heun</a></li><li><a href=#remark-we-found-that-the-explicit-methods-are-conditionally-stable-while-the-implicit-ones-are-unconditionally-stable>REMARK: We found that the explicit methods are conditionally stable, while the implicit ones are unconditionally stable.</a></li></ul></li><li><a href=#multistep-methods>Multistep Methods</a><ul><li><a href=#example>Example:</a></li><li><a href=#bdf-backward-difference-formulas>BDF (Backward Difference Formulas)</a></li><li><a href=#adams>Adams</a></li><li><a href=#a-rapid-recall-of-difference-equations-theory>A Rapid Recall of Difference Equations Theory</a></li><li><a href=#zero-stability-definition>Zero-Stability Definition</a></li><li><a href=#analysis-of-multistep-methods>Analysis of Multistep Methods</a></li><li><a href=#a-clarification-on-stability-concepts>A Clarification on Stability Concepts</a></li><li><a href=#example-extreme>Example (Extreme)</a></li><li><a href=#predictor-corrector-methods>Predictor-Corrector Methods</a></li><li><a href=#examples>Examples:</a></li><li><a href=#runge-kutta-methods>Runge-Kutta Methods</a></li><li><a href=#derivation-of-an-explicit-rk-method>Derivation of an Explicit RK Method</a></li><li><a href=#analysis-of-rk>Analysis of RK</a></li><li><a href=#absolute-stability>ABSOLUTE STABILITY</a></li><li><a href=#why-are-rk-so-popular>Why are RK so popular?</a></li><li><a href=#a-final-note-on-stiff-problems>A Final Note on Stiff problems</a></li><li><a href=#exercise-on-lmm>EXERCISE on LMM</a></li></ul></li></ul></nav></div><div class=button-next><a class=btn-next href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/Exercise/><strong>Next:</strong> 习题</a></div></div></aside></div><div class=toolbar-trigger></div><aside class=toolbar><button id=theme-toggle class=theme-toggle-btn>
<img src=/blackhole2.png alt="Switch Theme">
</button>
<button id=toc-toggle class=toc-toggle-btn>
<img src=/toc-icon.svg alt="Toggle TOC"></button><div class=font-size-control><button id=font-toggle class=font-size-btn>
A</button><div class=font-size-buttons><button id=font-increase class=font-control-btn>+</button>
<button id=font-reset class=font-control-btn>A</button>
<button id=font-decrease class=font-control-btn>−</button></div></div></aside><script>document.addEventListener("DOMContentLoaded",function(){var e=document.getElementById("book-toc"),t=!0;t?e.classList.remove("hidden"):e.classList.add("hidden"),document.getElementById("toc-toggle").addEventListener("click",function(){e.classList.toggle("hidden")})})</script><script src=/font-size.min.js></script><script src=/toolbar.min.js></script><script src=/toc-animation.min.js></script><script src=/menu-autohide.min.js></script></main><script>document.addEventListener("DOMContentLoaded",()=>{const e=document.getElementById("theme-toggle");if(!e)return;e.onclick=()=>{const s=e.getBoundingClientRect(),o=s.left+s.width/2,i=s.top+s.height/2,l=document.documentElement.getAttribute("data-theme"),r=l||"auto";let n;r==="auto"?n=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":n=r;const a=n==="dark"?"light":"dark";document.documentElement.setAttribute("data-theme",a),localStorage.setItem("theme",a);const c=n==="dark"?"#0b031c":"white",t=document.createElement("div");t.style.position="fixed",t.style.top="0",t.style.left="0",t.style.width="100vw",t.style.height="100vh",t.style.zIndex="9999",t.style.pointerEvents="none",t.style.setProperty("--cx",`${o}px`),t.style.setProperty("--cy",`${i}px`),t.style.setProperty("--r","0px"),t.style.backgroundImage=`radial-gradient(circle at var(--cx) var(--cy), transparent 0%, transparent var(--r), ${c} calc(var(--r) + 1px))`,document.body.appendChild(t);const u=window.innerWidth,h=window.innerHeight,m=Math.hypot(Math.max(o,u-o),Math.max(i,h-i)),f=600,p=performance.now();function d(e){const o=e-p,s=Math.min(o/f,1),n=s*m;if(t.style.setProperty("--r",`${n}px`),t.style.backgroundImage=`radial-gradient(circle at var(--cx) var(--cy), transparent 0%, transparent ${n}px, ${c} ${n+1}px)`,s<1)requestAnimationFrame(d);else{t.remove();const e=localStorage.getItem("theme")||"light";document.documentElement.setAttribute("data-theme",e)}}requestAnimationFrame(d)}})</script><script>window.__tikzjax_injected||(window.__tikzjax_injected=!0,fetch("/content.en/docs/.obsidian/plugins/obsidian-tikzjax/main.js").then(e=>e.text()).then(e=>{const t=e.match(/var tikzjax_default = `([\s\S]*?)`;?/);if(t){const n=t[1],e=document.createElement("script");e.id="tikzjax",e.type="text/javascript",e.innerHTML=n,document.head.appendChild(e)}else{const e=document.createElement("script");e.src="https://tikzjax.com/v1/tikzjax.js",e.defer=!0,document.head.appendChild(e)}}).catch(()=>{const e=document.createElement("script");e.src="https://tikzjax.com/v1/tikzjax.js",e.defer=!0,document.head.appendChild(e)}))</script><script>window.processTheoremBlocks=function(){document.querySelectorAll("blockquote").forEach(function(e){const t=e.querySelector("p");if(!t)return;const n=t.getAttribute("data-raw")||t.innerHTML,s=n.replace(/<[^>]*>/g,"").trim();for(const o of["definition","proposition","lemma","theorem","assumption","claim"]){const a=new RegExp(`^\\s*\\[!${o}\\|(\\*|[\\w\\.\\-]+)\\]\\s*`,"i"),i=s.match(a);if(i){console.log(`Found ${o} with label: ${i[1]}`);const c=i[1]!=="*",m=c?i[1]:"",f=n.replace(/<[^>]*>/g,""),p=f.split(/\n/),g=p[0]||"",l=g,d=l.replace(a,"").trim();if(e.classList.contains("math-theorem"))break;const v=Array.from(e.childNodes);e.innerHTML="",e.classList.add("math-theorem",o);const s=document.createElement("div");s.className="theorem-header";const u=o.charAt(0).toUpperCase()+o.slice(1),h=document.createElement("span");if(h.textContent=c?`${u} ${m}`:u,s.appendChild(h),d){const e=document.createElement("span");e.className="theorem-subtitle",e.textContent=` (${d})`,s.appendChild(e)}e.appendChild(s);const r=document.createElement("div");r.className="theorem-content",e.appendChild(r),v.forEach(function(e){let s=e.cloneNode(!0);if(e===t&&s.nodeType===1){const t=n.replace(/<[^>]*>/g,""),e=t.split(/\n/);if(e.length>1){const n=e.slice(1),t=n.join(`
`).trim();if(t)s.innerHTML="",s.appendChild(document.createTextNode(t));else return}else{const e=l.replace(a,"").trim();if(e)s.innerHTML="",s.appendChild(document.createTextNode(e));else return}}r.appendChild(s)});break}}})},document.addEventListener("DOMContentLoaded",function(){try{window.processTheoremBlocks()}catch(e){console.error(e)}})</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".copy-button");e.forEach(e=>{const t=e.innerHTML;e.addEventListener("click",function(){const n=this.parentElement.querySelector("pre code");if(!n)return;navigator.clipboard.writeText(n.innerText).then(()=>{e.textContent="Copied",setTimeout(()=>{e.innerHTML=t},1500)}).catch(n=>{console.error("Failed to copy code:",n),e.textContent="Error",setTimeout(()=>{e.innerHTML=t},1500)})})})})</script><script defer src=/partial-load.min.6b14bff963746e8ce83087045335ec7a6533e1157905e46f88d952cb415b7418.js integrity="sha256-axS/+WN0bozoMIcEUzXsemUz4RV5BeRviNlSy0FbdBg=" crossorigin=anonymous></script><script defer src=/menu-recursive-close.min.893276da918db977c98771d5746eab05ce5761bc6fa1ca400efe1db9be2ac1f3.js integrity="sha256-iTJ22pGNuXfJh3HVdG6rBc5XYbxvocpADv4dub4qwfM=" crossorigin=anonymous></script></body></html>