<!doctype html><html lang=en dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1314&amp;path=livereload" data-no-instant defer></script><script>(function(){const e=localStorage.getItem("theme");e&&document.documentElement.setAttribute("data-theme",e)})()</script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="arXiv:2504.15683v1 [cs.CL] 22 Apr 2025
Simon Jehnen Javier Villalba-Díez Joaquín Ordieres-Meré

  Abstract
  #

Recent advancements in information availability and computational capabilities have transformed the analysis of annual reports, integrating traditional financial metrics with insights from textual data. To extract valuable insights from this wealth of textual data, automated review processes, such as topic modeling, are crucial. This study examines the effectiveness of BERTopic, a state-of-the-art topic model relying on contextual embeddings, for analyzing Item 7 and Item 7A of 10-K filings from S&amp;P 500 companies (2016–2022). Moreover, we introduce FinTextSim, a finetuned sentence-transformer model optimized for clustering and semantic search in financial contexts. Compared to all-MiniLM-L6-v2, the most widely used sentence-transformer, FinTextSim increases intratopic similarity by 81% and reduces intertopic similarity by 100%, significantly enhancing organizational clarity. We assess BERTopic’s performance using embeddings from both FinTextSim and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and distinct economic topic clusters when paired with FinTextSim’s embeddings. Without FinTextSim, BERTopic struggles with misclassification and overlapping topics. Thus, FinTextSim is pivotal for advancing financial text analysis. FinTextSim’s enhanced contextual embeddings, tailored for the financial domain, elevate the quality of future research and financial information. This improved quality of financial information will enable stakeholders to gain a competitive advantage, streamlining resource allocation and decision-making processes. Moreover, the improved insights have the potential to leverage business valuation and stock price prediction models."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1314/docs/Clippings/LLM/FinTextSim-Enhancing-Financial-Text-Analysis-with-BERTopic/"><meta property="og:site_name" content="学习笔记"><meta property="og:title" content="FinTextSim: Enhancing Financial Text Analysis with BERTopic"><meta property="og:description" content="arXiv:2504.15683v1 [cs.CL] 22 Apr 2025
Simon Jehnen Javier Villalba-Díez Joaquín Ordieres-Meré
Abstract # Recent advancements in information availability and computational capabilities have transformed the analysis of annual reports, integrating traditional financial metrics with insights from textual data. To extract valuable insights from this wealth of textual data, automated review processes, such as topic modeling, are crucial. This study examines the effectiveness of BERTopic, a state-of-the-art topic model relying on contextual embeddings, for analyzing Item 7 and Item 7A of 10-K filings from S&amp;P 500 companies (2016–2022). Moreover, we introduce FinTextSim, a finetuned sentence-transformer model optimized for clustering and semantic search in financial contexts. Compared to all-MiniLM-L6-v2, the most widely used sentence-transformer, FinTextSim increases intratopic similarity by 81% and reduces intertopic similarity by 100%, significantly enhancing organizational clarity. We assess BERTopic’s performance using embeddings from both FinTextSim and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and distinct economic topic clusters when paired with FinTextSim’s embeddings. Without FinTextSim, BERTopic struggles with misclassification and overlapping topics. Thus, FinTextSim is pivotal for advancing financial text analysis. FinTextSim’s enhanced contextual embeddings, tailored for the financial domain, elevate the quality of future research and financial information. This improved quality of financial information will enable stakeholders to gain a competitive advantage, streamlining resource allocation and decision-making processes. Moreover, the improved insights have the potential to leverage business valuation and stock price prediction models."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:tag" content="Clippings"><meta property="article:tag" content="BERTopic"><meta property="article:tag" content="Clustering"><title>FinTextSim: Enhancing Financial Text Analysis with BERTopic | 学习笔记</title>
<link rel=icon href=/topo.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1314/docs/Clippings/LLM/FinTextSim-Enhancing-Financial-Text-Analysis-with-BERTopic/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/computer-modern-font@1.0.1/index.min.css><link rel=stylesheet href=/book.min.b3dea21ef7167d137d5c3d91df2df1ae008874baa457a6eb3fa1217af8da3509.css integrity="sha256-s96iHvcWfRN9XD2R3y3xrgCIdLqkV6brP6EhevjaNQk=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.3ae190a43814770b9417b53ab11c4184bcb5f814bf8e8ffe683dd8439b5c0822.js integrity="sha256-OuGQpDgUdwuUF7U6sRxBhLy1+BS/jo/+aD3YQ5tcCCI=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class=container><div class=book-layout><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center justify-center" href=http://localhost:1314/><span>学习笔记</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><nav class=subject-menu><div class=subject-tabs><button class="subject-tab active" data-subject=数学>数学</button>
<button class=subject-tab data-subject=物理>物理</button></div><div class=subject-content><div class=subject-panel data-subject=数学><ul><li class=book-section-flat><span>点集拓扑学基础</span><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/%E6%8B%93%E6%89%91%E5%AD%A6/T-n%E5%88%86%E7%A6%BB%E5%85%AC%E7%90%86/>T N分离公理</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-2626c0c7c40e5ba289bbf96779ed7be9 class=toggle>
<label for=section-2626c0c7c40e5ba289bbf96779ed7be9 class="flex justify-between"><a role=button>微分几何</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/1.-Overview-of-Differential-Geometry/>1. Overview of Differential Geometry</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/2.-Lecture-2-ACTUAL/>2. Lecture 2 ( Actual)</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/2.-Parametrized-Curves-and-Surfaces/>2. Parametrized Curves and Surfaces</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/Differential-Geometry/Tangent-Vectors-on-a-Surface/>Tangent Vectors on a Surface</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-6e5f2db787427bb8bccbb3c3eafa6cb6 class=toggle>
<label for=section-6e5f2db787427bb8bccbb3c3eafa6cb6 class="flex justify-between"><a role=button>概率机器学习</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/2.-Autoregressive%E6%A8%A1%E5%9E%8B/>2. Autoregressive模型</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/Probabilistic-Machine-Learning/3.-Autoregressive%E6%A8%A1%E5%9E%8B-2/>3. Autoregressive模型 2</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-e9a29b9c49921437d6f50841fa0b7a2c class=toggle>
<label for=section-e9a29b9c49921437d6f50841fa0b7a2c class="flex justify-between"><a role=button>实分析 II</a></label><ul><li><input type=checkbox id=section-e63b59f3907b6b5c0ca40a1419c50fbe class=toggle>
<label for=section-e63b59f3907b6b5c0ca40a1419c50fbe class="flex justify-between"><a role=button>第六章 可微映射</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.4-Sufficient-Conditions-for-Differentiability/>6.4 可微分性的必要条件</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.6-Product-Rule-and-Gradients/>6.6 乘积法则与梯度</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.9-Taylor-Higher-Dimentions/>6.9 泰勒公式的高维形式</a></li></ul></li><li><input type=checkbox id=section-516b62a4eda22b0c44d32f044080ba6a class=toggle>
<label for=section-516b62a4eda22b0c44d32f044080ba6a class="flex justify-between"><a role=button>第七章 逆函数和隐函数定理</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1-Inverse-Function-Theorem/>7.1 反函数定理</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.1.1-Inverse-FT-Proof/>7.1.1 反函数定理（证明）</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.2-Implicit-Function-Theorem/>7.2 隐函数定理</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B8%83%E7%AB%A0/7.6-Morse-Lemma/>7.6 莫尔斯引理</a></li></ul></li><li><input type=checkbox id=section-7be78291145a12f96f25aeffb3b16554 class=toggle>
<label for=section-7be78291145a12f96f25aeffb3b16554 class="flex justify-between"><a role=button>第八章 度量理论</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.1-%E5%BA%A6%E9%87%8F%E7%90%86%E8%AE%BA/>8.1 度量理论</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.2-Criterion-for-Integrability/>8.2 Criterion for Integrability</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.3-Proof-of-Lebesgues-Theorem/>8.3 Proof of Lebesgue's Theorem</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.5-Improper-Integral/>8.5 不定积分</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.6-Lebesgue-Convergence-Theorem/>8.6 勒贝格收敛定理</a></li></ul></li><li><input type=checkbox id=section-56e664b694094194eea41293621edc8e class=toggle>
<label for=section-56e664b694094194eea41293621edc8e class="flex justify-between"><a role=button>第九章</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1-Computing-Integrals/>积分计算</a></li></ul></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-412-Real-Analysis-II/%E7%AC%AC%E5%8D%81%E7%AB%A0/10.1-Fourier-Analysis/>10.1 Fourier Analysis</a></li></ul></li><li class=book-section-flat><input type=checkbox id=section-358a11d2cd962ab8fdb9c9ba7faf532a class=toggle>
<label for=section-358a11d2cd962ab8fdb9c9ba7faf532a class="flex justify-between"><a role=button>偏微分方程</a></label><ul><li><input type=checkbox id=section-7097c8722aa477fcc3c6ccc5730510d5 class=toggle>
<label for=section-7097c8722aa477fcc3c6ccc5730510d5 class="flex justify-between"><a role=button>数值方法</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/01-Mid-Point-Method/>01 Mid Point Method</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/02-Three-Point-Backward-Differentiation-Formula/>02 Three Point Backward Differentiation Formula</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/03-Numerical-Solution-of-Ordinary-Differential-Equations/>常微分方程的数值解</a></li><li><input type=checkbox id=section-039c2604ef8d15d0cdd62911e85f20cb class=toggle>
<label for=section-039c2604ef8d15d0cdd62911e85f20cb class="flex justify-between"><a role=button>第四章 有限元分析</a></label><ul></ul></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E5%8D%81%E7%AB%A0/Finite-Element-Method/>Finite Element Method</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E5%8D%81%E7%AB%A0/Quiz-4/>Quiz 4</a></li><li><input type=checkbox id=section-24cf0d34881526d98bfa6ff3655d07cb class=toggle>
<label for=section-24cf0d34881526d98bfa6ff3655d07cb class="flex justify-between"><a role=button>第九章</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1-Boundary-Value-Problems/>9.1 边值问题的近似</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.2-Finite-Difference/>9.2 有限差分法</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.3-Advection-Diffusion-Equation/>9.3 对流-扩散方程</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.4-2D-Problem/>9.4 4.1 二维（2D）偏微分方程问题</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/MATH-352-PDE-in-Action/%E7%AC%AC%E4%B9%9D%E7%AB%A0/Exercise/>习题</a></li></ul></li></ul></li><li><input type=checkbox id=section-2e9d41360137a1ae6194db40069654e3 class=toggle>
<label for=section-2e9d41360137a1ae6194db40069654e3 class="flex justify-between"><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/Heat-Equation/>热方程</a></label><ul><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/Heat-Equation/Heat-Equation-Solution/>Heat Equation Solution</a></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/MATH-351-352/Heat-Equation/The-Fourier-Series/>傅立叶级数</a></li></ul></li><li><input type=checkbox id=section-7f42ebdf414be775241761c5b3262618 class=toggle>
<label for=section-7f42ebdf414be775241761c5b3262618 class="flex justify-between"><a role=button>波方程</a></label><ul></ul></li><li><input type=checkbox id=section-3cdad47fb9156c74a9aac21d7840334d class=toggle>
<label for=section-3cdad47fb9156c74a9aac21d7840334d class="flex justify-between"><a role=button>拉普拉斯方程</a></label><ul></ul></li></ul></li><li><a href=/docs/%E6%95%B0%E5%AD%A6/PS/>Ps</a></li></ul></div><div class="subject-panel hidden" data-subject=物理><ul></ul></div></div></nav><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".subject-tab"),s=document.querySelectorAll(".subject-panel");function t(t){e.forEach(e=>e.classList.remove("active")),s.forEach(e=>e.classList.add("hidden"));const n=document.querySelector(`.subject-tab[data-subject="${t}"]`);n&&n.classList.add("active");const o=document.querySelector(`.subject-panel[data-subject="${t}"]`);o&&o.classList.remove("hidden"),localStorage.setItem("activeSubjectTab",t)}const n=localStorage.getItem("activeSubjectTab");n&&t(n),e.forEach(e=>{e.addEventListener("click",function(){const n=e.getAttribute("data-subject");t(n)})})})</script><div class=after-menu-offset><ul><li><a href=/posts/>Blog</a></li><li><a href=https://github.com/EriseHe/notebook target=_blank rel=noopener>Github</a></li><li><a href=https://themes.gohugo.io/themes/hugo-book/ target=_blank rel=noopener>Hugo Themes</a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>FinTextSim: Enhancing Financial Text Analysis with BERTopic</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><ul><li></li></ul></li><li><a href=#acknowledgement>Acknowledgement</a></li><li><a href=#1-introduction>1 Introduction</a></li><li><a href=#2-state-of-the-art>2 State of the Art</a><ul><li><a href=#21-evolution-of-contemporary-topic-modeling-approaches>2.1 Evolution of Contemporary Topic Modeling Approaches</a></li><li><a href=#22-bertopic>2.2 BERTopic</a></li><li><a href=#23-topic-modeling-of-item-7-and-item-7a>2.3 Topic Modeling of Item 7 and Item 7A</a></li></ul></li><li><a href=#3-materials-and-methods>3 Materials and Methods</a><ul><li><a href=#31-dataset>3.1 Dataset</a></li><li><a href=#32-keyword-list>3.2 Keyword List</a></li><li><a href=#33-fintextsim>3.3 FinTextSim</a></li><li><a href=#34-model-creation>3.4 Model Creation</a></li><li><a href=#35-evaluation-metrics>3.5 Evaluation Metrics</a></li></ul></li><li><a href=#4-results-and-discussion>4 Results and Discussion</a><ul><li><a href=#41-fintextsim---leveraging-contextual-embeddings-for-the-financial-domain>4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain</a></li><li><a href=#42-topic-quality>4.2 Topic Quality</a></li><li><a href=#43-organizing-power>4.3 Organizing Power</a></li><li><a href=#44-wrapup-of-results-and-discussion>4.4 Wrapup of Results and Discussion</a></li></ul></li><li><a href=#5-conclusion>5 Conclusion</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></aside></header><article class=book-article><h1 style="text-align:center;font-family:computer modern,cmu serif,serif">FinTextSim: Enhancing Financial Text Analysis with BERTopic</h1><div class="markdown posts" x-data><p data-raw='arXiv:2504.15683v1 [cs.CL] 22 Apr 2025'>arXiv:2504.15683v1 [cs.CL] 22 Apr 2025</p><p data-raw='Simon Jehnen Javier Villalba-Díez Joaquín Ordieres-Meré'>Simon Jehnen Javier Villalba-Díez Joaquín Ordieres-Meré</p><h6 id=abstract>Abstract
<a class=anchor href=#abstract>#</a></h6><p data-raw='Recent advancements in information availability and computational capabilities have transformed the analysis of annual reports, integrating traditional financial metrics with insights from textual data. To extract valuable insights from this wealth of textual data, automated review processes, such as topic modeling, are crucial. This study examines the effectiveness of BERTopic, a state-of-the-art topic model relying on contextual embeddings, for analyzing Item 7 and Item 7A of 10-K filings from S&amp;P 500 companies (2016–2022). Moreover, we introduce FinTextSim, a finetuned sentence-transformer model optimized for clustering and semantic search in financial contexts. Compared to all-MiniLM-L6-v2, the most widely used sentence-transformer, FinTextSim increases intratopic similarity by 81% and reduces intertopic similarity by 100%, significantly enhancing organizational clarity. We assess BERTopic’s performance using embeddings from both FinTextSim and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and distinct economic topic clusters when paired with FinTextSim’s embeddings. Without FinTextSim, BERTopic struggles with misclassification and overlapping topics. Thus, FinTextSim is pivotal for advancing financial text analysis. FinTextSim’s enhanced contextual embeddings, tailored for the financial domain, elevate the quality of future research and financial information. This improved quality of financial information will enable stakeholders to gain a competitive advantage, streamlining resource allocation and decision-making processes. Moreover, the improved insights have the potential to leverage business valuation and stock price prediction models.'>Recent advancements in information availability and computational capabilities have transformed the analysis of annual reports, integrating traditional financial metrics with insights from textual data. To extract valuable insights from this wealth of textual data, automated review processes, such as topic modeling, are crucial. This study examines the effectiveness of BERTopic, a state-of-the-art topic model relying on contextual embeddings, for analyzing Item 7 and Item 7A of 10-K filings from S&amp;P 500 companies (2016–2022). Moreover, we introduce FinTextSim, a finetuned sentence-transformer model optimized for clustering and semantic search in financial contexts. Compared to all-MiniLM-L6-v2, the most widely used sentence-transformer, FinTextSim increases intratopic similarity by 81% and reduces intertopic similarity by 100%, significantly enhancing organizational clarity. We assess BERTopic’s performance using embeddings from both FinTextSim and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and distinct economic topic clusters when paired with FinTextSim’s embeddings. Without FinTextSim, BERTopic struggles with misclassification and overlapping topics. Thus, FinTextSim is pivotal for advancing financial text analysis. FinTextSim’s enhanced contextual embeddings, tailored for the financial domain, elevate the quality of future research and financial information. This improved quality of financial information will enable stakeholders to gain a competitive advantage, streamlining resource allocation and decision-making processes. Moreover, the improved insights have the potential to leverage business valuation and stock price prediction models.</p><h6 id=keywords>keywords:
<a class=anchor href=#keywords>#</a></h6><p data-raw='Topic Modeling, 10-K, Artificial Intelligence, BERTopic, MD&amp;A, FinTextSim, Sentence Transformers'>Topic Modeling, 10-K, Artificial Intelligence, BERTopic, MD&amp;A, FinTextSim, Sentence Transformers</p><p data-raw='[inst1]organization=Universidad Politécnica de Madrid, DEGIN doctoral program, Department of Industrial Management. ETSII,addressline=c. José Gutiérrez Abascal 2, city=Madrid, postcode=28006, state=Madrid, country=Spain'>[inst1]organization=Universidad Politécnica de Madrid, DEGIN doctoral program, Department of Industrial Management. ETSII,addressline=c. José Gutiérrez Abascal 2, city=Madrid, postcode=28006, state=Madrid, country=Spain</p><p data-raw='[inst2]organization=Fakultät für Wirtschaft, Hochschule Heilbronn,addressline=Bildungscampus, Max-Planck-Straße 39, city=Heilbronn, postcode=74081, state=State Two, country=Germany'>[inst2]organization=Fakultät für Wirtschaft, Hochschule Heilbronn,addressline=Bildungscampus, Max-Planck-Straße 39, city=Heilbronn, postcode=74081, state=State Two, country=Germany</p><p data-raw='[inst3]organization=Escuela Técnica Superior de Ingeniería Industrial, Universidad de la Rioja,addressline=C. Luis de Ulloa, 4, city=Logroño, postcode=26004, state=La Rioja, country=Spain'>[inst3]organization=Escuela Técnica Superior de Ingeniería Industrial, Universidad de la Rioja,addressline=C. Luis de Ulloa, 4, city=Logroño, postcode=26004, state=La Rioja, country=Spain</p><p data-raw='[inst4]organization=Beta Klinik GmbH,addressline=Joseph-Schumpeter-Allee 15, city=Bonn, postcode=53227, country=Germany'>[inst4]organization=Beta Klinik GmbH,addressline=Joseph-Schumpeter-Allee 15, city=Bonn, postcode=53227, country=Germany</p><h2 id=acknowledgement>Acknowledgement
<a class=anchor href=#acknowledgement>#</a></h2><p data-raw='The second and third authors want to acknowledge the partial support by the Spanish “Agencia Estatal de Investigación” through the grant PID2022-137748OB-C31 funded by MCIN/AEI/10.13039/501100011033 and ”ERDF A way of making Europe”.'>The second and third authors want to acknowledge the partial support by the Spanish “Agencia Estatal de Investigación” through the grant PID2022-137748OB-C31 funded by MCIN/AEI/10.13039/501100011033 and ”ERDF A way of making Europe”.</p><h2 id=1-introduction>1 Introduction
<a class=anchor href=#1-introduction>#</a></h2><p data-raw='In recent years, the increasing availability of information <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> and advances in computational capabilities have transformed the way we analyze annual reports, including 10-K filings. 10-K filings are among the most critical annual reports <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, providing a standardized snapshot of a company’s financial situation through both numerical and textual data <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. The information contained carries predictive power for future profitability <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Hence, various stakeholders, such as investors and financial analysts, rely on 10-K reports to make informed decisions <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Traditionally, evaluation focused on retrospective quantitative financial metrics. However, there is a growing recognition of the value embedded in qualitative textual data. For example, <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> demonstrated that language and tone in financial reports correlate with future company returns. Therefore, integrating retrospective financial metrics with textual analysis offers a fuller picture of a company, improving various decision-making processes <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>.'>In recent years, the increasing availability of information <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> and advances in computational capabilities have transformed the way we analyze annual reports, including 10-K filings. 10-K filings are among the most critical annual reports <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, providing a standardized snapshot of a company’s financial situation through both numerical and textual data <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. The information contained carries predictive power for future profitability <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>. Hence, various stakeholders, such as investors and financial analysts, rely on 10-K reports to make informed decisions <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. Traditionally, evaluation focused on retrospective quantitative financial metrics. However, there is a growing recognition of the value embedded in qualitative textual data. For example, <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> demonstrated that language and tone in financial reports correlate with future company returns. Therefore, integrating retrospective financial metrics with textual analysis offers a fuller picture of a company, improving various decision-making processes <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>.</p><p data-raw='Item 7 and Item 7A of 10-K filings contain valuable information regarding companies listed in the Standard and Poor’s 500 (S&amp;P 500). Among the 15 items included in 10-K reports, they stand out as particularly crucial. Item 7 is the Management Discussion & Analysis (MD&amp;A) section. In this section, the management presents the company’s perspective on various aspects, including operations, performance, risks, opportunities, and strategies to address future challenges <sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Item 7A contains qualitative and quantitative disclosures about market risk. As the submission of a 10-K filing is mandatory for publicly traded companies, there is a wealth of information. We need clearly defined review processes to evaluate and use this information <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>.'>Item 7 and Item 7A of 10-K filings contain valuable information regarding companies listed in the Standard and Poor’s 500 (S&amp;P 500). Among the 15 items included in 10-K reports, they stand out as particularly crucial. Item 7 is the Management Discussion & Analysis (MD&amp;A) section. In this section, the management presents the company’s perspective on various aspects, including operations, performance, risks, opportunities, and strategies to address future challenges <sup id=fnref1:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. Item 7A contains qualitative and quantitative disclosures about market risk. As the submission of a 10-K filing is mandatory for publicly traded companies, there is a wealth of information. We need clearly defined review processes to evaluate and use this information <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>.</p><p data-raw='Automated review processes offer various advantages over manual approaches. First, manual review of textual data is time-consuming and prone to subjectivity bias <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. Moreover, the vast and growing amount of data <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> can lead to information overload, particularly for stakeholders with limited attention <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. Thus, attention must be efficiently allocated <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Topic modeling, a technique from Natural Language Processing (NLP), addresses these challenges by uncovering latent topics within textual datasets. Hence, they help to organize and summarize large text corpora <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>.'>Automated review processes offer various advantages over manual approaches. First, manual review of textual data is time-consuming and prone to subjectivity bias <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>. Moreover, the vast and growing amount of data <sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> can lead to information overload, particularly for stakeholders with limited attention <sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>. Thus, attention must be efficiently allocated <sup id=fnref1:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. Topic modeling, a technique from Natural Language Processing (NLP), addresses these challenges by uncovering latent topics within textual datasets. Hence, they help to organize and summarize large text corpora <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>.</p><p data-raw='Recently developed neural topic models address the limitations of classical topic modeling approaches, which still dominate applied topic modeling. Classical topic modeling approaches have been discussed in the literature since the introduction of Latent Semantic Indexing in 1990 <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Until 2015, Bayesian probabilistic models, most notably Latent Dirichlet Allocation (LDA), were considered state-of-the-art. Relying on the bag-of-words (BoW) assumption, each document is treated as a collection of words, disregarding their sequential order. However, this approach limits the model’s ability to capture the semantic meaning of text. Neural topic modeling approaches address this issue by employing contextual embeddings <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>, allowing them to capture richer semantic and contextual relationships within the data <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>.'>Recently developed neural topic models address the limitations of classical topic modeling approaches, which still dominate applied topic modeling. Classical topic modeling approaches have been discussed in the literature since the introduction of Latent Semantic Indexing in 1990 <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>. Until 2015, Bayesian probabilistic models, most notably Latent Dirichlet Allocation (LDA), were considered state-of-the-art. Relying on the bag-of-words (BoW) assumption, each document is treated as a collection of words, disregarding their sequential order. However, this approach limits the model’s ability to capture the semantic meaning of text. Neural topic modeling approaches address this issue by employing contextual embeddings <sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>, allowing them to capture richer semantic and contextual relationships within the data <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>.</p><p data-raw='Recent advances in contextual embeddings have transformed NLP, driven by key innovations such as the transformer architecture, encoder-only models, and sentence-transformers. The transformer architecture, introduced by <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>, revolutionized NLP by relying entirely on attention mechanisms, allowing models to capture long-range dependencies and rich contextual information. This made transformers the state-of-the-art approach for Natural Language Understanding tasks <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. Building on this foundation, Bidirectional Encoder Representations from Transformers (BERT) established a new standard for deep contextualized language modeling <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. BERT relies on the encoder from the transformers architecture, allowing it to processes text bidirectionally and capture nuanced semantic relationships. More recently, <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> introduced improvements to this architecture, increasing efficiency and surpassing BERT in classification and retrieval tasks. Despite their strengths, encoder-only models are less effective for large-scale semantic similarity comparison and clustering <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. To address these limitations, sentence-transformers refine encoder-only models using siamese or triplet network architectures, enabling efficient and precise similarity assessments <sup id="fnref1:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. Sentence-transformers encode text into dense vector representations—contextual embeddings—that quantify semantic similarity by mapping similar texts closer in a shared vector space. Neural topic models, such as BERTopic, leverage these contextual embeddings to enhance topic modeling by clustering semantically related documents, enabling more accurate topic discovery <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>.'>Recent advances in contextual embeddings have transformed NLP, driven by key innovations such as the transformer architecture, encoder-only models, and sentence-transformers. The transformer architecture, introduced by <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>, revolutionized NLP by relying entirely on attention mechanisms, allowing models to capture long-range dependencies and rich contextual information. This made transformers the state-of-the-art approach for Natural Language Understanding tasks <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>. Building on this foundation, Bidirectional Encoder Representations from Transformers (BERT) established a new standard for deep contextualized language modeling <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>. BERT relies on the encoder from the transformers architecture, allowing it to processes text bidirectionally and capture nuanced semantic relationships. More recently, <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup> introduced improvements to this architecture, increasing efficiency and surpassing BERT in classification and retrieval tasks. Despite their strengths, encoder-only models are less effective for large-scale semantic similarity comparison and clustering <sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>. To address these limitations, sentence-transformers refine encoder-only models using siamese or triplet network architectures, enabling efficient and precise similarity assessments <sup id=fnref1:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>. Sentence-transformers encode text into dense vector representations—contextual embeddings—that quantify semantic similarity by mapping similar texts closer in a shared vector space. Neural topic models, such as BERTopic, leverage these contextual embeddings to enhance topic modeling by clustering semantically related documents, enabling more accurate topic discovery <sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>.</p><p data-raw='While these advancements have demonstrated significant improvements in general text processing, it remains unclear how these sophisticated methods perform when applied to tasks specifically relevant to the finance and accounting domains <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. Additionally, there is little evidence that the customization of financial text leads to benefits in performance <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. Traditional algorithms continue to dominate applied topic modeling, hindering the generation of new knowledge <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. Although extensive studies on various topic modeling approaches <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup> have been conducted, the domain of Management Accounting and Finance, particularly Item 7 and Item 7A of 10-K reports from S&amp;P 500 companies, remains significantly under-researched. This presents a critical opportunity to integrate Machine Learning (ML)-based methods to fully exploit the value hidden in financial textual data <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. Furthermore, it opens avenues for refining domain-specific contextual embeddings by incorporating domain expertise <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. Such an approach aims to enhance the quality of contextual embeddings, allowing them to capture terminology and concepts unique to finance with greater precision <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>.'>While these advancements have demonstrated significant improvements in general text processing, it remains unclear how these sophisticated methods perform when applied to tasks specifically relevant to the finance and accounting domains <sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup>. Additionally, there is little evidence that the customization of financial text leads to benefits in performance <sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup>. Traditional algorithms continue to dominate applied topic modeling, hindering the generation of new knowledge <sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup>. Although extensive studies on various topic modeling approaches <sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup> have been conducted, the domain of Management Accounting and Finance, particularly Item 7 and Item 7A of 10-K reports from S&amp;P 500 companies, remains significantly under-researched. This presents a critical opportunity to integrate Machine Learning (ML)-based methods to fully exploit the value hidden in financial textual data <sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup>. Furthermore, it opens avenues for refining domain-specific contextual embeddings by incorporating domain expertise <sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup>. Such an approach aims to enhance the quality of contextual embeddings, allowing them to capture terminology and concepts unique to finance with greater precision <sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>.</p><p data-raw='To address this gap, we introduce FinTextSim, a finetuned sentence transformer leveraging the capabilities of contextual embeddings for the financial domain. We benchmark FinTextSim against all-miniLM-L6-v2 (AM), the most widely used general-purpose sentence transformer. To isolate the effect of the selected sentence-transformer, we generate contextual embeddings for our dataset using both FinTextSim and AM. We apply these embeddings to BERTopic, a state-of-the-art neural topic modeling approach, while keeping all other model parameters identical. This comparison allows us to determine whether domain-specific fine-tuning enhances topic modeling and financial text interpretation.'>To address this gap, we introduce FinTextSim, a finetuned sentence transformer leveraging the capabilities of contextual embeddings for the financial domain. We benchmark FinTextSim against all-miniLM-L6-v2 (AM), the most widely used general-purpose sentence transformer. To isolate the effect of the selected sentence-transformer, we generate contextual embeddings for our dataset using both FinTextSim and AM. We apply these embeddings to BERTopic, a state-of-the-art neural topic modeling approach, while keeping all other model parameters identical. This comparison allows us to determine whether domain-specific fine-tuning enhances topic modeling and financial text interpretation.</p><p data-raw='We will explore the following research questions based on Item 7 and Item 7A from S&amp;P500 companies between 2016 and 2022:'>We will explore the following research questions based on Item 7 and Item 7A from S&amp;P500 companies between 2016 and 2022:</p><ol><li>How can we leverage the capabilities of contextual embeddings for the financial domain?</li><li>Which embedding model — FinTextSim or AM — produces more qualitative and coherent topics when used as input for BERTopic?</li><li>Which embedding model — FinTextSim or AM — better supports BERTopic in organizing and summarizing large-scale financial text corpora?</li></ol><p data-raw='By addressing the research questions, our work makes the following significant contributions:'>By addressing the research questions, our work makes the following significant contributions:</p><ol><li>We identify the most effective approach for extracting meaningful topics from Item 7 and Item 7A of S&amp;P500 companies. This comparison provides valuable insights for researchers and practitioners selecting embedding models for NLP tasks.</li><li>We introduce FinTextSim, a finetuned sentence-transformer, improving the analysis of financial text for various downstream tasks. Hence, FinTextSim will boost future research quality.</li><li>By enhancing BERTopic for financial text with FinTextSim, we generate higher quality financial information regarding companies, sectors as well as whole markets and economies. Thus, we enable managers, financial analysts, investors, regulators and other stakeholders to gain a competitive advantage which aids in allocating resources more efficiently and making rational operational and strategic decisions. Moreover, the improved insights have the potential to leverage business valuation and stock price prediction models.</li></ol><p data-raw='The rest of the paper hereinafter is organized as follows. Section 2 reviews the state-of-the-art literature and methodologies. Section 3 describes our study’s materials and methods, including the training procedure of FinTextSim. Section 4 presents and discusses the main findings. Finally, Section 5 provides the conclusion. This structure ensures a clear and logical progression, enabling a thorough understanding of our study’s contributions.'>The rest of the paper hereinafter is organized as follows. Section 2 reviews the state-of-the-art literature and methodologies. Section 3 describes our study’s materials and methods, including the training procedure of FinTextSim. Section 4 presents and discusses the main findings. Finally, Section 5 provides the conclusion. This structure ensures a clear and logical progression, enabling a thorough understanding of our study’s contributions.</p><h2 id=2-state-of-the-art>2 State of the Art
<a class=anchor href=#2-state-of-the-art>#</a></h2><p data-raw='The following subsections provide an overview of the evolution of contemporary topic modeling techniques and a detailed examination of BERTopic, a state-of-the-art topic modeling approach.'>The following subsections provide an overview of the evolution of contemporary topic modeling techniques and a detailed examination of BERTopic, a state-of-the-art topic modeling approach.</p><h3 id=21-evolution-of-contemporary-topic-modeling-approaches>2.1 Evolution of Contemporary Topic Modeling Approaches
<a class=anchor href=#21-evolution-of-contemporary-topic-modeling-approaches>#</a></h3><p data-raw='Recent advancements in topic modeling have seen the integration of contextual embeddings, offering both significant benefits and challenges. Modern methodologies address the limitations of classical models by utilizing advanced text embedding techniques, moving beyond simple BoW representations. This enables them to better capture semantic relationships within text <sup id="fnref1:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. While classical models require extensive customization and become increasingly complex with larger datasets, modern approaches offer enhanced flexibility and scalability <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Moreover, neural topic models simplify the inference problem, enabling parallelization <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>. Among other innovative methods <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>, contextual vector representations are combined with centroid-based clustering techniques <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. They assume that the centroid of a cluster represents the topic. Words closest to the centroid are considered as the most representative ones for the topic. However, this assumption is fragile as clusters may not always conform to a spherical distribution around the centroid. As a result, misrepresentation of topics may occur <sup id="fnref1:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. A promising approach for topic modeling based on contextual embeddings, addressing centroid-based issues, is BERTopic.'>Recent advancements in topic modeling have seen the integration of contextual embeddings, offering both significant benefits and challenges. Modern methodologies address the limitations of classical models by utilizing advanced text embedding techniques, moving beyond simple BoW representations. This enables them to better capture semantic relationships within text <sup id=fnref1:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>. While classical models require extensive customization and become increasingly complex with larger datasets, modern approaches offer enhanced flexibility and scalability <sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>. Moreover, neural topic models simplify the inference problem, enabling parallelization <sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup>. Among other innovative methods <sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup>, contextual vector representations are combined with centroid-based clustering techniques <sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup>. They assume that the centroid of a cluster represents the topic. Words closest to the centroid are considered as the most representative ones for the topic. However, this assumption is fragile as clusters may not always conform to a spherical distribution around the centroid. As a result, misrepresentation of topics may occur <sup id=fnref1:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>. A promising approach for topic modeling based on contextual embeddings, addressing centroid-based issues, is BERTopic.</p><h3 id=22-bertopic>2.2 BERTopic
<a class=anchor href=#22-bertopic>#</a></h3><p data-raw='BERTopic structures topic modeling into five sequential steps. First, document embeddings are generated using a pre-trained sentence transformer. A method that offers enduring benefits by leveraging advancements in language models <sup id="fnref2:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. Second, the dimensionality of these embeddings is reduced. Subsequently, the reduced embeddings are clustered into semantically similar groups, i.e., topics. The reduction of dimensionality is deliberately placed before clustering to increase computational efficiency as well as clustering accuracy <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. In the fourth step, topics are tokenized. Finally, tokens are weighted. To enhance the quality of extracted topic representations, <sup id="fnref3:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> introduces class-based tfidf (c-tfidf), which weighs the importance of tokens within topics, enabling a more efficient extraction of topic representations.'>BERTopic structures topic modeling into five sequential steps. First, document embeddings are generated using a pre-trained sentence transformer. A method that offers enduring benefits by leveraging advancements in language models <sup id=fnref2:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>. Second, the dimensionality of these embeddings is reduced. Subsequently, the reduced embeddings are clustered into semantically similar groups, i.e., topics. The reduction of dimensionality is deliberately placed before clustering to increase computational efficiency as well as clustering accuracy <sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup>. In the fourth step, topics are tokenized. Finally, tokens are weighted. To enhance the quality of extracted topic representations, <sup id=fnref3:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup> introduces class-based tfidf (c-tfidf), which weighs the importance of tokens within topics, enabling a more efficient extraction of topic representations.</p><p data-raw='Despite its significant advantages, BERTopic also faces several drawbacks. It tends to produce a manifold of closely interconnected topics which may vary upon repeated modeling attempts <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. This variability contributes to inconsistency in producing meaningful results, further complicated by the complexity of interpreting hyperparameters, hindering troubleshooting and diminishing the reliability of results <sup id="fnref1:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>. Moreover, BERTopic assumes that each document relates to a single topic, potentially oversimplifying real-world document complexity <sup id="fnref4:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. Additionally, sentence-transformer models used for document embedding perform optimally with sentences or paragraphs <sup id="fnref2:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. Furthermore, high computation times can result from processing large amounts of data <sup id="fnref5:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>.'>Despite its significant advantages, BERTopic also faces several drawbacks. It tends to produce a manifold of closely interconnected topics which may vary upon repeated modeling attempts <sup id=fnref:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup>. This variability contributes to inconsistency in producing meaningful results, further complicated by the complexity of interpreting hyperparameters, hindering troubleshooting and diminishing the reliability of results <sup id=fnref1:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup>. Moreover, BERTopic assumes that each document relates to a single topic, potentially oversimplifying real-world document complexity <sup id=fnref4:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>. Additionally, sentence-transformer models used for document embedding perform optimally with sentences or paragraphs <sup id=fnref2:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>. Furthermore, high computation times can result from processing large amounts of data <sup id=fnref5:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>.</p><p data-raw='Due to its novelty, applications and enhancements of BERTopic are still in their infancy. In a financial context, <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup> utilized BERTopic on Item 1A from 10-K filings. They assessed whether identified topics can enhance the accuracy of ESG rating predictions and quantify each topic’s relative contribution to the final rating prediction. In other contexts, BERTopic has been applied in various studies: <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup> analyzed customer reviews, <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup> explored its application with pre-trained Arabic language models, <sup id="fnref1:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup> evaluated its performance on Twitter data, and <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup> extended BERTopic to predict individual’s responses to a questionnaire based on their social media activity.'>Due to its novelty, applications and enhancements of BERTopic are still in their infancy. In a financial context, <sup id=fnref:34><a href=#fn:34 class=footnote-ref role=doc-noteref>34</a></sup> utilized BERTopic on Item 1A from 10-K filings. They assessed whether identified topics can enhance the accuracy of ESG rating predictions and quantify each topic’s relative contribution to the final rating prediction. In other contexts, BERTopic has been applied in various studies: <sup id=fnref:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup> analyzed customer reviews, <sup id=fnref:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup> explored its application with pre-trained Arabic language models, <sup id=fnref1:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup> evaluated its performance on Twitter data, and <sup id=fnref:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup> extended BERTopic to predict individual’s responses to a questionnaire based on their social media activity.</p><h3 id=23-topic-modeling-of-item-7-and-item-7a>2.3 Topic Modeling of Item 7 and Item 7A
<a class=anchor href=#23-topic-modeling-of-item-7-and-item-7a>#</a></h3><p data-raw='Our research is driven by several motivations regarding the choice of documents and analysis techniques. Item 7 and Item 7A stand out as particularly crucial sections in 10-K reports <sup id="fnref1:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. The MD&amp;A section (Item 7) provides a narrative that contextualizes the presented numbers, covering topics such as performance, liquidity, risks, and operations. In this section, management offers its individual perspective, which is essential for understanding the company’s strategic direction and potential challenges. Additionally, the MD&amp;A section offers the most leeway and flexibility, making it rich with insights and indicative of future performance <sup id="fnref2:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Item 7A focuses on market risks, containing valuable information regarding the company’s prospective performance. Hence, analyzing Item 7 and Item 7A allows us to uncover hidden textual information that has the potential to support the prediction of a company’s future performance. A technique that shows promise in addressing this challenge is topic modeling <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. Despite advancements in computational capabilities and the emergence of new topic modeling techniques, there remains a gap in applying topic modeling methods to financial texts, particularly Item 7 and Item 7A. Furthermore, LDA continues to dominate applied topic modeling, although newer approaches like BERTopic offer potential improvements <sup id="fnref1:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>.'>Our research is driven by several motivations regarding the choice of documents and analysis techniques. Item 7 and Item 7A stand out as particularly crucial sections in 10-K reports <sup id=fnref1:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup>. The MD&amp;A section (Item 7) provides a narrative that contextualizes the presented numbers, covering topics such as performance, liquidity, risks, and operations. In this section, management offers its individual perspective, which is essential for understanding the company’s strategic direction and potential challenges. Additionally, the MD&amp;A section offers the most leeway and flexibility, making it rich with insights and indicative of future performance <sup id=fnref2:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. Item 7A focuses on market risks, containing valuable information regarding the company’s prospective performance. Hence, analyzing Item 7 and Item 7A allows us to uncover hidden textual information that has the potential to support the prediction of a company’s future performance. A technique that shows promise in addressing this challenge is topic modeling <sup id=fnref1:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup>. Despite advancements in computational capabilities and the emergence of new topic modeling techniques, there remains a gap in applying topic modeling methods to financial texts, particularly Item 7 and Item 7A. Furthermore, LDA continues to dominate applied topic modeling, although newer approaches like BERTopic offer potential improvements <sup id=fnref1:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup>.</p><p data-raw='In this paper, we aim to demonstrate how FinTextSim, a finetuned sentence transformer, outperforms the most widely used sentence transformer AM on text from Item 7 and Item 7A from 10-K reports of S&amp;P 500 companies. Additionally, we hypothesize that combining BERTopic with FinTextSim will significantly enhance the quality of financial information, providing better insights for stakeholders. We foresee that FinTextSim will also facilitate the application of aspect-based sentiment analysis, eventually improving business valuation and stock price prediction models.'>In this paper, we aim to demonstrate how FinTextSim, a finetuned sentence transformer, outperforms the most widely used sentence transformer AM on text from Item 7 and Item 7A from 10-K reports of S&amp;P 500 companies. Additionally, we hypothesize that combining BERTopic with FinTextSim will significantly enhance the quality of financial information, providing better insights for stakeholders. We foresee that FinTextSim will also facilitate the application of aspect-based sentiment analysis, eventually improving business valuation and stock price prediction models.</p><h2 id=3-materials-and-methods>3 Materials and Methods
<a class=anchor href=#3-materials-and-methods>#</a></h2><p data-raw='In the following subsections, we outline the materials and methods of our study. This section is divided into several parts: sourcing the dataset, creating an enhanced financial keyword list, training FinTextSim, creating the topic models, and presenting the metrics used to evaluate the performance of the topic models.'>In the following subsections, we outline the materials and methods of our study. This section is divided into several parts: sourcing the dataset, creating an enhanced financial keyword list, training FinTextSim, creating the topic models, and presenting the metrics used to evaluate the performance of the topic models.</p><h3 id=31-dataset>3.1 Dataset
<a class=anchor href=#31-dataset>#</a></h3><p data-raw='Our study focuses exclusively on Item 7 and Item 7A of 10-K reports while avoiding survivorship bias and ensuring the highest possible document comparability. Given their greater significance, we deliberately choose 10-K over 10-Q reports <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. We source our data from the Notre Dame Software Repository for Accounting and Finance in text-file format, which underwent a ’Stage One Parse’ to remove all HTML tags.<sup>1</sup> <sup>1</sup> 1 The data can be found at: 
  <a href="https://sraf.nd.edu/data/stage-one-10-x-parse-data/">https://sraf.nd.edu/data/stage-one-10-x-parse-data/</a>.'>Our study focuses exclusively on Item 7 and Item 7A of 10-K reports while avoiding survivorship bias and ensuring the highest possible document comparability. Given their greater significance, we deliberately choose 10-K over 10-Q reports <sup id=fnref1:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. We source our data from the Notre Dame Software Repository for Accounting and Finance in text-file format, which underwent a ’Stage One Parse’ to remove all HTML tags.<sup>1</sup> <sup>1</sup> 1 The data can be found at:
<a href=https://sraf.nd.edu/data/stage-one-10-x-parse-data/>https://sraf.nd.edu/data/stage-one-10-x-parse-data/</a>.</p><p data-raw='To prevent survivorship bias, we filter 10-K filings of all companies that have been listed in the S&amp;P 500 index between 2015 and 2022. Using a regular expression-based extractor, we isolate the text from the start of Item 7 to the start of Item 8. Through this endeavor, we obtain the raw text of Item 7 and Item 7A. We refer to this combination of Item 7 and Item 7A as ’documents’. In order to maintain comparability, documents containing fewer than 250 words are discarded.<sup>2</sup> <sup>2</sup> 2 Paragraphs typically consist of 100–200 words. Moreover, sentence-transformers, such as AM and FinTextSim are designed to capture the semantic information of sentences and short paragraphs. Input texts longer than 256-word pieces (approximately 170-210 words) are truncated by default. The 250-word threshold ensures that each document includes at least two paragraphs, enhancing relevance, as shorter texts often lack substantive or complete ideas.'>To prevent survivorship bias, we filter 10-K filings of all companies that have been listed in the S&amp;P 500 index between 2015 and 2022. Using a regular expression-based extractor, we isolate the text from the start of Item 7 to the start of Item 8. Through this endeavor, we obtain the raw text of Item 7 and Item 7A. We refer to this combination of Item 7 and Item 7A as ’documents’. In order to maintain comparability, documents containing fewer than 250 words are discarded.<sup>2</sup> <sup>2</sup> 2 Paragraphs typically consist of 100–200 words. Moreover, sentence-transformers, such as AM and FinTextSim are designed to capture the semantic information of sentences and short paragraphs. Input texts longer than 256-word pieces (approximately 170-210 words) are truncated by default. The 250-word threshold ensures that each document includes at least two paragraphs, enhancing relevance, as shorter texts often lack substantive or complete ideas.</p><p data-raw='Subsequently, we remove further outlier documents, identified by z-score. The z-score is a statistical measure that quantifies the distance of data points to the mean of a dataset, taking the standard deviation into account. We define data points as outliers if they deviate more than two standard deviations from the mean. Subsequently, we focus on documents from the period between 2016 and 2022. Finally, we apply classical text processing techniques to identify relevant documents, ensuring that only meaningful texts are included in the analysis. As part of this process, we normalize documents by removing stopwords, applying lemmatization, and performing tokenization. Subsequently, documents with low cosine similarity to others are filtered out. This procedure ensures that both classical (evaluated in 
  <a href="https://arxiv.org/html/2504.15683v1#A4" title="Appendix D Results and Discussion - Classical Topic Modeling Approaches ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">D</a>) and contemporary models are applied to the same set of documents for a direct comparison. The text fed into the sentence transformers remains unprocessed by classical techniques. The number of documents retained at each preprocessing step is shown in Table 
  <a href="https://arxiv.org/html/2504.15683v1#S3.T1" title="Table 1 ‣ 3.1 Dataset ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">1</a>.'>Subsequently, we remove further outlier documents, identified by z-score. The z-score is a statistical measure that quantifies the distance of data points to the mean of a dataset, taking the standard deviation into account. We define data points as outliers if they deviate more than two standard deviations from the mean. Subsequently, we focus on documents from the period between 2016 and 2022. Finally, we apply classical text processing techniques to identify relevant documents, ensuring that only meaningful texts are included in the analysis. As part of this process, we normalize documents by removing stopwords, applying lemmatization, and performing tokenization. Subsequently, documents with low cosine similarity to others are filtered out. This procedure ensures that both classical (evaluated in
<a href=https://arxiv.org/html/2504.15683v1#A4 title="Appendix D Results and Discussion - Classical Topic Modeling Approaches ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">D</a>) and contemporary models are applied to the same set of documents for a direct comparison. The text fed into the sentence transformers remains unprocessed by classical techniques. The number of documents retained at each preprocessing step is shown in Table
<a href=https://arxiv.org/html/2504.15683v1#S3.T1 title="Table 1 ‣ 3.1 Dataset ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">1</a>.</p><p data-raw='Table 1: Dataset.'>Table 1: Dataset.</p><table><thead><tr><th>Preprocessing-Step</th><th># documents</th></tr></thead><tbody><tr><td>Documents extracted related to S&amp;P 500 companies</td><td>4,600</td></tr><tr><td>Documents with less than 250 words</td><td>1,019</td></tr><tr><td>Documents with z-score greater than 2</td><td>165</td></tr><tr><td>Documents outside the timeframe of 2016-2022</td><td>373</td></tr><tr><td>Documents with low cosine similarity</td><td>1,604</td></tr><tr><td>Remaining documents in database</td><td>1,439</td></tr></tbody></table><h3 id=32-keyword-list>3.2 Keyword List
<a class=anchor href=#32-keyword-list>#</a></h3><p data-raw='To train FinTextSim we enhance and utilize a keyword list based on the works of <sup id="fnref1:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> and <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. The economic anchorword list for 10-K and 10-Q reports encompasses 11 distinct topics: sales, cost, profit/loss, operations, liquidity, investment, financing, litigation, employment, tax/regulation, and accounting <sup id="fnref2:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.<sup id="fnref1:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup> augmented it to a topic-word list by identifying semantically similar words, using a Word2Vec model trained on MD&amp;A sections of 10-K reports. We further enhance this list by adding key performance indicator names associated with company performance (e.g., EBITDA or EBIT) and supplementing the operations topic with terms related to logistics, supply chain and marketing. Additionally, we expand the topic-keyword list to 14 topics by introducing three topics that have garnered significant attention in recent years:'>To train FinTextSim we enhance and utilize a keyword list based on the works of <sup id=fnref1:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> and <sup id=fnref:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup>. The economic anchorword list for 10-K and 10-Q reports encompasses 11 distinct topics: sales, cost, profit/loss, operations, liquidity, investment, financing, litigation, employment, tax/regulation, and accounting <sup id=fnref2:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>.<sup id=fnref1:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup> augmented it to a topic-word list by identifying semantically similar words, using a Word2Vec model trained on MD&amp;A sections of 10-K reports. We further enhance this list by adding key performance indicator names associated with company performance (e.g., EBITDA or EBIT) and supplementing the operations topic with terms related to logistics, supply chain and marketing. Additionally, we expand the topic-keyword list to 14 topics by introducing three topics that have garnered significant attention in recent years:</p><ul><li>Energy, in response to the 2022 energy crisis <sup id=fnref:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>,</li><li>Environmental Sustainability, driven by the rising demand for ESG criteria <sup id=fnref:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup>, and</li><li>COVID-19 pandemic <sup id=fnref:41><a href=#fn:41 class=footnote-ref role=doc-noteref>41</a></sup>.</li></ul><p data-raw='An excerpt of the topics and the most important keywords is displayed in 
  <a href="https://arxiv.org/html/2504.15683v1#A5" title="Appendix E Excerpt of Keyword List ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">E</a>.'>An excerpt of the topics and the most important keywords is displayed in
<a href=https://arxiv.org/html/2504.15683v1#A5 title="Appendix E Excerpt of Keyword List ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">E</a>.</p><h3 id=33-fintextsim>3.3 FinTextSim
<a class=anchor href=#33-fintextsim>#</a></h3><p data-raw='To accurately cluster semantically similar financial text, we introduce FinTextSim, a sentence-transformer model specifically finetuned to enhance contextual embeddings for the financial domain. Given the financial jargon and its domain-specific nuances, off-the-shelf (OTS), general-purpose sentence transformers fall short. Existing models tailored for the financial domain are primarily optimized for sentiment analysis (e.g. <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>). Hence, they also prove insufficient. By fine-tuning FinTextSim on financial text, we aim to improve the quality of generated topics, enhancing semantic coherence and ensuring greater separation between topics. We hypothesize that this specialized fine-tuning will bridge the gap between general-purpose models and the specific demands of financial text analysis.'>To accurately cluster semantically similar financial text, we introduce FinTextSim, a sentence-transformer model specifically finetuned to enhance contextual embeddings for the financial domain. Given the financial jargon and its domain-specific nuances, off-the-shelf (OTS), general-purpose sentence transformers fall short. Existing models tailored for the financial domain are primarily optimized for sentiment analysis (e.g. <sup id=fnref:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup>). Hence, they also prove insufficient. By fine-tuning FinTextSim on financial text, we aim to improve the quality of generated topics, enhancing semantic coherence and ensuring greater separation between topics. We hypothesize that this specialized fine-tuning will bridge the gap between general-purpose models and the specific demands of financial text analysis.</p><p data-raw='To tailor FinTextSim for financial text, we create a labeled dataset by adopting a dictionary-based approach, relying on the keyword list presented in Section 
  <a href="https://arxiv.org/html/2504.15683v1#S3.SS2" title="3.2 Keyword List ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3.2</a>. We build upon our dataset and remove noise by replacing contractions as well as URLs and numerical characters. After tokenizing the documents into sentences, we remove noisy ones by discarding those with less than five and more than 50 words. To create a labeled dataset, we construct a keyword-sentence matrix. We iterate over each word in each sentence, checking if a sub-string matches a keyword. We deliberately use a sub-string approach to overcome issues with exact matches. For instance, our keyword list includes the word ’logistic’. Using sub-strings correctly allows words like ’logistics’ and ’logistical’ to be recognized as a keyword match. Additionally, we make minor adjustments to the presented keyword list to prevent the double-counting of keywords. For example, to avoid ’cashflow’ generating two entries for both ’cash’ and ’cashflow’, the refined list only includes ’cash’. After creating the keyword-sentence matrix, sentences containing two or more keywords from only one specific topic domain are labeled accordingly, ensuring topics are distinct. Moreover, we consider unique sentences to prevent overemphasis on particular wordings during training.'>To tailor FinTextSim for financial text, we create a labeled dataset by adopting a dictionary-based approach, relying on the keyword list presented in Section
<a href=https://arxiv.org/html/2504.15683v1#S3.SS2 title="3.2 Keyword List ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3.2</a>. We build upon our dataset and remove noise by replacing contractions as well as URLs and numerical characters. After tokenizing the documents into sentences, we remove noisy ones by discarding those with less than five and more than 50 words. To create a labeled dataset, we construct a keyword-sentence matrix. We iterate over each word in each sentence, checking if a sub-string matches a keyword. We deliberately use a sub-string approach to overcome issues with exact matches. For instance, our keyword list includes the word ’logistic’. Using sub-strings correctly allows words like ’logistics’ and ’logistical’ to be recognized as a keyword match. Additionally, we make minor adjustments to the presented keyword list to prevent the double-counting of keywords. For example, to avoid ’cashflow’ generating two entries for both ’cash’ and ’cashflow’, the refined list only includes ’cash’. After creating the keyword-sentence matrix, sentences containing two or more keywords from only one specific topic domain are labeled accordingly, ensuring topics are distinct. Moreover, we consider unique sentences to prevent overemphasis on particular wordings during training.</p><p data-raw='To address the initial imbalance in labeled sentence distribution and enhance FinTextSim’s generalization capabilities, we expand the dataset through multiple strategies. First, we incorporate an external dataset containing annual and sustainability reports from S&amp;P 500 companies <sup>3</sup> <sup>3</sup> 3 Dataset available at: 
  <a href="https://huggingface.co/datasets/lemousehunter/SnP500-annual-and-sustainability-reports">https://huggingface.co/datasets/lemousehunter/SnP500-annual-and-sustainability-reports</a>, following the same keyword-based labeling logic. This additional dataset broadens the scope of financial discourse captured by FinTextSim, ensuring a more comprehensive representation of domain-specific language. Second, for the underrepresented topics of ”Litigation” and ”Covid Pandemic” in the original Item 7 dataset, we relax keyword-matching constraints to increase the number of labeled instances. This adjustment ensures that these critical topics receive adequate representation in the training process. Finally, we augment sentences for the least represented topic of ”Litigation” using ChatGPT. Through this endeavor, we provide further balance and improve FinTextSim’s ability to accurately distinguish financial topics.'>To address the initial imbalance in labeled sentence distribution and enhance FinTextSim’s generalization capabilities, we expand the dataset through multiple strategies. First, we incorporate an external dataset containing annual and sustainability reports from S&amp;P 500 companies <sup>3</sup> <sup>3</sup> 3 Dataset available at:
<a href=https://huggingface.co/datasets/lemousehunter/SnP500-annual-and-sustainability-reports>https://huggingface.co/datasets/lemousehunter/SnP500-annual-and-sustainability-reports</a>, following the same keyword-based labeling logic. This additional dataset broadens the scope of financial discourse captured by FinTextSim, ensuring a more comprehensive representation of domain-specific language. Second, for the underrepresented topics of ”Litigation” and ”Covid Pandemic” in the original Item 7 dataset, we relax keyword-matching constraints to increase the number of labeled instances. This adjustment ensures that these critical topics receive adequate representation in the training process. Finally, we augment sentences for the least represented topic of ”Litigation” using ChatGPT. Through this endeavor, we provide further balance and improve FinTextSim’s ability to accurately distinguish financial topics.</p><p data-raw='Following these steps, our dataset comprises 180,435 labeled sentences, split into training and test sets with an 80/20 ratio. To ensure balanced learning across all topics, we perform the test-train-split topic-wise. Following these steps, we obtain 36,094 test- and 144,341 train-sentences.'>Following these steps, our dataset comprises 180,435 labeled sentences, split into training and test sets with an 80/20 ratio. To ensure balanced learning across all topics, we perform the test-train-split topic-wise. Following these steps, we obtain 36,094 test- and 144,341 train-sentences.</p><p data-raw='To train FinTextSim, we employ adaptive circle loss and follow methods outlined by <sup id="fnref3:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. As base model, we select ModernBERT, a recent advancement in encoder-only architectures introduced by <sup id="fnref1:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. We adapt ModernBERT with a mean pooling and a normalization layer to enhance its performance for sentence similarity tasks <sup id="fnref4:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. We train the model using adaptive circle loss, deliberately choosing it over common triplet-based approaches. While triplet loss minimizes the distance between similar samples and maximizes it for dissimilar ones, it applies equal penalty strength to positive and negative pairs. In contrast, circle loss dynamically adjusts penalties based on how far a similarity score deviates from its optimal value, allowing more targeted optimization. It focuses on less optimized pairs while mildly adjusting those already close to convergence. Additionally, its circular decision boundary reduces ambiguity in feature space separability <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. Adaptive circle loss further extends this approach by employing curriculum learning, dynamically adjusting margin and scale to improve convergence and stability. We train FinTextSim with a batch size of 200, an initial scale of 5, and an initial margin of 0.25, progressively increasing the scale to 16 and decreasing the margin to 0.1.'>To train FinTextSim, we employ adaptive circle loss and follow methods outlined by <sup id=fnref3:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>. As base model, we select ModernBERT, a recent advancement in encoder-only architectures introduced by <sup id=fnref1:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>. We adapt ModernBERT with a mean pooling and a normalization layer to enhance its performance for sentence similarity tasks <sup id=fnref4:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>. We train the model using adaptive circle loss, deliberately choosing it over common triplet-based approaches. While triplet loss minimizes the distance between similar samples and maximizes it for dissimilar ones, it applies equal penalty strength to positive and negative pairs. In contrast, circle loss dynamically adjusts penalties based on how far a similarity score deviates from its optimal value, allowing more targeted optimization. It focuses on less optimized pairs while mildly adjusting those already close to convergence. Additionally, its circular decision boundary reduces ambiguity in feature space separability <sup id=fnref:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup>. Adaptive circle loss further extends this approach by employing curriculum learning, dynamically adjusting margin and scale to improve convergence and stability. We train FinTextSim with a batch size of 200, an initial scale of 5, and an initial margin of 0.25, progressively increasing the scale to 16 and decreasing the margin to 0.1.</p><p data-raw='To evaluate performance, we encode the test dataset with AM and FinTextSim, comparing intra- and intertopic similarity (see 
  <a href="https://arxiv.org/html/2504.15683v1#S3.SS5.SSS2" title="3.5.2 Organizing Power ‣ 3.5 Evaluation Metrics ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3.5.2</a>). AM, the most downloaded model for sentence similarity tasks on Hugging Face, serves as a robust baseline for evaluating out-of-the-box sentence-transformer embeddings. By benchmarking FinTextSim against AM, we assess whether specialized contextual embeddings improve financial text analysis. We hypothesize that integrating domain-specific knowledge into contextual embeddings, will enable FinTextSim to generate more accurate and semantically meaningful representations of financial text than general-purpose models like AM. This improvement is crucial for precisely identifying meaningful financial topics and ensuring more reliable document organization. To further examine the structure of the learned embeddings, we visualize them by reducing dimensionality to two dimensions with Uniform Manifold Approximation and Projection (UMAP). Compared to alternatives, such as t-SNE or PCA, UMAP more effectively preserves both local and global structure <sup id="fnref1:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. For UMAP, we employ the following essential hyperparameters:'>To evaluate performance, we encode the test dataset with AM and FinTextSim, comparing intra- and intertopic similarity (see
<a href=https://arxiv.org/html/2504.15683v1#S3.SS5.SSS2 title="3.5.2 Organizing Power ‣ 3.5 Evaluation Metrics ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3.5.2</a>). AM, the most downloaded model for sentence similarity tasks on Hugging Face, serves as a robust baseline for evaluating out-of-the-box sentence-transformer embeddings. By benchmarking FinTextSim against AM, we assess whether specialized contextual embeddings improve financial text analysis. We hypothesize that integrating domain-specific knowledge into contextual embeddings, will enable FinTextSim to generate more accurate and semantically meaningful representations of financial text than general-purpose models like AM. This improvement is crucial for precisely identifying meaningful financial topics and ensuring more reliable document organization. To further examine the structure of the learned embeddings, we visualize them by reducing dimensionality to two dimensions with Uniform Manifold Approximation and Projection (UMAP). Compared to alternatives, such as t-SNE or PCA, UMAP more effectively preserves both local and global structure <sup id=fnref1:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup>. For UMAP, we employ the following essential hyperparameters:</p><ul><li>Minimum distance: 0, to encourage closely grouped data points, facilitating the formation of clusters representing semantically similar documents.</li><li>Distance metric: Cosine similarity, widely used in NLP and similarity tasks.</li><li>n_neighbors: 100, prioritizing global structures in our data to identify overarching macrotopics as well as hierarchically lower-ranked microtopics <sup id=fnref:44><a href=#fn:44 class=footnote-ref role=doc-noteref>44</a></sup>.</li></ul><h3 id=34-model-creation>3.4 Model Creation
<a class=anchor href=#34-model-creation>#</a></h3><p data-raw='Since BERTopic relies on sentence transformers optimized for sentence-level input and assumes that each document belongs to a single topic <sup id="fnref6:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>, we focus exclusively on sentence-level processing. To achieve this, we tokenize the documents displayed in Table 
  <a href="https://arxiv.org/html/2504.15683v1#S3.T1" title="Table 1 ‣ 3.1 Dataset ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">1</a> into individual sentences. This approach is necessary as full MD&amp;A sections typically cover multiple topics. Treating entire documents as single-topic texts would yield misleading results. Additionally, sentence-transformer models like AM and FinTextSim are designed for short texts, with a processing limit of 256-word pieces. Longer inputs are truncated, risking the loss of critical information. By working at the sentence level, we ensure compatibility with these models while preserving topic integrity.'>Since BERTopic relies on sentence transformers optimized for sentence-level input and assumes that each document belongs to a single topic <sup id=fnref6:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>, we focus exclusively on sentence-level processing. To achieve this, we tokenize the documents displayed in Table
<a href=https://arxiv.org/html/2504.15683v1#S3.T1 title="Table 1 ‣ 3.1 Dataset ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">1</a> into individual sentences. This approach is necessary as full MD&amp;A sections typically cover multiple topics. Treating entire documents as single-topic texts would yield misleading results. Additionally, sentence-transformer models like AM and FinTextSim are designed for short texts, with a processing limit of 256-word pieces. Longer inputs are truncated, risking the loss of critical information. By working at the sentence level, we ensure compatibility with these models while preserving topic integrity.</p><p data-raw='Although BERTopic is known to leverage noise to create contextualized embeddings, we fit the models on sentence and refined sentence input. Specifically, we refine the sentences by retaining those containing at least one keyword from our list as well as the pre- and succeeding sentences to maintain context and relevance <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>. As a result, we obtained 687,959 sentences and 678,218 refined sentences.'>Although BERTopic is known to leverage noise to create contextualized embeddings, we fit the models on sentence and refined sentence input. Specifically, we refine the sentences by retaining those containing at least one keyword from our list as well as the pre- and succeeding sentences to maintain context and relevance <sup id=fnref:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup>. As a result, we obtained 687,959 sentences and 678,218 refined sentences.</p><p data-raw='We generate contextual embeddings for our dataset using FinTextSim and AM, applying each to BERTopic with identical models and hyperparameter settings to ensure that only the embedding choice influences performance. In terms of dimensionality reduction, we opt for UMAP due to its ability to preserve both global and local structures <sup id="fnref2:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup> and its scalability to large datasets <sup id="fnref1:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>. We specifically configure UMAP with the same settings as displayed in Section 
  <a href="https://arxiv.org/html/2504.15683v1#S3.SS3" title="3.3 FinTextSim ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3.3</a>. To strike a balance between clustering efficiency and information retention, we reduce the dimensionality to ten components. For clustering, we choose Hierarchical Densitiy-Based Spatial Clustering of Applications with Noise (HDBSCAN) to address the limitations of centroid-based clustering algorithms in NLP and to effectively discern relevant from irrelevant data. As a soft clustering algorithm, HDBSCAN models noise as outliers, ensuring that documents not fitting into any cluster are not forcibly assigned <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>. Furthermore, HDBSCAN is proficient in detecting clusters of varying sizes and shapes, simplifies hyperparameter choices, and demonstrates computational scalability <sup id="fnref1:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>. Our specific choices for HDBSCAN’s hyperparameters include:'>We generate contextual embeddings for our dataset using FinTextSim and AM, applying each to BERTopic with identical models and hyperparameter settings to ensure that only the embedding choice influences performance. In terms of dimensionality reduction, we opt for UMAP due to its ability to preserve both global and local structures <sup id=fnref2:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup> and its scalability to large datasets <sup id=fnref1:44><a href=#fn:44 class=footnote-ref role=doc-noteref>44</a></sup>. We specifically configure UMAP with the same settings as displayed in Section
<a href=https://arxiv.org/html/2504.15683v1#S3.SS3 title="3.3 FinTextSim ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3.3</a>. To strike a balance between clustering efficiency and information retention, we reduce the dimensionality to ten components. For clustering, we choose Hierarchical Densitiy-Based Spatial Clustering of Applications with Noise (HDBSCAN) to address the limitations of centroid-based clustering algorithms in NLP and to effectively discern relevant from irrelevant data. As a soft clustering algorithm, HDBSCAN models noise as outliers, ensuring that documents not fitting into any cluster are not forcibly assigned <sup id=fnref:46><a href=#fn:46 class=footnote-ref role=doc-noteref>46</a></sup>. Furthermore, HDBSCAN is proficient in detecting clusters of varying sizes and shapes, simplifies hyperparameter choices, and demonstrates computational scalability <sup id=fnref1:46><a href=#fn:46 class=footnote-ref role=doc-noteref>46</a></sup>. Our specific choices for HDBSCAN’s hyperparameters include:</p><ul><li>Minimum Cluster Size: 1,250 to prioritize global topics over local ones.</li><li>Minimum Number of Samples: 10, to reduce the number of outliers. The minimum number of samples determines the level of conservativity during clustering. If the number of samples is high, more data points are considered as noise, and the clustering is restricted to more dense areas.</li></ul><p data-raw='We utilize a CountVectorizer for vectorization and exclude stopwords based on <sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup> as well as infrequently occurring words. To extract relevant economic topics, we use c-tfidf weighting, reduce common words, and guide the model by incorporating seed words based on the keyword list with a multiplier of 50.'>We utilize a CountVectorizer for vectorization and exclude stopwords based on <sup id=fnref:47><a href=#fn:47 class=footnote-ref role=doc-noteref>47</a></sup> as well as infrequently occurring words. To extract relevant economic topics, we use c-tfidf weighting, reduce common words, and guide the model by incorporating seed words based on the keyword list with a multiplier of 50.</p><h3 id=35-evaluation-metrics>3.5 Evaluation Metrics
<a class=anchor href=#35-evaluation-metrics>#</a></h3><p data-raw='To compare the performance of the topic models, we focus on two fundamental tasks <sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>:'>To compare the performance of the topic models, we focus on two fundamental tasks <sup id=fnref1:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>:</p><ol><li>Topic Quality: Uncover hidden topics in a collection of documents.</li><li>Organizing Power: Organizing and structuring documents.</li></ol><p data-raw='The following subsections provide detail in how we objectively measure the model’s capabilities for each task and their applicability for the financial domain.'>The following subsections provide detail in how we objectively measure the model’s capabilities for each task and their applicability for the financial domain.</p><h4 id=351-topic-quality>3.5.1 Topic Quality
<a class=anchor href=#351-topic-quality>#</a></h4><p data-raw='We use NPMI coherence, known for its alignment with human judgment, to evaluate the quality of the generated topics. NPMI, a metric used in information theory and NLP, measures the strength of association between words by accounting for their individual frequencies. It is derived from Pointwise Mutual Information, which quantifies the difference between the actual co-occurrence of two terms and their expected co-occurrence if they were statistically independent. NPMI coherence employs a sliding window approach to establish context vectors based on word co-occurrences <sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>. Given the typical sentence lengths in our dataset, we set the window size to 20, optimizing context coverage.<sup>4</sup> <sup>4</sup> 4 For classical topic modeling approaches, which are evaluated in 
  <a href="https://arxiv.org/html/2504.15683v1#A4" title="Appendix D Results and Discussion - Classical Topic Modeling Approaches ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">D</a>, we maintain the default window size of ten. However, due to the shorter sentence lengths resulting from stopword removal in classical models, we adjust the window size for BERTopic based on the ratio between sentence lengts of BERTopic versus classical models, guaranteeing comparable context coverage.Moreover, to mitigate vocabulary divergence effects, we lemmatize both the input texts and the extracted topic representations. We use the five most representative words per topic as topic representations. While the standard procedure is to use ten representative words, using five to eight words is recommended to maintain clarity and conciseness <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>.'>We use NPMI coherence, known for its alignment with human judgment, to evaluate the quality of the generated topics. NPMI, a metric used in information theory and NLP, measures the strength of association between words by accounting for their individual frequencies. It is derived from Pointwise Mutual Information, which quantifies the difference between the actual co-occurrence of two terms and their expected co-occurrence if they were statistically independent. NPMI coherence employs a sliding window approach to establish context vectors based on word co-occurrences <sup id=fnref:48><a href=#fn:48 class=footnote-ref role=doc-noteref>48</a></sup>. Given the typical sentence lengths in our dataset, we set the window size to 20, optimizing context coverage.<sup>4</sup> <sup>4</sup> 4 For classical topic modeling approaches, which are evaluated in
<a href=https://arxiv.org/html/2504.15683v1#A4 title="Appendix D Results and Discussion - Classical Topic Modeling Approaches ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">D</a>, we maintain the default window size of ten. However, due to the shorter sentence lengths resulting from stopword removal in classical models, we adjust the window size for BERTopic based on the ratio between sentence lengts of BERTopic versus classical models, guaranteeing comparable context coverage.Moreover, to mitigate vocabulary divergence effects, we lemmatize both the input texts and the extracted topic representations. We use the five most representative words per topic as topic representations. While the standard procedure is to use ten representative words, using five to eight words is recommended to maintain clarity and conciseness <sup id=fnref:49><a href=#fn:49 class=footnote-ref role=doc-noteref>49</a></sup>.</p><h4 id=352-organizing-power>3.5.2 Organizing Power
<a class=anchor href=#352-organizing-power>#</a></h4><p data-raw='To assess the organizing power of FinTextSim and our topic models, we employ intra- and intertopic similarity. To ensure clear, distinct and well-separated topic clusters, we aim to maximize intratopic similarity while simultaneously minimizing intertopic similarity.'>To assess the organizing power of FinTextSim and our topic models, we employ intra- and intertopic similarity. To ensure clear, distinct and well-separated topic clusters, we aim to maximize intratopic similarity while simultaneously minimizing intertopic similarity.</p><p data-raw='We calculate intratopic similarity following this approach:'>We calculate intratopic similarity following this approach:</p><ul><li>Topic Embedding Calculation: Compute the mean of all sentence embeddings for each topic to obtain topic embeddings.</li><li>Cosine Similarity Calculation per Topic: Determine the cosine similarity of each sentence embedding within a topic to the corresponding topic embedding.</li><li>Mean Calculation within Topics: The mean of these cosine similarities provides the intratopic similarity, representing the cohesion within a topic.</li><li>Mean Calculation for all Topics: The mean of the intratopic similarities of each topic represents the model’s intratopic similarity.</li></ul><p data-raw='Intertopic similarity is computed as follows:'>Intertopic similarity is computed as follows:</p><ul><li>Topic Embedding Calculation: Compute the mean of all sentence embeddings for each topic to obtain topic embeddings.</li><li>Cosine Similarity Matrix Computation: Calculate the cosine similarity between each topic embedding and all other topic embeddings, forming a pairwise similarity matrix.</li><li>Extraction of Upper Triangle Values: Extract the upper triangle of the similarity matrix, excluding the self-similarity values in the diagonal.</li><li>Mean Calculation for all Topics: The mean of these cosine similarities results in the model’s intertopic similarity.</li></ul><p data-raw='For the evaluation of FinTextSim, we use the topic labels from the test dataset. As we do not have any ground-truth labels for BERTopic, we employ its topic assignments.'>For the evaluation of FinTextSim, we use the topic labels from the test dataset. As we do not have any ground-truth labels for BERTopic, we employ its topic assignments.</p><h4 id=353-applicability-for-the-financial-domain>3.5.3 Applicability for the Financial Domain
<a class=anchor href=#353-applicability-for-the-financial-domain>#</a></h4><p data-raw='To evaluate the topic models specifically for the financial domain, we incorporate a topic-precision-based weighting into the metrics for topic quality and organizing power, using the previously described keyword list (see Section 
  <a href="https://arxiv.org/html/2504.15683v1#S3.SS2" title="3.2 Keyword List ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3.2</a>). Specifically, we weigh NPMI Coherence and intratopic similarity by multiplying them with topic-precision, while intertopic similarity is adjusted by dividing it by topic-precision. We calculate topic-precision as follows:'>To evaluate the topic models specifically for the financial domain, we incorporate a topic-precision-based weighting into the metrics for topic quality and organizing power, using the previously described keyword list (see Section
<a href=https://arxiv.org/html/2504.15683v1#S3.SS2 title="3.2 Keyword List ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3.2</a>). Specifically, we weigh NPMI Coherence and intratopic similarity by multiplying them with topic-precision, while intertopic similarity is adjusted by dividing it by topic-precision. We calculate topic-precision as follows:</p><ul><li>Determine the dominant topic: A topic is classified as dominant if it contains at least two keywords from a single topic domain and no more than one keyword from another domain.</li><li>Count True Positives (TP): The number of keywords belonging to the dominant topic’s original domain.</li><li>Count False Positives (FP): The number of keywords from other topic domains. Keywords not present in the keyword list are ignored.</li><li>Compute topic-precision:<table><tbody><tr><td></td><td><math><semantics><mrow><mrow><mrow><mi>T</mi> <mo>⁢</mo> <mi>o</mi> <mo>⁢</mo> <mi>p</mi> <mo>⁢</mo> <mi>i</mi> <mo>⁢</mo> <mi>c</mi></mrow> <mo>−</mo> <mrow><mi>P</mi> <mo>⁢</mo> <mi>r</mi> <mo>⁢</mo> <mi>e</mi> <mo>⁢</mo> <mi>c</mi> <mo>⁢</mo> <mi>i</mi> <mo>⁢</mo> <mi>s</mi> <mo>⁢</mo> <mi>i</mi> <mo>⁢</mo> <mi>o</mi> <mo>⁢</mo> <mi>n</mi></mrow></mrow> <mo>=</mo> <mfrac><mrow><mi>T</mi> <mo>⁢</mo> <mi>P</mi></mrow> <mrow><mo>(</mo><mrow><mrow><mi>T</mi> <mo>⁢</mo> <mi>P</mi></mrow> <mo>+</mo> <mrow><mi>F</mi> <mo>⁢</mo> <mi>P</mi></mrow></mrow><mo>)</mo></mrow></mfrac></mrow> <annotation-xml><apply><apply><apply><ci>𝑇</ci> <ci>𝑜</ci> <ci>𝑝</ci> <ci>𝑖</ci> <ci>𝑐</ci></apply> <apply><ci>𝑃</ci> <ci>𝑟</ci> <ci>𝑒</ci> <ci>𝑐</ci> <ci>𝑖</ci> <ci>𝑠</ci> <ci>𝑖</ci> <ci>𝑜</ci> <ci>𝑛</ci></apply></apply> <apply><apply><ci>𝑇</ci> <ci>𝑃</ci></apply> <apply><apply><ci>𝑇</ci> <ci>𝑃</ci></apply> <apply><ci>𝐹</ci> <ci>𝑃</ci></apply></apply></apply></apply></annotation-xml> <annotation>Topic-Precision=\frac{TP}{(TP+FP)}</annotation> <annotation>italic_T italic_o italic_p italic_i italic_c - italic_P italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n = divide start_ARG italic_T italic_P end_ARG start_ARG ( italic_T italic_P + italic_F italic_P ) end_ARG</annotation></semantics></math></td><td></td><td rowspan=1><span>(1)</span></td></tr></tbody></table></li><li>Penalty for missing topics: If a topic from the keyword list is not captured, its topic-precision is set to 0.</li><li>Assess overall performance: The model’s ability to capture financial topics is evaluated by averaging topic-precision across all topics.</li></ul><p data-raw='This approach ensures that the detection of financial topics is the most crucial aspect of the topic models. If the identified topics lack financial relevance, their structural organization becomes obsolete. Moreover, if no financial topics are detected, the scores are penalized, reflecting the model’s unsuitability for financial text analysis.'>This approach ensures that the detection of financial topics is the most crucial aspect of the topic models. If the identified topics lack financial relevance, their structural organization becomes obsolete. Moreover, if no financial topics are detected, the scores are penalized, reflecting the model’s unsuitability for financial text analysis.</p><h2 id=4-results-and-discussion>4 Results and Discussion
<a class=anchor href=#4-results-and-discussion>#</a></h2><p data-raw='We structure the results and discussion section according to our research questions:'>We structure the results and discussion section according to our research questions:</p><ul><li>FinTextSim: Leveraging the quality of contextual embeddings for the financial domain</li><li>Topic Quality: Creating qualitative, coherent topic representations</li><li>Organizing Power: Organizing large textual datasets</li></ul><p data-raw='For FinTextSim, we highlight its result on the test dataset as well as its value in combination with BERTopic for topic modeling tasks. The results are presented and contextualized in the following subsections.'>For FinTextSim, we highlight its result on the test dataset as well as its value in combination with BERTopic for topic modeling tasks. The results are presented and contextualized in the following subsections.</p><h3 id=41-fintextsim---leveraging-contextual-embeddings-for-the-financial-domain>4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain
<a class=anchor href=#41-fintextsim---leveraging-contextual-embeddings-for-the-financial-domain>#</a></h3><p data-raw='FinTextSim generates improved clusters and notably reduces the number of outliers in comparison to AM. As illustrated in Figure 
  <a href="https://arxiv.org/html/2504.15683v1#S4.F1" title="Figure 1 ‣ 4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">1</a> and Table 
  <a href="https://arxiv.org/html/2504.15683v1#S4.T2" title="Table 2 ‣ 4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">2</a>, FinTextSim leads to a significant increase of intratopic similarity, while simultaneously reducing intertopic similarity compared to AM on the test dataset. Specifically, FinTextSim increases intratopic similarity by 81%, achieving a score of 0.9972, compared to 0.5498 for AM. Moreover, FinTextSim reduces intertopic similarity by 100%, resulting in a score of 0.002, whereas AM has a score of 0.4647. When combining BERTopic with AM, we identified 226,605 outliers. In contrast, FinTextSim generated 184,470 outliers, reducing the number of outliers by 19%.'>FinTextSim generates improved clusters and notably reduces the number of outliers in comparison to AM. As illustrated in Figure
<a href=https://arxiv.org/html/2504.15683v1#S4.F1 title="Figure 1 ‣ 4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">1</a> and Table
<a href=https://arxiv.org/html/2504.15683v1#S4.T2 title="Table 2 ‣ 4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">2</a>, FinTextSim leads to a significant increase of intratopic similarity, while simultaneously reducing intertopic similarity compared to AM on the test dataset. Specifically, FinTextSim increases intratopic similarity by 81%, achieving a score of 0.9972, compared to 0.5498 for AM. Moreover, FinTextSim reduces intertopic similarity by 100%, resulting in a score of 0.002, whereas AM has a score of 0.4647. When combining BERTopic with AM, we identified 226,605 outliers. In contrast, FinTextSim generated 184,470 outliers, reducing the number of outliers by 19%.</p><p data-raw='Table 2: FinTextSim vs. AM: Intra- and Intertopic Similarity on test dataset.'>Table 2: FinTextSim vs. AM: Intra- and Intertopic Similarity on test dataset.</p><table><thead><tr><th>Model</th><th>Intratopic Similarity</th><th>Intertopic Similarity</th><th>Outliers within BERTopic</th></tr></thead><tbody><tr><td>FinTextSim</td><td>0.9972</td><td>0.0002</td><td>184,470</td></tr><tr><td>AM</td><td>0.5498</td><td>0.4647</td><td>226,605</td></tr></tbody></table><p data-raw='
  <img src="https://arxiv.org/html/extracted/6377241/FinTextSim_v28_embeddings_testset.png" alt="Refer to caption" />'><img src=https://arxiv.org/html/extracted/6377241/FinTextSim_v28_embeddings_testset.png alt="Refer to caption"></p><p data-raw='(a) UMAP reduced sentence embeddings - FinTextSim.'>(a) UMAP reduced sentence embeddings - FinTextSim.</p><p data-raw='The results indicate that FinTextSim creates significantly better clusters of semantically similar concepts compared to AM. FinTextSim’s clusters are characterized by high intratopic similarity and low intertopic similarity. In contrast, AM fails to accurately detect topic specifics, resulting in an indistinguishable mash of data points (see Figure 
  <a href="https://arxiv.org/html/2504.15683v1#S4.F1" title="Figure 1 ‣ 4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">1</a>). This demonstrates that OTS sentence transformers are unsuitable for financial text.'>The results indicate that FinTextSim creates significantly better clusters of semantically similar concepts compared to AM. FinTextSim’s clusters are characterized by high intratopic similarity and low intertopic similarity. In contrast, AM fails to accurately detect topic specifics, resulting in an indistinguishable mash of data points (see Figure
<a href=https://arxiv.org/html/2504.15683v1#S4.F1 title="Figure 1 ‣ 4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">1</a>). This demonstrates that OTS sentence transformers are unsuitable for financial text.</p><p data-raw='
  <img src="https://arxiv.org/html/extracted/6377241/hr_doc_2050_bertopic.png" alt="Refer to caption" />'><img src=https://arxiv.org/html/extracted/6377241/hr_doc_2050_bertopic.png alt="Refer to caption"></p><p data-raw='Figure 2: Topic representations - FinTextSim vs. AM - HR. Original cleaned sentence: ’a majority of employees belong to labor unions’.'>Figure 2: Topic representations - FinTextSim vs. AM - HR. Original cleaned sentence: ’a majority of employees belong to labor unions’.</p><p data-raw='Turning to a practical example, BERTopic with FinTextSim is able to accurately capture the underlying topic of a sentence, while the results with AM are misleading. Figure 
  <a href="https://arxiv.org/html/2504.15683v1#S4.F2" title="Figure 2 ‣ 4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">2</a> displays word clouds for topics assigned to the same sentence using BERTopic with FinTextSim and AM. FinTextSim correctly identifies that the sentence focuses on ’HR and Employment’. In contrast, AM fails to detect the topic, leading the analyst to associate it incorrectly with revenue and products.'>Turning to a practical example, BERTopic with FinTextSim is able to accurately capture the underlying topic of a sentence, while the results with AM are misleading. Figure
<a href=https://arxiv.org/html/2504.15683v1#S4.F2 title="Figure 2 ‣ 4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">2</a> displays word clouds for topics assigned to the same sentence using BERTopic with FinTextSim and AM. FinTextSim correctly identifies that the sentence focuses on ’HR and Employment’. In contrast, AM fails to detect the topic, leading the analyst to associate it incorrectly with revenue and products.</p><h3 id=42-topic-quality>4.2 Topic Quality
<a class=anchor href=#42-topic-quality>#</a></h3><p data-raw='As displayed in Section 
  <a href="https://arxiv.org/html/2504.15683v1#S3.SS5" title="3.5 Evaluation Metrics ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3.5</a>, we use NPMI coherence weighted by topic-precision to objectively measure the topic quality of each topic model. Table  
  <a href="https://arxiv.org/html/2504.15683v1#S4.T3" title="Table 3 ‣ 4.2 Topic Quality ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3</a> displays the topic-precision scores for each topic model.'>As displayed in Section
<a href=https://arxiv.org/html/2504.15683v1#S3.SS5 title="3.5 Evaluation Metrics ‣ 3 Materials and Methods ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3.5</a>, we use NPMI coherence weighted by topic-precision to objectively measure the topic quality of each topic model. Table 
<a href=https://arxiv.org/html/2504.15683v1#S4.T3 title="Table 3 ‣ 4.2 Topic Quality ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3</a> displays the topic-precision scores for each topic model.</p><p data-raw='Table 3: Topic-Precision Scores.'>Table 3: Topic-Precision Scores.</p><table><thead><tr><th></th><th colspan=2>Type</th></tr><tr><th>Model</th><th>Sentences</th><th>Refined Sentences</th></tr></thead><tbody><tr><th>AM</th><td>0.311</td><td>0.684</td></tr><tr><th>FinTextSim</th><td>1.0</td><td>1.0</td></tr></tbody></table><p data-raw='Using BERTopic with FinTextSim significantly enhances the identification of economic topics. For sentence-level input, AM achieves a topic-precision of 0.311 while failing to detect ten out of 14 economic topics. Refining sentence input improves AM’s topic-precision to 0.684, reducing the number of missed topics to six. However, even with refinement, AM struggles to identify key financial themes such as Operations, Liquidity and Solvency, and Investment. In contrast, FinTextSim correctly identifies all 14 economic topics, achieving a perfect topic-precision of 1.0.<sup>5</sup> <sup>5</sup> 5 The wordclouds for each model are displayed in Figures  
  <a href="https://arxiv.org/html/2504.15683v1#A1.F6" title="Figure 6 ‣ Appendix A Wordclouds BERTopic models ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">6</a> to 
  <a href="https://arxiv.org/html/2504.15683v1#A1.F9" title="Figure 9 ‣ Appendix A Wordclouds BERTopic models ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">9</a>.'>Using BERTopic with FinTextSim significantly enhances the identification of economic topics. For sentence-level input, AM achieves a topic-precision of 0.311 while failing to detect ten out of 14 economic topics. Refining sentence input improves AM’s topic-precision to 0.684, reducing the number of missed topics to six. However, even with refinement, AM struggles to identify key financial themes such as Operations, Liquidity and Solvency, and Investment. In contrast, FinTextSim correctly identifies all 14 economic topics, achieving a perfect topic-precision of 1.0.<sup>5</sup> <sup>5</sup> 5 The wordclouds for each model are displayed in Figures 
<a href=https://arxiv.org/html/2504.15683v1#A1.F6 title="Figure 6 ‣ Appendix A Wordclouds BERTopic models ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">6</a> to
<a href=https://arxiv.org/html/2504.15683v1#A1.F9 title="Figure 9 ‣ Appendix A Wordclouds BERTopic models ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">9</a>.</p><p data-raw='Although refining sentence input improves AM’s topic precision, general-purpose embedding models are insufficient for comprehensive financial text analysis. AM models fail to capture the full scope of financial topics, limiting their practical application. In contrast, FinTextSim not only identifies all relevant topics but also minimizes conceptual overlap, ensuring clearer topic distinctions and more effective document organization. These findings underscore the necessity of domain-specific embeddings for financial text processing, as general-purpose models fail to capture the nuances of economic language.'>Although refining sentence input improves AM’s topic precision, general-purpose embedding models are insufficient for comprehensive financial text analysis. AM models fail to capture the full scope of financial topics, limiting their practical application. In contrast, FinTextSim not only identifies all relevant topics but also minimizes conceptual overlap, ensuring clearer topic distinctions and more effective document organization. These findings underscore the necessity of domain-specific embeddings for financial text processing, as general-purpose models fail to capture the nuances of economic language.</p><p data-raw='Table 4: Coherence Scores.'>Table 4: Coherence Scores.</p><table><thead><tr><th></th><th colspan=2>Type</th></tr><tr><th>Model</th><th>Sentences</th><th>Refined Sentences</th></tr></thead><tbody><tr><th>AM</th><td>0.106 (0.341)</td><td>0.257 (0.376)</td></tr><tr><th>FinTextSim</th><td>0.279 (0.279)</td><td>0.301 (0.301)</td></tr></tbody></table><p data-raw='Table 
  <a href="https://arxiv.org/html/2504.15683v1#S4.T4" title="Table 4 ‣ 4.2 Topic Quality ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">4</a> presents the coherence scores, with non-topic-precision weighted coherence shown in parentheses. When incorporating topic-precision weighting, we find that BERTopic generates higher-quality financial topics using FinTextSim compared to AM. For sentence-level input, FinTextSim achieves a topic-precision weighted coherence score of 0.279, surpassing AM’s 0.106 by 163%. With refined sentence input, coherence scores improve across both models. For AM, this increase is primarily attributed to its increased topic-precision. However, FinTextSim still outperforms AM by 17%.'>Table
<a href=https://arxiv.org/html/2504.15683v1#S4.T4 title="Table 4 ‣ 4.2 Topic Quality ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">4</a> presents the coherence scores, with non-topic-precision weighted coherence shown in parentheses. When incorporating topic-precision weighting, we find that BERTopic generates higher-quality financial topics using FinTextSim compared to AM. For sentence-level input, FinTextSim achieves a topic-precision weighted coherence score of 0.279, surpassing AM’s 0.106 by 163%. With refined sentence input, coherence scores improve across both models. For AM, this increase is primarily attributed to its increased topic-precision. However, FinTextSim still outperforms AM by 17%.</p><p data-raw='Contrary to our expectations, BERTopic achieves higher coherence with AM than with FinTextSim when topic-precision-based weighting is not applied. Although FinTextSim better captures the distinct structure of financial text, as reflected by its high topic-precision, it yields lower coherence scores. However, this result is misleading in the context of financial text analysis. AM generates significantly more outliers and fails to capture key economic topics, leading to a loss of valuable information, potentially compromising studies. We suspect that the higher coherence of BERTopic with AM results from the increased number of outliers, which simplifies the compression and generation of topics. Another factor is the vocabulary of the financial domain. Financial terms are often standalone words that do not necessarily co-occur within a sliding window. Therefore, coherence scores do not always align with human judgment of topic quality.<sup>6</sup> <sup>6</sup> 6 This trend is also observed for classical models (see 
  <a href="https://arxiv.org/html/2504.15683v1#A4" title="Appendix D Results and Discussion - Classical Topic Modeling Approaches ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">D</a>).Relying solely on standard coherence metrics without domain-specific weighting can be problematic. In financial text analysis, ensuring high topic-precision is crucial as the meaningful organization of domain-specific knowledge must take precedence over raw coherence scores. Since topic evaluation requires domain expertise and subjective interpretation <sup id="fnref7:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>, standard coherence metrics alone are insufficient. Without domain-specific weighting, coherence fails to reflect practical applicability, making topic-precision essential for capturing meaningful financial insights.'>Contrary to our expectations, BERTopic achieves higher coherence with AM than with FinTextSim when topic-precision-based weighting is not applied. Although FinTextSim better captures the distinct structure of financial text, as reflected by its high topic-precision, it yields lower coherence scores. However, this result is misleading in the context of financial text analysis. AM generates significantly more outliers and fails to capture key economic topics, leading to a loss of valuable information, potentially compromising studies. We suspect that the higher coherence of BERTopic with AM results from the increased number of outliers, which simplifies the compression and generation of topics. Another factor is the vocabulary of the financial domain. Financial terms are often standalone words that do not necessarily co-occur within a sliding window. Therefore, coherence scores do not always align with human judgment of topic quality.<sup>6</sup> <sup>6</sup> 6 This trend is also observed for classical models (see
<a href=https://arxiv.org/html/2504.15683v1#A4 title="Appendix D Results and Discussion - Classical Topic Modeling Approaches ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">D</a>).Relying solely on standard coherence metrics without domain-specific weighting can be problematic. In financial text analysis, ensuring high topic-precision is crucial as the meaningful organization of domain-specific knowledge must take precedence over raw coherence scores. Since topic evaluation requires domain expertise and subjective interpretation <sup id=fnref7:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>, standard coherence metrics alone are insufficient. Without domain-specific weighting, coherence fails to reflect practical applicability, making topic-precision essential for capturing meaningful financial insights.</p><p data-raw='
  <img src="https://arxiv.org/html/extracted/6377241/cost_doc_2065_bertopic.png" alt="Refer to caption" />'><img src=https://arxiv.org/html/extracted/6377241/cost_doc_2065_bertopic.png alt="Refer to caption"></p><p data-raw='Figure 3: Topic representations - FinTextSim vs. AM - Cost. Original cleaned sentence: ’business is vulnerable to fluctuations in fuel costs and disruptions in fuel supplies’.'>Figure 3: Topic representations - FinTextSim vs. AM - Cost. Original cleaned sentence: ’business is vulnerable to fluctuations in fuel costs and disruptions in fuel supplies’.</p><p data-raw='To illustrate this in a practical example, we refer to Figure 
  <a href="https://arxiv.org/html/2504.15683v1#S4.F3" title="Figure 3 ‣ 4.2 Topic Quality ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3</a>. Once again, FinTextSim correctly identifies the underlying ’Cost’ topic, while AM misclassifies it, associating it with currency and exchange rates. Despite this misclassification, AM receives a higher coherence score of 0.448 compared to FinTextSim’s 0.324.'>To illustrate this in a practical example, we refer to Figure
<a href=https://arxiv.org/html/2504.15683v1#S4.F3 title="Figure 3 ‣ 4.2 Topic Quality ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">3</a>. Once again, FinTextSim correctly identifies the underlying ’Cost’ topic, while AM misclassifies it, associating it with currency and exchange rates. Despite this misclassification, AM receives a higher coherence score of 0.448 compared to FinTextSim’s 0.324.</p><p data-raw='
  <img src="https://arxiv.org/html/extracted/6377241/operations_doc_1171_bertopic.png" alt="Refer to caption" />'><img src=https://arxiv.org/html/extracted/6377241/operations_doc_1171_bertopic.png alt="Refer to caption"></p><p data-raw='Figure 4: Topic representations - FinTextSim vs. AM - Operations. Original cleaned sentence: ’the company manufactures markets and distributes spices seasoning mixes condiments and other flavorful products to the entire food industry retailers food manufacturers and foodservice businesses’.'>Figure 4: Topic representations - FinTextSim vs. AM - Operations. Original cleaned sentence: ’the company manufactures markets and distributes spices seasoning mixes condiments and other flavorful products to the entire food industry retailers food manufacturers and foodservice businesses’.</p><p data-raw='For further clarity, Figure 
  <a href="https://arxiv.org/html/2504.15683v1#S4.F4" title="Figure 4 ‣ 4.2 Topic Quality ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">4</a> presents another practical example. FinTextSim correctly identifies the ’Operations’ topic, whereas AM misclassifies it as a non-economic concept. Yet, FinTextSim receives a lower coherence score of 0.205 compared to AM’s 0.262. These discrepancies underscore the limitations of coherence scores in evaluating financial text, as they fail to account for domain relevance and topic-precision.'>For further clarity, Figure
<a href=https://arxiv.org/html/2504.15683v1#S4.F4 title="Figure 4 ‣ 4.2 Topic Quality ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">4</a> presents another practical example. FinTextSim correctly identifies the ’Operations’ topic, whereas AM misclassifies it as a non-economic concept. Yet, FinTextSim receives a lower coherence score of 0.205 compared to AM’s 0.262. These discrepancies underscore the limitations of coherence scores in evaluating financial text, as they fail to account for domain relevance and topic-precision.</p><h3 id=43-organizing-power>4.3 Organizing Power
<a class=anchor href=#43-organizing-power>#</a></h3><p data-raw='To efficiently organize and structure large collections of documents, maximizing intratopic similarity while simultaneously minimizing intertopic similarity is desirable. The results for intratopic similarity of our models are displayed in Table 
  <a href="https://arxiv.org/html/2504.15683v1#S4.T5" title="Table 5 ‣ 4.3 Organizing Power ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">5</a>. Non-topic-precision weighted scores are illustrated in parentheses.'>To efficiently organize and structure large collections of documents, maximizing intratopic similarity while simultaneously minimizing intertopic similarity is desirable. The results for intratopic similarity of our models are displayed in Table
<a href=https://arxiv.org/html/2504.15683v1#S4.T5 title="Table 5 ‣ 4.3 Organizing Power ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">5</a>. Non-topic-precision weighted scores are illustrated in parentheses.</p><p data-raw='Table 5: Intratopic Similarity.'>Table 5: Intratopic Similarity.</p><table><thead><tr><th></th><th colspan=2>Type</th></tr><tr><th>Model</th><th>Sentences</th><th>Refined Sentences</th></tr></thead><tbody><tr><th>AM</th><td>0.206 (0.661)</td><td>0.441 (0.644)</td></tr><tr><th>FinTextSim</th><td>0.925 (0.925)</td><td>0.929 (0.929)</td></tr></tbody></table><p data-raw='FinTextSim consistently achieves higher intratopic similarity than AM, regardless of input type or the application of topic-precision weighting. When topic-precision weighting is considered, FinTextSim outperforms AM by a wide margin, achieving an intratopic similarity of 0.925 compared to 0.206 for sentence input, representing a 350% improvement. For refined sentence input, FinTextSim reaches 0.929, outperforming AM’s 0.441 by 111%. Even without topic-precision weighting, FinTextSim maintains a strong advantage, with a 40% increase for sentence input, scoring 0.925 compared to AM’s 0.661. The trend remains consistent for refined sentence input, where FinTextSim achieves 0.929, exceeding AM’s 0.644 by 44%. These results further highlight FinTextSim’s ability to generate more cohesive topic clusters, reinforcing its suitability for financial text analysis.'>FinTextSim consistently achieves higher intratopic similarity than AM, regardless of input type or the application of topic-precision weighting. When topic-precision weighting is considered, FinTextSim outperforms AM by a wide margin, achieving an intratopic similarity of 0.925 compared to 0.206 for sentence input, representing a 350% improvement. For refined sentence input, FinTextSim reaches 0.929, outperforming AM’s 0.441 by 111%. Even without topic-precision weighting, FinTextSim maintains a strong advantage, with a 40% increase for sentence input, scoring 0.925 compared to AM’s 0.661. The trend remains consistent for refined sentence input, where FinTextSim achieves 0.929, exceeding AM’s 0.644 by 44%. These results further highlight FinTextSim’s ability to generate more cohesive topic clusters, reinforcing its suitability for financial text analysis.</p><p data-raw='The intertopic similarities of the models are displayed in Table 
  <a href="https://arxiv.org/html/2504.15683v1#S4.T6" title="Table 6 ‣ 4.3 Organizing Power ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">6</a>. Non-topic-precision weighted scores are illustrated in parentheses. A good separation of the generated topics is represented by low intertopic similarities.'>The intertopic similarities of the models are displayed in Table
<a href=https://arxiv.org/html/2504.15683v1#S4.T6 title="Table 6 ‣ 4.3 Organizing Power ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">6</a>. Non-topic-precision weighted scores are illustrated in parentheses. A good separation of the generated topics is represented by low intertopic similarities.</p><p data-raw='Table 6: Intertopic Similarity.'>Table 6: Intertopic Similarity.</p><table><thead><tr><th></th><th colspan=2>Type</th></tr><tr><th>Model</th><th>Sentences</th><th>Refined Sentences</th></tr></thead><tbody><tr><th>AM</th><td>1.315 (0.409)</td><td>0.631 (0.432)</td></tr><tr><th>FinTextSim</th><td>0.064 (0.064)</td><td>0.066 (0.066)</td></tr></tbody></table><p data-raw='FinTextSim significantly reduces intertopic similarity compared to AM, indicating enhanced topic separation. When topic-precision weighting is applied, FinTextSim achieves a 95% reduction in intertopic similarity, scoring 0.064 compared to AM’s 1.315. With refined sentence input, intertopic similarity remains 90% lower, with FinTextSim at 0.066 compared to AM’s 0.631. Neglecting topic-precision weighting, FinTextSim continues to outperform AM by a wide margin, reducing intertopic similarity by 84% for sentence input, with scores of 0.064 versus 0.409. For refined sentence input, FinTextSim outperforms AM by 85%, where it achieves 0.066 compared to AM’s 0.432. These results underscore FinTextSim’s effectiveness in minimizing topic overlap, leading to clearer and more distinct financial topic clusters.'>FinTextSim significantly reduces intertopic similarity compared to AM, indicating enhanced topic separation. When topic-precision weighting is applied, FinTextSim achieves a 95% reduction in intertopic similarity, scoring 0.064 compared to AM’s 1.315. With refined sentence input, intertopic similarity remains 90% lower, with FinTextSim at 0.066 compared to AM’s 0.631. Neglecting topic-precision weighting, FinTextSim continues to outperform AM by a wide margin, reducing intertopic similarity by 84% for sentence input, with scores of 0.064 versus 0.409. For refined sentence input, FinTextSim outperforms AM by 85%, where it achieves 0.066 compared to AM’s 0.432. These results underscore FinTextSim’s effectiveness in minimizing topic overlap, leading to clearer and more distinct financial topic clusters.</p><p data-raw='
  <img src="https://arxiv.org/html/extracted/6377241/accounting_doc_698_bertopic.png" alt="Refer to caption" />'><img src=https://arxiv.org/html/extracted/6377241/accounting_doc_698_bertopic.png alt="Refer to caption"></p><p data-raw='Figure 5: Topic representations - FinTextSim vs. AM - Accounting. Original cleaned sentence: ’critical accounting policies estimates and judgments our consolidated financial statements are based on gaap which requires us to make estimates and assumptions about future events that affect the amounts reported in our consolidated financial statements’.'>Figure 5: Topic representations - FinTextSim vs. AM - Accounting. Original cleaned sentence: ’critical accounting policies estimates and judgments our consolidated financial statements are based on gaap which requires us to make estimates and assumptions about future events that affect the amounts reported in our consolidated financial statements’.</p><p data-raw='Figure 
  <a href="https://arxiv.org/html/2504.15683v1#S4.F5" title="Figure 5 ‣ 4.3 Organizing Power ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">5</a> illustrates this situation. In this practical case, BERTopic with FinTextSim can accurately capture the underlying topic of the sentence, while the results with AM are misleading. FinTextSim correctly recognizes that the sentence pertains to ’Accounting,’ ensuring a precise and domain-relevant topic assignment. In contrast, AM fails to detect the specific topic, causing the analyst to misinterpret the sentence as relating to multiple themes, such as cost, tax, and HR. This inability to distinguish between economic topics results in high intertopic similarity and low intratopic similarity, highlighting the limitations of AM for financial text analysis.'>Figure
<a href=https://arxiv.org/html/2504.15683v1#S4.F5 title="Figure 5 ‣ 4.3 Organizing Power ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">5</a> illustrates this situation. In this practical case, BERTopic with FinTextSim can accurately capture the underlying topic of the sentence, while the results with AM are misleading. FinTextSim correctly recognizes that the sentence pertains to ’Accounting,’ ensuring a precise and domain-relevant topic assignment. In contrast, AM fails to detect the specific topic, causing the analyst to misinterpret the sentence as relating to multiple themes, such as cost, tax, and HR. This inability to distinguish between economic topics results in high intertopic similarity and low intratopic similarity, highlighting the limitations of AM for financial text analysis.</p><h3 id=44-wrapup-of-results-and-discussion>4.4 Wrapup of Results and Discussion
<a class=anchor href=#44-wrapup-of-results-and-discussion>#</a></h3><p data-raw='We find that BERTopic is highly on financial text when combined with FinTextSim. AM, on the other hand, generates more general topics and fails to capture economic topics, leading to significant gaps in coverage. The quality of topics improves significantly when FinTextSim is used. Only in combination with FinTextSim, BERTopic produce clear, distinct clusters of economic topics. In contrast, AM leads to frequent misclassifications.'>We find that BERTopic is highly on financial text when combined with FinTextSim. AM, on the other hand, generates more general topics and fails to capture economic topics, leading to significant gaps in coverage. The quality of topics improves significantly when FinTextSim is used. Only in combination with FinTextSim, BERTopic produce clear, distinct clusters of economic topics. In contrast, AM leads to frequent misclassifications.</p><p data-raw='Our findings support the hypothesis of <sup id="fnref1:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>, demonstrating that a fine-tuned model grounded in a specialized dataset significantly improves both performance and domain-specific understanding. Furthermore, as <sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup> indicates, finetuning a foundational base model enhances performance on complex tasks. Relying solely on OTS models may compromise reliability and introduce systematic errors, highlighting the importance of integrating fine-tuned models like FinTextSim for extracting meaningful and reliable insights. However, the extent to which FinTextSim generalizes beyond 10-K reports remains an open question. A small-scale experiment on Item 1 yielded similar results as described in Section 
  <a href="https://arxiv.org/html/2504.15683v1#S4.SS1" title="4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">4.1</a>, suggesting that FinTextSim’s effectiveness extends to other sections of 10-K filings. Expanding its training data to include diverse financial sources, such as news articles, conference call transcripts, and analyst reports, could further enhance its generalization capabilities. Additionally, incorporating researcher-labeled data may provide further improvements in FinTextSim’s adaptability and robustness across financial contexts.'>Our findings support the hypothesis of <sup id=fnref1:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>, demonstrating that a fine-tuned model grounded in a specialized dataset significantly improves both performance and domain-specific understanding. Furthermore, as <sup id=fnref:50><a href=#fn:50 class=footnote-ref role=doc-noteref>50</a></sup> indicates, finetuning a foundational base model enhances performance on complex tasks. Relying solely on OTS models may compromise reliability and introduce systematic errors, highlighting the importance of integrating fine-tuned models like FinTextSim for extracting meaningful and reliable insights. However, the extent to which FinTextSim generalizes beyond 10-K reports remains an open question. A small-scale experiment on Item 1 yielded similar results as described in Section
<a href=https://arxiv.org/html/2504.15683v1#S4.SS1 title="4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain ‣ 4 Results and Discussion ‣ FinTextSim: Enhancing Financial Text Analysis with BERTopic">4.1</a>, suggesting that FinTextSim’s effectiveness extends to other sections of 10-K filings. Expanding its training data to include diverse financial sources, such as news articles, conference call transcripts, and analyst reports, could further enhance its generalization capabilities. Additionally, incorporating researcher-labeled data may provide further improvements in FinTextSim’s adaptability and robustness across financial contexts.</p><p data-raw='Regarding our results, it is important to note that the displayed metrics should never be considered in isolation, especially those regarding organizing power. For instance, even if AM would show high intratopic and low intertopic similarity, it does not necessarily produce ’good’ clusters, as the quality of the generated topics may remain low. In such cases, its ability to enhance organizational clarity would still be limited. Therefore, evaluating topics requires looking beyond the raw metrics to consider their true quality.'>Regarding our results, it is important to note that the displayed metrics should never be considered in isolation, especially those regarding organizing power. For instance, even if AM would show high intratopic and low intertopic similarity, it does not necessarily produce ’good’ clusters, as the quality of the generated topics may remain low. In such cases, its ability to enhance organizational clarity would still be limited. Therefore, evaluating topics requires looking beyond the raw metrics to consider their true quality.</p><p data-raw='Hence, evaluating topic models remains challenging <sup id="fnref1:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Our analysis reveals the limitations of coherence as a measure. For instance, BERTopic with AM achieves higher coherence scores than FinTextSim. Yet, we identified low topic-precision scores, indicating numerous missing economic topics and/or overlapping concepts. This suggests that higher coherence does not necessarily correlate with higher topic quality. Our findings underscore the necessity for new coherence or topic quality measures, particularly for domain-specific texts like finance. In such texts, topic words often stand alone and may not co-occur within a sliding window. Hence, traditional coherence metrics cannot capture the ’true’ quality of the generated topics.'>Hence, evaluating topic models remains challenging <sup id=fnref1:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>. Our analysis reveals the limitations of coherence as a measure. For instance, BERTopic with AM achieves higher coherence scores than FinTextSim. Yet, we identified low topic-precision scores, indicating numerous missing economic topics and/or overlapping concepts. This suggests that higher coherence does not necessarily correlate with higher topic quality. Our findings underscore the necessity for new coherence or topic quality measures, particularly for domain-specific texts like finance. In such texts, topic words often stand alone and may not co-occur within a sliding window. Hence, traditional coherence metrics cannot capture the ’true’ quality of the generated topics.</p><p data-raw='While BERTopic enhances topic modeling compared to the classical approaches, there is still significant room for improvement. The transformer architecture, which BERTopic heavily relies on, may not be fully optimized yet. Thus, more sophisticated and computationally efficient alternatives should be explored <sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>. Further advancements in encoder-only models could enhance sentence transformers by improving their contextual understanding of language <sup id="fnref2:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. Moreover, applying domain-specific pre-training methods to optimized BERT variants may deepen the model’s understanding of financial language, leading to more effective downstream task performance <sup id="fnref1:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. Additionally, BERTopic’s inconsistency in producing meaningful results, compounded by the complex hyperparameters of the underlying models, compromises reliability <sup id="fnref2:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>. Hence, future research should focus on developing an objective standard for selecting models and tuning hyperparameters. Specifically, we plan to investigate the impact of hyperparameter tuning for both dimensionality reduction and clustering techniques on contextual embeddings. This approach aims to streamline the process of topic modeling, objectively determining hyperparameters. Eventually, this will improve topic clustering and extraction, thus enhancing the analysis of textual data.'>While BERTopic enhances topic modeling compared to the classical approaches, there is still significant room for improvement. The transformer architecture, which BERTopic heavily relies on, may not be fully optimized yet. Thus, more sophisticated and computationally efficient alternatives should be explored <sup id=fnref:51><a href=#fn:51 class=footnote-ref role=doc-noteref>51</a></sup>. Further advancements in encoder-only models could enhance sentence transformers by improving their contextual understanding of language <sup id=fnref2:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>. Moreover, applying domain-specific pre-training methods to optimized BERT variants may deepen the model’s understanding of financial language, leading to more effective downstream task performance <sup id=fnref1:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup>. Additionally, BERTopic’s inconsistency in producing meaningful results, compounded by the complex hyperparameters of the underlying models, compromises reliability <sup id=fnref2:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup>. Hence, future research should focus on developing an objective standard for selecting models and tuning hyperparameters. Specifically, we plan to investigate the impact of hyperparameter tuning for both dimensionality reduction and clustering techniques on contextual embeddings. This approach aims to streamline the process of topic modeling, objectively determining hyperparameters. Eventually, this will improve topic clustering and extraction, thus enhancing the analysis of textual data.</p><h2 id=5-conclusion>5 Conclusion
<a class=anchor href=#5-conclusion>#</a></h2><p data-raw='Increased availability of information and enhanced computational capabilities have transformed the analysis of annual reports, recognizing the value embedded within qualitative textual data. Automated review processes, such as topic modeling, are crucial for analyzing this data. However, in our domain, the use of those ML-based methods, including contextual embeddings, remains underexplored <sup id="fnref2:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. We address these issues by introducing FinTextSim, a finetuned sentence transformer enhancing analysis of financial text with BERTopic.'>Increased availability of information and enhanced computational capabilities have transformed the analysis of annual reports, recognizing the value embedded within qualitative textual data. Automated review processes, such as topic modeling, are crucial for analyzing this data. However, in our domain, the use of those ML-based methods, including contextual embeddings, remains underexplored <sup id=fnref2:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup>. We address these issues by introducing FinTextSim, a finetuned sentence transformer enhancing analysis of financial text with BERTopic.</p><p data-raw='Our study reveals the significant advantages of FinTextSim over OTS sentence-transformer models. FinTextSim excels in generating distinct clusters of topics, substantially outperforming OTS sentence-transformer models on financial text. This highlights the need for domain-specifically finetuned sentence-transformer models. Additionally, FinTextSim allows BERTopic to identify the most qualitative topics for the financial domain. While FinTextSim captures all relevant financial topics, OTS sentence-transformers miss valuable information. Combining BERTopic with FinTextSim notably enhances its capability to create well-separated clusters of economic topics. Hence, the domain-specific fine-tuning of sentence transformers is crucial for achieving optimal topic modeling outcomes.'>Our study reveals the significant advantages of FinTextSim over OTS sentence-transformer models. FinTextSim excels in generating distinct clusters of topics, substantially outperforming OTS sentence-transformer models on financial text. This highlights the need for domain-specifically finetuned sentence-transformer models. Additionally, FinTextSim allows BERTopic to identify the most qualitative topics for the financial domain. While FinTextSim captures all relevant financial topics, OTS sentence-transformers miss valuable information. Combining BERTopic with FinTextSim notably enhances its capability to create well-separated clusters of economic topics. Hence, the domain-specific fine-tuning of sentence transformers is crucial for achieving optimal topic modeling outcomes.</p><p data-raw='Through our efforts, we make several significant contributions. First, we enhance contextual embeddings for our domain with FinTextSim, amplifying results and insights from future studies. Moreover, FinTextSim improves the quality of financial information, empowering stakeholders to gain a competitive advantage through more efficient allocation of resources and improved decision-making processes. Furthermore, integrating FinTextSim into business valuation and stock price prediction models promises enhancements in accuracy and effectiveness, providing valuable tools for financial analysts and investors alike.'>Through our efforts, we make several significant contributions. First, we enhance contextual embeddings for our domain with FinTextSim, amplifying results and insights from future studies. Moreover, FinTextSim improves the quality of financial information, empowering stakeholders to gain a competitive advantage through more efficient allocation of resources and improved decision-making processes. Furthermore, integrating FinTextSim into business valuation and stock price prediction models promises enhancements in accuracy and effectiveness, providing valuable tools for financial analysts and investors alike.</p><p data-raw='It is important to note that the evaluation of topic models remains a challenging task. Assessing each model’s performance requires considering all presented metrics simultaneously, as relying on one measure in isolation may be misleading. Moreover, a qualitative analysis of topic representations is necessary.'>It is important to note that the evaluation of topic models remains a challenging task. Assessing each model’s performance requires considering all presented metrics simultaneously, as relying on one measure in isolation may be misleading. Moreover, a qualitative analysis of topic representations is necessary.</p><p data-raw='Although BERTopic outperforms classical approaches, significant room for advancements in the field of topic modeling and their applications in the financial domain remains. Interesting directions for future research include the creation of an improved metric to objectively measure the quality of generated topic representations, particularly for domain-specific text like finance. Additionally, advancing the architecture of sentence transformers or exploring new embedding techniques holds promise for further enhancement of topic modeling. Moreover, there is a need for streamlining processes to determine optimal models and hyperparameters within BERTopic. Through this endeavor, topic instability will be addressed, enhancing the reliability and validity of results. Integrating contemporary topic modeling approaches like BERTopic with FinTextSim could offer substantial benefits to new and ongoing studies, leveraging their results. Finally, harnessing the improved topic extraction and organizational structure generated with BERTopic in combination with FinTextSim holds the potential to significantly enhance business valuation and stock price prediction models, providing valuable insights for financial analysts and investors.'>Although BERTopic outperforms classical approaches, significant room for advancements in the field of topic modeling and their applications in the financial domain remains. Interesting directions for future research include the creation of an improved metric to objectively measure the quality of generated topic representations, particularly for domain-specific text like finance. Additionally, advancing the architecture of sentence transformers or exploring new embedding techniques holds promise for further enhancement of topic modeling. Moreover, there is a need for streamlining processes to determine optimal models and hyperparameters within BERTopic. Through this endeavor, topic instability will be addressed, enhancing the reliability and validity of results. Integrating contemporary topic modeling approaches like BERTopic with FinTextSim could offer substantial benefits to new and ongoing studies, leveraging their results. Finally, harnessing the improved topic extraction and organizational structure generated with BERTopic in combination with FinTextSim holds the potential to significantly enhance business valuation and stock price prediction models, providing valuable insights for financial analysts and investors.</p><h2 id=references>References
<a class=anchor href=#references>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p data-raw='Gupta, A., Dengre, V., Kheruwala, H.A., Shah, M., 2020.Comprehensive review of text-mining applications in finance.Financial Innovation 6, 1–25.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Gupta, A., Dengre, V., Kheruwala, H.A., Shah, M., 2020.Comprehensive review of text-mining applications in finance.Financial Innovation 6, 1–25.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p data-raw='Griffin, P.A., 2003.Got information? investor response to form 10-k and form 10-q edgar filings.Review of Accounting Studies 8, 433–460.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Griffin, P.A., 2003.Got information? investor response to form 10-k and form 10-q edgar filings.Review of Accounting Studies 8, 433–460.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p data-raw='Masson, C., Paroubek, P., 2020.Nlp analytics in finance with dore: a french 257m tokens corpus of corporate annual reports, in: Language Resources and Evaluation Conference (LREC 2020), ELRA. pp. 2261–2267.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Masson, C., Paroubek, P., 2020.Nlp analytics in finance with dore: a french 257m tokens corpus of corporate annual reports, in: Language Resources and Evaluation Conference (LREC 2020), ELRA. pp. 2261–2267.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p data-raw='You, H., Zhang, X.j., 2009.Financial reporting complexity and investor underreaction to 10-k information.Review of Accounting studies 14, 559–586.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>You, H., Zhang, X.j., 2009.Financial reporting complexity and investor underreaction to 10-k information.Review of Accounting studies 14, 559–586.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p data-raw='Liu, M., 2022.Assessing human information processing in lending decisions: A machine learning approach.Journal of Accounting Research 60, 607–651.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Liu, M., 2022.Assessing human information processing in lending decisions: A machine learning approach.Journal of Accounting Research 60, 607–651.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p data-raw='Cohen, L., Malloy, C., Nguyen, Q., 2020.Lazy prices.The Journal of Finance 75, 1371–1415.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref2:6" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Cohen, L., Malloy, C., Nguyen, Q., 2020.Lazy prices.The Journal of Finance 75, 1371–1415.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p data-raw='Hsieh, H.T., Hristova, D., 2022.Transformer-based summarization and sentiment analysis of sec 10-k annual reports for company performance prediction, in: Proceedings of the 55th Hawaii International Conference on System Sciences, Hawaii International Conference on System Sciences. pp. 1759–1768.URL: 
  <a href="https://hdl.handle.net/10125/79550">https://hdl.handle.net/10125/79550</a>, doi:
  <a href="http://dx.doi.org/10.24251/hicss.2022.218">10.24251/hicss.2022.218</a>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Hsieh, H.T., Hristova, D., 2022.Transformer-based summarization and sentiment analysis of sec 10-k annual reports for company performance prediction, in: Proceedings of the 55th Hawaii International Conference on System Sciences, Hawaii International Conference on System Sciences. pp. 1759–1768.URL:
<a href=https://hdl.handle.net/10125/79550>https://hdl.handle.net/10125/79550</a>, doi:
<a href=http://dx.doi.org/10.24251/hicss.2022.218>10.24251/hicss.2022.218</a>.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p data-raw='Dutta, S., Fuksa, M., Macaulay, K., 2019.Determinants of md&amp;a sentiment in canada.International Review of Economics & Finance 60, 130–148.URL: 
  <a href="https://www.sciencedirect.com/science/article/pii/S105905601630332X">https://www.sciencedirect.com/science/article/pii/S105905601630332X</a>, doi:
  <a href="http://dx.doi.org/https://doi.org/10.1016/j.iref.2018.12.017">https://doi.org/10.1016/j.iref.2018.12.017</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Dutta, S., Fuksa, M., Macaulay, K., 2019.Determinants of md&amp;a sentiment in canada.International Review of Economics & Finance 60, 130–148.URL:
<a href=https://www.sciencedirect.com/science/article/pii/S105905601630332X>https://www.sciencedirect.com/science/article/pii/S105905601630332X</a>, doi:
<a href=http://dx.doi.org/https://doi.org/10.1016/j.iref.2018.12.017>https://doi.org/10.1016/j.iref.2018.12.017</a>.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p data-raw='Li, F., 2010a.The Information Content of Forward-Looking Statements in Corporate Filings—A Naïve Bayesian Machine Learning Approach.Journal of Accounting Research 48, 1049–1102.doi:
  <a href="http://dx.doi.org/10.1111/j.1475-679X.2010.00382.x">10.1111/j.1475-679X.2010.00382.x</a>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:9" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref2:9" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Li, F., 2010a.The Information Content of Forward-Looking Statements in Corporate Filings—A Naïve Bayesian Machine Learning Approach.Journal of Accounting Research 48, 1049–1102.doi:
<a href=http://dx.doi.org/10.1111/j.1475-679X.2010.00382.x>10.1111/j.1475-679X.2010.00382.x</a>.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p data-raw='Lu, J., 2022.Limited attention: Implications for financial reporting.Journal of Accounting Research 60, 1991–2027.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Lu, J., 2022.Limited attention: Implications for financial reporting.Journal of Accounting Research 60, 1991–2027.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p data-raw='Blei, D.M., Ng, A.Y., Jordan, M.I., 2003.Latent dirichlet allocation.Journal of machine Learning research 3, 993–1022.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Blei, D.M., Ng, A.Y., Jordan, M.I., 2003.Latent dirichlet allocation.Journal of machine Learning research 3, 993–1022.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p data-raw='Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., Harshman, R., 1990.Indexing by latent semantic analysis.Journal of the American society for information science 41, 391–407.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., Harshman, R., 1990.Indexing by latent semantic analysis.Journal of the American society for information science 41, 391–407.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p data-raw='Blair, S.J., Bi, Y., Mulvenna, M.D., 2020.Aggregated topic models for increasing social media topic coherence.Applied Intelligence 50, 138–156.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:13" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Blair, S.J., Bi, Y., Mulvenna, M.D., 2020.Aggregated topic models for increasing social media topic coherence.Applied Intelligence 50, 138–156.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p data-raw='Booker, A., Chiu, V., Groff, N., Richardson, V.J., 2024.Ais research opportunities utilizing machine learning: From a meta-theory of accounting literature.International Journal of Accounting Information Systems 52, 100661.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Booker, A., Chiu, V., Groff, N., Richardson, V.J., 2024.Ais research opportunities utilizing machine learning: From a meta-theory of accounting literature.International Journal of Accounting Information Systems 52, 100661.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p data-raw='Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017.Attention Is All You Need, in: 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. pp. 1–15.
  <a href="http://arxiv.org/abs/1706.03762">arXiv:1706.03762</a>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017.Attention Is All You Need, in: 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. pp. 1–15.
<a href=http://arxiv.org/abs/1706.03762>arXiv:1706.03762</a>.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p data-raw='Caron, M., Müller, O., 2020.Hardening soft information: A transformer-based approach to forecasting stock return volatility, in: 2020 IEEE International Conference on Big Data (Big Data), IEEE. pp. 4383–4391.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Caron, M., Müller, O., 2020.Hardening soft information: A transformer-based approach to forecasting stock return volatility, in: 2020 IEEE International Conference on Big Data (Big Data), IEEE. pp. 4383–4391.&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p data-raw='Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019.Bert: Pre-training of deep bidirectional transformers for language understanding. arxiv.arXiv preprint arXiv:1810.04805.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019.Bert: Pre-training of deep bidirectional transformers for language understanding. arxiv.arXiv preprint arXiv:1810.04805.&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p data-raw='Warner, B., Chaffin, A., Clavié, B., Weller, O., Hallström, O., Taghadouini, S., Gallagher, A., Biswas, R., Ladhak, F., Aarsen, T., et al., 2024.Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference.arXiv preprint arXiv:2412.13663.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:18" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref2:18" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Warner, B., Chaffin, A., Clavié, B., Weller, O., Hallström, O., Taghadouini, S., Gallagher, A., Biswas, R., Ladhak, F., Aarsen, T., et al., 2024.Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference.arXiv preprint arXiv:2412.13663.&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p data-raw='Reimers, N., Gurevych, I., 2019.Sentence-bert: Sentence embeddings using siamese bert-networks.arXiv preprint arXiv:1908.10084.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref2:19" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref3:19" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref4:19" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Reimers, N., Gurevych, I., 2019.Sentence-bert: Sentence embeddings using siamese bert-networks.arXiv preprint arXiv:1908.10084.&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p data-raw='Grootendorst, M., 2022.Bertopic: Neural topic modeling with a class-based tf-idf procedure.arXiv preprint arXiv:2203.05794.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:20" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref2:20" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref3:20" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref4:20" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref5:20" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref6:20" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref7:20" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Grootendorst, M., 2022.Bertopic: Neural topic modeling with a class-based tf-idf procedure.arXiv preprint arXiv:2203.05794.&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref5:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref6:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref7:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p data-raw='Bhattacharya, I., Mickovic, A., 2024.Accounting fraud detection using contextual language learning.International Journal of Accounting Information Systems 53, 100682.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:21" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Bhattacharya, I., Mickovic, A., 2024.Accounting fraud detection using contextual language learning.International Journal of Accounting Information Systems 53, 100682.&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p data-raw='Huang, A.H., Wang, H., Yang, Y., 2023.Finbert: A large language model for extracting information from financial text.Contemporary Accounting Research 40, 806–841.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:22" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Huang, A.H., Wang, H., Yang, Y., 2023.Finbert: A large language model for extracting information from financial text.Contemporary Accounting Research 40, 806–841.&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p data-raw='Egger, R., Yu, J., 2021.Identifying hidden semantic structures in instagram data: a topic modelling comparison.Tourism Review 77, 1234–1246.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:23" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Egger, R., Yu, J., 2021.Identifying hidden semantic structures in instagram data: a topic modelling comparison.Tourism Review 77, 1234–1246.&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p data-raw='Albalawi, R., Yeap, T.H., Benyoucef, M., 2020.Using topic modeling methods for short-text data: A comparative analysis.Frontiers in artificial intelligence 3, 42.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Albalawi, R., Yeap, T.H., Benyoucef, M., 2020.Using topic modeling methods for short-text data: A comparative analysis.Frontiers in artificial intelligence 3, 42.&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p data-raw='Ranta, M., Ylinen, M., Järvenpää, M., 2022.Machine Learning in Management Accounting Research: Literature Review and Pathways for the Future.European Accounting Review, 1–30doi:
  <a href="http://dx.doi.org/10.1080/09638180.2022.2137221">10.1080/09638180.2022.2137221</a>.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref2:25" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Ranta, M., Ylinen, M., Järvenpää, M., 2022.Machine Learning in Management Accounting Research: Literature Review and Pathways for the Future.European Accounting Review, 1–30doi:
<a href=http://dx.doi.org/10.1080/09638180.2022.2137221>10.1080/09638180.2022.2137221</a>.&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p data-raw='Murphy, B., Feeney, O., Rosati, P., Lynn, T., 2024.Exploring accounting and ai using topic modelling.International Journal of Accounting Information Systems 55, 100709.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Murphy, B., Feeney, O., Rosati, P., Lynn, T., 2024.Exploring accounting and ai using topic modelling.International Journal of Accounting Information Systems 55, 100709.&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p data-raw='Dong, M.M., Stratopoulos, T.C., Wang, V.X., 2024.A scoping review of chatgpt research in accounting and finance.International Journal of Accounting Information Systems 55, 100715.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:27" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Dong, M.M., Stratopoulos, T.C., Wang, V.X., 2024.A scoping review of chatgpt research in accounting and finance.International Journal of Accounting Information Systems 55, 100715.&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28><p data-raw='Zhao, H., Phung, D., Huynh, V., Jin, Y., Du, L., Buntine, W., 2021.Topic modelling meets deep neural networks: A survey.arXiv preprint arXiv:2103.00498.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:28" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Zhao, H., Phung, D., Huynh, V., Jin, Y., Du, L., Buntine, W., 2021.Topic modelling meets deep neural networks: A survey.arXiv preprint arXiv:2103.00498.&#160;<a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29><p data-raw='Abdelrazek, A., Eid, Y., Gawish, E., Medhat, W., Hassan, A., 2023.Topic modeling algorithms and applications: A survey.Information Systems 112, 102131.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:29" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref2:29" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Abdelrazek, A., Eid, Y., Gawish, E., Medhat, W., Hassan, A., 2023.Topic modeling algorithms and applications: A survey.Information Systems 112, 102131.&#160;<a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30><p data-raw='Wu, X., Nguyen, T., Luu, A.T., 2024.A survey on neural topic models: methods, applications, and challenges.Artificial Intelligence Review 57, 18.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Wu, X., Nguyen, T., Luu, A.T., 2024.A survey on neural topic models: methods, applications, and challenges.Artificial Intelligence Review 57, 18.&#160;<a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31><p data-raw='Sia, S., Dalmia, A., Mielke, S.J., 2020.Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Online. pp. 1728–1736.doi:
  <a href="http://dx.doi.org/10.18653/v1/2020.emnlp-main.135">10.18653/v1/2020.emnlp-main.135</a>.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Sia, S., Dalmia, A., Mielke, S.J., 2020.Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Online. pp. 1728–1736.doi:
<a href=http://dx.doi.org/10.18653/v1/2020.emnlp-main.135>10.18653/v1/2020.emnlp-main.135</a>.&#160;<a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32><p data-raw='Allaoui, M., Kherfi, M.L., Cheriet, A., 2020.Considerably improving clustering algorithms using umap dimensionality reduction technique: a comparative study, in: International conference on image and signal processing, Springer. pp. 317–325.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:32" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref2:32" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Allaoui, M., Kherfi, M.L., Cheriet, A., 2020.Considerably improving clustering algorithms using umap dimensionality reduction technique: a comparative study, in: International conference on image and signal processing, Springer. pp. 317–325.&#160;<a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:33><p data-raw='Egger, R., Yu, J., 2022.A topic modeling comparison between lda, nmf, top2vec, and bertopic to demystify twitter posts.Frontiers in sociology 7, 886498.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:33" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Egger, R., Yu, J., 2022.A topic modeling comparison between lda, nmf, top2vec, and bertopic to demystify twitter posts.Frontiers in sociology 7, 886498.&#160;<a href=#fnref:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:34><p data-raw='Kim, M.G., Kim, K.S., Lee, K.C., 2022.Analyzing the effects of topics underlying companies’ financial disclosures about risk factors on prediction of esg risk ratings: Emphasis on bertopic, in: 2022 IEEE International Conference on Big Data (Big Data), IEEE. pp. 4520–4527.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Kim, M.G., Kim, K.S., Lee, K.C., 2022.Analyzing the effects of topics underlying companies’ financial disclosures about risk factors on prediction of esg risk ratings: Emphasis on bertopic, in: 2022 IEEE International Conference on Big Data (Big Data), IEEE. pp. 4520–4527.&#160;<a href=#fnref:34 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:35><p data-raw='Sánchez-Franco, M.J., Rey-Moreno, M., 2022.Do travelers’ reviews depend on the destination? an analysis in coastal and urban peer-to-peer lodgings.Psychology & marketing 39, 441–459.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Sánchez-Franco, M.J., Rey-Moreno, M., 2022.Do travelers’ reviews depend on the destination? an analysis in coastal and urban peer-to-peer lodgings.Psychology & marketing 39, 441–459.&#160;<a href=#fnref:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:36><p data-raw='Abuzayed, A., Al-Khalifa, H., 2021.Bert for arabic topic modeling: An experimental study on bertopic technique.Procedia computer science 189, 191–194.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Abuzayed, A., Al-Khalifa, H., 2021.Bert for arabic topic modeling: An experimental study on bertopic technique.Procedia computer science 189, 191–194.&#160;<a href=#fnref:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:37><p data-raw='Grigore, D.N., Pintilie, I., 2023.Transformer-based topic modeling to measure the severity of eating disorder symptoms., in: CLEF (Working Notes), pp. 684–692.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Grigore, D.N., Pintilie, I., 2023.Transformer-based topic modeling to measure the severity of eating disorder symptoms., in: CLEF (Working Notes), pp. 684–692.&#160;<a href=#fnref:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:38><p data-raw='Fengler, M., Phan, M.T., 2023.A Topic Model for 10-K Management Disclosures.Economics Working Paper Series 2307. University of St. Gallen, School of Economics and Political Science.URL: 
  <a href="https://econpapers.repec.org/RePEc:usg:econwp:2023:07">https://EconPapers.repec.org/RePEc:usg:econwp:2023:07</a>.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:38" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Fengler, M., Phan, M.T., 2023.A Topic Model for 10-K Management Disclosures.Economics Working Paper Series 2307. University of St. Gallen, School of Economics and Political Science.URL:
<a href=https://econpapers.repec.org/RePEc:usg:econwp:2023:07>https://EconPapers.repec.org/RePEc:usg:econwp:2023:07</a>.&#160;<a href=#fnref:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:39><p data-raw='Ferriani, F., Gazzani, A., 2023.The invasion of ukraine and the energy crisis: Comparative advantages in equity valuations.Finance Research Letters 58, 104604.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Ferriani, F., Gazzani, A., 2023.The invasion of ukraine and the energy crisis: Comparative advantages in equity valuations.Finance Research Letters 58, 104604.&#160;<a href=#fnref:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:40><p data-raw='Cohen, S., Kadach, I., Ormazabal, G., Reichelstein, S., 2023.Executive compensation tied to esg performance: International evidence.Journal of Accounting Research 61, 805–853.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Cohen, S., Kadach, I., Ormazabal, G., Reichelstein, S., 2023.Executive compensation tied to esg performance: International evidence.Journal of Accounting Research 61, 805–853.&#160;<a href=#fnref:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:41><p data-raw='Borrero-Domínguez, C., Cortijo-Gallego, V., Escobar-Rodríguez, T., 2024.Digital transformation voluntary disclosure: Insights from leading european companies.International Journal of Accounting Information Systems 55, 100711.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Borrero-Domínguez, C., Cortijo-Gallego, V., Escobar-Rodríguez, T., 2024.Digital transformation voluntary disclosure: Insights from leading european companies.International Journal of Accounting Information Systems 55, 100711.&#160;<a href=#fnref:41 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:42><p data-raw='Araci, D., 2019.Finbert: Financial sentiment analysis with pre-trained language models.arXiv preprint arXiv:1908.10063.&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Araci, D., 2019.Finbert: Financial sentiment analysis with pre-trained language models.arXiv preprint arXiv:1908.10063.&#160;<a href=#fnref:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:43><p data-raw='Sun, Y., Cheng, C., Zhang, Y., Zhang, C., Zheng, L., Wang, Z., Wei, Y., 2020.Circle loss: A unified perspective of pair similarity optimization, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6398–6407.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Sun, Y., Cheng, C., Zhang, Y., Zhang, C., Zheng, L., Wang, Z., Wei, Y., 2020.Circle loss: A unified perspective of pair similarity optimization, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6398–6407.&#160;<a href=#fnref:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:44><p data-raw='Angelov, D., 2020.Top2vec: Distributed representations of topics.arXiv preprint arXiv:2008.09470.&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:44" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Angelov, D., 2020.Top2vec: Distributed representations of topics.arXiv preprint arXiv:2008.09470.&#160;<a href=#fnref:44 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:44 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:45><p data-raw='Altaweel, M., Bone, C., Abrams, J., 2019.Documents as data: a content analysis and topic modeling approach for analyzing responses to ecological disturbances.Ecological Informatics 51, 82–95.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Altaweel, M., Bone, C., Abrams, J., 2019.Documents as data: a content analysis and topic modeling approach for analyzing responses to ecological disturbances.Ecological Informatics 51, 82–95.&#160;<a href=#fnref:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:46><p data-raw='McInnes, L., Healy, J., 2017.Accelerated Hierarchical Density Clustering, in: 2017 IEEE International Conference on Data Mining Workshops (ICDMW), pp. 33–42.doi:
  <a href="http://dx.doi.org/10.1109/ICDMW.2017.12">10.1109/ICDMW.2017.12</a>, 
  <a href="http://arxiv.org/abs/1705.07321">arXiv:1705.07321</a>.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>&#160;<a href="#fnref1:46" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>McInnes, L., Healy, J., 2017.Accelerated Hierarchical Density Clustering, in: 2017 IEEE International Conference on Data Mining Workshops (ICDMW), pp. 33–42.doi:
<a href=http://dx.doi.org/10.1109/ICDMW.2017.12>10.1109/ICDMW.2017.12</a>,
<a href=http://arxiv.org/abs/1705.07321>arXiv:1705.07321</a>.&#160;<a href=#fnref:46 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:46 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:47><p data-raw='Loughran, T., McDonald, B., 2011.When is a liability not a liability? textual analysis, dictionaries, and 10-ks.The Journal of finance 66, 35–65.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Loughran, T., McDonald, B., 2011.When is a liability not a liability? textual analysis, dictionaries, and 10-ks.The Journal of finance 66, 35–65.&#160;<a href=#fnref:47 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:48><p data-raw='Röder, M., Both, A., Hinneburg, A., 2015.Exploring the space of topic coherence measures, in: Proceedings of the eighth ACM international conference on Web search and data mining, pp. 399–408.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Röder, M., Both, A., Hinneburg, A., 2015.Exploring the space of topic coherence measures, in: Proceedings of the eighth ACM international conference on Web search and data mining, pp. 399–408.&#160;<a href=#fnref:48 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:49><p data-raw='Agrawal, A., Fu, W., Menzies, T., 2018.What is wrong with topic modeling? and how to fix it using search-based software engineering.Information and Software Technology 98, 74–88.&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Agrawal, A., Fu, W., Menzies, T., 2018.What is wrong with topic modeling? and how to fix it using search-based software engineering.Information and Software Technology 98, 74–88.&#160;<a href=#fnref:49 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:50><p data-raw='Gu, H., Schreyer, M., Moffitt, K., Vasarhelyi, M., 2024a.Artificial intelligence co-piloted auditing.International Journal of Accounting Information Systems 54, 100698.&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Gu, H., Schreyer, M., Moffitt, K., Vasarhelyi, M., 2024a.Artificial intelligence co-piloted auditing.International Journal of Accounting Information Systems 54, 100698.&#160;<a href=#fnref:50 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:51><p data-raw='Karami, M., Ghodsi, A., 2024.Orchid: Flexible and data-dependent convolution for sequence modeling.arXiv preprint arXiv:2402.18508.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#8617;&#xfe0e;</a>'>Karami, M., Ghodsi, A., 2024.Orchid: Flexible and data-dependent convolution for sequence modeling.arXiv preprint arXiv:2402.18508.&#160;<a href=#fnref:51 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/EriseHe/notebook/edit/main//content.en/docs/Clippings/LLM/FinTextSim%20Enhancing%20Financial%20Text%20Analysis%20with%20BERTopic.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside id=book-toc class="book-toc hidden"><div class=book-toc-content><div class=button-prev><a class=btn-prev href=/docs/Clippings/LLM/Attention-Is-All-You-Need/><strong>Previous:</strong> Attention Is All You Need</a></div><div class=toc-entries><nav id=TableOfContents><ul><li><ul><li><ul><li></li></ul></li><li><a href=#acknowledgement>Acknowledgement</a></li><li><a href=#1-introduction>1 Introduction</a></li><li><a href=#2-state-of-the-art>2 State of the Art</a><ul><li><a href=#21-evolution-of-contemporary-topic-modeling-approaches>2.1 Evolution of Contemporary Topic Modeling Approaches</a></li><li><a href=#22-bertopic>2.2 BERTopic</a></li><li><a href=#23-topic-modeling-of-item-7-and-item-7a>2.3 Topic Modeling of Item 7 and Item 7A</a></li></ul></li><li><a href=#3-materials-and-methods>3 Materials and Methods</a><ul><li><a href=#31-dataset>3.1 Dataset</a></li><li><a href=#32-keyword-list>3.2 Keyword List</a></li><li><a href=#33-fintextsim>3.3 FinTextSim</a></li><li><a href=#34-model-creation>3.4 Model Creation</a></li><li><a href=#35-evaluation-metrics>3.5 Evaluation Metrics</a></li></ul></li><li><a href=#4-results-and-discussion>4 Results and Discussion</a><ul><li><a href=#41-fintextsim---leveraging-contextual-embeddings-for-the-financial-domain>4.1 FinTextSim - Leveraging Contextual Embeddings for the Financial Domain</a></li><li><a href=#42-topic-quality>4.2 Topic Quality</a></li><li><a href=#43-organizing-power>4.3 Organizing Power</a></li><li><a href=#44-wrapup-of-results-and-discussion>4.4 Wrapup of Results and Discussion</a></li></ul></li><li><a href=#5-conclusion>5 Conclusion</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></div><div class=button-next><a class=btn-next href=/docs/Clippings/Psychoanalysis/From-Hallucination-to-Suture-Insights-from-Language-Philosophy-to-Enhance-Large-Language-Models/><strong>Next:</strong> From "Hallucination" to "Suture": Insights from Language Philosophy to Enhance Large Language Models</a></div></div></aside></div><div class=toolbar-trigger></div><aside class=toolbar><button id=theme-toggle class=theme-toggle-btn>
<img src=/blackhole2.png alt="Switch Theme">
</button>
<button id=toc-toggle class=toc-toggle-btn>
<img src=/toc-icon.svg alt="Toggle TOC"></button><div class=font-size-control><button id=font-toggle class=font-size-btn>
A</button><div class=font-size-buttons><button id=font-increase class=font-control-btn>+</button>
<button id=font-reset class=font-control-btn>A</button>
<button id=font-decrease class=font-control-btn>−</button></div></div></aside><script>document.addEventListener("DOMContentLoaded",function(){var e=document.getElementById("book-toc"),t=!0;t?e.classList.remove("hidden"):e.classList.add("hidden"),document.getElementById("toc-toggle").addEventListener("click",function(){e.classList.toggle("hidden")})})</script><script src=/font-size.min.js></script><script src=/toolbar.min.js></script><script src=/toc-animation.min.js></script><script src=/menu-autohide.min.js></script></main><script>document.addEventListener("DOMContentLoaded",()=>{const e=document.getElementById("theme-toggle");if(!e)return;e.onclick=()=>{const s=e.getBoundingClientRect(),o=s.left+s.width/2,i=s.top+s.height/2,l=document.documentElement.getAttribute("data-theme"),r=l||"auto";let n;r==="auto"?n=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":n=r;const a=n==="dark"?"light":"dark";document.documentElement.setAttribute("data-theme",a),localStorage.setItem("theme",a);const c=n==="dark"?"#0b031c":"white",t=document.createElement("div");t.style.position="fixed",t.style.top="0",t.style.left="0",t.style.width="100vw",t.style.height="100vh",t.style.zIndex="9999",t.style.pointerEvents="none",t.style.setProperty("--cx",`${o}px`),t.style.setProperty("--cy",`${i}px`),t.style.setProperty("--r","0px"),t.style.backgroundImage=`radial-gradient(circle at var(--cx) var(--cy), transparent 0%, transparent var(--r), ${c} calc(var(--r) + 1px))`,document.body.appendChild(t);const u=window.innerWidth,h=window.innerHeight,m=Math.hypot(Math.max(o,u-o),Math.max(i,h-i)),f=600,p=performance.now();function d(e){const o=e-p,s=Math.min(o/f,1),n=s*m;if(t.style.setProperty("--r",`${n}px`),t.style.backgroundImage=`radial-gradient(circle at var(--cx) var(--cy), transparent 0%, transparent ${n}px, ${c} ${n+1}px)`,s<1)requestAnimationFrame(d);else{t.remove();const e=localStorage.getItem("theme")||"light";document.documentElement.setAttribute("data-theme",e)}}requestAnimationFrame(d)}})</script><script>document.addEventListener("DOMContentLoaded",function(){function e(e){return e.replace(/\\(\[[0-9]+(?:\.[0-9]+)?(?:pt|em|ex|px|cm|mm|in)\])/g,"\\\\$1")}function t(){const t=document.querySelectorAll("p, div");t.forEach(function(t){let n=t.innerHTML,s=!1;n=n.replace(/\$\$([\s\S]*?)\$\$/g,function(t,n){const o=e(n);return o!==n&&(s=!0),"$$"+o+"$$"}),n=n.replace(/\$([^$]+)\$/g,function(t,n){const o=e(n);return o!==n&&(s=!0),"$"+o+"$"}),s&&(t.innerHTML=n)})}t()})</script><script>window.__tikzjax_injected||(window.__tikzjax_injected=!0,fetch("/content.en/docs/.obsidian/plugins/obsidian-tikzjax/main.js").then(e=>e.text()).then(e=>{const t=e.match(/var tikzjax_default = `([\s\S]*?)`;?/);if(t){const n=t[1],e=document.createElement("script");e.id="tikzjax",e.type="text/javascript",e.innerHTML=n,document.head.appendChild(e)}else{const e=document.createElement("script");e.src="https://tikzjax.com/v1/tikzjax.js",e.defer=!0,document.head.appendChild(e)}}).catch(()=>{const e=document.createElement("script");e.src="https://tikzjax.com/v1/tikzjax.js",e.defer=!0,document.head.appendChild(e)}))</script><script>window.processTheoremBlocks=function(){document.querySelectorAll("blockquote").forEach(function(e){const t=e.querySelector("p");if(!t)return;const n=t.getAttribute("data-raw")||t.innerHTML,s=n.replace(/<[^>]*>/g,"").trim();for(const o of["definition","proposition","lemma","theorem","assumption","claim"]){const a=new RegExp(`^\\s*\\[!${o}\\|(\\*|[\\w\\.\\-]+)\\]\\s*`,"i"),i=s.match(a);if(i){console.log(`Found ${o} with label: ${i[1]}`);const c=i[1]!=="*",m=c?i[1]:"",f=n.replace(/<[^>]*>/g,""),p=f.split(/\n/),g=p[0]||"",l=g,d=l.replace(a,"").trim();if(e.classList.contains("math-theorem"))break;const v=Array.from(e.childNodes);e.innerHTML="",e.classList.add("math-theorem",o);const s=document.createElement("div");s.className="theorem-header";const u=o.charAt(0).toUpperCase()+o.slice(1),h=document.createElement("span");if(h.textContent=c?`${u} ${m}`:u,s.appendChild(h),d){const e=document.createElement("span");e.className="theorem-subtitle",e.textContent=` (${d})`,s.appendChild(e)}e.appendChild(s);const r=document.createElement("div");r.className="theorem-content",e.appendChild(r),v.forEach(function(e){let s=e.cloneNode(!0);if(e===t&&s.nodeType===1){const t=n.replace(/<[^>]*>/g,""),e=t.split(/\n/);if(e.length>1){const n=e.slice(1),t=n.join(`
`).trim();if(t)s.innerHTML="",s.appendChild(document.createTextNode(t));else return}else{const e=l.replace(a,"").trim();if(e)s.innerHTML="",s.appendChild(document.createTextNode(e));else return}}r.appendChild(s)});break}}})},document.addEventListener("DOMContentLoaded",function(){try{window.processTheoremBlocks()}catch(e){console.error(e)}})</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".copy-button");e.forEach(e=>{const t=e.innerHTML;e.addEventListener("click",function(){const n=this.parentElement.querySelector("pre code");if(!n)return;navigator.clipboard.writeText(n.innerText).then(()=>{e.textContent="Copied",setTimeout(()=>{e.innerHTML=t},1500)}).catch(n=>{console.error("Failed to copy code:",n),e.textContent="Error",setTimeout(()=>{e.innerHTML=t},1500)})})})})</script><script defer src=/partial-load.min.6b14bff963746e8ce83087045335ec7a6533e1157905e46f88d952cb415b7418.js integrity="sha256-axS/+WN0bozoMIcEUzXsemUz4RV5BeRviNlSy0FbdBg=" crossorigin=anonymous></script><script defer src=/menu-recursive-close.min.893276da918db977c98771d5746eab05ce5761bc6fa1ca400efe1db9be2ac1f3.js integrity="sha256-iTJ22pGNuXfJh3HVdG6rBc5XYbxvocpADv4dub4qwfM=" crossorigin=anonymous></script></body></html>