<!doctype html><html lang=en dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1314&amp;path=livereload" data-no-instant defer></script><script>(function(){const e=localStorage.getItem("theme");e&&document.documentElement.setAttribute("data-theme",e)})()</script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Abstract page for arXiv paper 1706.03762: Attention Is All You Need"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1314/docs/Clippings/LLM/Attention-Is-All-You-Need/"><meta property="og:site_name" content="学习笔记"><meta property="og:title" content="Attention Is All You Need"><meta property="og:description" content="Abstract page for arXiv paper 1706.03762: Attention Is All You Need"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:tag" content="Clippings"><meta property="article:tag" content="Attention"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="CNN"><title>Attention Is All You Need | 学习笔记</title>
<link rel=icon href=/topo.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1314/docs/Clippings/LLM/Attention-Is-All-You-Need/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/computer-modern-font@1.0.1/index.min.css><link rel=stylesheet href=/book.min.53bd3023ca7fa105b30e0db0d2cbc02702dc40cef7efbac83facecd39e55ed16.css integrity="sha256-U70wI8p/oQWzDg2w0svAJwLcQM7377rIP6zs055V7RY=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.02146d62883b84f93e0524565edbcf1e9c978da904f495247ceef863dda858c9.js integrity="sha256-AhRtYog7hPk+BSRWXtvPHpyXjakE9JUkfO74Y92oWMk=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class=container><div class=book-layout><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center justify-center" href=http://localhost:1314/><span>学习笔记</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><nav class=subject-menu><div class=subject-tabs><button class="subject-tab active" data-subject=数学>数学</button>
<button class=subject-tab data-subject=物理>物理</button></div><div class=subject-content><div class=subject-panel data-subject=数学></div><div class="subject-panel hidden" data-subject=物理></div></div></nav><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".subject-tab"),s=document.querySelectorAll(".subject-panel");function t(t){e.forEach(e=>e.classList.remove("active")),s.forEach(e=>e.classList.add("hidden"));const n=document.querySelector(`.subject-tab[data-subject="${t}"]`);n&&n.classList.add("active");const o=document.querySelector(`.subject-panel[data-subject="${t}"]`);o&&o.classList.remove("hidden"),localStorage.setItem("activeSubjectTab",t)}const n=localStorage.getItem("activeSubjectTab");n&&t(n),e.forEach(e=>{e.addEventListener("click",function(){const n=e.getAttribute("data-subject");t(n)})})})</script><div class=after-menu-offset><ul><li><a href=/posts/>Blog</a></li><li><a href=https://github.com/EriseHe/notebook target=_blank rel=noopener>Github</a></li><li><a href=https://themes.gohugo.io/themes/hugo-book/ target=_blank rel=noopener>Hugo Themes</a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Attention Is All You Need</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#titleattention-is-all-you-need>Title:Attention Is All You Need</a></li><li><a href=#1-motivation>1. Motivation</a></li><li><a href=#2-architecture-core>2. Architecture core</a><ul><li><a href=#21-encoder-decoder-structure>2.1 Encoder-decoder structure</a></li></ul></li></ul></li></ul></nav></aside></header><article class=book-article><h1 style="text-align:center;font-family:computer modern,cmu serif,serif">Attention Is All You Need</h1><div class="markdown posts" x-data><p data-raw='[Submitted on 12 Jun 2017 (
  <a href="https://arxiv.org/abs/1706.03762v1">v1</a>), last revised 2 Aug 2023 (this version, v7)]'>[Submitted on 12 Jun 2017 (
<a href=https://arxiv.org/abs/1706.03762v1>v1</a>), last revised 2 Aug 2023 (this version, v7)]</p><h2 id=titleattention-is-all-you-need>Title:Attention Is All You Need
<a class=anchor href=#titleattention-is-all-you-need>#</a></h2><p data-raw='
  <a href="https://arxiv.org/pdf/1706.03762">View PDF</a> 
  <a href="https://arxiv.org/html/1706.03762v7">HTML (experimental)</a>'><a href=https://arxiv.org/pdf/1706.03762>View PDF</a>
<a href=https://arxiv.org/html/1706.03762v7>HTML (experimental)</a></p><blockquote><p data-raw='Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'>Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</p></blockquote><h2 id=1-motivation>1. Motivation
<a class=anchor href=#1-motivation>#</a></h2><p data-raw='Before 2017, sequence-to-sequence models relied on <strong>RNNs with attention on top</strong>, which has two limits:'>Before 2017, sequence-to-sequence models relied on <strong>RNNs with attention on top</strong>, which has two limits:</p><ol><li>Sequential computation $\rightarrow$ hard to parallelize.</li><li>Long-range dependencies $\rightarrow$ information degraded over many steps.</li></ol><p data-raw='The <strong>Transformer</strong> solved both by:'>The <strong>Transformer</strong> solved both by:</p><ul><li>Using <em>self-attention</em> $\rightarrow$ connect every position in the input to every other, weighting them according to learned relevance.</li><li>Removing <em>recurrence</em> (or convolution) $\rightarrow$ whole sequence can be processed in parallel.</li></ul><h2 id=2-architecture-core>2. Architecture core
<a class=anchor href=#2-architecture-core>#</a></h2><h3 id=21-encoder-decoder-structure>2.1 Encoder-decoder structure
<a class=anchor href=#21-encoder-decoder-structure>#</a></h3><blockquote><p data-raw='Sutskever et al. (2014) introduced the RNN-based seq2seq encoder–decoder for machine translation.'>Sutskever et al. (2014) introduced the RNN-based seq2seq encoder–decoder for machine translation.</p><p data-raw='Bahdanau et al. (2015) added attention on top of RNN encoder–decoder models.'>Bahdanau et al. (2015) added attention on top of RNN encoder–decoder models.</p></blockquote><h4 id=211-encoder>2.1.1 Encoder
<a class=anchor href=#211-encoder>#</a></h4><p data-raw='stacks of self-attention + feed-forward layers to produce contextual representations.'>stacks of self-attention + feed-forward layers to produce contextual representations.</p><h4 id=212-decoder>2.1.2 Decoder
<a class=anchor href=#212-decoder>#</a></h4><p data-raw='similar, but with an extra attention over encoder outputs and causal masking to prevent future-token access.'>similar, but with an extra attention over encoder outputs and causal masking to prevent future-token access.</p><h4 id=213-scaled-dot-product-attention>2.1.3 Scaled dot-product attention
<a class=anchor href=#213-scaled-dot-product-attention>#</a></h4><ul><li>Input: query (Q), key (K), value (V) matrices.</li><li>Attention $(\mathrm{Q}, \mathrm{K}, \mathrm{V})=\operatorname{softmax}\left(\left(\mathrm{QK}^{\mathrm{T}}\right) / \sqrt{\mathrm{d}_{\mathrm{k}}}\right) \mathrm{V}$</li><li>QKT: similarity between query and all keys.</li><li>$\sqrt{ } \mathrm{d}_{\mathrm{k}}$ : scaling to stabilize gradients.</li><li>Softmax: turns similarities into a probability distribution.</li><li>Multiply by V to blend values according to attention weights.</li></ul><h4 id=214-multi-head-attention>2.1.4 Multi-head attention
<a class=anchor href=#214-multi-head-attention>#</a></h4><ul><li>Split Q, K, V into h subspaces.</li><li>Apply attention in each, then concatenate results.</li><li>Allows capturing different types of relationships in parallel.</li></ul><h4 id=215-positional-encoding>2.1.5 Positional encoding
<a class=anchor href=#215-positional-encoding>#</a></h4><ul><li>Since there&rsquo;s no recurrence, add position information to token embeddings (sinusoidal or learned).</li></ul><h4 id=216-feed-forward-and-residual-connections>2.1.6 Feed-forward and residual connections
<a class=anchor href=#216-feed-forward-and-residual-connections>#</a></h4><ul><li>Position-wise feed-forward layers after attention.</li><li>Residual + layer norm stabilize deep training.</li></ul></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/EriseHe/notebook/edit/main//content.en/docs/Clippings/LLM/Attention%20Is%20All%20You%20Need.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside id=book-toc class="book-toc hidden"><div class=book-toc-content><div class=button-prev><a class=btn-prev href=/docs/Clippings/Psychoanalysis/Artificial-intelligence-and-psychoanalysis-is-it-time-for-psychoanalyst.AI-PMC/><strong>Previous:</strong> Artificial intelligence and psychoanalysis: is it time for psychoanalyst.AI? - PMC</a></div><div class=toc-entries><nav id=TableOfContents><ul><li><ul><li><a href=#titleattention-is-all-you-need>Title:Attention Is All You Need</a></li><li><a href=#1-motivation>1. Motivation</a></li><li><a href=#2-architecture-core>2. Architecture core</a><ul><li><a href=#21-encoder-decoder-structure>2.1 Encoder-decoder structure</a></li></ul></li></ul></li></ul></nav></div><div class=button-next><a class=btn-next href=/docs/Clippings/LLM/FinTextSim-Enhancing-Financial-Text-Analysis-with-BERTopic/><strong>Next:</strong> FinTextSim: Enhancing Financial Text Analysis with BERTopic</a></div></div></aside></div><div class=toolbar-trigger></div><aside class=toolbar><button id=theme-toggle class=theme-toggle-btn>
<img src=/blackhole2.png alt="Switch Theme">
</button>
<button id=toc-toggle class=toc-toggle-btn>
<img src=/toc-icon.svg alt="Toggle TOC"></button><div class=font-size-control><button id=font-toggle class=font-size-btn>
A</button><div class=font-size-buttons><button id=font-increase class=font-control-btn>+</button>
<button id=font-reset class=font-control-btn>A</button>
<button id=font-decrease class=font-control-btn>−</button></div></div></aside><script>document.addEventListener("DOMContentLoaded",function(){var e=document.getElementById("book-toc"),t=!0;t?e.classList.remove("hidden"):e.classList.add("hidden"),document.getElementById("toc-toggle").addEventListener("click",function(){e.classList.toggle("hidden")})})</script><script src=/font-size.min.js></script><script src=/toolbar.min.js></script><script src=/toc-animation.min.js></script><script src=/menu-autohide.min.js></script></main><script>document.addEventListener("DOMContentLoaded",()=>{const e=document.getElementById("theme-toggle");if(!e)return;e.onclick=()=>{const s=e.getBoundingClientRect(),o=s.left+s.width/2,i=s.top+s.height/2,l=document.documentElement.getAttribute("data-theme"),r=l||"auto";let n;r==="auto"?n=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":n=r;const a=n==="dark"?"light":"dark";document.documentElement.setAttribute("data-theme",a),localStorage.setItem("theme",a);const c=n==="dark"?"#0b031c":"white",t=document.createElement("div");t.style.position="fixed",t.style.top="0",t.style.left="0",t.style.width="100vw",t.style.height="100vh",t.style.zIndex="9999",t.style.pointerEvents="none",t.style.setProperty("--cx",`${o}px`),t.style.setProperty("--cy",`${i}px`),t.style.setProperty("--r","0px"),t.style.backgroundImage=`radial-gradient(circle at var(--cx) var(--cy), transparent 0%, transparent var(--r), ${c} calc(var(--r) + 1px))`,document.body.appendChild(t);const u=window.innerWidth,h=window.innerHeight,m=Math.hypot(Math.max(o,u-o),Math.max(i,h-i)),f=600,p=performance.now();function d(e){const o=e-p,s=Math.min(o/f,1),n=s*m;if(t.style.setProperty("--r",`${n}px`),t.style.backgroundImage=`radial-gradient(circle at var(--cx) var(--cy), transparent 0%, transparent ${n}px, ${c} ${n+1}px)`,s<1)requestAnimationFrame(d);else{t.remove();const e=localStorage.getItem("theme")||"light";document.documentElement.setAttribute("data-theme",e)}}requestAnimationFrame(d)}})</script><script>window.__tikzjax_injected||(window.__tikzjax_injected=!0,fetch("/content.en/docs/.obsidian/plugins/obsidian-tikzjax/main.js").then(e=>e.text()).then(e=>{const t=e.match(/var tikzjax_default = `([\s\S]*?)`;?/);if(t){const n=t[1],e=document.createElement("script");e.id="tikzjax",e.type="text/javascript",e.innerHTML=n,document.head.appendChild(e)}else{const e=document.createElement("script");e.src="https://tikzjax.com/v1/tikzjax.js",e.defer=!0,document.head.appendChild(e)}}).catch(()=>{const e=document.createElement("script");e.src="https://tikzjax.com/v1/tikzjax.js",e.defer=!0,document.head.appendChild(e)}))</script><script>window.processTheoremBlocks=function(){document.querySelectorAll("blockquote").forEach(function(e){const t=e.querySelector("p");if(!t)return;const n=t.getAttribute("data-raw")||t.innerHTML,s=n.replace(/<[^>]*>/g,"").trim();for(const o of["definition","proposition","lemma","theorem","assumption","claim"]){const a=new RegExp(`^\\s*\\[!${o}\\|(\\*|[\\w\\.\\-]+)\\]\\s*`,"i"),i=s.match(a);if(i){console.log(`Found ${o} with label: ${i[1]}`);const c=i[1]!=="*",m=c?i[1]:"",f=n.replace(/<[^>]*>/g,""),p=f.split(/\n/),g=p[0]||"",l=g,d=l.replace(a,"").trim();if(e.classList.contains("math-theorem"))break;const v=Array.from(e.childNodes);e.innerHTML="",e.classList.add("math-theorem",o);const s=document.createElement("div");s.className="theorem-header";const u=o.charAt(0).toUpperCase()+o.slice(1),h=document.createElement("span");if(h.textContent=c?`${u} ${m}`:u,s.appendChild(h),d){const e=document.createElement("span");e.className="theorem-subtitle",e.textContent=` (${d})`,s.appendChild(e)}e.appendChild(s);const r=document.createElement("div");r.className="theorem-content",e.appendChild(r),v.forEach(function(e){let s=e.cloneNode(!0);if(e===t&&s.nodeType===1){const t=n.replace(/<[^>]*>/g,""),e=t.split(/\n/);if(e.length>1){const n=e.slice(1),t=n.join(`
`).trim();if(t)s.innerHTML="",s.appendChild(document.createTextNode(t));else return}else{const e=l.replace(a,"").trim();if(e)s.innerHTML="",s.appendChild(document.createTextNode(e));else return}}r.appendChild(s)});break}}})},document.addEventListener("DOMContentLoaded",function(){try{window.processTheoremBlocks()}catch(e){console.error(e)}})</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".copy-button");e.forEach(e=>{const t=e.innerHTML;e.addEventListener("click",function(){const n=this.parentElement.querySelector("pre code");if(!n)return;navigator.clipboard.writeText(n.innerText).then(()=>{e.textContent="Copied",setTimeout(()=>{e.innerHTML=t},1500)}).catch(n=>{console.error("Failed to copy code:",n),e.textContent="Error",setTimeout(()=>{e.innerHTML=t},1500)})})})})</script><script defer src=/partial-load.min.6b14bff963746e8ce83087045335ec7a6533e1157905e46f88d952cb415b7418.js integrity="sha256-axS/+WN0bozoMIcEUzXsemUz4RV5BeRviNlSy0FbdBg=" crossorigin=anonymous></script><script defer src=/menu-recursive-close.min.893276da918db977c98771d5746eab05ce5761bc6fa1ca400efe1db9be2ac1f3.js integrity="sha256-iTJ22pGNuXfJh3HVdG6rBc5XYbxvocpADv4dub4qwfM=" crossorigin=anonymous></script></body></html>